 that the proportions of the different senses of each word were the same between BC and WSJ, accuracy improved by 9%. In this paper, we explore domain adaptation of WSD systems, by adding training examples from the new domain as additional training data to a WSD system. To reduce the effort required to adapt a WSD system to a new domain, we employ an active learning strategy (Lewis and Gale, 1994) to select examples to annotate from the new domain of interest. To our knowledge, our work is the first to use active learning for domain adaptation for WSD. A similar work is the recent research by Chen et al. (2006), where active learning was used successfully to reduce the annotation effort for WSD of 5 English verbs using coarse-grained evaluation. In that work, the authors only used active learning to reduce the annotation effort and did not deal with the porting of a WSD system to a new domain. Domain adaptation is necessary when the training and target domains are different. In this paper, Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 49–56, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics we perform domain adaptation
WSJ adaptation examples per noun. DT +— the set of BC training examples DA +— the set ofuntagged WSJ adaptation examples r +— WSD system trained on DT repeat pmin +— 00 for each d E DA do s+— word sense prediction for d using r p +— confidence of prediction s ifp < pmin then pmin +— p, dmin +— d end data, and the rest of the WSJ examples are designated as in-domain adaptation data. The row 21 nouns in Table 1 shows some information about these 21 nouns. For instance, these nouns have an average of 6.7 senses in BC and 6.8 senses in WSJ. This is slightly higher than the 5.8 senses per verb in (Chen et al., 2006), where the experiments were conducted using coarse-grained evaluation. Assuming we have access to an “oracle” which determines the predominant sense, or most frequent sense (MFS), of each noun in our WSJ test data perfectly, and we assign this most frequent sense to each noun in the test data, we will have achieved an accuracy of 61.1% as shown in the column MFS accuracy of Table 1. Finally, we note that we have an average of 310 BC training examples and 406 WSJ adaptation examples per noun. 3 Active Learning For our experiments, we use naive Bayes as the learning algorithm. The knowledge sou
n in our WSJ test data perfectly, and we assign this most frequent sense to each noun in the test data, we will have achieved an accuracy of 61.1% as shown in the column MFS accuracy of Table 1. Finally, we note that we have an average of 310 BC training examples and 406 WSJ adaptation examples per noun. 3 Active Learning For our experiments, we use naive Bayes as the learning algorithm. The knowledge sources we use include parts-of-speech, local collocations, and surrounding words. These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work (Lee and Ng, 2002). In performing WSD with a naive Bayes classifier, the sense s assigned to an example with features f1, ... , fn is chosen so as to maximize: In our domain adaptation study, we start with a WSD system built using training examples drawn from BC. We then investigate the utility of adding additional in-domain training data from WSJ. In the baseline approach, the additional WSJ examples are randomly selected. With active learning (Lewis and Gale, 1994), we use uncertainty sampling as shown end DA +— DA — dmin provide correct sense s for dmin and add dmin to DT r +— WSD system trained on new DT en
ess of the adaptation process by using the predicted predominant sense of the new domain and adopting the count-merging technique. 7 Related Work In applying active learning for domain adaptation, Zhang et al. (2003) presented work on sentence boundary detection using generalized Winnow, while Tur et al. (2004) performed language model adaptation of automatic speech recognition systems. In both papers, out-of-domain and indomain data were simply mixed together without MAP estimation such as count-merging. For WSD, Fujii et al. (1998) used selective sampling for a Japanese language WSD system, Chen et al. (2006) used active learning for 5 verbs using coarse-grained evaluation, and H. T. Dang (2004) employed active learning for another set of 5 verbs. However, their work only investigated the use of active learning to reduce the annotation effort necessary for WSD, but 82 81 80 79 78 77 76 75 74 73 72 71 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 50 49 48 46 45 44 43 70 69 52 51 47 a-c-estPred a-truePred a-estPred a r 55 did not deal with the porting of a WSD system to a different domain. Escudero et al. (2000) used the DSO corpus to highlight the importance of the issue of domain dependence of W
