{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.925073333333334","#text":"\nLinguistic Data Consortium (LDC). We pre-\nprocessed the dictionary as follows:\n1. Stem Chinese words via a simple algorithm\nto remove common suffixes and prefixes.\n2. Use the Porter stemmer on English words.\n3. Split English phrases into words. If an\n"},{"#tail":"\n","@confidence":"0.4524855","#text":"\n87% of their performance.\n\\[--*- Short Query 4-- Medium Query J\n"}],"figure":[{"#tail":"\n","@confidence":"0.946114227272727","#text":"\nQ a query\nEnglish query\na document\na document in foreign language y\ndocument is relevant\na word\nan English corpus\na corpus in language x\nQX\nD\nDr\nD isR\nW\nGx\nCx\nWx\nBL\nan English word\nforeign language y\nWy a word in\na bilingual dictionary\nA Glossary of Notation used in Formulas\n"},{"#tail":"\n","@confidence":"0.994768142857143","#text":"\nQuery sets\nTrec5C-medium\nTrec6C-medium\nTrec4S\n(+4%)\nDegree of Disambiguation\nNone Manual % of\nMono-\nlingual\n0.2449 0.2873 84%\n(+17%)\n0.3872 0.3830 83%\n(-1%)\n0.1729 0.1799 80%\n"},{"#tail":"\n","@confidence":"0.99346775","#text":"\n0.35\n0.3\no.25\n== o.2\n0.15\n~. 0.1\nO.O5\n0\n0 10000 20000 30000 40000 50000 60000\nLexicon Size\n\\[ -*-- Short + Medium \\]\n_-- 120\no lO0I\n~g 00\n0 o o_ 60\n,f. o\n0\nO,,\n10000 20000 30000 40000 5(X)O0 60000\nLexicon Size\n"}],"address":[{"#tail":"\n","@confidence":"0.827893","#text":"\n70 Fawcett St.\nCambridge, MA, USA 02138\n"},{"#tail":"\n","@confidence":"0.799653","#text":"\n70 Fawcett St.\nCambridge, MA, USA 02138\n"}],"author":[{"#tail":"\n","@confidence":"0.802942","#text":"\nJinxi Xu\n"},{"#tail":"\n","@confidence":"0.493814","#text":"\nRalph Weischedel\n"}],"equation":[{"#tail":"\n","@confidence":"0.6431205","#text":"\n? e (WlD) =\nlength of D\n"},{"#tail":"\n","@confidence":"0.819141","#text":"\nprobability P(WxIDy):\nP(WxIDy)= ~P(WylDy)P(WxIWy)\nW inBL y\n"},{"#tail":"\n","@confidence":"0.574773","#text":"\nP(Qx IDr /sR) = I~I(aetwx IG,)+O-a)P(W~ IDy))\nw.~,o.\n"},{"#tail":"\n","@confidence":"0.9643686","#text":"\nco(e i , c)\nP_ corpus(ell c) =\ni=n\nMAX(df (c ) , ~ co(e i, c))\ni=1\n"}],"subsectionHeader":{"#tail":"\n","@confidence":"0.450433","#text":"\n10 Using a Parallel Corpus\n"},"footnote":{"#tail":"\n","@confidence":"0.5654095","#text":"\n1 Clearly, this is not correct; however, it\nsimplified implementation.\n"},"title":{"#tail":"\n","@confidence":"0.750425","#text":"\nCross-lingual Information Retrieval using Hidden Markov Models\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.985156621621621","#text":"\nL. Ballesteros and W.B. Croft 1997. &quot;Phrasal\ntranslation and query expansion techniques for\ncross-language information retrieval.&quot; Proceedings\nof the 20th ACM SIGIR International Conference\non Research and Development in Information\nRetrieval 1997, pp. 84-91.\nL. Ballesteros and W.B. Croft, 1998. &quot;Resolving\nambiguity for cross-language retrieval.&quot;\nProceedings of the 21st ACM SIGIR Conference on\nResearch and Development in Information\nRetrieval, 1998, pp. 64-71.\nJ.P. Callan, W.B. Croft and J. Broglio. 1995. &quot;TREC\nand TIPSTER Experiments with INQUERY&quot;.\nInformation Processing and Management, pages\n327-343, 1995.\nJ. Carbonell, Y. Yang, R. Frederking, R. Brown, Y.\nGeng and D. Lee, 1997. &quot;Translingual information\nretrieval: a comparative evaluation.&quot; In\nProceedings of the 15th International Joint\nConference on Artificial Intelligence, 1997.\nM. Davis and W. Ogden, 1997. &quot;QUILT:\nImplementing a Large Scale Cross-language Text\nRetrieval System.&quot; Proceedings of ACM SIGIR\nConference, 1997.\nA. Diekema, F. Oroumchain, P. Sheridan and E.\nLiddy, 1999. &quot;TREC-7 Evaluation of Conceptual\nInterlingual Document Retrieval (CINDOR) in\nEnglish and French.&quot; TREC7 Proceedings, NIST\nspecial publication.\nP. Fung and K. Mckeown. &quot;Finding Terminology\nTranslations from Non-parallel Corpora.&quot; The 5 'h\nAnnual Workshop on Very Large Corpora, Hong\nKong: August 1997, 192n202\nF. Gey, J. He and A. Chen, 1999. &quot;Manual queries\nand Machine Translation in cross-language\nretrieval at TREC-7&quot;. In TREC7 Proceedings,\nNIST Special Publication, 1999.\n"},{"#tail":"\n","@confidence":"0.999502125","#text":"\nHarman, 1996. The TREC-4 Proceedings. NIST\nSpecial publication, 1996.\nD. Hiemstra nd F. de Jong, 1999. &quot;Disambiguafion\nstrategies for Cross-language Information\nRetrieval.&quot; Proceedings of the third European\nConference on Research and Advanced Technology\nfor Digital Libraries, pp. 274-293, 1999.\nD. Hiemstra and W. Kraaij, 1999. &quot;Twenty-One at\nTREC-7: ad-hoc and cross-language track.&quot; In\nTREC-7 Proceedings, NIST Special Publication,\n1999.\nD. Hull, 1993. &quot;Using Statistical Testing in the\nEvaluation of Retrieval Experiments.&quot; Proceedings\nof the 16th Annual International ACM SIGIR\nConference on Research and Development in\nInformation Retrieval, pages 329-338, 1993.\nD. A. Hull and G. Grefenstette, 1996. &quot;A dictionary-\nbased approach to multilingual information\nretrieval&quot;. Proceedings of ACM SIGIR Conference,\n1996.\nD. A. Hull, 1997. &quot;Using structured queries for\ndisambiguation in cross-language information\nretrieval.&quot; In AAAI Symposium on Cross-Language\nText and Speech Retrieval. AAAI, 1997.\nM. E. Maron and K. L. Kuhns, 1960. &quot;On\nRelevance, Probabilistic Indexing and Information\nRetrieval.&quot; Journal of the Association for\n&quot;: Computing Machinery, 1960, pp 216-244.\nD. Miller, T. Leek and R. Schwartz, 1999. &quot;A\nHidden Markov Model Information Retrieval\nSystem.&quot; Proceedings of the 22nd Annual\nInternational ACM S1GIR Conference on Research\nand Development in Information Retrieval, pages\n214-221, 1999.\nD.W. Oard, 1998. &quot;A comparative study of query and\ndocument translation for cross-language\ninformation retrieval.&quot; In Proceedings of the Third\nConference of the Association for Machine\nTranslation in America (AMTA ), 1998.\nAri Pirkola, 1998. &quot;The effects of query structure\nand dictionary setups in dictionary-based cross-\nlanguage information retrieval.&quot; Proceedings of\nACM SIGIR Conference, 1998, pp 55-63.\nJ. Ponte and W.B. Croft, 1998. &quot;A Language\nModeling Approach to Information Retrieval.&quot;\nProceedings of the 21st Annual International ACM\nS1GIR Conference on Research and Development\nin Information Retrieval, pages 275-281, 1998.\nL. Rabiner, 1989. &quot;A tutorial on hidden Markov\nmodels and selected applications in speech\nrecognition.&quot; Proc. IEEE 77, pp. 257-286, 1989.\nM. Sanderson. &quot;Word sense disambiguation and\ninformation retrieval.&quot; Proceedings of ACM SIGIR\nConference, 1994, pp 142-15 I.\nVoorhees and Harman, 1997. TREC-5 Proceedings.\nE. Voorhees and D. Harman, Editors. NIST\nspecial publication.\nVoorhees and Harman, 1998. TREC-6 Proceedings.\nE. Voorhees and D. Harrnan, Editors. NIST\nspecial publication.\nJ. Xu and W.B. Croft, 1998. &quot;Corpus-based\nstemming using co-occurrence of word variants&quot;.\nACM Transactions on Information Systems,\nJanuary 1998, vol 16, no. 1.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.996736941176471","#text":"\nThis paper presents empirical results in\ncross-lingual information retrieval using\nEnglish queries to access Chinese\ndocuments (TREC-5 and TREC-6) and\nSpanish documents (TREC-4). Since our\ninterest is in languages where resources\nmay be minimal, we use an integrated\nprobabilistic model that requires only a\nbilingual dictionary as a resource. We\nexplore how a combined probability\nmodel of term translation and retrieval can\nreduce the effect of translation ambiguity.\nIn addition, we estimate an upper bound\non performance, if translation ambiguity\nwere a solved problem. We also measure\nperformance as a function of bilingual\ndictionary size.\n"},{"#tail":"\n","@confidence":"0.993503466666667","#text":"\nCross-language information retrieval (CLIR) can\nserve both those users with a smattering of\nknowledge of other languages and also those\nfluent in them. For those with limited\nknowledge of the other language(s), CLIR offers\na wide pool of documents, even though the user\ndoes not have the skill to prepare ahigh quality\nquery in the other language(s). Once documents\nare retrieved, machine translation or human\ntranslation, if desired, can make the documents\nusable. For the user who is fluent in two or\nmore languages, even though e/she may be able\nto formulate good queries in each of the source\nlanguages, CLIR relieves the user from having\nto do so.\nMost CLIR studies have been based on a variant\nof tf-idf; our experiments instead use a hidden\nMarkov model (HMM) to estimate the\nprobability that a document is relevant given the\nquery. We integrated two simple estimates of\nterm translation probability into the mono-\nlingual HMM model, giving an estimate of the\nprobability that a document is relevant given a\nquery in another language.\nIn this paper we address the following questions:\n? How can a combined probability model of\nterm translation and retrieval minimize the\neffect of translation ambiguity? (Sections 3,\n5, 6, 7, and 10)\n? What is the upper bound performance using\nbilingual dictionary lookup for term\ntranslation? (Section 8)\n? How much does performance d grade due to\nomissions from the bilingual dictionary and\nhow does performance vary with size of\nsuch a dictionary? (Sections 8-9)\nAll experiments were performed using a\ncommon baseline, an HMM-based (mono-\nlingual) indexing and retrieval engine. In order\nto design controlled experiments for the\nquestions above, the IR system was run without\nsophisticated query expansion techniques.\nOur experiments are based on the Chinese\nmaterials of TREC-5 and TREC-6 and the\nSpanish materials of TREC-4.\n"},{"#tail":"\n","@confidence":"0.9792735","#text":"\nFollowing Miller et al, 1999, the IR system\nranks documents according to the probability\nthat a document D is relevant given the query Q,\nP(D is R IQ). Using Bayes Rule, and the fact\nthat P(Q) is constant for a given query, and our\ninitial assumption of a uniform a priori\n"},{"#tail":"\n","@confidence":"0.965698956521739","#text":"\nprobability that a document is relevant, ranking\ndocuments according to P(Q\\[D is R) is the same\nas ranking them according to P(D is RIQ). The\napproach therefore estimates the probability that\na query Q is generated, given the document D is\nrelevant. (A glossary of symbols used appears\nbelow.)\nWe use x to represent the language (e.g.\nEnglish) for which retrieval is carried out.\nAccording to that model of monolingual\nretrieval, it can be shown that\np(Q \\[ D is R) = I I (aP(W \\[ Gx) + (1- a)e(w ID)),\nW inQ\nwhere W's are query words in Q. Miller et al\nestimated probabilities as follows:\n* The transition probability a is 0.7 using the\nEM algorithm (Rabiner, 1989) on the TREC4\nad-hoc query set.\nnumber of occurrences of W in C x\n? e0e IGx)= length of Cx\nwhich is the general language probability for\nword W in language x.\nnumber of occurrences of W in D\n"},{"#tail":"\n","@confidence":"0.997565","#text":"\nIn principle, any large corpus Cx that is\nrepresentative of language x can be used in\ncomputing the general language probabilities.\nIn practice, the collection to be searched is\nused for that purpose. The length of a\n"},{"#tail":"\n","@confidence":"0.907951","#text":"\ncollection is the sum of the document\nlengths.\n"},{"#tail":"\n","@confidence":"0.996737705882353","#text":"\nFor CLIR we extend the query generation\nprocess o that a document Dy written in\nlanguage y can generate a query Qx in language\nx. We use Wx to denote aword in x and Wy to\ndenote aword in y. As before, to model general\nquery words from language x, we estimate P(Wx\n\\]Gx) by using a large corpus Cx in language x.\nAlso as before, we estimate P(WyIDy) to be the\nsample distribution of Wy in Dy.\nWe use P(Wx\\[Wy) to denote the probability that\nWy is translated as Wx. Though terms often\nshould not be translated independent of their\ncontext, we make that simplifying assumption\nhere. We assume that the possible translations\nare specified by a bilingual lexicon BL. Since\nthe event spaces for Wy's in P(WyIDy) are\nmutually exclusive, we can compute the output\n"},{"#tail":"\n","@confidence":"0.944688","#text":"\nWe compute P(Q~IDy is R) as below:\n"},{"#tail":"\n","@confidence":"0.9991453","#text":"\nThe above model generates queries from\ndocuments, that is, it attempts o determine how\nlikely a particular query is given a relevant\ndocument. The retrieval system, however, can\nuse either query translation or document\ntranslation. We chose query translation over\ndocument translation for its flexibility, since it\nallowed us to experiment with a new method of\nestimating the translation probabilities without\nchanging the index structure.\n"},{"#tail":"\n","@confidence":"0.986600857142857","#text":"\nFor retrieval using English queries to search\nChinese documents, we used the TREC5 and\nTREC6 Chinese data which consists of 164,789\ndocuments from the Xinhua News Agency and\nPeople's Daily, averaging 450 Chinese\ncharacters/document. Each of the TREC topics\nhas three Chinese fields: title, description and\n"},{"#tail":"\n","@confidence":"0.9980315","#text":"\nnarrative, plus manually translated, English\nversions of each. We corrected some of the\nEnglish queries that contained errors, such as\n&quot;Dali Lama&quot; instead of the correct &quot;Dalai Lama&quot;\nand &quot;Medina&quot; instead of &quot;Medellin.&quot; Stop\nwords and stop phrases were removed. We\ncreated three versions of Chinese queries and\nthree versions of English queries: short (title\nonly), medium (title and description), and long\n(all three fields).\nFor retrieval using English queries to search\nSpanish documents, we used the TREC4\nSpanish data, which has 57,868 documents. It\nhas 25 queries in Spanish with manual\ntranslations toEnglish. We will denote the\nChinese data sets as Trec5C and Trec6C and the\nSpanish data set as Trec4S.\nWe used a Chinese-English lexicon from the\n"},{"#tail":"\n","@confidence":"0.969296671875","#text":"\nEnglish phrase is a translation for a Chinese\nword, each word in the phrase is taken as a\nseparate translation for the Chinese word. ~\n4. Estimate the translation probabilities. (We\nfirst report results assuming a uniform\ndistribution on a word's translations. If a\nChinese word c has n translations el, e2, ...en.\neach of them will be assigned equal probability,\ni.e., P(ei lc)=l/n. Section 10 supplements this\nwith a corpus-based distribution.)\n5. Invert he lexicon to make it an English-\nChinese lexicon. That is, for each English word\ne, we associate it with a list of Chinese words cl,\nc2, ... Cm together with non-zero translation\nprobabilities P( elc~).\nThe resulting English-Chinese l xicon has\n80,000 English words. On average, each\nEnglish word has 2.3 Chinese translations.\nFor Spanish, we downloaded a bilingual\nEnglish-Spanish lexicon from the Internet\n(http://www.activa.arrakis.es) containing around\n22,000 English words (16,000 English stems)\nand processed it similarly. Each English word\nhas around 1.5 translations on average. A co-\noccurrence based stemmer (Xu and Croft, 1998)\nwas used to stem Spanish words. One\ndifference from the treatment of Chinese is to\ninclude the English word as one of its own\ntranslations in addition to its Spanish\ntranslations in the lexicon. This is useful for\ntranslating proper nouns, which often have\nidentical spellings in English and Spanish but\nare routinely excluded from a lexicon.\nOne problem is the segmentation f Chinese\ntext, since Chinese has no spaces between\nwords. In these initial experiments, we relied on\na simple sub-string matching algorithm to\nextract words from Chinese text. To extract\nwords from a string of Chinese characters, the\nalgorithm examines any sub-string of length 2 or\ngreater and recognizes it as a Chinese word if it\nis in a predefined dictionary (the LDC lexicon in\nour case). In addition, any single character\nwhich is not part of any recognized Chinese\nwords in the first step is taken as a Chinese\nword. Note that this algorithm can extract a\ncompound Chinese word as well as its\ncomponents. For example, the Chinese word for\n&quot;particle physics&quot; as well as the Chinese words\nfor &quot;particle&quot; and &quot;physics&quot; will be extracted.\nThis seems desirable because it ensures the\nretrieval algorithm will match both the\ncompound words as well as their components.\nThe above algorithm was used in processing\nChinese documents and Chinese queries.\nEnglish data from the 2 GB of TREC disks l&2\nwas used to estimate P(WlG,..ngti~h), the general\nlanguage probabilities for English words. The\nevaluation metric used in this study is the\naverage precision using the trec_eval program\n(Voorhees and Harman, 1997). Mono-lingual\nretrieval results (using the Chinese and Spanish\nqueries) provided our baseline, with the HMM\nretrieval system (Miller et al 1999).\n"},{"#tail":"\n","@confidence":"0.993115576923077","#text":"\nTable 2 reports average precision for mono-\nlingual retrieval, average precision for cross-\nlingual, and the relative performance ratio of\ncross-lingual retrieval to mono-lingual.\nRelative performance of cross-lingual IR varies\nbetween 67% and 84% of mono-lingual IR.\nTrec6 Chinese queries have a somewhat higher\nrelative performance than Trec5 Chinese\nqueries. Longer queries have higher elative\nperformance than short queries in general.\nOverall, cross-lingual performance using our\nHMM retrieval model is around 76% of mono-\nlingual retrieval. A comparison of our mono-\nlingual results with Trec5 Chinese and Trec6\nChinese results published in the TREC\nproceedings (Voorhees and Harman, 1997,\n1998) shows that our mono-lingual results are\nclose to the top performers in the TREC\nconferences. Our Spanish mono-lingual\nperformance is also comparable tothe top\nautomatic runs of the TREC4 Spanish task\n(Harrnan, 1996). Since these mono-lingual\nresults were obtained without using\nsophisticated query processing techniques such\nas query expansion, we believe the mono-lingual\nresults form a valid baseline.\n"},{"#tail":"\n","@confidence":"0.501074333333333","#text":"\nlingual retrieval performance. The scores on\nthe monolingual and cross-lingual columns are\naverage precision.\n"},{"#tail":"\n","@confidence":"0.985188078431373","#text":"\nIn this section we compare our approach with\ntwo other approaches. One approach is &quot;simple\nsubstitution&quot;, i.e., replacing a query term with\nall its translations and treating the translated\nquery as a bag of words in mono-lingual\nretrieval. Suppose we have a simple query\nQ=(a, b), the translations for a are al, a2, a3, and\nthe translations for b are bl, b2. The translated\nquery would be (at, a2, a3, b~, b2). Since all terms\nare treated as equal in the translated query, this\ngives terms with more translations (potentially\nthe more common terms) more credit in\nretrieval, even though such terms hould\npotentially be given less credit if they are more\ncommon. Also, a document matching different\ntranslations ofone term in the original query\nmay be ranked higher than a document that\nmatches translations ofdifferent terms in the\noriginal query. That is, a document that\ncontains terms at, a2 and a3 may be ranked\nhigher than a document which contains terms at\nand bl. However, the second ocument is more\nlikely to be relevant since correct translations of\nthe query terms are more likely to co-occur\n(Ballesteros and Croft, 1998).\nA second method is to structure the translated\nquery, separating the translations for one term\nfrom translations for other terms. This approach\nlimits how much credit he retrieval algorithm\ncan give to a single term in the original query\nand prevents the translations ofone or a few\nterms from swamping the whole query. There\nare several variations of such a method\n(Ballesteros and Croft, 1998; Pirkola, 1998; Hull\n1997). One such method is to treat different\ntranslations ofthe same term as synonyms.\nBallesteros, for example, used the INQUERY\n(Callan et al 1995) synonym operator to group\ntranslations ofdifferent query terms. However,\nif a term has two translations inthe target\nlanguage, it will treat hem as equal even though\none of them is more likely to be the correct\ntranslation than the other. By contrast, our\nHMM approach supports translation\nprobabilities. The synonym approach is\nequivalent to changing all non-zero translation\nprobabilities P(W~\\[ Wy)'s to 1 in our retrieyal\nfunction. Even estimating uniform translation\nprobabilities gives higher weights to\nunambiguous translations and lower weights to\nhighly ambiguous translations.\n"},{"#tail":"\n","@confidence":"0.978244454545455","#text":"\nThese intuitions are supported empirically by the\nresults in Table 3. We can see that the HMM\nperforms best for every query set. Simple\nsubstitution performs worst. The synonym\napproach is significantly better than substitution,\nbut is consistently worse than the HMM\ntranslations were kept in disambiguation, the\nimprovement would be 4% for Trec6C-medium.\nThe results of this manual disambiguation\nsuggest that there are limits to automatic\ndisambiguation.\n"},{"#tail":"\n","@confidence":"0.987588724137931","#text":"\nTo get an upper bound on performance ofany\ndisambiguation technique, we manually\ndisambiguated the Trec5C-medium, Trec6C-\nmedium and Trec4S queries. That is, for each\nEnglish query term, a native Chinese or Spanish\nspeaker scanned the list of translations in the\nbilingual exicon and kept one translation\ndeemed to be the best for the English term and\ndiscarded the rest. If none of the translations\nwas correct, the first one was chosen.\nThe results in Table 4 show that manual\ndisambiguation improves performance by 17%\non Trec5C, 4% on Trec4S, but not at all on\nTrec6C. Furthermore, the improvement on\nTrec5C appears to be caused by big\nimprovements for a small number of queries.\nThe one-sided t-test (Hull, 1993) at significance\nlevel 0.05 indicated that the improvement on\nTrec5C is not statistically significant.\nIt seems urprising that disambiguation does not\nhelp at all for Trec6C. We found that many\nterms have more than one valid translation. For\nexample, the word &quot;flood&quot; (as in &quot;flood\ncontrol&quot;) has 4 valid Chinese translations. Using\nall of them achieves the desirable ffect of query\nexpansion. It appears that for Trec6C, the benefit\nof disambiguation is cancelled by choosing only\none of several alternatives, discarding those\nother good translations. If multiple correct\n"},{"#tail":"\n","@confidence":"0.76131","#text":"\nare average precision.\n"},{"#tail":"\n","@confidence":"0.974481961538462","#text":"\nResults in the previous ection showed that\nmanual disambiguation can bring performance\nof cross-lingual IR to around 82% of mono-\nlingual IR. The remaining performance gap\nbetween mono-lingual nd cross-lingual IR is\nlikely to be caused by the incompleteness of the\nbilingual exicon used for query translation, i.e.,\nmissing translations for some query terms. This\nmay be a more serious problem for cross-lingual\nIR than ambiguity. To test the conjecture, for\neach English query term, a native speaker in\nChinese or Spanish manually checked whether\nthe bilingual exicon contains acorrect\ntranslation for the term in the context of the\nquery. If it does not, a correct ranslation for the\nterm was added to the lexicon. For the query\nsets Trec5C-medium and Trec6C-medium, there\nare 100 query terms for which the lexicon does\nnot have a correct ranslation. This represents\n19% of the 520 query terms (a term is counted\nonly once in one query). For the query set\nTrec4S, the percentage is 12%.\nThe results in Table 5 show that with augmented\nlexicons, performance of cross-lingual IR is\n91%, 99% and 95% of mono-lingual IR on\nTrec5C-mediurn, Trec6C-medium and Trec4S.\n"},{"#tail":"\n","@confidence":"0.998417666666667","#text":"\nThe improvement over using the original exicon\nis 28%, 18% and 23% respectively. The results\ndemonstrate the importance cff a complete\nlexicon. Compared with the results in section 7,\nthe results here suggest that missing translations\nhave a much larger impact on cross-lingual IR\n"},{"#tail":"\n","@confidence":"0.995309321428572","#text":"\nIn this section we measure CLIR performance as\na function of lexicon size. We sorted the\nEnglish words from TREC disks l&2 in order of\ndecreasing frequency. For a lexicon of size n,\nwe keep only the n most frequent English words.\nThe upper graph in Figure 1 shows the curve of\ncross-lingual IR performance asa function of the\nsize of the lexicon based on the Chinese short\nand medium-length queries. Retrieval\nperformance was averaged over Trec5C and\nTrec6C. Initially retrieval performance increases\nsharply with lexicon size. After the dictionary\nexceeds 20,000, performance l vels off. An\nexamination of the translated queries hows that\nwords not appearing in the 20,000-word lexicon\nusually do not appear in the larger lexicons\neither. Thus, increases in the general lexicon\nbeyond 20,000 words did not result in a\nsubstantial increase in the coverage of the query\nterms.\nThe lower graph in Figure 1 plots the retrieval\nperformance asa function of the percent of the\nfull lexicon. The figure shows that short queries\nare more susceptible toincompleteness of the\nlexicon than longer queries. Using a 7,000-word\nlexicon, the short queries only achieve 75% of\ntheir performance with the full lexicon. In\ncomparison, the medium-length queries achieve\n"},{"#tail":"\n","@confidence":"0.998601529411764","#text":"\nWe categorized the missing terms and found that\nmost of them are proper nouns (especially\nlocations and person ames), highly technical\nterms, or numbers. Such words understandably\ndo not normally appear in traditional lexicons.\nTranslation of numbers can be solved using\nsimple rules. Transliteration, a technique that\nguesses the likely translations of a word based\non pronunciation, can be readily used in\ntranslating proper nouns.\nAnother technique is automatic discovery of\ntranslations from parallel or non-parallel corpora\n(Fung and Mckeown, 1997). Since traditional\nlexicons are more or less static repositories of\nknowledge, techniques that discover translation\nfrom newly published materials can supplement\nthem with corpus-specific vocabularies.\n"},{"#tail":"\n","@confidence":"0.992684789473684","#text":"\nIn this section we estimate translation\nprobabilities from a parallel corpus rather than\nassuming uniform likelihood as in section 4. A\nHong Kong News corpus obtained from the\nLinguistic Data Consortium has 9,769 news\nstories in Chinese with English translations. It\nhas 3.4 million English words. Since the\ndocuments are not exact ranslations of each\nother, occasionally having extra or missing\nsentences, we used document-level co-\noccurrence toestimate translation probabilities.\nThe Chinese documents were &quot;segmented&quot; using\nthe technique discussed in section 4. Let co(e,c)\nbe the number of parallel documents where an\nEnglish word e and a Chinese word c co-occur,\nand df(c) be the document frequency of c. If a\nChinese word c has n possible translations el to\nen in the bilingual exicon, we estimate the\ncorpus translation probability as:\n"},{"#tail":"\n","@confidence":"0.986291695652174","#text":"\nSince several translations for c may co-occur in\na document, ~co(e~ c) can be greater than df(c).\nUsing the maximum of the two ensures that\nE P_corpus(eilc)_<l.\nInstead of relying solely on corpus-based\nestimates from a small parallel corpus, we\nemploy a mixture model as follows:\nP( e I c) = ~ P _ corpus( eI c) + (1- #)P_ lexicon( e\\[ c)\nThe retrieval results in Table 6 show that\ncombining the probability estimates from the\nlexicon and the parallel corpus does improve\nretrieval performance. The best results are\nobtained when 13=0.7; this is better than using\nuniform probabilities by 9% on Trec5C-medium\nand 4% on Trec6C-medium. Using the corpus\nprobability estimates alone results in a\nsignificant drop in performance, the parallel\ncorpus is not large enough nor diverse nough\nfor reliable stimation of the translation\nprobabilities. In fact, many words do not appear\nin the corpus at all. With a larger and better\nparallel corpus, more weight should be given to\nthe probability estimates from the corpus.\n"},{"#tail":"\n","@confidence":"0.999122413793104","#text":"\nOther studies which view IR as a query\ngeneration process include Maron and Kuhns,\n1960; Hiemstra nd Kraaij, 1999; Ponte and\nCroft, 1998; Miller et al 1999. Our work has\nfocused on cross-lingual retrieval.\nMany approaches tocross-lingual IR have been\npublished. One common approach is using\nMachine Translation (MT) to translate the\nqueries to the language of the documents or\ntranslate documents othe language of the\nqueries (Gey et al 1999; Oard, 1998). For most\nlanguages, there are no MT systems at all. Our\nfocus is on languages where no MT exists, but a\nbilingual dictionary may exist or may be\nderived.\nAnother common approach is term translation,\ne.g., via a bilingual exicon. (Davis and Ogden,\n1997; Ballesteros and Croft, 1997; Hull and\nGrefenstette, 1996). While word sense\ndisambiguation has been a central topic in\nprevious tudies for cross-lingual IR, our study\nsuggests that using multiple weighted\ntranslations and compensating for the\nincompleteness of the lexicon may be more\nvaluable. Other studies on the value of\ndisambiguation for cross-lingual IR include\nHiernstra nd de Jong, 1999; Hull, 1997.\nSanderson, 1994 studied the issue of\ndisarnbiguation for mono-lingual IR.\n"},{"#tail":"\n","@confidence":"0.997436222222222","#text":"\nThe third approach to cross-lingual retrieval is to\nmap queries and documents o some\nintermediate r presentation, e.g latent semantic\nindexing (LSI) (Littman et al 1998), or the\nGeneral Vector space model (GVSM),\n(Carbonell et al 1997). We believe our\napproach is computationally ess costly than\n(LSI and GVSM) and assumes less resources\n(WordNet in Diekema et al, 1999).\n"},{"#tail":"\n","@confidence":"0.992656555555555","#text":"\nWe proposed an approach to cross-lingual IR\nbased on hidden Markov models, where the\nsystem estimates the probability that a query in\none language could be generated from a\ndocument in another language. Experiments\nusing the TREC5 and TREC6 Chinese test sets\nand the TREC4 Spanish test set show the\nfollowing:\n? Our retrieval model can reduce the\nperformance d gradation due to translation\nambiguity This had been a major limiting\nfactor for other query-translation\napproaches.\n? Some earlier studies uggested that query\ntranslation is not an effective approach to\ncross-lingual IR (Carbonell et al 1997).\nHowever, our results uggest that query\ntranslation can be effective particularly if a\nbilingual dictionary is the primary bilingual\nresource available.\n? Manual selection from the translations in the\nbilingual dictionary improves performance\nlittle over the HMM.\n? We believe an algorithm cannot rule out a\npossible translation with absolute\nconfidence; it is more effective to rely on\nprobability estimation/re-estimation to\ndifferentiate likely translations and unlikely\ntranslations.\n? Rather than translation ambiguity, a more\nserious limitation to effective cross-lingual\nIR is incompleteness of the bilingual exicon\nused for query translation.\n? Cross-lingual IR performance is typically\n75% that of mono-lingual for our HMM on\nthe Chinese and Spanish collections.\nFuture improvements in cross-lingual IR will\ncome by attacking the incompleteness of\nbilingual dictionaries and by improved query\nexpansion and context-dependent translation.\nOur current model assumes that query terms are\ngenerated one at time. We would like to extend\nthe model to allow phrase generation i the\nquery generation process. We also wish to\nexplore techniques to extend bilingual exicons.\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.482095","#text":"\nBBN Technologies\n"},{"#tail":"\n","@confidence":"0.305984","#text":"\nBBN Technologies\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.988","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.995342","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.997212","@genericHeader":"introduction","#text":"\n2 HMM for Mono-Lingual Retrieval\n"},{"#tail":"\n","@confidence":"0.998037","@genericHeader":"method","#text":"\n3 HMM for Cross-lingual IR\n"},{"#tail":"\n","@confidence":"0.99974","@genericHeader":"method","#text":"\n4 Experimental Set-up\n"},{"#tail":"\n","@confidence":"0.994289","@genericHeader":"method","#text":"\n5 Retrieval Results\n"},{"#tail":"\n","@confidence":"0.662286","@genericHeader":"method","#text":"\n6 Comparison with other Methods\n"},{"#tail":"\n","@confidence":"0.941557","@genericHeader":"method","#text":"\n7 Impact of Translation Ambiguity\n"},{"#tail":"\n","@confidence":"0.798836","@genericHeader":"method","#text":"\n8 Impact of Missing Translations\n"},{"#tail":"\n","@confidence":"0.644331","@genericHeader":"method","#text":"\n9 Impact of Lexicon Size\n"},{"#tail":"\n","@confidence":"0.994014","@genericHeader":"related work","#text":"\n11 Related Work\n"},{"#tail":"\n","@confidence":"0.911979","@genericHeader":"conclusions","#text":"\n12 Conclusions and Future Work\n"},{"#tail":"\n","@confidence":"0.973255","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.993827","#text":"\nTable 2: Comparing mono-lingual and cross-\n"},{"#tail":"\n","@confidence":"0.603519","#text":"\nTable 3: Comparing different methods of\nquery translation. All numbers are average\nprecision.\n"},{"#tail":"\n","@confidence":"0.804983","#text":"\nTable 4: The effect of disambiguation on\nretrieval performance. The scores reported\n"},{"#tail":"\n","@confidence":"0.949802333333333","#text":"\nTable 5: The impact of missing the right\ntranslations on retrieval performance. All\nscores are average precision.\n"},{"#tail":"\n","@confidence":"0.973574","#text":"\nTable 6: Performance with different values\nof 13. All scores are average precision.\n"}],"page":[{"#tail":"\n","@confidence":"0.991846","#text":"\n95\n"},{"#tail":"\n","@confidence":"0.996062","#text":"\n96\n"},{"#tail":"\n","@confidence":"0.996579","#text":"\n97\n"},{"#tail":"\n","@confidence":"0.997597","#text":"\n98\n"},{"#tail":"\n","@confidence":"0.995637","#text":"\n99\n"},{"#tail":"\n","@confidence":"0.8329","#text":"\n100\n"},{"#tail":"\n","@confidence":"0.992546","#text":"\n101\n"},{"#tail":"\n","@confidence":"0.974726","#text":"\n102\n"},{"#tail":"\n","@confidence":"0.999684","#text":"\n103\n"}],"figureCaption":{"#tail":"\n","@confidence":"0.7850355","#text":"\nFigure 1 Impact of lexicon size on cross-lingual IR\nperformance\n"},"table":[{"#tail":"\n","@confidence":"0.9111064","#text":"\nQuery sets Mono- Cross- % of\nlingual lingual Mono-\nlingual\nTrec5C-short 0.2830 0.1889 67%\nTrec5C-medium 0.3427 0.2449 72%\nTrec5C-long 0.3750 0.2735 73%\nTrec6C-short 0.3423 0.2617 77%\nTrec6C-medium 0.4606 0.3872 84%\nTrec6C-long 0.5104 0.4206 82%\nTrec4S 0.2252 0.1729 77%\n"},{"#tail":"\n","@confidence":"0.9947006","#text":"\nSubsti- Synonym HMM\ntution\nTrec5C-long 0.0391 0.2306 0.2735\nTrec6C-long 0.0941 0.3842 0.4206\nTrec4S 0.0935 0.1594 0.1729\n"},{"#tail":"\n","@confidence":"0.9133288","#text":"\nthan translation ambiguity does.\nQuery sets Original Augmented % o f\nlexicon lexicon Mono-\nlingual\nTrec5C- 0.2449 0.3131 91%\nmedium (+28%)\nTrec6C- 0.3872 0.4589 99%\nmedium (+18%)\nTrec4S 0.1729 0.2128 95%\n(+23%)\n"},{"#tail":"\n","@confidence":"0.994518857142857","#text":"\nTrec5 - Trec6-\nmedium medium\nP_lexicon 0.2449 0.3872\n13=0.3 0.2557 0.3980\n13=0.5 0.2605 0.4021\n13=0.7 0.2658 0.4035\nP_corpus 0.2293 0.2971\n"}],"email":[{"#tail":"\n","@confidence":"0.893544","#text":"\njxu@bbn.com\n"},{"#tail":"\n","@confidence":"0.911327","#text":"\nweischedel @bbn.com\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.942232","#tail":"\n","@no":"0","address":[{"#tail":"\n","@confidence":"0.9982615","#text":"70 Fawcett St. Cambridge, MA, USA 02138"},{"#tail":"\n","@confidence":"0.9981925","#text":"70 Fawcett St. Cambridge, MA, USA 02138"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.991024","#text":"BBN Technologies"},{"#tail":"\n","@confidence":"0.975278","#text":"BBN Technologies"}],"author":[{"#tail":"\n","@confidence":"0.999527","#text":"Jinxi Xu"},{"#tail":"\n","@confidence":"0.999755","#text":"Ralph Weischedel"}],"abstract":{"#tail":"\n","@confidence":"0.999302611111111","#text":"This paper presents empirical results in cross-lingual information retrieval using English queries to access Chinese documents (TREC-5 and TREC-6) and Spanish documents (TREC-4). Since our interest is in languages where resources may be minimal, we use an integrated probabilistic model that requires only a bilingual dictionary as a resource. We explore how a combined probability model of term translation and retrieval can reduce the effect of translation ambiguity. In addition, we estimate an upper bound on performance, if translation ambiguity were a solved problem. We also measure performance as a function of bilingual dictionary size."},"title":{"#tail":"\n","@confidence":"0.9962","#text":"Cross-lingual Information Retrieval using Hidden Markov Models"},"email":[{"#tail":"\n","@confidence":"0.999816","#text":"jxu@bbn.com"},{"#tail":"\n","@confidence":"0.997978","#text":"weischedel@bbn.com"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"L. Ballesteros and W.B. Croft 1997. &quot;Phrasal translation and query expansion techniques for cross-language information retrieval.&quot; Proceedings of the 20th ACM SIGIR International Conference on Research and Development in Information Retrieval 1997, pp. 84-91."},"#text":"\n","pages":{"#tail":"\n","#text":"84--91"},"marker":{"#tail":"\n","#text":"Ballesteros, Croft, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"1999; Ponte and Croft, 1998; Miller et al 1999. Our work has focused on cross-lingual retrieval. Many approaches tocross-lingual IR have been published. One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents othe language of the queries (Gey et al 1999; Oard, 1998). For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous tudies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra nd de Jong, 1999; Hull, 1997. Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR. 101 The third approach to cross-lingual retrieval is to map queries and documents o some intermediate r presentation, e.g latent semantic indexi","@endWordPosition":"3752","@position":"23370","annotationId":"T1","@startWordPosition":"3749","@citStr":"Ballesteros and Croft, 1997"}},"title":{"#tail":"\n","#text":"Phrasal translation and query expansion techniques for cross-language information retrieval.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 20th ACM SIGIR International Conference on Research and Development in Information Retrieval"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L Ballesteros"},{"#tail":"\n","#text":"W B Croft"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"L. Ballesteros and W.B. Croft, 1998. &quot;Resolving ambiguity for cross-language retrieval.&quot; Proceedings of the 21st ACM SIGIR Conference on Research and Development in Information Retrieval, 1998, pp. 64-71."},"#text":"\n","pages":{"#tail":"\n","#text":"64--71"},"marker":{"#tail":"\n","#text":"Ballesteros, Croft, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"slations (potentially the more common terms) more credit in retrieval, even though such terms hould potentially be given less credit if they are more common. Also, a document matching different translations ofone term in the original query may be ranked higher than a document that matches translations ofdifferent terms in the original query. That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at and bl. However, the second ocument is more likely to be relevant since correct translations of the query terms are more likely to co-occur (Ballesteros and Croft, 1998). A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit he retrieval algorithm can give to a single term in the original query and prevents the translations ofone or a few terms from swamping the whole query. There are several variations of such a method (Ballesteros and Croft, 1998; Pirkola, 1998; Hull 1997). One such method is to treat different translations ofthe same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al 1995) synonym operator to group transl","@endWordPosition":"2074","@position":"12829","annotationId":"T2","@startWordPosition":"2071","@citStr":"Ballesteros and Croft, 1998"}},"title":{"#tail":"\n","#text":"Resolving ambiguity for cross-language retrieval.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 21st ACM SIGIR Conference on Research and Development in Information Retrieval,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L Ballesteros"},{"#tail":"\n","#text":"W B Croft"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"J.P. Callan, W.B. Croft and J. Broglio. 1995. &quot;TREC and TIPSTER Experiments with INQUERY&quot;."},"#text":"\n","marker":{"#tail":"\n","#text":"Callan, Croft, Broglio, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"re likely to co-occur (Ballesteros and Croft, 1998). A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit he retrieval algorithm can give to a single term in the original query and prevents the translations ofone or a few terms from swamping the whole query. There are several variations of such a method (Ballesteros and Croft, 1998; Pirkola, 1998; Hull 1997). One such method is to treat different translations ofthe same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al 1995) synonym operator to group translations ofdifferent query terms. However, if a term has two translations inthe target language, it will treat hem as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities. The synonym approach is equivalent to changing all non-zero translation probabilities P(W~\\[ Wy)'s to 1 in our retrieyal function. Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations. 98 These int","@endWordPosition":"2166","@position":"13396","annotationId":"T3","@startWordPosition":"2163","@citStr":"Callan et al 1995"}},"title":{"#tail":"\n","#text":"TREC and TIPSTER Experiments with INQUERY&quot;."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J P Callan"},{"#tail":"\n","#text":"W B Croft"},{"#tail":"\n","#text":"J Broglio"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Information Processing and Management, pages 327-343, 1995."},"#text":"\n","pages":{"#tail":"\n","#text":"327--343"},"marker":{"#tail":"\n","#text":"1995"},"booktitle":{"#tail":"\n","#text":"Information Processing and Management,"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"J. Carbonell, Y. Yang, R. Frederking, R. Brown, Y. Geng and D. Lee, 1997. &quot;Translingual information retrieval: a comparative evaluation.&quot; In Proceedings of the 15th International Joint Conference on Artificial Intelligence, 1997."},"#text":"\n","marker":{"#tail":"\n","#text":"Carbonell, Yang, Frederking, Brown, Geng, Lee, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" central topic in previous tudies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra nd de Jong, 1999; Hull, 1997. Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR. 101 The third approach to cross-lingual retrieval is to map queries and documents o some intermediate r presentation, e.g latent semantic indexing (LSI) (Littman et al 1998), or the General Vector space model (GVSM), (Carbonell et al 1997). We believe our approach is computationally ess costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al, 1999). 12 Conclusions and Future Work We proposed an approach to cross-lingual IR based on hidden Markov models, where the system estimates the probability that a query in one language could be generated from a document in another language. Experiments using the TREC5 and TREC6 Chinese test sets and the TREC4 Spanish test set show the following: ? Our retrieval model can reduce the performance d gradation due to translation ambiguity This had been a major limiting f","@endWordPosition":"3857","@position":"24065","annotationId":"T4","@startWordPosition":"3854","@citStr":"Carbonell et al 1997"}},"title":{"#tail":"\n","#text":"Translingual information retrieval: a comparative evaluation.&quot;"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 15th International Joint Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Carbonell"},{"#tail":"\n","#text":"Y Yang"},{"#tail":"\n","#text":"R Frederking"},{"#tail":"\n","#text":"R Brown"},{"#tail":"\n","#text":"Y Geng"},{"#tail":"\n","#text":"D Lee"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"M. Davis and W. Ogden, 1997. &quot;QUILT: Implementing a Large Scale Cross-language Text Retrieval System.&quot; Proceedings of ACM SIGIR Conference, 1997. A. Diekema, F. Oroumchain, P. Sheridan and E."},"#text":"\n","marker":{"#tail":"\n","#text":"Davis, Ogden, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"0; Hiemstra nd Kraaij, 1999; Ponte and Croft, 1998; Miller et al 1999. Our work has focused on cross-lingual retrieval. Many approaches tocross-lingual IR have been published. One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents othe language of the queries (Gey et al 1999; Oard, 1998). For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous tudies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra nd de Jong, 1999; Hull, 1997. Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR. 101 The third approach to cross-lingual retrieval is to map queries and documents o some intermediate r presentatio","@endWordPosition":"3748","@position":"23341","annotationId":"T5","@startWordPosition":"3745","@citStr":"Davis and Ogden, 1997"}},"title":{"#tail":"\n","#text":"QUILT: Implementing a Large Scale Cross-language Text Retrieval System.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of ACM SIGIR Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Davis"},{"#tail":"\n","#text":"W Ogden"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Liddy, 1999. &quot;TREC-7 Evaluation of Conceptual Interlingual Document Retrieval (CINDOR) in English and French.&quot; TREC7 Proceedings, NIST special publication."},"#text":"\n","marker":{"#tail":"\n","#text":"Liddy, 1999"},"title":{"#tail":"\n","#text":"TREC-7 Evaluation of Conceptual Interlingual Document Retrieval (CINDOR) in English and French.&quot;"},"booktitle":{"#tail":"\n","#text":"TREC7 Proceedings, NIST special publication."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Liddy"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"P. Fung and K. Mckeown. &quot;Finding Terminology Translations from Non-parallel Corpora.&quot; The 5 'h Annual Workshop on Very Large Corpora, Hong Kong: August 1997, 192n202 F. Gey, J. He and A. Chen, 1999. &quot;Manual queries and Machine Translation in cross-language retrieval at TREC-7&quot;. In TREC7 Proceedings, NIST Special Publication, 1999."},"#text":"\n","marker":{"#tail":"\n","#text":"Fung, Mckeown, 1997"},"location":{"#tail":"\n","#text":"Hong Kong:"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" Size Figure 1 Impact of lexicon size on cross-lingual IR performance We categorized the missing terms and found that most of them are proper nouns (especially locations and person ames), highly technical terms, or numbers. Such words understandably do not normally appear in traditional lexicons. Translation of numbers can be solved using simple rules. Transliteration, a technique that guesses the likely translations of a word based on pronunciation, can be readily used in translating proper nouns. Another technique is automatic discovery of translations from parallel or non-parallel corpora (Fung and Mckeown, 1997). Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies. 100 10 Using a Parallel Corpus In this section we estimate translation probabilities from a parallel corpus rather than assuming uniform likelihood as in section 4. A Hong Kong News corpus obtained from the Linguistic Data Consortium has 9,769 news stories in Chinese with English translations. It has 3.4 million English words. Since the documents are not exact ranslations of each other, occasio","@endWordPosition":"3245","@position":"20237","annotationId":"T6","@startWordPosition":"3242","@citStr":"Fung and Mckeown, 1997"}},"title":{"#tail":"\n","#text":"Finding Terminology Translations from Non-parallel Corpora.&quot; The 5 'h Annual Workshop on Very Large Corpora,"},"booktitle":{"#tail":"\n","#text":"In TREC7 Proceedings, NIST Special Publication,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P Fung"},{"#tail":"\n","#text":"K Mckeown"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Harman, 1996. The TREC-4 Proceedings. NIST Special publication, 1996."},"#text":"\n","marker":{"#tail":"\n","#text":"Harman, 1996"},"booktitle":{"#tail":"\n","#text":"The TREC-4 Proceedings. NIST Special publication,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Harman"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"D. Hiemstra nd F. de Jong, 1999. &quot;Disambiguafion strategies for Cross-language Information Retrieval.&quot; Proceedings of the third European Conference on Research and Advanced Technology for Digital Libraries, pp. 274-293, 1999."},"#text":"\n","pages":{"#tail":"\n","#text":"274--293"},"marker":{"#tail":"\n","#text":"de Jong, 1999"},"title":{"#tail":"\n","#text":"Disambiguafion strategies for Cross-language Information Retrieval.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the third European Conference on Research and Advanced Technology for Digital Libraries,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Hiemstra nd F de Jong"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"D. Hiemstra and W. Kraaij, 1999. &quot;Twenty-One at TREC-7: ad-hoc and cross-language track.&quot; In TREC-7 Proceedings, NIST Special Publication, 1999."},"#text":"\n","marker":{"#tail":"\n","#text":"Hiemstra, Kraaij, 1999"},"title":{"#tail":"\n","#text":"Twenty-One at TREC-7: ad-hoc and cross-language track.&quot;"},"booktitle":{"#tail":"\n","#text":"In TREC-7 Proceedings, NIST Special Publication,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Hiemstra"},{"#tail":"\n","#text":"W Kraaij"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"D. Hull, 1993. &quot;Using Statistical Testing in the Evaluation of Retrieval Experiments.&quot; Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 329-338, 1993."},"#text":"\n","pages":{"#tail":"\n","#text":"329--338"},"marker":{"#tail":"\n","#text":"Hull, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"medium, Trec6C- medium and Trec4S queries. That is, for each English query term, a native Chinese or Spanish speaker scanned the list of translations in the bilingual exicon and kept one translation deemed to be the best for the English term and discarded the rest. If none of the translations was correct, the first one was chosen. The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C. Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries. The one-sided t-test (Hull, 1993) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant. It seems urprising that disambiguation does not help at all for Trec6C. We found that many terms have more than one valid translation. For example, the word &quot;flood&quot; (as in &quot;flood control&quot;) has 4 valid Chinese translations. Using all of them achieves the desirable ffect of query expansion. It appears that for Trec6C, the benefit of disambiguation is cancelled by choosing only one of several alternatives, discarding those other good translations. If multiple correct Query sets Trec5C-medium Tre","@endWordPosition":"2472","@position":"15411","annotationId":"T7","@startWordPosition":"2471","@citStr":"Hull, 1993"}},"title":{"#tail":"\n","#text":"Using Statistical Testing in the Evaluation of Retrieval Experiments.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Hull"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"D. A. Hull and G. Grefenstette, 1996. &quot;A dictionarybased approach to multilingual information retrieval&quot;. Proceedings of ACM SIGIR Conference, 1996."},"#text":"\n","marker":{"#tail":"\n","#text":"Hull, Grefenstette, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Miller et al 1999. Our work has focused on cross-lingual retrieval. Many approaches tocross-lingual IR have been published. One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents othe language of the queries (Gey et al 1999; Oard, 1998). For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous tudies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra nd de Jong, 1999; Hull, 1997. Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR. 101 The third approach to cross-lingual retrieval is to map queries and documents o some intermediate r presentation, e.g latent semantic indexing (LSI) (Littman et al 1998),","@endWordPosition":"3756","@position":"23400","annotationId":"T8","@startWordPosition":"3753","@citStr":"Hull and Grefenstette, 1996"}},"title":{"#tail":"\n","#text":"A dictionarybased approach to multilingual information retrieval&quot;."},"booktitle":{"#tail":"\n","#text":"Proceedings of ACM SIGIR Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D A Hull"},{"#tail":"\n","#text":"G Grefenstette"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"D. A. Hull, 1997. &quot;Using structured queries for disambiguation in cross-language information retrieval.&quot; In AAAI Symposium on Cross-Language Text and Speech Retrieval. AAAI, 1997."},"#text":"\n","marker":{"#tail":"\n","#text":"Hull, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"contains terms at and bl. However, the second ocument is more likely to be relevant since correct translations of the query terms are more likely to co-occur (Ballesteros and Croft, 1998). A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit he retrieval algorithm can give to a single term in the original query and prevents the translations ofone or a few terms from swamping the whole query. There are several variations of such a method (Ballesteros and Croft, 1998; Pirkola, 1998; Hull 1997). One such method is to treat different translations ofthe same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al 1995) synonym operator to group translations ofdifferent query terms. However, if a term has two translations inthe target language, it will treat hem as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities. The synonym approach is equivalent to changing all non-zero translation probabilities P(W~\\[ Wy)'s to 1 in our retrieyal function. Even estimating uni","@endWordPosition":"2143","@position":"13252","annotationId":"T9","@startWordPosition":"2142","@citStr":"Hull 1997"},{"#tail":"\n","#text":". Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous tudies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra nd de Jong, 1999; Hull, 1997. Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR. 101 The third approach to cross-lingual retrieval is to map queries and documents o some intermediate r presentation, e.g latent semantic indexing (LSI) (Littman et al 1998), or the General Vector space model (GVSM), (Carbonell et al 1997). We believe our approach is computationally ess costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al, 1999). 12 Conclusions and Future Work We proposed an approach to cross-lingual IR based on hidden Markov models, where the system estimates the probability","@endWordPosition":"3809","@position":"23750","annotationId":"T10","@startWordPosition":"3808","@citStr":"Hull, 1997"}]},"title":{"#tail":"\n","#text":"Using structured queries for disambiguation in cross-language information retrieval.&quot;"},"booktitle":{"#tail":"\n","#text":"In AAAI Symposium on Cross-Language Text and Speech Retrieval. AAAI,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D A Hull"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1960"},"rawString":{"#tail":"\n","#text":"M. E. Maron and K. L. Kuhns, 1960. &quot;On Relevance, Probabilistic Indexing and Information Retrieval.&quot; Journal of the Association for &quot;: Computing Machinery, 1960, pp 216-244."},"journal":{"#tail":"\n","#text":"Journal of the Association for &quot;: Computing Machinery,"},"#text":"\n","pages":{"#tail":"\n","#text":"216--244"},"marker":{"#tail":"\n","#text":"Maron, Kuhns, 1960"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rmance, the parallel corpus is not large enough nor diverse nough for reliable stimation of the translation probabilities. In fact, many words do not appear in the corpus at all. With a larger and better parallel corpus, more weight should be given to the probability estimates from the corpus. Trec5 - Trec6- medium medium P_lexicon 0.2449 0.3872 13=0.3 0.2557 0.3980 13=0.5 0.2605 0.4021 13=0.7 0.2658 0.4035 P_corpus 0.2293 0.2971 Table 6: Performance with different values of 13. All scores are average precision. 11 Related Work Other studies which view IR as a query generation process include Maron and Kuhns, 1960; Hiemstra nd Kraaij, 1999; Ponte and Croft, 1998; Miller et al 1999. Our work has focused on cross-lingual retrieval. Many approaches tocross-lingual IR have been published. One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents othe language of the queries (Gey et al 1999; Oard, 1998). For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (D","@endWordPosition":"3646","@position":"22721","annotationId":"T11","@startWordPosition":"3643","@citStr":"Maron and Kuhns, 1960"}},"title":{"#tail":"\n","#text":"On Relevance, Probabilistic Indexing and Information Retrieval.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M E Maron"},{"#tail":"\n","#text":"K L Kuhns"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"D. Miller, T. Leek and R. Schwartz, 1999. &quot;A Hidden Markov Model Information Retrieval System.&quot; Proceedings of the 22nd Annual International ACM S1GIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999."},"#text":"\n","pages":{"#tail":"\n","#text":"214--221"},"marker":{"#tail":"\n","#text":"Miller, Leek, Schwartz, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"or term translation? (Section 8) ? How much does performance d grade due to omissions from the bilingual dictionary and how does performance vary with size of such a dictionary? (Sections 8-9) All experiments were performed using a common baseline, an HMM-based (mono- lingual) indexing and retrieval engine. In order to design controlled experiments for the questions above, the IR system was run without sophisticated query expansion techniques. Our experiments are based on the Chinese materials of TREC-5 and TREC-6 and the Spanish materials of TREC-4. 2 HMM for Mono-Lingual Retrieval Following Miller et al, 1999, the IR system ranks documents according to the probability that a document D is relevant given the query Q, P(D is R IQ). Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori 95 probability that a document is relevant, ranking documents according to P(Q\\[D is R) is the same as ranking them according to P(D is RIQ). The approach therefore estimates the probability that a query Q is generated, given the document D is relevant. (A glossary of symbols used appears below.) We use x to represent the language (e.g. English) for whi","@endWordPosition":"442","@position":"2837","annotationId":"T12","@startWordPosition":"439","@citStr":"Miller et al, 1999"},{"#tail":"\n","#text":"ted. This seems desirable because it ensures the retrieval algorithm will match both the compound words as well as their components. The above algorithm was used in processing Chinese documents and Chinese queries. English data from the 2 GB of TREC disks l&2 was used to estimate P(WlG,..ngti~h), the general language probabilities for English words. The evaluation metric used in this study is the average precision using the trec_eval program (Voorhees and Harman, 1997). Mono-lingual retrieval results (using the Chinese and Spanish queries) provided our baseline, with the HMM retrieval system (Miller et al 1999). 1 Clearly, this is not correct; however, it simplified implementation. 97 5 Retrieval Results Table 2 reports average precision for mono- lingual retrieval, average precision for cross- lingual, and the relative performance ratio of cross-lingual retrieval to mono-lingual. Relative performance of cross-lingual IR varies between 67% and 84% of mono-lingual IR. Trec6 Chinese queries have a somewhat higher relative performance than Trec5 Chinese queries. Longer queries have higher elative performance than short queries in general. Overall, cross-lingual performance using our HMM retrieval model","@endWordPosition":"1648","@position":"10040","annotationId":"T13","@startWordPosition":"1645","@citStr":"Miller et al 1999"},{"#tail":"\n","#text":"liable stimation of the translation probabilities. In fact, many words do not appear in the corpus at all. With a larger and better parallel corpus, more weight should be given to the probability estimates from the corpus. Trec5 - Trec6- medium medium P_lexicon 0.2449 0.3872 13=0.3 0.2557 0.3980 13=0.5 0.2605 0.4021 13=0.7 0.2658 0.4035 P_corpus 0.2293 0.2971 Table 6: Performance with different values of 13. All scores are average precision. 11 Related Work Other studies which view IR as a query generation process include Maron and Kuhns, 1960; Hiemstra nd Kraaij, 1999; Ponte and Croft, 1998; Miller et al 1999. Our work has focused on cross-lingual retrieval. Many approaches tocross-lingual IR have been published. One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents othe language of the queries (Gey et al 1999; Oard, 1998). For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenst","@endWordPosition":"3658","@position":"22789","annotationId":"T14","@startWordPosition":"3655","@citStr":"Miller et al 1999"}]},"title":{"#tail":"\n","#text":"A Hidden Markov Model Information Retrieval System.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 22nd Annual International ACM S1GIR Conference on Research and Development in Information Retrieval,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Miller"},{"#tail":"\n","#text":"T Leek"},{"#tail":"\n","#text":"R Schwartz"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"D.W. Oard, 1998. &quot;A comparative study of query and document translation for cross-language information retrieval.&quot; In Proceedings of the Third Conference of the Association for Machine Translation in America (AMTA ), 1998."},"#text":"\n","marker":{"#tail":"\n","#text":"Oard, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":".2605 0.4021 13=0.7 0.2658 0.4035 P_corpus 0.2293 0.2971 Table 6: Performance with different values of 13. All scores are average precision. 11 Related Work Other studies which view IR as a query generation process include Maron and Kuhns, 1960; Hiemstra nd Kraaij, 1999; Ponte and Croft, 1998; Miller et al 1999. Our work has focused on cross-lingual retrieval. Many approaches tocross-lingual IR have been published. One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents othe language of the queries (Gey et al 1999; Oard, 1998). For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous tudies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for c","@endWordPosition":"3704","@position":"23088","annotationId":"T15","@startWordPosition":"3703","@citStr":"Oard, 1998"}},"title":{"#tail":"\n","#text":"A comparative study of query and document translation for cross-language information retrieval.&quot;"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Third Conference of the Association for Machine Translation in America (AMTA ),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D W Oard"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Ari Pirkola, 1998. &quot;The effects of query structure and dictionary setups in dictionary-based crosslanguage information retrieval.&quot; Proceedings of ACM SIGIR Conference, 1998, pp 55-63."},"#text":"\n","pages":{"#tail":"\n","#text":"55--63"},"marker":{"#tail":"\n","#text":"Pirkola, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"document which contains terms at and bl. However, the second ocument is more likely to be relevant since correct translations of the query terms are more likely to co-occur (Ballesteros and Croft, 1998). A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit he retrieval algorithm can give to a single term in the original query and prevents the translations ofone or a few terms from swamping the whole query. There are several variations of such a method (Ballesteros and Croft, 1998; Pirkola, 1998; Hull 1997). One such method is to treat different translations ofthe same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al 1995) synonym operator to group translations ofdifferent query terms. However, if a term has two translations inthe target language, it will treat hem as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities. The synonym approach is equivalent to changing all non-zero translation probabilities P(W~\\[ Wy)'s to 1 in our retrieyal function. Even es","@endWordPosition":"2141","@position":"13240","annotationId":"T16","@startWordPosition":"2140","@citStr":"Pirkola, 1998"}},"title":{"#tail":"\n","#text":"The effects of query structure and dictionary setups in dictionary-based crosslanguage information retrieval.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of ACM SIGIR Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Ari Pirkola"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"J. Ponte and W.B. Croft, 1998. &quot;A Language Modeling Approach to Information Retrieval.&quot; Proceedings of the 21st Annual International ACM S1GIR Conference on Research and Development in Information Retrieval, pages 275-281, 1998."},"#text":"\n","pages":{"#tail":"\n","#text":"275--281"},"marker":{"#tail":"\n","#text":"Ponte, Croft, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"or diverse nough for reliable stimation of the translation probabilities. In fact, many words do not appear in the corpus at all. With a larger and better parallel corpus, more weight should be given to the probability estimates from the corpus. Trec5 - Trec6- medium medium P_lexicon 0.2449 0.3872 13=0.3 0.2557 0.3980 13=0.5 0.2605 0.4021 13=0.7 0.2658 0.4035 P_corpus 0.2293 0.2971 Table 6: Performance with different values of 13. All scores are average precision. 11 Related Work Other studies which view IR as a query generation process include Maron and Kuhns, 1960; Hiemstra nd Kraaij, 1999; Ponte and Croft, 1998; Miller et al 1999. Our work has focused on cross-lingual retrieval. Many approaches tocross-lingual IR have been published. One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents othe language of the queries (Gey et al 1999; Oard, 1998). For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997","@endWordPosition":"3654","@position":"22770","annotationId":"T17","@startWordPosition":"3651","@citStr":"Ponte and Croft, 1998"}},"title":{"#tail":"\n","#text":"A Language Modeling Approach to Information Retrieval.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 21st Annual International ACM S1GIR Conference on Research and Development in Information Retrieval,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Ponte"},{"#tail":"\n","#text":"W B Croft"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"L. Rabiner, 1989. &quot;A tutorial on hidden Markov models and selected applications in speech recognition.&quot; Proc. IEEE 77, pp. 257-286, 1989."},"#text":"\n","pages":{"#tail":"\n","#text":"257--286"},"marker":{"#tail":"\n","#text":"Rabiner, 1989"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ments according to P(Q\\[D is R) is the same as ranking them according to P(D is RIQ). The approach therefore estimates the probability that a query Q is generated, given the document D is relevant. (A glossary of symbols used appears below.) We use x to represent the language (e.g. English) for which retrieval is carried out. According to that model of monolingual retrieval, it can be shown that p(Q \\[ D is R) = I I (aP(W \\[ Gx) + (1- a)e(w ID)), W inQ where W's are query words in Q. Miller et al estimated probabilities as follows: * The transition probability a is 0.7 using the EM algorithm (Rabiner, 1989) on the TREC4 ad-hoc query set. number of occurrences of W in C x ? e0e IGx)= length of Cx which is the general language probability for word W in language x. number of occurrences of W in D ? e (WlD) = length of D In principle, any large corpus Cx that is representative of language x can be used in computing the general language probabilities. In practice, the collection to be searched is used for that purpose. The length of a Q a query English query a document a document in foreign language y document is relevant a word an English corpus a corpus in language x QX D Dr D isR W Gx Cx Wx BL an ","@endWordPosition":"609","@position":"3752","annotationId":"T18","@startWordPosition":"608","@citStr":"Rabiner, 1989"}},"title":{"#tail":"\n","#text":"A tutorial on hidden Markov models and selected applications in speech recognition.&quot;"},"booktitle":{"#tail":"\n","#text":"Proc. IEEE 77,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"L Rabiner"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"M. Sanderson. &quot;Word sense disambiguation and information retrieval.&quot; Proceedings of ACM SIGIR Conference, 1994, pp 142-15 I."},"#text":"\n","pages":{"#tail":"\n","#text":"142--15"},"marker":{"#tail":"\n","#text":"Sanderson, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual exicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous tudies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra nd de Jong, 1999; Hull, 1997. Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR. 101 The third approach to cross-lingual retrieval is to map queries and documents o some intermediate r presentation, e.g latent semantic indexing (LSI) (Littman et al 1998), or the General Vector space model (GVSM), (Carbonell et al 1997). We believe our approach is computationally ess costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al, 1999). 12 Conclusions and Future Work We proposed an approach to cross-lingual IR based on hidden Markov models, where the system estimates the probability that a query in ","@endWordPosition":"3811","@position":"23767","annotationId":"T19","@startWordPosition":"3810","@citStr":"Sanderson, 1994"}},"title":{"#tail":"\n","#text":"Word sense disambiguation and information retrieval.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of ACM SIGIR Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Sanderson"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"note":{"#tail":"\n","#text":"NIST special publication."},"rawString":{"#tail":"\n","#text":"Voorhees and Harman, 1997. TREC-5 Proceedings. E. Voorhees and D. Harman, Editors. NIST special publication."},"#text":"\n","pages":{"#tail":"\n","#text":"5"},"marker":{"#tail":"\n","#text":"Voorhees, Harman, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" as well as its components. For example, the Chinese word for &quot;particle physics&quot; as well as the Chinese words for &quot;particle&quot; and &quot;physics&quot; will be extracted. This seems desirable because it ensures the retrieval algorithm will match both the compound words as well as their components. The above algorithm was used in processing Chinese documents and Chinese queries. English data from the 2 GB of TREC disks l&2 was used to estimate P(WlG,..ngti~h), the general language probabilities for English words. The evaluation metric used in this study is the average precision using the trec_eval program (Voorhees and Harman, 1997). Mono-lingual retrieval results (using the Chinese and Spanish queries) provided our baseline, with the HMM retrieval system (Miller et al 1999). 1 Clearly, this is not correct; however, it simplified implementation. 97 5 Retrieval Results Table 2 reports average precision for mono- lingual retrieval, average precision for cross- lingual, and the relative performance ratio of cross-lingual retrieval to mono-lingual. Relative performance of cross-lingual IR varies between 67% and 84% of mono-lingual IR. Trec6 Chinese queries have a somewhat higher relative performance than Trec5 Chinese querie","@endWordPosition":"1627","@position":"9895","annotationId":"T20","@startWordPosition":"1624","@citStr":"Voorhees and Harman, 1997"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Voorhees"},{"#tail":"\n","#text":"Harman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"note":{"#tail":"\n","#text":"NIST special publication."},"rawString":{"#tail":"\n","#text":"Voorhees and Harman, 1998. TREC-6 Proceedings. E. Voorhees and D. Harrnan, Editors. NIST special publication."},"#text":"\n","pages":{"#tail":"\n","#text":"6"},"marker":{"#tail":"\n","#text":"Voorhees, Harman, 1998"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Voorhees"},{"#tail":"\n","#text":"Harman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"J. Xu and W.B. Croft, 1998. &quot;Corpus-based stemming using co-occurrence of word variants&quot;."},"#text":"\n","marker":{"#tail":"\n","#text":"Xu, Croft, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"t an English- Chinese lexicon. That is, for each English word e, we associate it with a list of Chinese words cl, c2, ... Cm together with non-zero translation probabilities P( elc~). The resulting English-Chinese l xicon has 80,000 English words. On average, each English word has 2.3 Chinese translations. For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A co- occurrence based stemmer (Xu and Croft, 1998) was used to stem Spanish words. One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon. One problem is the segmentation f Chinese text, since Chinese has no spaces between words. In these initial experiments, we relied on a simple sub-string matching algorithm to extract words from Chinese text. To extract words from a string of Chinese chara","@endWordPosition":"1365","@position":"8308","annotationId":"T21","@startWordPosition":"1362","@citStr":"Xu and Croft, 1998"}},"title":{"#tail":"\n","#text":"Corpus-based stemming using co-occurrence of word variants&quot;."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Xu"},{"#tail":"\n","#text":"W B Croft"}]}},{"volume":{"#tail":"\n","#text":"16"},"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"ACM Transactions on Information Systems, January 1998, vol 16, no. 1."},"journal":{"#tail":"\n","#text":"ACM Transactions on Information Systems,"},"#text":"\n","marker":{"#tail":"\n","#text":"1998"},"@valid":"true"}]}}]}}
