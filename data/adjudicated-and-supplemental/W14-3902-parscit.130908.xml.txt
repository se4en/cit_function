ing code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved the highest scores. Another very recent work on this topic is (Nguyen and Do˘gru¨oz, 2013). They report on language identification experiments performed on Turki
suited best for our data. Hence, we consider this as our baseline language identification system. Dictionary Accuracy(%) BNC 80.09 SemevalTwitter 77.61 LexNormList 79.86 Training Data 90.21 LexNormList+TrainingData (Baseline) 93.12 Table 4: Average cross-validation accuracy of dictionary-based detection 5.2 Word-Level Classification without Contextual Clues The following feature types are employed: 1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers. Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; Wagner et al., 2014) and create multiple features for length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization
al Clues The following feature types are employed: 1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers. Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; Wagner et al., 2014) and create multiple features for length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized. We perform experiments with an SVM classifier (linear kernel) for different combination of these features.3 Parameter optimizations (C rang
g feature types are employed: 1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers. Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; Wagner et al., 2014) and create multiple features for length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized. We perform experiments with an SVM classifier (linear kernel) for different combination of these features.3 Parameter optimizations (C range 2-15 to 210) for SVM
