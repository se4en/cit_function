{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"note":{"#tail":"\n","@confidence":"0.589058","#text":"\nProceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1317?1325,\n"},"listItem":[{"#tail":"\n","@confidence":"0.918462166666667","#text":"\nH1: If a query q hits a title t, then q and\nt are likely to be paraphrases.\nH2: If queries q1 and q2 hit the same title t,\nq1 and q2 are likely to be paraphrases.\nH3: If a query q hits titles t1 and t2, then\nt1 and t2 are likely to be paraphrases.\n"},{"#tail":"\n","@confidence":"0.998870090909091","#text":"\n1. FOR any q ? Q and t ? T\n2. IF q hits t\n3. IF IsParaphrase(q, t)\n4. Add ?q, t? to Pqt\n5. END IF\n6. END IF\n7. END FOR\n8. FOR any q1, q2 ? Q and t ? T\n9. IF ?q1, t? ? Pqt and ?q2, t? ? Pqt\n10. IF IsParaphrase(q1, q2)\n11. Add ?q1, q2? to Pqq\n12. END IF\n13. END IF\n14. END FOR\n15. FOR any t1, t2 ? T and q ? Q\n16. IF ?q, t1? ? Pqt and ?q, t2? ? Pqt\n17. IF IsParaphrase(t1, t2)\n18. Add ?t1, t2? to Ptt\n19. END IF\n20. END IF\n21. END FOR\n22. RETURN ParaSet = Pqt ? Pqq ? Ptt\n"},{"#tail":"\n","@confidence":"0.920089214285714","#text":"\n? Trivial change (12.61%): changes of punctu-\nation or stop words, such as ??? ?? ?\n??? and ??????????.\n? Word or phrase replacement (68.38%): re-\nplacements of synonymous words or phrases,\nsuch as ??? ? ? ?? ?? ? (how\nmach is ...)? and ??? ? ? ?? ??\n??? (what is the price of ...)?.\n? Structure change (7.10%): changes of both\nwords and word orders, such as ?????\n? ?? ? ?? (what fruit can I eat on a\ndiet)? and ?? ?? ?? ?? ?? (what\nfruit can help loss weight)?.\n? Others (11.90%): candidate Q-T that cannot\n"}],"@no":"0","figure":[{"#tail":"\n","@confidence":"0.996952428571429","#text":"\n(a) Q-T classification\n0\n0.2\n0.4\n0.6\n0.8\n1\n"},{"#tail":"\n","@confidence":"0.980457714285714","#text":"\n(b) Q-Q classification\n0\n0.2\n0.4\n0.6\n0.8\n1\n"},{"#tail":"\n","@confidence":"0.981299","#text":"\n(c) T-T classification\n0\n0.2\n0.4\n0.6\n0.8\n1\n"}],"address":{"#tail":"\n","@confidence":"0.583879","#text":"\nBeijing, August 2010\n"},"author":{"#tail":"\n","@confidence":"0.932027","#text":"\nShiqi Zhao??, Haifeng Wang?, and Ting Liu?\n"},"equation":[{"#tail":"\n","@confidence":"0.9996234","#text":"\nFF (s1, s2) = {\nc(s1,s2)\nC if c(s1, s2) < C\n1 if c(s1, s2) ? C\n(1)\n"},{"#tail":"\n","@confidence":"0.988002","#text":"\nFWOR(s1, s2) =\ncw(s1 ? s2)\nmax{cw(s1), cw(s2)}\n(3)\n"},{"#tail":"\n","@confidence":"0.9917615","#text":"\nFCS(s1, s2) =\nvecw(s1) ? vecw(s2)\n?vecw(s1)? ? ?vecw(s2)?\n(4)\n"},{"#tail":"\n","@confidence":"0.977436","#text":"\nW (w) = tf(w)? log( Nc(w) + 0.1) (5)\n"},{"#tail":"\n","@confidence":"0.86894675","#text":"\nFED(s1, s2) = 1?\nED(s1, s2)\nmax{cw(s1), cw(s2)}\n(6)\n"},{"#tail":"\n","@confidence":"0.99848925","#text":"\nFNE(s1, s2) =\ncne(s1 ? s2) + 1\nmax{cne(s1), cne(s2)}+ 1\n(7)\n"},{"#tail":"\n","@confidence":"0.998942","#text":"\nFPF (s1, s2) = maxp\n1\nf(p) (8)\n"},{"#tail":"\n","@confidence":"0.9998084","#text":"\nP = ?Sa ? Sm??Sa?\n(9)\nR = ?Sa ? Sm??Sm?\n(10)\nF = 2? P ?RP +R (11)\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.989202","#text":"\n2.1 Paraphrase Extraction\n"},{"#tail":"\n","@confidence":"0.994063","#text":"\n2.2 Query Log Mining in IR\n"},{"#tail":"\n","@confidence":"0.995965","#text":"\n3.1 Basic Idea\n"},{"#tail":"\n","@confidence":"0.996887","#text":"\n3.2 Algorithm\n"},{"#tail":"\n","@confidence":"0.728759","#text":"\n3.3 Features for Paraphrase Validation\n"},{"#tail":"\n","@confidence":"0.983969","#text":"\n3.4 Generating Paraphrase Patterns\n"},{"#tail":"\n","@confidence":"0.999835","#text":"\n4.1 Evaluation of Candidate Q-T\n"},{"#tail":"\n","@confidence":"0.99773","#text":"\n4.2 Evaluation of Paraphrase Q-T\n"},{"#tail":"\n","@confidence":"0.999434","#text":"\n4.3 Evaluation of Paraphrase Q-Q and T-T\n"},{"#tail":"\n","@confidence":"0.999294","#text":"\n4.4 Evaluation of Paraphrase Patterns\n"},{"#tail":"\n","@confidence":"0.982781","#text":"\n4.5 Analysis\n"}],"footnote":[{"#tail":"\n","@confidence":"0.930128","#text":"\n1www.baidu.com\n"},{"#tail":"\n","@confidence":"0.4320789","#text":"\n2The similarity is computed based on word overlap rate,\nwhich will be described in detail in section 3.3. We set T =\n0.6 in the experiments.\n? Length Rate Feature FLR:\nFLR(s1, s2) =\nmin{cw(s1), cw(s2)}\nmax{cw(s1), cw(s2)}\n(2)\nwhere cw(s) denotes the number of words in s.\n? Word Overlap Rate Feature FWOR:\n"},{"#tail":"\n","@confidence":"0.7305865","#text":"\n3In FCOR, cw(s) of Equation (3) denotes the number of\ncharacters in s.\n"},{"#tail":"\n","@confidence":"0.924108","#text":"\n4FPF is not used in paraphrase Q-T validation.\n"},{"#tail":"\n","@confidence":"0.695818","#text":"\n5We use libsvm-2.82 toolkit, which can be downloaded\nfrom http://www.csie.ntu.edu.tw/ cjlin/libsvm/\n"},{"#tail":"\n","@confidence":"0.800611","#text":"\nbeled data. The experimental setups for Q-Q and\n6We assume all possible paraphrases are included in the\ncandidates, thus its recall is 100%.\n"}],"title":{"#tail":"\n","@confidence":"0.88875","#text":"\nParaphrasing with Search Engine Query Logs\n"},"reference":{"#tail":"\n","@confidence":"0.994790135416666","#text":"\nFarooq Ahmad and Grzegorz Kondrak. 2005. Learn-\ning a Spelling Error Model from Search Query\nLogs. In Proceedings of HLT/EMNLP, pages 955-\n962.\nColin Bannard and Chris Callison-Burch. 2005. Para-\nphrasing with Bilingual Parallel Corpora. In Pro-\nceedings of ACL, pages 597-604.\n1324\nRegina Barzilay and Lillian Lee. 2003. Learning\nto Paraphrase: An Unsupervised Approach Using\nMultiple-Sequence Alignment. In Proceedings of\nHLT-NAACL, pages 16-23.\nRegina Barzilay and Kathleen R. McKeown. 2001.\nExtracting Paraphrases from a Parallel Corpus. In\nProceedings of ACL/EACL, pages 50-57.\nRahul Bhagat and Deepak Ravichandran. 2008. Large\nScale Acquisition of Paraphrases for Learning Sur-\nface Patterns. In Proceedings of ACL-08: HLT,\npages 674-682.\nChris Callison-Burch, Philipp Koehn, and Miles Os-\nborne. 2006. Improved Statistical Machine Trans-\nlation Using Paraphrases. In Proceedings of HLT-\nNAACL, pages 17-24.\nChris Callison-Burch. 2008. Syntactic Constraints\non Paraphrases Extracted from Parallel Corpora. In\nProceedings of EMNLP, pages 196-205.\nHang Cui, Ji-Rong Wen, Jian-Yun Nie, Wei-Ying Ma.\n2002. Probabilistic Query Expansion Using Query\nLogs In Proceedings of WWW, pages 325-332.\nBill Dolan, Chris Quirk, and Chris Brockett. 2004.\nUnsupervised Construction of Large Paraphrase\nCorpora: Exploiting Massively Parallel News\nSources. In Proceedings of COLING, pages 350-\n356.\nPablo Ariel Duboue and Jennifer Chu-Carroll. 2006.\nAnswering the Question You Wish They Had\nAsked: The Impact of Paraphrasing for Question\nAnswering. In Proceedings of HLT-NAACL, pages\n33-36.\nWei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian\nHu, Kam-Fai Wong, and Hsiao-Wuen Hon. 2007.\nCross-Lingual Query Suggestion Using Query Logs\nof Different Languages. In Proceedings of SIGIR,\npages 463-470.\nAli Ibrahim, Boris Katz, Jimmy Lin. 2003. Extract-\ning Structural Paraphrases from Aligned Monolin-\ngual Corpora. In Proceedings of IWP, pages 57-64.\nLidija Iordanskaja, Richard Kittredge, and Alain\nPolgue`re. 1991. Lexical Selection and Paraphrase\nin a Meaning-Text Generation Model. In Ce?cile L.\nParis, William R. Swartout, and William C. Mann\n(Eds.): Natural Language Generation in Artificial\nIntelligence and Computational Linguistics, pages\n293-312.\nDavid Kauchak and Regina Barzilay. 2006. Para-\nphrasing for Automatic Evaluation. In Proceedings\nof HLT-NAACL, pages 455-462.\nDe-Kang Lin and Patrick Pantel. 2001. Discovery of\nInference Rules for Question Answering. In Natu-\nral Language Engineering 7(4): 343-360.\nMarius Pas?ca and Pe?ter Dienes. 2005. Aligning Nee-\ndles in a Haystack: Paraphrase Acquisition Across\nthe Web. In Proceedings of IJCNLP, pages 119-\n130.\nMarius Pas?ca. 2007. Weakly-supervised Discovery\nof Named Entities using Web Search Queries. In\nProceedings of CIKM, pages 683-690.\nDeepak Ravichandran and Eduard Hovy. 2002. Learn-\ning Surface Text Patterns for a Question Answering\nSystem. In Proceedings of ACL, pages 41-47.\nMatthew Richardson. 2008. Learning about the World\nthrough Long-Term Query Logs. In ACM Transac-\ntions on the Web 2(4): 1-27.\nStefan Riezler, Alexander Vasserman, Ioannis\nTsochantaridis, Vibhu Mittal and Yi Liu. 2007.\nStatistical Machine Translation for Query Expan-\nsion in Answer Retrieval. In Proceedings of ACL,\npages 464-471.\nSatoshi Sekine and Hisami Suzuki. 2007. Acquiring\nOntological Knowledge from Query Logs. In Pro-\nceedings of WWW, pages 1223-1224.\nYusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.\n2002. Automatic Paraphrase Acquisition from\nNews Articles. In Proceedings of HLT, pages 40-\n46.\nJi-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.\n2002. Query Clustering Using User Logs. In ACM\nTransactions on Information Systems 20(1): 59-81,\n2002.\nShiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.\n2008. Pivot Approach for Extracting Paraphrase\nPatterns from Bilingual Corpora. In Proceedings of\nACL-08:HLT, pages 780-788.\nShiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning\nQuestion Paraphrases for QA from Encarta Logs. In\nProceedings of IJCAI, pages 1795-1800.\n"},"#tail":"\n","@confidence":"0.000000","bodyText":[{"#tail":"\n","@confidence":"0.99927725","#text":"\nThis paper proposes a method that extracts\nparaphrases from search engine query\nlogs. The method first extracts paraphrase\nquery-title pairs based on an assumption\nthat a search query and its correspond-\ning clicked document titles may mean the\nsame thing. It then extracts paraphrase\nquery-query and title-title pairs from the\nquery-title paraphrases with a pivot ap-\nproach. Paraphrases extracted in each step\nare validated with a binary classifier. We\nevaluate the method using a query log\nfrom Baidu1, a Chinese search engine.\nExperimental results show that the pro-\nposed method is effective, which extracts\nmore than 3.5 million pairs of paraphrases\nwith a precision of over 70%. The results\nalso show that the extracted paraphrases\ncan be used to generate high-quality para-\nphrase patterns.\n"},{"#tail":"\n","@confidence":"0.992611583333333","#text":"\nThe use of paraphrases is ubiquitous in hu-\nman languages, which also presents a challenge\nfor natural language processing (NLP). Previous\nstudies have shown that paraphrasing can play im-\nportant roles in plenty of areas, such as machine\ntranslation (MT) (Callison-Burch et al, 2006;\nKauchak and Barzilay, 2006), question answer-\ning (QA) (Duboue and Chu-Carroll, 2006; Riezler\net al, 2007), natural language generation (NLG)\n(Iordanskaja et al, 1991), and so on. As a result,\nthe research on paraphrasing and its applications\nhave attracted significant interest.\n"},{"#tail":"\n","@confidence":"0.998626055555556","#text":"\nThis paper proposes a method that uses search\nengine query logs for extracting paraphrases,\nwhich is illustrated in Figure 1. Specifically, three\nkinds of paraphrases can be extracted with our\nmethod, which include (1) query-title (Q-T): a\nquery and a document title that users clicked on;\n(2) query-query (Q-Q): two queries, for which\nusers clicked on the same document title; (3) title-\ntitle (T-T): two titles that users clicked on for the\nsame query. We train a classifier for each kind to\nfilter incorrect pairs and refine the paraphrases.\nExtracting paraphrases using query logs has\nmany advantages. First, query logs keep growing,\nwhich have no scale limitation. Second, query\nlogs reflect web users? real needs, hence the ex-\ntracted paraphrases may be more useful than that\nfrom other kinds of corpora. Third, paraphrases\nextracted from query logs can be directed applied\nin search engines for query suggestion and doc-\nument reranking. In addition, we find that both\nqueries and titles contain a good many question\nsentences, which can be useful in developing QA\nsystems.\nWe conduct experiments using a query log of\na commercial Chinese search engine Baidu, from\nwhich we extracted about 2.7 million pairs of\nparaphrase Q-T, 0.4 million pairs of paraphrase Q-\nQ, and 0.4 million pairs of paraphrase T-T. The\nprecision of the paraphrases is above 70%. In\naddition, we generate paraphrase patterns using\nthe extracted paraphrases. The results show that\n73,484 pairs of paraphrase patterns have been gen-\nerated, with a precision of over 78%.\nIn the rest of the paper, we first review related\nwork in Section 2. Section 3 describes our method\nin detail. Section 4 presents the evaluation and re-\n"},{"#tail":"\n","@confidence":"0.88503475","#text":"\nparaphrase Q-T extraction\nquery title both query and title\nparaphrase Q-Q extraction paraphrase T-T extraction\nparaphrase relation\n"},{"#tail":"\n","@confidence":"0.499225","#text":"\nsults. Section 5 concludes the paper and discusses\nfuture directions.\n"},{"#tail":"\n","@confidence":"0.995626666666667","#text":"\nIn this section, we briefly review previous studies\non paraphrase extraction and query log mining in\ninformation retrieval (IR).\n"},{"#tail":"\n","@confidence":"0.997879441176471","#text":"\nA variety of data resources have been exploited\nfor paraphrase extraction. For example, some re-\nsearchers extract paraphrases from multiple trans-\nlations of the same foreign novel (Barzilay and\nMcKeown, 2001; Ibrahim et al, 2003), while\nsome others make use of comparable news arti-\ncles that report on the same event within a small\ntime interval (Shinyama et al, 2002; Barzilay and\nLee, 2003; Dolan et al, 2004). Besides the mono-\nlingual corpora, bilingual parallel corpora have\nalso been used for extracting paraphrases (Ban-\nnard and Callison-Burch, 2005; Callison-Burch,\n2008; Zhao et al, 2008). Their basic assumption\nis that phrases that align with the same foreign\nphrase may have the same meaning.\nThe above methods have achieved promising\nresults. However, their performances are usually\nconstrained due to the scale and domain limita-\ntion. As an alternative, researchers have tried\nto acquire paraphrases from large-scale web cor-\npora (Lin and Pantel, 2001; Pas?ca and Dienes,\n2005; Bhagat and Ravichandran, 2008) or directly\nbased on web mining (Ravichandran and Hovy,\n2002). These methods are guided by an extended\nversion of distributional hypothesis, namely, if\ntwo phrases often occur in similar contexts, their\nmeanings tend to be similar. The disadvantage\nof these methods is that the underlying assump-\ntion does not always hold. Phrases with opposite\nmeanings can also occur in similar contexts, such\nas ?X solves Y? and ?X worsens Y? (Lin and Pan-\ntel, 2001). In addition, the extracted paraphrases\nare generally short fragments with two slots (vari-\nables) at both ends.\n"},{"#tail":"\n","@confidence":"0.999864105263158","#text":"\nQuery logs are widely used in the IR commu-\nnity, especially for mining similar queries. For ex-\nample, Wen et al (2002) clustered queries based\non user click information. Their basic idea is\nthat if some queries result in similar user clicks,\nthe meanings of these queries should be similar.\nSuch methods have also been investigated in (Gao\net al, 2007) for cross-lingual query suggestion\nand (Zhao et al, 2007) for synonymous questions\nidentification. This paper is partly inspired by\ntheir studies. However, we do not simply use click\ninformation as clues for mining similar queries.\nInstead, we mine paraphrases across queries and\nclicked document titles.\nIn addition, query logs can be used for query\nexpansion. For instance, Cui et al (2002)\nextract probabilistic correlations between query\nterms and document terms by analyzing query\nlogs, which are then used to select high-quality\n"},{"#tail":"\n","@confidence":"0.989103666666667","#text":"\nexpansion terms for new queries. Note that the\nexpansion terms are merely related terms of the\nqueries, not necessarily paraphrases.\nThere are other studies that use query logs\nfor constructing ontologies (Sekine and Suzuki,\n2007), learning named entities (Pas?ca, 2007),\nbuilding user profiles (Richardson, 2008), correct-\ning spelling errors (Ahmad and Kondrak, 2005),\nand so forth.\n"},{"#tail":"\n","@confidence":"0.999053620689655","#text":"\nNowadays, more and more users tend to search\nlong queries with search engines. Many users\neven directly search questions to get exact an-\nswers. By analyzing our query log that records\nrich information including user queries, clicked\nurls, titles, etc., we find that most titles of clicked\ndocuments are highly related with search queries.\nEspecially, paraphrases can be easily found from\nlong queries and the corresponding clicked ti-\ntles. This motivates us to extract paraphrases from\nquery-title pairs. Here we introduce a concept hit\nthat will be frequently used: given a query q, a\nweb document d, and d?s title t, if there exist some\nusers that click on d when searching q, then we\nsay q hits t.\nThe hypothesis for extracting paraphrase Q-T\nis shown in Table 1 (H1). In addition, we find\nthat when several queries hit the same title, the\nqueries are likely to be paraphrases of each other.\nThe other way round, when a query hits several\ntitles, paraphrases can also be found among the ti-\ntles. We therefore further extract paraphrase Q-Q\nand T-T from the paraphrase Q-T. The underly-\ning hypotheses can be found in Table 1 (H2 and\nINPUT: Q: query space, T : title space\nOUTPUT: Pqt: the set of paraphrase Q-T,\nPqq: the set of paraphrase Q-Q,\nPtt: the set of paraphrase T-T,\nParaSet: the set of paraphrases\n"},{"#tail":"\n","@confidence":"0.956631666666667","#text":"\nH3). Note that, based on H2 and H3, paraphrase\nQ-Q and T-T can be directly extracted from raw\nQ-T pairs. However, in consideration of preci-\nsion, we extract them from paraphrase Q-T. We\ncall our paraphrase Q-Q and T-T extraction ap-\nproach as a pivot approach, since we use titles as\npivots (queries as targets) when extracting para-\nphrase Q-Q and use queries as pivots (titles as tar-\ngets) when extracting paraphrase T-T.\n"},{"#tail":"\n","@confidence":"0.989428555555556","#text":"\nphrase Q-T from the query log. Lines 8?14 and\n15?21 extract paraphrase Q-Q and T-T, respec-\ntively. Line 22 combines the paraphrase Q-T, Q-\nQ, and T-T together. To filter noise, the extracted\nQ-T, Q-Q, and T-T pairs are all validated using\na function IsParaphrase(s1, s2). In this work,\nwe recast paraphrase validation as a binary clas-\nsification problem. Any pair of ?s1, s2? is classi-\nfied as 1 (paraphrase) or 0 (non-paraphrase) with\na support vector machine (SVM) classifier. The\nfeatures used for classification will be detailed in\nSection 3.3.\nIn practice, we exploit a query log that contains\n287 million Q-T pairs, which are then filtered us-\ning the following constraints: (1) exclude Q-T\npairs that are too short, i.e., either query q or tittle\nt contains less than three terms; (2) exclude Q-T\npairs where q subsumes t or vice versa, e.g., ??\n? (beef)? and ?????? (cooking method of\nbeef)?; (3) exclude Q-T pairs in which the similar-\nity between q and t is below a predefined threshold\nT 2; (4) exclude Q-T pairs whose t contains fre-\nquent internet terms, such as ??? (home page)?,\n??? (web site)?, ??? (online)?, since such ti-\ntles are mostly organization home pages, online\nvideos, downloadable resources, etc., which are\nuseless for our purpose of paraphrase extraction.\n"},{"#tail":"\n","@confidence":"0.992451714285714","#text":"\nGiven a pair of candidate paraphrases ?s1, s2?, in\nwhich s1 and s2 can be either a query or a title, we\nexploit the following features in the classification-\nbased paraphrase validation.\n? Frequency Feature FF . FF is defined based\non each ?s1, s2??s frequency. We expect that more\nfrequent ?s1, s2? should be more reliable.\n"},{"#tail":"\n","@confidence":"0.990909","#text":"\nwhere c(s1, s2) denotes the number of times that\nthe ?s1, s2? pair occurs in the corpus. C is a nor-\nmalizing factor (C = 10 in our experiments).\n"},{"#tail":"\n","@confidence":"0.871492571428571","#text":"\nwhere ?s1 ? s2? is the intersection of s1 and s2.\n? Character Overlap Rate Feature FCOR. Chi-\nnese words are composed of characters. It is quite\noften that words with similar characters share\nsimilar meanings, such as ??? (comfortable)?\nand ??? (comfortable)?, ??? (sell)? and ??\n? (sell)?. Here we use FCOR to measure the sim-\nilarity between s1 and s2 at the character level.\nDetailedly, we segment s1 and s2 into sets of\ncharacters and compute the overlap rate based on\nEquation (3)3.\n? Cosine Similarity Feature FCS . In FCS , both\ns1 and s2 are represented as vectors and their co-\nsine similarity is computed as:\n"},{"#tail":"\n","@confidence":"0.9012828","#text":"\nwhere vecw(s) is the vector of words in s, ??? de-\nnotes the dot product of two vectors, ?vecw(s)?\nis the norm of a vector. Here, the weight of each\nword w in a vector is computed using a heuristic\nsimilar to tf-idf:\n"},{"#tail":"\n","@confidence":"0.706287833333333","#text":"\nwhere tf(w) is the frequency of w in the given s,\nc(w) is the number of times that w occurs in the\ncorpus, N = maxw c(w).\n? Edit Distance Feature FED. Let ED(s1, s2)\nbe the edit distance at the word level between s1\nand s2, we compute FED as follows:\n"},{"#tail":"\n","@confidence":"0.84252575","#text":"\n? Named Entity (NE) Similarity Feature FNE .\nNE information is critical in paraphrase identifica-\ntion (Shinyama et al, 2002). We therefore com-\npute the NE similarity between s1 and s2 and take\nit as a feature. We employ a Chinese NE recog-\nnition tool that can recognize person names, loca-\ntions, organizations, and numerals. The NE simi-\nlarity is computed as:\n"},{"#tail":"\n","@confidence":"0.971432636363636","#text":"\nwhere cne(s) denotes the number of NEs in s.\nEquation (7) guarantees FNE = 1 if there are no\nNEs in either s1 or s2.\n? Pivot Fertility Feature FPF : FPF is a fea-\nture specially designed for paraphrase Q-Q and\nT-T extraction, which are based on the pivot ap-\nproach4. Specifically, we define fertility of a pivot\nas the number of targets it corresponds to. Our ob-\nservation indicates that the larger the fertility of a\npivot is, the more noisy the targets are. Hence we\ndefine FPF as:\n"},{"#tail":"\n","@confidence":"0.9844384","#text":"\nwhere s1 = q1, s2 = q2, p = t when classifying\nQ-Q, while s1 = t1, s2 = t2, p = q when classi-\nfying T-T. f(p) denotes the fertility of the pivot p.\nThe value is maximized over p if s1 and s2 can be\nextracted with multiple pivots.\n"},{"#tail":"\n","@confidence":"0.818945714285714","#text":"\nA key feature of our method is that the extracted\nparaphrases are particularly suitable for generat-\ning paraphrase patterns, especially for the hot do-\nmains that are frequently searched. For example,\nthere are quite a few paraphrases concerning the\ntherapy of various diseases, from which we can\neasily induce patterns expressing the meaning of\n?How to treat [X] disease?, such as ?[X] ? ?\n? ???, ??? ?? [X] ??, and ?[X] ? ?\n?? ???. Therefore, in this work, we try to\ngenerate paraphrase patterns using the extracted\nparaphrases.\nIn our preliminary experiments, we only induce\nparaphrase patterns from paraphrases that contain\n"},{"#tail":"\n","@confidence":"0.897583666666667","#text":"\nno more than 6 words. In addition, only one slot\nis allowed in each pair of paraphrase patterns. Let\ns1 and s2 be a pair of paraphrases extracted above.\nIf there exist words w ? s1 and v ? s2 that satisfy\n(1) w = v, (2) w and v are not stop words, then\nwe can induce a pair of paraphrase patterns by re-\nplacing w in s1 and v in s2 with a slot ?[X]?. It is\nobvious that several pairs of paraphrase patterns\nmay be induced from one pair of paraphrases.\n"},{"#tail":"\n","@confidence":"0.994597066666667","#text":"\nWe experiment with a query log that contains a\ntotal of 284,316,659 queries. Statistics reveal that\n170,315,807 queries (59.90%) lead to at least one\nuser click, each having 1.69 clicks on average. We\nextract 287,129,850 raw Q-T pairs using the query\nlog, from which 4,448,347 pairs of candidate Q-\nT are left after filtering as described in Section\n3.2. Almost all queries and titles are written in\nChinese, though some of them contain English or\nJapanese words. The preprocessing of candidate\nQ-T includes Chinese word segmentation (WSeg)\nand NE recognition (NER). Our WSeg tool is im-\nplemented based on forward maximum matching,\nwhile the NER tool is based on a NE dictionary\nmined from the web.\n"},{"#tail":"\n","@confidence":"0.995591285714286","#text":"\nWe first evaluate candidate Q-T without valida-\ntion. To this end, we randomly sampled 5000\npairs of candidate Q-T and labeled them manu-\nally. Each pair is labeled into one of the 3 classes:\nSAME - q and t have the same meaning; RELA - q\nand t have related meanings; DIFF - q and t have\nclearly different meanings. The labeling results\nare listed in Table 3. We can see that no candidate\nQ-T is in the DIFF class. This is not surprising,\nsince users are unlikely to click on web pages un-\nrelated to their queries.\nTo gain a better insight into the data, we ana-\nlyzed the subtle types of candidate Q-T in both\nSAME and RELA classes. In detail, we sampled\n"},{"#tail":"\n","@confidence":"0.95443","#text":"\n1000 pairs of candidate Q-T from the 5000 pairs\nlabeled above, in which 563 are in the SAME\nclass, while the other 437 are in the RELA class.\nOur analysis suggests that candidate Q-T in the\nSAME class can be divided into 4 subtle types:\n"},{"#tail":"\n","@confidence":"0.912535125","#text":"\nbe classified into the 3 types above.\nThe above analysis reveals that more than two\nthirds of candidate Q-T in the SAME class are in\nthe ?word or phrase replacement? type, while the\nones with structure changes are slightly more than\n7%. We believe this is mainly because queries\nand titles are relatively short and their structures\nare simple. Thus structure rewriting can hardly be\nconducted. This distribution is in line with that\nreported in (Zhao et al, 2008).\nAs for the RELA class, we find that 42.33% of\nsuch candidate Q-T share a problem of named en-\ntity mismatch, such as ??? (US) ?? ??\n??? and ??? (China) ?? ?? ?? ?\n??. This indicates that the NE similarity feature\nis necessary in paraphrase validation.\n"},{"#tail":"\n","@confidence":"0.998784166666666","#text":"\nThe candidate Q-T extracted above are classified\nwith a SVM classifier5 under its default setting.\nTo evaluate the classifier, we run 5-fold cross val-\nidation with the 5000 human annotated data, in\nwhich we use 4000 for training and the rest 1000\nfor testing in each run. The evaluation criteria are\n"},{"#tail":"\n","@confidence":"0.8870845","#text":"\nprecision (P), recall (R), and f-measure (F), which\nare defined as follows:\n"},{"#tail":"\n","@confidence":"0.989941235294118","#text":"\nwhere Sa is the set of paraphrases automatically\nrecognized with the classifier, Sm is the set of\nparaphrases manually annotated. Precision, re-\ncall, and f-measure are averaged over 5 runs in\nthe 5-fold cross validation.\nFigure 2 (a) shows the classification results\n(dark bars). For comparison, we also show the\nprecision, recall6, and f-measure of the candidate\nQ-T (light bars). As can be seen, the precision is\nimproved from 0.5592 to 0.7444 after classifica-\ntion. F-measure is also evidently enhanced. This\nresult indicates that the classification-based para-\nphrase validation is effective. We then use all of\nthe 5000 annotated data to train a classifier and\nclassify all the candidate Q-T. Results show that\n2,762,291 out of 4,448,347 pairs of candidate Q-\nT are classified as paraphrases.\n"},{"#tail":"\n","@confidence":"0.993899133333333","#text":"\nFrom the paraphrase Q-T, we further extracted\n934,758 pairs of candidate Q-Q and 438,954 pairs\nof candidate T-T (without validation). We ran-\ndomly sampled 5000 from each for human an-\nnotation. The results show that the precisions of\ncandidate Q-Q and T-T are 0.4672 and 0.6860, re-\nspectively. As can be seen, the precision of can-\ndidate Q-Q is much lower than that of candidate\nT-T. Our analysis reveals that it is mainly because\ncandidate Q-Q are more noisy, since user queries\ncontain quite a lot of spelling mistakes and infor-\nmal expressions.\nThe candidate Q-Q and T-T are also refined\nbased on classification. We first evaluate the clas-\nsification performance using the 5000 human la-\n"},{"#tail":"\n","@confidence":"0.967708","#text":"\nT-T classification are the same as that of Q-T clas-\nsification, in which we run 5-fold cross validation\nwith a SVM classifier using its default parameters.\nFigure 2 (b) and (c) give the classification results\n(dark bars) as well as the precision, recall, and f-\nmeasure of the candidates (light bars).\nWe can see that the precision of Q-Q is signifi-\ncantly enhanced from 0.4672 to 0.7345 after clas-\nsification, which suggests that a substantial part\nof errors and noise are removed. The increase of\nf-measure demonstrates the effectiveness of clas-\nsification despite the decrease of recall. Mean-\nwhile, the quality of candidate T-T is not clearly\nimproved after classification. The reason should\nbe that the precision of candidate T-T is already\npretty high. We then use all 5000 human labeled\ndata to train a classifier for Q-Q and T-T respec-\ntively and classify all candidate Q-Q and T-T. Re-\nsults show that 390,920 pairs of paraphrase Q-Q\nand 415,539 pairs of paraphrase T-T are extracted\nafter classification.\n"},{"#tail":"\n","@confidence":"0.815089769230769","#text":"\nUsing the method introduced in Section 3.4, we\nhave generated 73,484 pairs of paraphrase pat-\nterns that appear at least two times in the cor-\npus. We randomly selected 500 pairs and labeled\nthem manually. The results show that the preci-\nsion is 78.4%. Two examples are shown in Ta-\nble 4, in which p1 and p2 are paraphrase patterns.\nSome slot fillers are also listed below. We real-\np1 [X]??????\np2 ???? [X]??\n(how to open [X] file)\nslot 7z; ashx; aspx; bib; cda; cdfs; cmp;\ncpi; csf; csv; cur; dat; dek...\n"},{"#tail":"\n","@confidence":"0.998618714285714","#text":"\nize that the method currently used for inducing\nparaphrase patterns is simple. Hence we will im-\nprove the method in our following experiments.\nSpecifically, multiple slots will be allowed in a\npair of patterns. In addition, we will try to ap-\nply the alignment techniques in the generation of\nparaphrase patterns, as Zhao et al (2008) did.\n"},{"#tail":"\n","@confidence":"0.996311","#text":"\nFeature Contribution. To investigate the contri-\nbutions of different features used in classification,\nwe tried different feature combinations for each of\nour three classifiers. The results are shown in Ta-\nble 5, in which ?+? means the feature has contri-\nbution to the corresponding classifier. As can be\nseen, the character overlap rate feature (FCOR),\ncosine similarity feature (FCS), and NE similarity\n"},{"#tail":"\n","@confidence":"0.998575926829268","#text":"\nfeature (FNE) are the most useful, which play im-\nportant roles in all the three classifiers. The other\nfeatures are useful in some of the classifiers ex-\ncept the word overlap rate feature (FWOR). The\nclassification results reported in prior sections are\nall achieved with the optimal feature combination.\nAnalysis of the Paraphrases. We combine the\nextracted paraphrase Q-T, Q-Q and T-T and get\na total of 3,560,257 pairs of unique paraphrases.\nStatistics show that only 8380 pairs (0.24%) are\nfrom more than one source, which indicates that\nthe intersection among the three sets is very small.\nFurther statistics show that the average length of\nthe queries and titles in the paraphrases is 6.69\n(words).\nTo have a detailed analysis of the extracted\nparaphrases, we randomly selected 1000 pairs and\nmanually labeled the precision, types, and do-\nmains. It is found that more than 43% of the para-\nphrases are paraphrase questions, in which how\n(36%), what (19%), and yes/no (14%) questions\nare the most common. In addition, we find that\nthe precision of paraphrase questions (84.26%)\nis evidently higher than non-question paraphrases\n(65.14%). Those paraphrase questions are useful\nin question analysis and expansion in QA, which\ncan hardly be extracted from other kinds of cor-\npora.\nAs expected, the paraphrases we extract cover\na variety of domains. However, around 50% of\nthem are in the 7 most popular domains7, includ-\ning: (1) health and medicine, (2) documentary\ndownload, (3) entertainment, (4) software, (5) ed-\n7Note that pornographic queries have been filtered from\nthe query log beforehand.\nucation and study, (6) computer game, (7) econ-\nomy and finance. This analysis reflects what web\nusers are most concerned about. These domains,\nespecially (4) and (6), are not well covered by the\nparallel and comparable corpora previously used\nfor paraphrase extraction.\n"},{"#tail":"\n","@confidence":"0.99991344","#text":"\nIn this paper, we put forward a novel method that\nextracts paraphrases from search engine query\nlogs. Our contribution is that we, for the first\ntime, propose to extract paraphrases from user\nqueries and the corresponding clicked document\ntitles. Specifically, three kinds of paraphrases\nare extracted, which can be (1) a query and a\nhit title, (2) two queries that hit the same title,\nand (3) two titles hit by the same query. The\nextracted paraphrases are refined based on clas-\nsification. Using the proposed method, we ex-\ntracted over 3.5 million pairs of paraphrases from\na query log of Baidu. Human evaluation results\nshow that the precision of the paraphrases is above\n70%. The results also show that we can gener-\nate high-quality paraphrase patterns from the ex-\ntracted paraphrases.\nOur future research will be conducted along the\nfollowing directions. Firstly, we will use a much\nlarger query log for paraphrase extraction, so as to\nenhance the coverage of paraphrases. Secondly,\nwe plan to have a deeper study of the transitivity\nof paraphrasing. Simply speaking, we want to find\nout whether we can extract ?s1, s3? as paraphrases\ngiven that ?s1, s2? and ?s2, s3? are paraphrases.\n"},{"#tail":"\n","@confidence":"0.998316","#text":"\nWe would like to thank Wanxiang Che, Hua Wu,\nand the anonymous reviewers for their useful\ncomments on this paper.\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.471841","#text":"\n?Baidu Inc.\n?HIT Center for Information Retrieval, Harbin Institute of Technology\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.990209","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998077","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.999001","@genericHeader":"related work","#text":"\n2 Related Work\n"},{"#tail":"\n","@confidence":"0.980593","@genericHeader":"method","#text":"\n3 The Proposed Method\n"},{"#tail":"\n","@confidence":"0.99946","@genericHeader":"method","#text":"\n4 Experiments\n"},{"#tail":"\n","@confidence":"0.997179","@genericHeader":"conclusions","#text":"\n5 Conclusions and Future Directions\n"},{"#tail":"\n","@confidence":"0.999088","@genericHeader":"acknowledgments","#text":"\n6 Acknowledgments\n"},{"#tail":"\n","@confidence":"0.986198","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.956519","#text":"\nTable 1: Hypotheses for extracting paraphrases.\n"},{"#tail":"\n","@confidence":"0.853714","#text":"\nTable 2: Algorithm for extracting paraphrases.\n"},{"#tail":"\n","@confidence":"0.726185","#text":"\nOur paraphrase extraction algorithm is shown in\nTable 2. In particular, lines 1?7 extract para-\n"},{"#tail":"\n","@confidence":"0.998937","#text":"\nTable 3: Human labeling of candidate Q-T.\n"},{"#tail":"\n","@confidence":"0.997386","#text":"\nTable 4: Examples of paraphrase patterns.\n"},{"#tail":"\n","@confidence":"0.980471","#text":"\nTable 5: Feature contribution.\n"}],"page":[{"#tail":"\n","@confidence":"0.959616","#text":"\n1317\n"},{"#tail":"\n","@confidence":"0.993044","#text":"\n1318\n"},{"#tail":"\n","@confidence":"0.991844","#text":"\n1319\n"},{"#tail":"\n","@confidence":"0.627326","#text":"\n1320\n"},{"#tail":"\n","@confidence":"0.858853","#text":"\n1321\n"},{"#tail":"\n","@confidence":"0.761157","#text":"\n1322\n"},{"#tail":"\n","@confidence":"0.910568","#text":"\n1.2\n"},{"#tail":"\n","@confidence":"0.898573","#text":"\n1.2\n"},{"#tail":"\n","@confidence":"0.949379","#text":"\n1.2\n"},{"#tail":"\n","@confidence":"0.672776","#text":"\n1323\n"},{"#tail":"\n","@confidence":"0.956985","#text":"\n1325\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.991955","#text":"\nFigure 1: Illustration of the proposed method.\n"},{"#tail":"\n","@confidence":"0.987847","#text":"\nFigure 2: Classification precision (P), recall (R), and f-measure (F).\n"}],"table":[{"#tail":"\n","@confidence":"0.915349","#text":"\nSAME RELA DIFF\npercent (%) 55.92 44.08 -\n"},{"#tail":"\n","@confidence":"0.841689","#text":"\ncand. 0.5592 1 0.7173\npara. 0.7444 0.8391 0.7887\nP R F\n"},{"#tail":"\n","@confidence":"0.824995","#text":"\ncand. 0.4672 1 0.6369\npara. 0.7345 0.6575 0.6938\nP R F\n"},{"#tail":"\n","@confidence":"0.972107","#text":"\ncand. 0.686 1 0.8138\npara. 0.7056 0.9776 0.8196\nP R F\n"},{"#tail":"\n","@confidence":"0.9656322","#text":"\np1 ?? [X]???\np2 ?? [X]???\n(poems about [X])\nslot ?? (prairies);?? (Yangtze River);\n?? (Mount Tai);?? (nostalgia)...\n"},{"#tail":"\n","@confidence":"0.590823666666667","#text":"\nFeature Q-T Q-Q T-T\nFF +\nFLR +\nFWOR\nFCOR + + +\nFCS + + +\nFED +\nFNE + + +\nFPF +\n"}],"email":{"#tail":"\n","@confidence":"0.771245","#text":"\n{zhaoshiqi, wanghaifeng}@baidu.com, tliu@ir.hit.edu.cn\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.697447","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.9083015","#text":"Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1317?1325, Beijing, August 2010"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.9933665","#text":"Baidu Inc. ?HIT Center for Information Retrieval, Harbin Institute of Technology"},"author":[{"#tail":"\n","@confidence":"0.989529","#text":"Shiqi Zhao"},{"#tail":"\n","@confidence":"0.989529","#text":"Haifeng Wang"},{"#tail":"\n","@confidence":"0.989529","#text":"Ting Liu"}],"abstract":{"#tail":"\n","@confidence":"0.998681761904762","#text":"This paper proposes a method that extracts paraphrases from search engine query logs. The method first extracts paraphrase query-title pairs based on an assumption that a search query and its corresponding clicked document titles may mean the same thing. It then extracts paraphrase query-query and title-title pairs from the query-title paraphrases with a pivot approach. Paraphrases extracted in each step are validated with a binary classifier. We evaluate the method using a query log from Baidu1, a Chinese search engine. Experimental results show that the proposed method is effective, which extracts more than 3.5 million pairs of paraphrases with a precision of over 70%. The results also show that the extracted paraphrases can be used to generate high-quality paraphrase patterns."},"title":{"#tail":"\n","@confidence":"0.998467","#text":"Paraphrasing with Search Engine Query Logs"},"email":[{"#tail":"\n","@confidence":"0.89827","#text":"zhaoshiqi@baidu.com,tliu@ir.hit.edu.cn"},{"#tail":"\n","@confidence":"0.89827","#text":"wanghaifeng@baidu.com,tliu@ir.hit.edu.cn"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a Spelling Error Model from Search Query Logs. In Proceedings of HLT/EMNLP, pages 955-962."},"#text":"\n","pages":{"#tail":"\n","#text":"955--962"},"marker":{"#tail":"\n","#text":"Ahmad, Kondrak, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" and t are likely to be paraphrases. H2: If queries q1 and q2 hit the same title t, q1 and q2 are likely to be paraphrases. H3: If a query q hits titles t1 and t2, then t1 and t2 are likely to be paraphrases. Table 1: Hypotheses for extracting paraphrases. expansion terms for new queries. Note that the expansion terms are merely related terms of the queries, not necessarily paraphrases. There are other studies that use query logs for constructing ontologies (Sekine and Suzuki, 2007), learning named entities (Pas?ca, 2007), building user profiles (Richardson, 2008), correcting spelling errors (Ahmad and Kondrak, 2005), and so forth. 3 The Proposed Method 3.1 Basic Idea Nowadays, more and more users tend to search long queries with search engines. Many users even directly search questions to get exact answers. By analyzing our query log that records rich information including user queries, clicked urls, titles, etc., we find that most titles of clicked documents are highly related with search queries. Especially, paraphrases can be easily found from long queries and the corresponding clicked titles. This motivates us to extract paraphrases from query-title pairs. Here we introduce a concept hit that will be","@endWordPosition":"1108","@position":"7015","annotationId":"T1","@startWordPosition":"1105","@citStr":"Ahmad and Kondrak, 2005"}},"title":{"#tail":"\n","#text":"Learning a Spelling Error Model from Search Query Logs."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT/EMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Farooq Ahmad"},{"#tail":"\n","#text":"Grzegorz Kondrak"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of ACL, pages 597-604."},"#text":"\n","pages":{"#tail":"\n","#text":"597--604"},"marker":{"#tail":"\n","#text":"Bannard, Callison-Burch, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ion and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al, 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypoth","@endWordPosition":"681","@position":"4407","annotationId":"T2","@startWordPosition":"677","@citStr":"Bannard and Callison-Burch, 2005"}},"title":{"#tail":"\n","#text":"Paraphrasing with Bilingual Parallel Corpora."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Colin Bannard"},{"#tail":"\n","#text":"Chris Callison-Burch"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Regina Barzilay and Lillian Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment. In Proceedings of HLT-NAACL, pages 16-23."},"#text":"\n","pages":{"#tail":"\n","#text":"16--23"},"marker":{"#tail":"\n","#text":"Barzilay, Lee, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" Section 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al, 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bhag","@endWordPosition":"657","@position":"4245","annotationId":"T3","@startWordPosition":"654","@citStr":"Barzilay and Lee, 2003"}},"title":{"#tail":"\n","#text":"Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT-NAACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Regina Barzilay"},{"#tail":"\n","#text":"Lillian Lee"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Regina Barzilay and Kathleen R. McKeown. 2001. Extracting Paraphrases from a Parallel Corpus. In Proceedings of ACL/EACL, pages 50-57."},"#text":"\n","pages":{"#tail":"\n","#text":"50--57"},"marker":{"#tail":"\n","#text":"Barzilay, McKeown, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"17 paraphrase Q-T extraction query title both query and title paraphrase Q-Q extraction paraphrase T-T extraction paraphrase relation Figure 1: Illustration of the proposed method. sults. Section 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al, 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due","@endWordPosition":"624","@position":"4062","annotationId":"T4","@startWordPosition":"621","@citStr":"Barzilay and McKeown, 2001"}},"title":{"#tail":"\n","#text":"Extracting Paraphrases from a Parallel Corpus."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL/EACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Regina Barzilay"},{"#tail":"\n","#text":"Kathleen R McKeown"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Rahul Bhagat and Deepak Ravichandran. 2008. Large Scale Acquisition of Paraphrases for Learning Surface Patterns. In Proceedings of ACL-08: HLT, pages 674-682."},"#text":"\n","pages":{"#tail":"\n","#text":"674--682"},"marker":{"#tail":"\n","#text":"Bhagat, Ravichandran, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur in similar contexts, their meanings tend to be similar. The disadvantage of these methods is that the underlying assumption does not always hold. Phrases with opposite meanings can also occur in similar contexts, such as ?X solves Y? and ?X worsens Y? (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs","@endWordPosition":"752","@position":"4871","annotationId":"T5","@startWordPosition":"749","@citStr":"Bhagat and Ravichandran, 2008"}},"title":{"#tail":"\n","#text":"Large Scale Acquisition of Paraphrases for Learning Surface Patterns."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL-08: HLT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Rahul Bhagat"},{"#tail":"\n","#text":"Deepak Ravichandran"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved Statistical Machine Translation Using Paraphrases. In Proceedings of HLTNAACL, pages 17-24."},"#text":"\n","pages":{"#tail":"\n","#text":"17--24"},"marker":{"#tail":"\n","#text":"Callison-Burch, Koehn, Osborne, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" the method using a query log from Baidu1, a Chinese search engine. Experimental results show that the proposed method is effective, which extracts more than 3.5 million pairs of paraphrases with a precision of over 70%. The results also show that the extracted paraphrases can be used to generate high-quality paraphrase patterns. 1 Introduction The use of paraphrases is ubiquitous in human languages, which also presents a challenge for natural language processing (NLP). Previous studies have shown that paraphrasing can play important roles in plenty of areas, such as machine translation (MT) (Callison-Burch et al, 2006; Kauchak and Barzilay, 2006), question answering (QA) (Duboue and Chu-Carroll, 2006; Riezler et al, 2007), natural language generation (NLG) (Iordanskaja et al, 1991), and so on. As a result, the research on paraphrasing and its applications have attracted significant interest. 1www.baidu.com This paper proposes a method that uses search engine query logs for extracting paraphrases, which is illustrated in Figure 1. Specifically, three kinds of paraphrases can be extracted with our method, which include (1) query-title (Q-T): a query and a document title that users clicked on; (2) query-query","@endWordPosition":"213","@position":"1448","annotationId":"T6","@startWordPosition":"210","@citStr":"Callison-Burch et al, 2006"}},"title":{"#tail":"\n","#text":"Improved Statistical Machine Translation Using Paraphrases."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLTNAACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chris Callison-Burch"},{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Miles Osborne"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Chris Callison-Burch. 2008. Syntactic Constraints on Paraphrases Extracted from Parallel Corpora. In Proceedings of EMNLP, pages 196-205."},"#text":"\n","pages":{"#tail":"\n","#text":"196--205"},"marker":{"#tail":"\n","#text":"Callison-Burch, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ation retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al, 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two p","@endWordPosition":"683","@position":"4429","annotationId":"T7","@startWordPosition":"682","@citStr":"Callison-Burch, 2008"}},"title":{"#tail":"\n","#text":"Syntactic Constraints on Paraphrases Extracted from Parallel Corpora."},"booktitle":{"#tail":"\n","#text":"In Proceedings of EMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Chris Callison-Burch"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Hang Cui, Ji-Rong Wen, Jian-Yun Nie, Wei-Ying Ma. 2002. Probabilistic Query Expansion Using Query Logs In Proceedings of WWW, pages 325-332."},"#text":"\n","pages":{"#tail":"\n","#text":"325--332"},"marker":{"#tail":"\n","#text":"Cui, Wen, Nie, Ma, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"queries based on user click information. Their basic idea is that if some queries result in similar user clicks, the meanings of these queries should be similar. Such methods have also been investigated in (Gao et al, 2007) for cross-lingual query suggestion and (Zhao et al, 2007) for synonymous questions identification. This paper is partly inspired by their studies. However, we do not simply use click information as clues for mining similar queries. Instead, we mine paraphrases across queries and clicked document titles. In addition, query logs can be used for query expansion. For instance, Cui et al (2002) extract probabilistic correlations between query terms and document terms by analyzing query logs, which are then used to select high-quality 1318 H1: If a query q hits a title t, then q and t are likely to be paraphrases. H2: If queries q1 and q2 hit the same title t, q1 and q2 are likely to be paraphrases. H3: If a query q hits titles t1 and t2, then t1 and t2 are likely to be paraphrases. Table 1: Hypotheses for extracting paraphrases. expansion terms for new queries. Note that the expansion terms are merely related terms of the queries, not necessarily paraphrases. There are other studies","@endWordPosition":"972","@position":"6204","annotationId":"T8","@startWordPosition":"969","@citStr":"Cui et al (2002)"}},"title":{"#tail":"\n","#text":"Probabilistic Query Expansion Using Query Logs"},"booktitle":{"#tail":"\n","#text":"In Proceedings of WWW,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hang Cui"},{"#tail":"\n","#text":"Ji-Rong Wen"},{"#tail":"\n","#text":"Jian-Yun Nie"},{"#tail":"\n","#text":"Wei-Ying Ma"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. In Proceedings of COLING, pages 350-356."},"#text":"\n","pages":{"#tail":"\n","#text":"350--356"},"marker":{"#tail":"\n","#text":"Dolan, Quirk, Brockett, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al, 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bhagat and Ravichandran,","@endWordPosition":"661","@position":"4265","annotationId":"T9","@startWordPosition":"658","@citStr":"Dolan et al, 2004"}},"title":{"#tail":"\n","#text":"Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources."},"booktitle":{"#tail":"\n","#text":"In Proceedings of COLING,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Bill Dolan"},{"#tail":"\n","#text":"Chris Quirk"},{"#tail":"\n","#text":"Chris Brockett"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. Answering the Question You Wish They Had Asked: The Impact of Paraphrasing for Question Answering. In Proceedings of HLT-NAACL, pages 33-36."},"#text":"\n","pages":{"#tail":"\n","#text":"33--36"},"marker":{"#tail":"\n","#text":"Duboue, Chu-Carroll, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"esults show that the proposed method is effective, which extracts more than 3.5 million pairs of paraphrases with a precision of over 70%. The results also show that the extracted paraphrases can be used to generate high-quality paraphrase patterns. 1 Introduction The use of paraphrases is ubiquitous in human languages, which also presents a challenge for natural language processing (NLP). Previous studies have shown that paraphrasing can play important roles in plenty of areas, such as machine translation (MT) (Callison-Burch et al, 2006; Kauchak and Barzilay, 2006), question answering (QA) (Duboue and Chu-Carroll, 2006; Riezler et al, 2007), natural language generation (NLG) (Iordanskaja et al, 1991), and so on. As a result, the research on paraphrasing and its applications have attracted significant interest. 1www.baidu.com This paper proposes a method that uses search engine query logs for extracting paraphrases, which is illustrated in Figure 1. Specifically, three kinds of paraphrases can be extracted with our method, which include (1) query-title (Q-T): a query and a document title that users clicked on; (2) query-query (Q-Q): two queries, for which users clicked on the same document title; (3) titleti","@endWordPosition":"225","@position":"1532","annotationId":"T10","@startWordPosition":"222","@citStr":"Duboue and Chu-Carroll, 2006"}},"title":{"#tail":"\n","#text":"Answering the Question You Wish They Had Asked: The Impact of Paraphrasing for Question Answering."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT-NAACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Pablo Ariel Duboue"},{"#tail":"\n","#text":"Jennifer Chu-Carroll"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu, Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-Lingual Query Suggestion Using Query Logs of Different Languages. In Proceedings of SIGIR, pages 463-470."},"#text":"\n","pages":{"#tail":"\n","#text":"463--470"},"marker":{"#tail":"\n","#text":"Gao, Niu, Nie, Zhou, Hu, Wong, Hon, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"hrases with opposite meanings can also occur in similar contexts, such as ?X solves Y? and ?X worsens Y? (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs are widely used in the IR community, especially for mining similar queries. For example, Wen et al (2002) clustered queries based on user click information. Their basic idea is that if some queries result in similar user clicks, the meanings of these queries should be similar. Such methods have also been investigated in (Gao et al, 2007) for cross-lingual query suggestion and (Zhao et al, 2007) for synonymous questions identification. This paper is partly inspired by their studies. However, we do not simply use click information as clues for mining similar queries. Instead, we mine paraphrases across queries and clicked document titles. In addition, query logs can be used for query expansion. For instance, Cui et al (2002) extract probabilistic correlations between query terms and document terms by analyzing query logs, which are then used to select high-quality 1318 H1: If a query q hits a title t, then q and t are likely to","@endWordPosition":"911","@position":"5811","annotationId":"T11","@startWordPosition":"908","@citStr":"Gao et al, 2007"}},"title":{"#tail":"\n","#text":"Cross-Lingual Query Suggestion Using Query Logs of Different Languages."},"booktitle":{"#tail":"\n","#text":"In Proceedings of SIGIR,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Wei Gao"},{"#tail":"\n","#text":"Cheng Niu"},{"#tail":"\n","#text":"Jian-Yun Nie"},{"#tail":"\n","#text":"Ming Zhou"},{"#tail":"\n","#text":"Jian Hu"},{"#tail":"\n","#text":"Kam-Fai Wong"},{"#tail":"\n","#text":"Hsiao-Wuen Hon"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Ali Ibrahim, Boris Katz, Jimmy Lin. 2003. Extracting Structural Paraphrases from Aligned Monolingual Corpora. In Proceedings of IWP, pages 57-64."},"#text":"\n","pages":{"#tail":"\n","#text":"57--64"},"marker":{"#tail":"\n","#text":"Ibrahim, Katz, Lin, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" query title both query and title paraphrase Q-Q extraction paraphrase T-T extraction paraphrase relation Figure 1: Illustration of the proposed method. sults. Section 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al, 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and doma","@endWordPosition":"628","@position":"4084","annotationId":"T12","@startWordPosition":"625","@citStr":"Ibrahim et al, 2003"}},"title":{"#tail":"\n","#text":"Extracting Structural Paraphrases from Aligned Monolingual Corpora."},"booktitle":{"#tail":"\n","#text":"In Proceedings of IWP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ali Ibrahim"},{"#tail":"\n","#text":"Boris Katz"},{"#tail":"\n","#text":"Jimmy Lin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Lidija Iordanskaja, Richard Kittredge, and Alain Polgue`re. 1991. Lexical Selection and Paraphrase in a Meaning-Text Generation Model. In Ce?cile L. Paris, William R. Swartout, and William C. Mann (Eds.): Natural Language Generation in Artificial Intelligence and Computational Linguistics, pages 293-312."},"#text":"\n","pages":{"#tail":"\n","#text":"293--312"},"marker":{"#tail":"\n","#text":"Iordanskaja, Kittredge, Polgue`re, 1991"},"title":{"#tail":"\n","#text":"Lexical Selection and Paraphrase in a Meaning-Text Generation Model. In Ce?cile"},"booktitle":{"#tail":"\n","#text":"Mann (Eds.): Natural Language Generation in Artificial Intelligence and Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Lidija Iordanskaja"},{"#tail":"\n","#text":"Richard Kittredge"},{"#tail":"\n","#text":"Alain Polgue`re"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"David Kauchak and Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation. In Proceedings of HLT-NAACL, pages 455-462."},"#text":"\n","pages":{"#tail":"\n","#text":"455--462"},"marker":{"#tail":"\n","#text":"Kauchak, Barzilay, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"g from Baidu1, a Chinese search engine. Experimental results show that the proposed method is effective, which extracts more than 3.5 million pairs of paraphrases with a precision of over 70%. The results also show that the extracted paraphrases can be used to generate high-quality paraphrase patterns. 1 Introduction The use of paraphrases is ubiquitous in human languages, which also presents a challenge for natural language processing (NLP). Previous studies have shown that paraphrasing can play important roles in plenty of areas, such as machine translation (MT) (Callison-Burch et al, 2006; Kauchak and Barzilay, 2006), question answering (QA) (Duboue and Chu-Carroll, 2006; Riezler et al, 2007), natural language generation (NLG) (Iordanskaja et al, 1991), and so on. As a result, the research on paraphrasing and its applications have attracted significant interest. 1www.baidu.com This paper proposes a method that uses search engine query logs for extracting paraphrases, which is illustrated in Figure 1. Specifically, three kinds of paraphrases can be extracted with our method, which include (1) query-title (Q-T): a query and a document title that users clicked on; (2) query-query (Q-Q): two queries, for whic","@endWordPosition":"217","@position":"1477","annotationId":"T13","@startWordPosition":"214","@citStr":"Kauchak and Barzilay, 2006"}},"title":{"#tail":"\n","#text":"Paraphrasing for Automatic Evaluation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT-NAACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Kauchak"},{"#tail":"\n","#text":"Regina Barzilay"}]}},{"date":{"#tail":"\n","#text":"2001"},"issue":{"#tail":"\n","#text":"4"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"erval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur in similar contexts, their meanings tend to be similar. The disadvantage of these methods is that the underlying assumption does not always hold. Phrases with opposite meanings can also occur in similar contexts, such as ?X solves Y? and ?X worsens Y? (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (varia","@endWordPosition":"744","@position":"4814","annotationId":"T14","@startWordPosition":"741","@citStr":"Lin and Pantel, 2001"}},"title":{"#tail":"\n","#text":"Discovery of Inference Rules for Question Answering."},"volume":{"#tail":"\n","#text":"7"},"#tail":"\n","rawString":{"#tail":"\n","#text":"De-Kang Lin and Patrick Pantel. 2001. Discovery of Inference Rules for Question Answering. In Natural Language Engineering 7(4): 343-360."},"journal":{"#tail":"\n","#text":"In Natural Language Engineering"},"#text":"\n","pages":{"#tail":"\n","#text":"343--360"},"marker":{"#tail":"\n","#text":"Lin, Pantel, 2001"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"De-Kang Lin"},{"#tail":"\n","#text":"Patrick Pantel"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Marius Pas?ca and Pe?ter Dienes. 2005. Aligning Needles in a Haystack: Paraphrase Acquisition Across the Web. In Proceedings of IJCNLP, pages 119-130."},"#text":"\n","pages":{"#tail":"\n","#text":"119--130"},"marker":{"#tail":"\n","#text":"Pasca, Dienes, 2005"},"title":{"#tail":"\n","#text":"Aligning Needles in a Haystack: Paraphrase Acquisition Across the Web. In"},"booktitle":{"#tail":"\n","#text":"Proceedings of IJCNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Marius Pasca"},{"#tail":"\n","#text":"Peter Dienes"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Marius Pas?ca. 2007. Weakly-supervised Discovery of Named Entities using Web Search Queries. In Proceedings of CIKM, pages 683-690."},"#text":"\n","pages":{"#tail":"\n","#text":"683--690"},"marker":{"#tail":"\n","#text":"Pasca, 2007"},"title":{"#tail":"\n","#text":"Weakly-supervised Discovery of Named Entities using Web Search Queries."},"booktitle":{"#tail":"\n","#text":"In Proceedings of CIKM,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Marius Pasca"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Deepak Ravichandran and Eduard Hovy. 2002. Learning Surface Text Patterns for a Question Answering System. In Proceedings of ACL, pages 41-47."},"#text":"\n","pages":{"#tail":"\n","#text":"41--47"},"marker":{"#tail":"\n","#text":"Ravichandran, Hovy, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"gual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur in similar contexts, their meanings tend to be similar. The disadvantage of these methods is that the underlying assumption does not always hold. Phrases with opposite meanings can also occur in similar contexts, such as ?X solves Y? and ?X worsens Y? (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs are widely used in the IR community, especially for mining si","@endWordPosition":"762","@position":"4933","annotationId":"T15","@startWordPosition":"759","@citStr":"Ravichandran and Hovy, 2002"}},"title":{"#tail":"\n","#text":"Learning Surface Text Patterns for a Question Answering System. In"},"booktitle":{"#tail":"\n","#text":"Proceedings of ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Deepak Ravichandran"},{"#tail":"\n","#text":"Eduard Hovy"}]}},{"date":{"#tail":"\n","#text":"2008"},"issue":{"#tail":"\n","#text":"4"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ty 1318 H1: If a query q hits a title t, then q and t are likely to be paraphrases. H2: If queries q1 and q2 hit the same title t, q1 and q2 are likely to be paraphrases. H3: If a query q hits titles t1 and t2, then t1 and t2 are likely to be paraphrases. Table 1: Hypotheses for extracting paraphrases. expansion terms for new queries. Note that the expansion terms are merely related terms of the queries, not necessarily paraphrases. There are other studies that use query logs for constructing ontologies (Sekine and Suzuki, 2007), learning named entities (Pas?ca, 2007), building user profiles (Richardson, 2008), correcting spelling errors (Ahmad and Kondrak, 2005), and so forth. 3 The Proposed Method 3.1 Basic Idea Nowadays, more and more users tend to search long queries with search engines. Many users even directly search questions to get exact answers. By analyzing our query log that records rich information including user queries, clicked urls, titles, etc., we find that most titles of clicked documents are highly related with search queries. Especially, paraphrases can be easily found from long queries and the corresponding clicked titles. This motivates us to extract paraphrases from query-tit","@endWordPosition":"1100","@position":"6961","annotationId":"T16","@startWordPosition":"1099","@citStr":"Richardson, 2008"}},"title":{"#tail":"\n","#text":"Learning about the World through Long-Term Query Logs. In"},"volume":{"#tail":"\n","#text":"2"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Matthew Richardson. 2008. Learning about the World through Long-Term Query Logs. In ACM Transactions on the Web 2(4): 1-27."},"journal":{"#tail":"\n","#text":"ACM Transactions on the Web"},"#text":"\n","pages":{"#tail":"\n","#text":"1--27"},"marker":{"#tail":"\n","#text":"Richardson, 2008"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Matthew Richardson"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi Liu. 2007. Statistical Machine Translation for Query Expansion in Answer Retrieval. In Proceedings of ACL, pages 464-471."},"#text":"\n","pages":{"#tail":"\n","#text":"464--471"},"marker":{"#tail":"\n","#text":"Riezler, 2007"},"title":{"#tail":"\n","#text":"Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal"},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Stefan Riezler"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Satoshi Sekine and Hisami Suzuki. 2007. Acquiring Ontological Knowledge from Query Logs. In Proceedings of WWW, pages 1223-1224."},"#text":"\n","pages":{"#tail":"\n","#text":"1223--1224"},"marker":{"#tail":"\n","#text":"Sekine, Suzuki, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"terms and document terms by analyzing query logs, which are then used to select high-quality 1318 H1: If a query q hits a title t, then q and t are likely to be paraphrases. H2: If queries q1 and q2 hit the same title t, q1 and q2 are likely to be paraphrases. H3: If a query q hits titles t1 and t2, then t1 and t2 are likely to be paraphrases. Table 1: Hypotheses for extracting paraphrases. expansion terms for new queries. Note that the expansion terms are merely related terms of the queries, not necessarily paraphrases. There are other studies that use query logs for constructing ontologies (Sekine and Suzuki, 2007), learning named entities (Pas?ca, 2007), building user profiles (Richardson, 2008), correcting spelling errors (Ahmad and Kondrak, 2005), and so forth. 3 The Proposed Method 3.1 Basic Idea Nowadays, more and more users tend to search long queries with search engines. Many users even directly search questions to get exact answers. By analyzing our query log that records rich information including user queries, clicked urls, titles, etc., we find that most titles of clicked documents are highly related with search queries. Especially, paraphrases can be easily found from long queries and the co","@endWordPosition":"1090","@position":"6878","annotationId":"T17","@startWordPosition":"1087","@citStr":"Sekine and Suzuki, 2007"}},"title":{"#tail":"\n","#text":"Acquiring Ontological Knowledge from Query Logs."},"booktitle":{"#tail":"\n","#text":"In Proceedings of WWW,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Satoshi Sekine"},{"#tail":"\n","#text":"Hisami Suzuki"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo. 2002. Automatic Paraphrase Acquisition from News Articles. In Proceedings of HLT, pages 40-46."},"#text":"\n","pages":{"#tail":"\n","#text":"40--46"},"marker":{"#tail":"\n","#text":"Shinyama, Sekine, Sudo, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"roposed method. sults. Section 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al, 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?c","@endWordPosition":"653","@position":"4221","annotationId":"T18","@startWordPosition":"650","@citStr":"Shinyama et al, 2002"},{"#tail":"\n","#text":"eight of each word w in a vector is computed using a heuristic similar to tf-idf: W (w) = tf(w)? log( Nc(w) + 0.1) (5) where tf(w) is the frequency of w in the given s, c(w) is the number of times that w occurs in the corpus, N = maxw c(w). ? Edit Distance Feature FED. Let ED(s1, s2) be the edit distance at the word level between s1 and s2, we compute FED as follows: FED(s1, s2) = 1? ED(s1, s2) max{cw(s1), cw(s2)} (6) 3In FCOR, cw(s) of Equation (3) denotes the number of characters in s. 1320 ? Named Entity (NE) Similarity Feature FNE . NE information is critical in paraphrase identification (Shinyama et al, 2002). We therefore compute the NE similarity between s1 and s2 and take it as a feature. We employ a Chinese NE recognition tool that can recognize person names, locations, organizations, and numerals. The NE similarity is computed as: FNE(s1, s2) = cne(s1 ? s2) + 1 max{cne(s1), cne(s2)}+ 1 (7) where cne(s) denotes the number of NEs in s. Equation (7) guarantees FNE = 1 if there are no NEs in either s1 or s2. ? Pivot Fertility Feature FPF : FPF is a feature specially designed for paraphrase Q-Q and T-T extraction, which are based on the pivot approach4. Specifically, we define fertility of a pivot","@endWordPosition":"2226","@position":"13055","annotationId":"T19","@startWordPosition":"2223","@citStr":"Shinyama et al, 2002"}]},"title":{"#tail":"\n","#text":"Automatic Paraphrase Acquisition from News Articles."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yusuke Shinyama"},{"#tail":"\n","#text":"Satoshi Sekine"},{"#tail":"\n","#text":"Kiyoshi Sudo"}]}},{"date":{"#tail":"\n","#text":"2002"},"issue":{"#tail":"\n","#text":"1"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"y an extended version of distributional hypothesis, namely, if two phrases often occur in similar contexts, their meanings tend to be similar. The disadvantage of these methods is that the underlying assumption does not always hold. Phrases with opposite meanings can also occur in similar contexts, such as ?X solves Y? and ?X worsens Y? (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs are widely used in the IR community, especially for mining similar queries. For example, Wen et al (2002) clustered queries based on user click information. Their basic idea is that if some queries result in similar user clicks, the meanings of these queries should be similar. Such methods have also been investigated in (Gao et al, 2007) for cross-lingual query suggestion and (Zhao et al, 2007) for synonymous questions identification. This paper is partly inspired by their studies. However, we do not simply use click information as clues for mining similar queries. Instead, we mine paraphrases across queries and clicked document titles. In addition, query logs can be used for query expansion. For","@endWordPosition":"872","@position":"5577","annotationId":"T20","@startWordPosition":"869","@citStr":"Wen et al (2002)"}},"title":{"#tail":"\n","#text":"Query Clustering Using User Logs. In"},"volume":{"#tail":"\n","#text":"20"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang. 2002. Query Clustering Using User Logs. In ACM Transactions on Information Systems 20(1): 59-81, 2002."},"journal":{"#tail":"\n","#text":"ACM Transactions on Information Systems"},"#text":"\n","pages":{"#tail":"\n","#text":"59--81"},"marker":{"#tail":"\n","#text":"Wen, Nie, Zhang, 2002"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ji-Rong Wen"},{"#tail":"\n","#text":"Jian-Yun Nie"},{"#tail":"\n","#text":"Hong-Jiang Zhang"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2008. Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora. In Proceedings of ACL-08:HLT, pages 780-788."},"#text":"\n","pages":{"#tail":"\n","#text":"780--788"},"marker":{"#tail":"\n","#text":"Zhao, Wang, Liu, Li, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al, 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al, 2002; Barzilay and Lee, 2003; Dolan et al, 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al, 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur ","@endWordPosition":"687","@position":"4448","annotationId":"T21","@startWordPosition":"684","@citStr":"Zhao et al, 2008"},{"#tail":"\n","#text":" as ????? ? ?? ? ?? (what fruit can I eat on a diet)? and ?? ?? ?? ?? ?? (what fruit can help loss weight)?. ? Others (11.90%): candidate Q-T that cannot be classified into the 3 types above. The above analysis reveals that more than two thirds of candidate Q-T in the SAME class are in the ?word or phrase replacement? type, while the ones with structure changes are slightly more than 7%. We believe this is mainly because queries and titles are relatively short and their structures are simple. Thus structure rewriting can hardly be conducted. This distribution is in line with that reported in (Zhao et al, 2008). As for the RELA class, we find that 42.33% of such candidate Q-T share a problem of named entity mismatch, such as ??? (US) ?? ?? ??? and ??? (China) ?? ?? ?? ? ??. This indicates that the NE similarity feature is necessary in paraphrase validation. 4.2 Evaluation of Paraphrase Q-T The candidate Q-T extracted above are classified with a SVM classifier5 under its default setting. To evaluate the classifier, we run 5-fold cross validation with the 5000 human annotated data, in which we use 4000 for training and the rest 1000 for testing in each run. The evaluation criteria are 5We use libsvm-2","@endWordPosition":"3129","@position":"17928","annotationId":"T22","@startWordPosition":"3126","@citStr":"Zhao et al, 2008"},{"#tail":"\n","#text":"realp1 [X]?????? p2 ???? [X]?? (how to open [X] file) slot 7z; ashx; aspx; bib; cda; cdfs; cmp; cpi; csf; csv; cur; dat; dek... p1 ?? [X]??? p2 ?? [X]??? (poems about [X]) slot ?? (prairies);?? (Yangtze River); ?? (Mount Tai);?? (nostalgia)... Table 4: Examples of paraphrase patterns. ize that the method currently used for inducing paraphrase patterns is simple. Hence we will improve the method in our following experiments. Specifically, multiple slots will be allowed in a pair of patterns. In addition, we will try to apply the alignment techniques in the generation of paraphrase patterns, as Zhao et al (2008) did. 4.5 Analysis Feature Contribution. To investigate the contributions of different features used in classification, we tried different feature combinations for each of our three classifiers. The results are shown in Table 5, in which ?+? means the feature has contribution to the corresponding classifier. As can be seen, the character overlap rate feature (FCOR), cosine similarity feature (FCS), and NE similarity 1323 Feature Q-T Q-Q T-T FF + FLR + FWOR FCOR + + + FCS + + + FED + FNE + + + FPF + Table 5: Feature contribution. feature (FNE) are the most useful, which play important roles in ","@endWordPosition":"3967","@position":"22822","annotationId":"T23","@startWordPosition":"3964","@citStr":"Zhao et al (2008)"}]},"title":{"#tail":"\n","#text":"Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL-08:HLT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Shiqi Zhao"},{"#tail":"\n","#text":"Haifeng Wang"},{"#tail":"\n","#text":"Ting Liu"},{"#tail":"\n","#text":"Sheng Li"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning Question Paraphrases for QA from Encarta Logs. In Proceedings of IJCAI, pages 1795-1800."},"#text":"\n","pages":{"#tail":"\n","#text":"1795--1800"},"marker":{"#tail":"\n","#text":"Zhao, Zhou, Liu, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ontexts, such as ?X solves Y? and ?X worsens Y? (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs are widely used in the IR community, especially for mining similar queries. For example, Wen et al (2002) clustered queries based on user click information. Their basic idea is that if some queries result in similar user clicks, the meanings of these queries should be similar. Such methods have also been investigated in (Gao et al, 2007) for cross-lingual query suggestion and (Zhao et al, 2007) for synonymous questions identification. This paper is partly inspired by their studies. However, we do not simply use click information as clues for mining similar queries. Instead, we mine paraphrases across queries and clicked document titles. In addition, query logs can be used for query expansion. For instance, Cui et al (2002) extract probabilistic correlations between query terms and document terms by analyzing query logs, which are then used to select high-quality 1318 H1: If a query q hits a title t, then q and t are likely to be paraphrases. H2: If queries q1 and q2 hit the same tit","@endWordPosition":"920","@position":"5869","annotationId":"T24","@startWordPosition":"917","@citStr":"Zhao et al, 2007"}},"title":{"#tail":"\n","#text":"Learning Question Paraphrases for QA from Encarta Logs."},"booktitle":{"#tail":"\n","#text":"In Proceedings of IJCAI,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Shiqi Zhao"},{"#tail":"\n","#text":"Ming Zhou"},{"#tail":"\n","#text":"Ting Liu"}]}}]}}]}}
