ystem designed to accept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, prov
cept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step sca
uage input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potenti
rt experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional
fferent tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadv
ning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of ad
nt approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a differe
oring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for 
stem can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al, 2006; Graesser et al, 1999; Aleven et al, 2001; Buckley and Wolska, 2007; Nielsen et al, 2008; VanLehn et al, 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al, 1994; Litman et al, 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue sta
l problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations. The BEETLE II system architecture is designed to overcome these limitations (Callaway et al, 2007). It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically. This allows the system to consistently apply the same tutorial policy across a range of questions. To some extent, this comes at the expense of being able to address individual student misconceptions. However, the system?s modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning. A distinguishing feature of the system is
m are typed. Students read pre-authored curriculum slides and carry out exercises which involve experimenting with the circuit simulator and explaining the observed behavior. The system also asks some high-level questions, such as ?What is voltage??. The system architecture is shown in Figure 2. The system uses a standard interpretation pipeline, with domain-independent parsing and generation components supported by domain specific reasoners for decision making. The architecture is discussed in detail in the rest of this section. 2.1 Interpretation Components We use the TRIPS dialogue parser (Allen et al, 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al, 2008a) to produce a domain-specific semantic representation of the student?s output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For
?. The system architecture is shown in Figure 2. The system uses a standard interpretation pipeline, with domain-independent parsing and generation components supported by domain specific reasoners for decision making. The architecture is discussed in detail in the rest of this section. 2.1 Interpretation Components We use the TRIPS dialogue parser (Allen et al, 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al, 2008a) to produce a domain-specific semantic representation of the student?s output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question ?Which bulbs will be on and which bulbs will be off in this diagram??, ?off? can be taken to mean ?all bulbs in the diagram will be off.? The resulting output is then passed on to 
The system uses a standard interpretation pipeline, with domain-independent parsing and generation components supported by domain specific reasoners for decision making. The architecture is discussed in detail in the rest of this section. 2.1 Interpretation Components We use the TRIPS dialogue parser (Allen et al, 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al, 2008a) to produce a domain-specific semantic representation of the student?s output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question ?Which bulbs will be on and which bulbs will be off in this diagram??, ?off? can be taken to mean ?all bulbs in the diagram will be off.? The resulting output is then passed on to the domain reasoning and diagnosis components. 2.2 Domain 
presented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question ?Which bulbs will be on and which bulbs will be off in this diagram??, ?off? can be taken to mean ?all bulbs in the diagram will be off.? The resulting output is then passed on to the domain reasoning and diagnosis components. 2.2 Domain Reasoning and Diagnosis The system uses a knowledge base implemented in the KM representation language (Clark and Porter, 1999; Dzikovska et al, 2006) to represent the state of the world. At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits. Student explanations are checked on two levels, verifying factual and explanation correctness. For example, for a question ?Why is bulb A lit??, if the student says ?it is in a closed path?, the system checks two things: a) is the bulb indeed in a closed path? and b) is being in a closed path a reasonable explanation for the bulb being lit? Different remediation strategies need to be used dep
tracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question ?Which bulbs will be on and which bulbs will be off in this diagram??, ?off? can be taken to mean ?all bulbs in the diagram will be off.? The resulting output is then passed on to the domain reasoning and diagnosis components. 2.2 Domain Reasoning and Diagnosis The system uses a knowledge base implemented in the KM representation language (Clark and Porter, 1999; Dzikovska et al, 2006) to represent the state of the world. At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits. Student explanations are checked on two levels, verifying factual and explanation correctness. For example, for a question ?Why is bulb A lit??, if the student says ?it is in a closed path?, the system checks two things: a) is the bulb indeed in a closed path? and b) is being in a closed path a reasonable explanation for the bulb being lit? Different remediation strategies need to be used depending on whether the st
nd b) is being in a closed path a reasonable explanation for the bulb being lit? Different remediation strategies need to be used depending on whether the student made a factual error (i.e., they misread the diagram and the bulb is not in a closed path) or produced an incorrect explanation (i.e., the bulb is indeed in a closed path, but they failed to mention that a battery needs to be in the same closed path for the bulb to light). The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student?s answer. At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al (2008). 2.3 Tutorial Planner The tutorial planner implements a set of generic tutoring strategies, as well as a policy to choose an appropriate strategy at each point of the interaction. It is designed so that different policies can be defined for the system. The currentl
but they failed to mention that a battery needs to be in the same closed path for the bulb to light). The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student?s answer. At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al (2008). 2.3 Tutorial Planner The tutorial planner implements a set of generic tutoring strategies, as well as a policy to choose an appropriate strategy at each point of the interaction. It is designed so that different policies can be defined for the system. The currently implemented strategies are: acknowledging the correct part of the answer; suggesting a slide to read with background material; prompting for missing parts of the answer; hinting (low- and high- specificity); and giving away the answer. Two or more strategies can be used together if necessary. The hint selection mechanism generates
gh-specificity, it attempts to hint at a two-place relation, for example, ?Here?s a hint: the battery is connected to something.? The tutorial policy makes a high-level decision as to which strategy to use (for example, ?acknowledge the correct part and give a high specificity hint?) based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers.1 In addition to a remediation policy, the tutorial planner implements an error recovery policy (Dzikovska et al, 2009). Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al, 2003) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ?I?m sorry, I?m having a problem understanding. I don?t know the word power.? The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers. 2.
gh-level decision as to which strategy to use (for example, ?acknowledge the correct part and give a high specificity hint?) based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers.1 In addition to a remediation policy, the tutorial planner implements an error recovery policy (Dzikovska et al, 2009). Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al, 2003) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ?I?m sorry, I?m having a problem understanding. I don?t know the word power.? The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers. 2.4 Generation The strategy decision made by the tutorial planner, together with relevant semantic content from the student?s answer (e.g., part of the answer 
or example, ?I?m sorry, I?m having a problem understanding. I don?t know the word power.? The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers. 2.4 Generation The strategy decision made by the tutorial planner, together with relevant semantic content from the student?s answer (e.g., part of the answer to confirm), is passed to content planning and generation. The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision, and a FUF/SURGE (Elhadad and Robin, 1992) generation system to produce the appropriate text. Templates are used to generate some stock phrases such as ?When you are ready, go on to the next slide.? 2.5 Dialogue Management Interaction between components is coordinated by the dialogue manager which uses the informationstate approach (Larsson and Traum, 2000). The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well (Callaway et al, 2007). of the answer. Once the complete a
 with relevant semantic content from the student?s answer (e.g., part of the answer to confirm), is passed to content planning and generation. The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision, and a FUF/SURGE (Elhadad and Robin, 1992) generation system to produce the appropriate text. Templates are used to generate some stock phrases such as ?When you are ready, go on to the next slide.? 2.5 Dialogue Management Interaction between components is coordinated by the dialogue manager which uses the informationstate approach (Larsson and Traum, 2000). The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well (Callaway et al, 2007). of the answer. Once the complete answer has been accumulated, the system accepts it and moves on. Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student. 3 Evaluation The first experimental evaluation involving 81 participants (undergraduates recruited fr
ategy decision, and a FUF/SURGE (Elhadad and Robin, 1992) generation system to produce the appropriate text. Templates are used to generate some stock phrases such as ?When you are ready, go on to the next slide.? 2.5 Dialogue Management Interaction between components is coordinated by the dialogue manager which uses the informationstate approach (Larsson and Traum, 2000). The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well (Callaway et al, 2007). of the answer. Once the complete answer has been accumulated, the system accepts it and moves on. Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student. 3 Evaluation The first experimental evaluation involving 81 participants (undergraduates recruited from a Southeastern University in the USA) was completed in 2009. Participants had little or no prior knowledge of the domain. Each participant took a pre-test, worked through a lesson with the system, took a post-test, and completed a user satisfac
rrived at an incorrect interpretation of a student utterance and took action on it. Such annotation can provide useful input for statistical learning algorithms to detect and recover from misunderstandings. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn?t explicitly explain the reason why different terminology is needed (Dzikovska et al, 2010). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006). Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail. From the point of view of tutoring research, we are planning to use the system to answer questions about the effectiveness of different approaches to tutoring, and the differences between human-human and human-computer tutoring. Previ
cover from misunderstandings. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn?t explicitly explain the reason why different terminology is needed (Dzikovska et al, 2010). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006). Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail. From the point of view of tutoring research, we are planning to use the system to answer questions about the effectiveness of different approaches to tutoring, and the differences between human-human and human-computer tutoring. Previous comparisons of human-human and humancomputer dialogue were limited to systems that asked short-answer questions (Litman et al, 2006; Rose? and Torrey, 2005). Having a system
