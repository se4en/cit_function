{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Collins, Michael and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 100\u2013110, College Park, MD."},"#text":"\n","pages":{"#tail":"\n","#text":"100--110"},"marker":{"#tail":"\n","#text":"Collins, Singer, 1999"},"location":{"#tail":"\n","#text":"College Park, MD."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"he extraction phase, for each sentence (or fragment of a sentence) in CTRW the program will decide which leaf class is expressed, with what strength, and what frequency. We use a decision-list algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDE-STYLE DISTINCTIONS. These are split further for each leaf class, as explained in Section 2.3. The algorithm we implemented is inspired by the work of Yarowsky (1995) on word sense disambiguation. He classified the senses of a word on the basis of other words that the given word co-occurs with. Collins and Singer (1999) classified proper names 1 We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project. 228 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Figure 4 Example of distinctions extracted from CTRW. as PERSON, ORGANIZATION, or LOCATION using contextual rules (that rely on other words appearing in the context of the proper names) and spelling rules (that rely on words in the proper names). Starting with a few spelling rules (using some proper-name features) in the decision list, their algorithm learns new contextual rules; using these rules, it ","@endWordPosition":"2369","@position":"15635","annotationId":"T1","@startWordPosition":"2366","@citStr":"Collins and Singer (1999)"},{"#tail":"\n","#text":"cision list that allows us to compute the confidence with which new patterns Figure 5 The architecture of the extraction module. 229 Computational Linguistics Volume 32, Number 2 Figure 6 The decision-list learning algorithm. are significant for the class. The confidence h(x) for a word x is computed with the formula: count(x, E') + α h(x) = (1) count(x, E) + kα where E' is the set of patterns selected for the class, and E is the set of all input data. So, we count how many times x is in the patterns selected for the class versus the total number of occurrences in the training data. Following Collins and Singer (1999), k = 2, because there are two partitions (relevant and irrelevant for the class). α = 0.1 is a smoothing parameter. In order to obtain input data, we replace all the near-synonyms in the text of the dictionary with the term near syn; then we chunk the text with Abney\u2019s chunker (Abney 1996). The training set E is composed of all the verb phrases, noun phrases, adjectival phrases, and adverbial phrases (denoted vx, nx, ax, rx, respectively) that occur more 230 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences than t times in the text of the dictionary (where t = 3 in our exp","@endWordPosition":"2790","@position":"18177","annotationId":"T2","@startWordPosition":"2787","@citStr":"Collins and Singer (1999)"}]},"title":{"#tail":"\n","#text":"Unsupervised models for named entity classification."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michael Collins"},{"#tail":"\n","#text":"Yoram Singer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Yarowsky, David. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189\u2013196, Cambridge, MA."},"#text":"\n","pages":{"#tail":"\n","#text":"189--196"},"marker":{"#tail":"\n","#text":"Yarowsky, 1995"},"location":{"#tail":"\n","#text":"Cambridge, MA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"al is to learn a set of words and expressions from CTRW\u2014that is, extraction patterns\u2014that characterizes descriptions of the class. Then, during the extraction phase, for each sentence (or fragment of a sentence) in CTRW the program will decide which leaf class is expressed, with what strength, and what frequency. We use a decision-list algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDE-STYLE DISTINCTIONS. These are split further for each leaf class, as explained in Section 2.3. The algorithm we implemented is inspired by the work of Yarowsky (1995) on word sense disambiguation. He classified the senses of a word on the basis of other words that the given word co-occurs with. Collins and Singer (1999) classified proper names 1 We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project. 228 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Figure 4 Example of distinctions extracted from CTRW. as PERSON, ORGANIZATION, or LOCATION using contextual rules (that rely on other words appearing in the context of the proper names) and spelling rules (that rely on words in the proper names). Sta","@endWordPosition":"2342","@position":"15480","annotationId":"T3","@startWordPosition":"2341","@citStr":"Yarowsky (1995)"}},"title":{"#tail":"\n","#text":"Unsupervised word sense disambiguation rivaling supervised methods."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David Yarowsky"}}}]}}}}
