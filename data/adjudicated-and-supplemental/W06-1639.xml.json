{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":{"#tail":"\n","@confidence":"0.538433833333333","#text":"\n6We are attempting to determine whether a speech seg-\nment represents support or not. This differs from the problem\nof determining what the speaker?s actual opinion is, a prob-\nlem that, as an anonymous reviewer put it, is complicated by\n?grandstanding, backroom deals, or, more innocently, plain\nchange of mind (?I voted for it before I voted against it?)?.\n"},"figure":[{"#tail":"\n","@confidence":"0.873668833333333","#text":"\nAgreement classifier\n(?reference?agreement??)\nDevel.\nset\nTest\nset\n"},{"#tail":"\n","@confidence":"0.792011166666667","#text":"\nSupport/oppose classifer\n(?speech segment?yea??)\nDevel.\nset\nTest\nset\n"},{"#tail":"\n","@confidence":"0.8554633","#text":"\nSupport/oppose classifer\n(?speech segment?yea??)\nDevel.\nset\nTest\nset\nSVM [speaker] 71.60 70.00\nSVM + agreement links . . .\nwith ?agr = 0 88.72 71.28\nwith ?agr = ? 84.44 76.05\n"}],"address":{"#tail":"\n","@confidence":"0.952107","#text":"\nIthaca, NY 14853-7501\n"},"author":{"#tail":"\n","@confidence":"0.987342","#text":"\nMatt Thomas, Bo Pang, and Lillian Lee\n"},"equation":[{"#tail":"\n","@confidence":"0.577246666666667","#text":"\n?\ns\nind(s, c(s))+\n?\ns,s?: c(s) 6=c(s?)\n?\n"},{"#tail":"\n","@confidence":"0.845794777777778","#text":"\nind(s,Y) def=\n?\n???\n???\n1 d(s) > 2?s;(\n1 + d(s)2?s\n)\n/2 |d(s) |? 2?s;\n0 d(s) < ?2?s\n"},{"#tail":"\n","@confidence":"0.946214142857143","#text":"\nagr(r) def=\n?\n??\n??\n0 d(r) < ?agr;\n? ? d(r)/4?r ?agr ? d(r) ? 4?r;\n? d(r) > 4?r.\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.996936","#text":"\n3.1 Classification framework\n"},{"#tail":"\n","@confidence":"0.994697","#text":"\n3.2 Classifying speech segments in isolation\n"},{"#tail":"\n","@confidence":"0.993172","#text":"\n3.3 Relationships between speech segments\n"},{"#tail":"\n","@confidence":"0.999434","#text":"\n4.1 Preliminaries: Reference classification\n"},{"#tail":"\n","@confidence":"0.820448","#text":"\n4.2 Segment-based speech-segment\n"},{"#tail":"\n","@confidence":"0.991284","#text":"\n4.3 Speaker-based speech-segment\n"},{"#tail":"\n","@confidence":"0.995406","#text":"\n4.4 ?Hard? agreement constraints\n"},{"#tail":"\n","@confidence":"0.934502","#text":"\n4.5 On the development/test set split\n"}],"footnote":[{"#tail":"\n","@confidence":"0.5038515","#text":"\nwhen Mosaic was not quite two years old and Altavista did\nnot yet exist.\n2E.g., ?Internet injects sweeping change into U.S. poli-\ntics?, Adam Nagourney, The New York Times, April 2, 2006.\n3E.g., ?The End of News??, Michael Massing, The New\nYork Review of Books, December 1, 2005.\n"},{"#tail":"\n","@confidence":"0.9711116","#text":"\n7One subtlety is that for the purposes of mining agree-\nment cues (but not for evaluating overall support/oppose\nclassification accuracy), we temporarily re-inserted into our\ndataset previously filtered speech segments containing the\nterm ?yield?, since the yielding of time on the House floor\ntypically indicates agreement even though the yield state-\nments contain little relevant text on their own.\n8We found good development-set performance using the\n30 tokens before, 20 tokens after, and the name itself.\n9Since we are concerned with references that potentially\n"}],"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.917851621621622","#text":"\nInternational Conference on Natural Language Pro-\ncessing (ICON).\nR. Agrawal, S. Rajagopalan, R. Srikant, Y. Xu. 2003.\nMining newsgroups using networks arising from so-\ncial behavior. In Proceedings of WWW, 529?535.\nN. Bansal, A. Blum, S. Chawla. 2002. Correla-\ntion clustering. In Proceedings of the Symposium\non Foundations of Computer Science (FOCS), 238?\n247. Journal version in Machine Learning Journal,\nspecial issue on theoretical advances in data cluster-\ning, 56(1-3):89?113 (2004).\nR. Barzilay, M. Lapata. 2005. Collective content selec-\ntion for concept-to-text generation. In Proceedings\nof HLT/EMNLP, 331?338.\nA. Blum, S. Chawla. 2001. Learning from labeled and\nunlabeled data using graph mincuts. In Proceedings\nof ICML, 19?26.\nC. Cardie, C. Farina, T. Bruce, E. Wagner. 2006. Us-\ning natural language processing to improve eRule-\nmaking. In Proceedings of Digital Government Re-\nsearch (dg.o).\nV. Carvalho, W. W. Cohen. 2005. On the collective\nclassification of email ?speech acts?. In Proceedings\nof SIGIR, 345?352.\nW. Daelemans, V. Hoste. 2002. Evaluation of ma-\nchine learning methods for natural language pro-\ncessing tasks. In Proceedings of the Third Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC), 755?760.\nS. Das, M. Chen. 2001. Yahoo! for Amazon: Extract-\ning market sentiment from stock message boards. In\nProceedings of the Asia Pacific Finance Association\nAnnual Conference (APFA).\nK. Dave, S. Lawrence, D. M. Pennock. 2003. Mining\nthe peanut gallery: Opinion extraction and semantic\nclassification of product reviews. In Proceedings of\nWWW, 519?528.\n"},{"#tail":"\n","@confidence":"0.9975284296875","#text":"\nM. Efron. 2004. Cultural orientation: Classifying sub-\njective documents by cociation [sic] analysis. In\nProceedings of the AAAI Fall Symposium on Style\nand Meaning in Language, Art, Music, and Design,\n41?48.\nA. Esuli. 2006. Sentiment classification bibliography.\nliinwww.ira.uka.de/bibliography/Misc/Sentiment.html.\nM. Galley, K. McKeown, J. Hirschberg, E. Shriberg.\n2004. Identifying agreement and disagreement in\nconversational speech: Use of Bayesian networks to\nmodel pragmatic dependencies. In Proceedings of\nthe 42nd ACL, 669?676.\nL. Getoor, N. Friedman, D. Koller, B. Taskar. 2002.\nLearning probabilistic models of relational structure.\nJournal of Machine Learning Research, 3:679?707.\nSpecial issue on the Eighteenth ICML.\nA. B. Goldberg, J. Zhu. 2006. Seeing stars\nwhen there aren?t many stars: Graph-based semi-\nsupervised learning for sentiment categorization.\nIn TextGraphs: HLT/NAACL Workshop on Graph-\nbased Algorithms for Natural Language Processing.\nG. Grefenstette, Y. Qu, J. G. Shanahan, D. A. Evans.\n2004. Coupling niche browsers and affect analysis\nfor an opinion mining application. In Proceedings\nof RIAO.\nM. Hearst. 1992. Direction-based text interpretation as\nan information access refinement. In P. Jacobs, ed.,\nText-Based Intelligent Systems, 257?274. Lawrence\nErlbaum Associates.\nD. Hillard, M. Ostendorf, E. Shriberg. 2003. Detection\nof agreement vs. disagreement in meetings: Train-\ning with unlabeled data. In Proceedings of HLT-\nNAACL.\nT. Joachims. 2003. Transductive learning via spectral\ngraph partitioning. In Proceedings of ICML, 290?\n297.\nR. I. Kondor, J. D. Lafferty. 2002. Diffusion kernels\non graphs and other discrete input spaces. In Pro-\nceedings of ICML, 315?322.\nN. Kwon, S. Shulman, E. Hovy. 2006. Multidimen-\nsional text analysis for eRulemaking. In Proceed-\nings of Digital Government Research (dg.o).\nJ. Lafferty, A. McCallum, F. Pereira. 2001. Condi-\ntional random fields: Probabilistic models for seg-\nmenting and labeling sequence data. In Proceedings\nof ICML, 282?289.\nM. Laver, K. Benoit, J. Garry. 2003. Extracting policy\npositions from political texts using words as data.\nAmerican Political Science Review.\nW. Lehnert, C. Cardie, E. Riloff. 1990. Analyzing re-\nsearch papers using citation sentences. In Program\nof the Twelfth Annual Conference of the Cognitive\nScience Society, 511?18.\nD. Marcu. 2000. The theory and practice of discourse\nparsing and summarization. MIT Press.\nA. McCallum, B. Wellner. 2004. Conditional mod-\nels of identity uncertainty with application to noun\ncoreference. In Proceedings of NIPS.\nT. Mullen, R. Malouf. 2006. A preliminary investiga-\ntion into sentiment analysis of informal political dis-\ncourse. In Proceedings of the AAAI Symposium on\nComputational Approaches to Analyzing Weblogs,\n159?162.\nA. Munson, C. Cardie, R. Caruana. 2005. Optimizing\nto arbitrary NLP metrics using ensemble selection.\nIn Proceedings of HLT-EMNLP, 539?546.\nJ. Neville, D. Jensen. 2000. Iterative classification in\nrelational data. In Proceedings of the AAAI Work-\nshop on Learning Statistical Models from Relational\nData, 13?20.\nB. Pang, L. Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. In Proceedings of the ACL,\n271?278.\nB. Pang, L. Lee. 2005. Seeing stars: Exploiting class\nrelationships for sentiment categorization with re-\nspect to rating scales. In Proceedings of the ACL.\nB. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs\nup? Sentiment classification using machine learning\ntechniques. In Proceedings of EMNLP, 79?86.\nS. Purpura, D. Hillard. 2006. Automated classifica-\ntion of congressional legislation. In Proceedings of\nDigital Government Research (dg.o).\nW. Sack. 1994. On the computation of point of view.\nIn Proceedings of AAAI, pg. 1488. Student abstract.\nS. Shulman, D. Schlosberg. 2002. Electronic rulemak-\ning: New frontiers in public participation. Prepared\nfor the Annual Meeting of the American Political\nScience Association.\nS. Shulman, J. Callan, E. Hovy, S. Zavestoski. 2005.\nLanguage processing technologies for electronic\nrulemaking: A project highlight. In Proceedings of\nDigital Government Research (dg.o), 87?88.\nS. S. Smith, J. M. Roberts, R. J. Vander Wielen. 2005.\nThe American Congress. Cambridge University\nPress, fourth edition.\nA. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-\nDykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-\ntin, M. Meteer. 2000. Dialogue act modeling for\nautomatic tagging and recognition of conversational\nspeech. Computational Linguistics, 26(3):339?373.\nB. Taskar, P. Abbeel, D. Koller. 2002. Discriminative\nprobabilistic models for relational data. In Proceed-\nings of UAI, Edmonton, Canada.\nB. Taskar, C. Guestrin, D. Koller. 2003. Max-margin\nMarkov networks. In Proceedings of NIPS.\nB. Taskar, V. Chatalbashev, D. Koller. 2004. Learn-\ning associative Markov networks. In Proceedings of\nICML.\nS. Teufel, M. Moens. 2002. Summarizing scientific\narticles: Experiments with relevance and rhetorical\nstatus. Computational Linguistics, 28(4):409?445.\nP. Turney. 2002. Thumbs up or thumbs down? Seman-\ntic orientation applied to unsupervised classification\nof reviews. In Proceedings of the ACL, 417?424.\nJ. M. Wiebe, W. J. Rapaport. 1988. A computational\ntheory of perspective and reference in narrative. In\nProceedings of the ACL, 131?138.\nJ. M. Wiebe. 1994. Tracking point of view in narrative.\nComputational Linguistics, 20(2):233?287.\nH. Yang, J. Callan. 2005. Near-duplicate detection\nfor eRulemaking. In Proceedings of Digital Gov-\nernment Research (dg.o).\nJ. Zhu. 2005. Semi-supervised learning literature\nsurvey. Computer Sciences Technical Report TR\n1530, University of Wisconsin-Madison. Available\nat http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf;\nhas been updated since the initial 2005 version.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.79634025","#text":"\nProceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327?335,\nSydney, July 2006. c?2006 Association for Computational Linguistics\nGet out the vote: Determining support or opposition from Congressional\nfloor-debate transcripts\n"},{"#tail":"\n","@confidence":"0.9955136","#text":"\nWe investigate whether one can determine\nfrom the transcripts of U.S. Congressional\nfloor debates whether the speeches repre-\nsent support of or opposition to proposed\nlegislation. To address this problem, we\nexploit the fact that these speeches occur\nas part of a discussion; this allows us to\nuse sources of information regarding re-\nlationships between discourse segments,\nsuch as whether a given utterance indicates\nagreement with the opinion expressed by\nanother. We find that the incorporation\nof such information yields substantial im-\nprovements over classifying speeches in\nisolation.\n"},{"#tail":"\n","@confidence":"0.989037117647059","#text":"\nOne ought to recognize that the present\npolitical chaos is connected with the de-\ncay of language, and that one can prob-\nably bring about some improvement by\nstarting at the verbal end. ? Orwell,\n?Politics and the English language?\nWe have entered an era where very large\namounts of politically oriented text are now avail-\nable online. This includes both official documents,\nsuch as the full text of laws and the proceedings of\nlegislative bodies, and unofficial documents, such\nas postings on weblogs (blogs) devoted to politics.\nIn some sense, the availability of such data is sim-\nply a manifestation of a general trend of ?every-\nbody putting their records on the Internet?.1 The\n1It is worth pointing out that the United States? Library of\nCongress was an extremely early adopter of Web technology:\nthe THOMAS database (http://thomas.loc.gov) of congres-\nonline accessibility of politically oriented texts in\nparticular, however, is a phenomenon that some\nhave gone so far as to say will have a potentially\nsociety-changing effect.\nIn the United States, for example, governmen-\ntal bodies are providing and soliciting political\ndocuments via the Internet, with lofty goals in\nmind: electronic rulemaking (eRulemaking) ini-\ntiatives involving the ?electronic collection, dis-\ntribution, synthesis, and analysis of public com-\nmentary in the regulatory rulemaking process?,\nmay ?[alter] the citizen-government relationship?\n(Shulman and Schlosberg, 2002). Additionally,\nmuch media attention has been focused recently\non the potential impact that Internet sites may have\non politics2, or at least on political journalism3.\nRegardless of whether one views such claims as\nclear-sighted prophecy or mere hype, it is obvi-\nously important to help people understand and an-\nalyze politically oriented text, given the impor-\ntance of enabling informed participation in the po-\nlitical process.\nEvaluative and persuasive documents, such as\na politician?s speech regarding a bill or a blog-\nger?s commentary on a legislative proposal, form a\nparticularly interesting type of politically oriented\ntext. People are much more likely to consult such\nevaluative statements than the actual text of a bill\nor law under discussion, given the dense nature of\nlegislative language and the fact that (U.S.) bills\noften reach several hundred pages in length (Smith\net al, 2005). Moreover, political opinions are ex-\nsional bills and related data was launched in January 1995,\n"},{"#tail":"\n","@confidence":"0.999215817307692","#text":"\nplicitly solicited in the eRulemaking scenario.\nIn the analysis of evaluative language, it is fun-\ndamentally necessary to determine whether the au-\nthor/speaker supports or disapproves of the topic\nof discussion. In this paper, we investigate the\nfollowing specific instantiation of this problem:\nwe seek to determine from the transcripts of\nU.S. Congressional floor debates whether each\n?speech? (continuous single-speaker segment of\ntext) represents support for or opposition to a pro-\nposed piece of legislation. Note that from an ex-\nperimental point of view, this is a very convenient\nproblem to work with because we can automati-\ncally determine ground truth (and thus avoid the\nneed for manual annotation) simply by consulting\npublicly available voting records.\nTask properties Determining whether or not a\nspeaker supports a proposal falls within the realm\nof sentiment analysis, an extremely active re-\nsearch area devoted to the computational treatment\nof subjective or opinion-oriented language (early\nwork includes Wiebe and Rapaport (1988), Hearst\n(1992), Sack (1994), and Wiebe (1994); see Esuli\n(2006) for an active bibliography). In particu-\nlar, since we treat each individual speech within\na debate as a single ?document?, we are consider-\ning a version of document-level sentiment-polarity\nclassification, namely, automatically distinguish-\ning between positive and negative documents (Das\nand Chen, 2001; Pang et al, 2002; Turney, 2002;\nDave et al, 2003).\nMost sentiment-polarity classifiers proposed in\nthe recent literature categorize each document in-\ndependently. A few others incorporate various\nmeasures of inter-document similarity between the\ntexts to be labeled (Agarwal and Bhattacharyya,\n2005; Pang and Lee, 2005; Goldberg and Zhu,\n2006). Many interesting opinion-oriented docu-\nments, however, can be linked through certain re-\nlationships that occur in the context of evaluative\ndiscussions. For example, we may find textual4\nevidence of a high likelihood of agreement be-\n4Because we are most interested in techniques applicable\nacross domains, we restrict consideration to NLP aspects of\nthe problem, ignoring external problem-specific information.\nFor example, although most votes in our corpus were almost\ncompletely along party lines (and despite the fact that same-\nparty information is easily incorporated via the methods we\npropose), we did not use party-affiliation data. Indeed, in\nother settings (e.g., a movie-discussion listserv) one may not\nbe able to determine the participants? political leanings, and\nsuch information may not lead to significantly improved re-\nsults even if it were available.\ntween two speakers, such as explicit assertions (?I\nsecond that!?) or quotation of messages in emails\nor postings (see Mullen and Malouf (2006) but cf.\nAgrawal et al (2003)). Agreement evidence can\nbe a powerful aid in our classification task: for ex-\nample, we can easily categorize a complicated (or\noverly terse) document if we find within it indica-\ntions of agreement with a clearly positive text.\nObviously, incorporating agreement informa-\ntion provides additional benefit only when the in-\nput documents are relatively difficult to classify\nindividually. Intuition suggests that this is true\nof the data with which we experiment, for several\nreasons. First, U.S. congressional debates contain\nvery rich language and cover an extremely wide\nvariety of topics, ranging from flag burning to in-\nternational policy to the federal budget. Debates\nare also subject to digressions, some fairly natural\nand others less so (e.g., ?Why are we discussing\nthis bill when the plight of my constituents regard-\ning this other issue is being ignored??)\nSecond, an important characteristic of persua-\nsive language is that speakers may spend more\ntime presenting evidence in support of their po-\nsitions (or attacking the evidence presented by\nothers) than directly stating their attitudes. An\nextreme example will illustrate the problems in-\nvolved. Consider a speech that describes the U.S.\nflag as deeply inspirational, and thus contains only\npositive language. If the bill under discussion is a\nproposed flag-burning ban, then the speech is sup-\nportive; but if the bill under discussion is aimed at\nrescinding an existing flag-burning ban, the speech\nmay represent opposition to the legislation. Given\nthe current state of the art in sentiment analysis,\nit is doubtful that one could determine the (proba-\nbly topic-specific) relationship between presented\nevidence and speaker opinion.\nQualitative summary of results The above dif-\nficulties underscore the importance of enhancing\nstandard classification techniques with new infor-\nmation sources that promise to improve accuracy,\nsuch as inter-document relationships between the\ndocuments to be labeled. In this paper, we demon-\nstrate that the incorporation of agreement model-\ning can provide substantial improvements over the\napplication of support vector machines (SVMs) in\nisolation, which represents the state of the art in\nthe individual classification of documents. The en-\nhanced accuracies are obtained via a fairly primi-\ntive automatically-acquired ?agreement detector?\n"},{"#tail":"\n","@confidence":"0.993831666666667","#text":"\nand a conceptually simple method for integrat-\ning isolated-document and agreement-based in-\nformation. We thus view our results as demon-\nstrating the potentially large benefits of exploiting\nsentiment-related discourse-segment relationships\nin sentiment-analysis tasks.\n"},{"#tail":"\n","@confidence":"0.994994052631579","#text":"\nThis section outlines the main steps of the process\nby which we created our corpus (download site:\nwww.cs.cornell.edu/home/llee/data/convote.html).\nGovTrack (http://govtrack.us) is an independent\nwebsite run by Joshua Tauberer that collects pub-\nlicly available data on the legislative and fund-\nraising activities of U.S. congresspeople. Due to\nits extensive cross-referencing and collating of in-\nformation, it was nominated for a 2006 ?Webby?\naward. A crucial characteristic of GovTrack from\nour point of view is that the information is pro-\nvided in a very convenient format; for instance,\nthe floor-debate transcripts are broken into sepa-\nrate HTML files according to the subject of the\ndebate, so we can trivially derive long sequences\nof speeches guaranteed to cover the same topic.\nWe extracted from GovTrack all available tran-\nscripts of U.S. floor debates in the House of Rep-\nresentatives for the year 2005 (3268 pages of tran-\nscripts in total), together with voting records for all\nroll-call votes during that year. We concentrated\non debates regarding ?controversial? bills (ones in\nwhich the losing side generated at least 20% of the\nspeeches) because these debates should presum-\nably exhibit more interesting discourse structure.\nEach debate consists of a series of speech seg-\nments, where each segment is a sequence of un-\ninterrupted utterances by a single speaker. Since\nspeech segments represent natural discourse units,\nwe treat them as the basic unit to be classified.\nEach speech segment was labeled by the vote\n(?yea? or ?nay?) cast for the proposed bill by the\nperson who uttered the speech segment.\nWe automatically discarded those speech seg-\nments belonging to a class of formulaic, generally\none-sentence utterances focused on the yielding\nof time on the house floor (for example, ?Madam\nSpeaker, I am pleased to yield 5 minutes to the\ngentleman from Massachusetts?), as such speech\nsegments are clearly off-topic. We also removed\nspeech segments containing the term ?amend-\nment?, since we found during initial inspection\nthat these speeches generally reflect a speaker?s\nopinion on an amendment, and this opinion may\ndiffer from the speaker?s opinion on the underly-\ning bill under discussion.\nWe randomly split the data into training, test,\nand development (parameter-tuning) sets repre-\nsenting roughly 70%, 20%, and 10% of our data,\nrespectively (see Table 1). The speech segments\nremained grouped by debate, with 38 debates as-\nsigned to the training set, 10 to the test set, and 5\nto the development set; we require that the speech\nsegments from an individual debate all appear in\nthe same set because our goal is to examine clas-\nsification of speech segments in the context of the\nsurrounding discussion.\n"},{"#tail":"\n","@confidence":"0.999564833333333","#text":"\nThe support/oppose classification problem can be\napproached through the use of standard classifiers\nsuch as support vector machines (SVMs), which\nconsider each text unit in isolation. As discussed\nin Section 1, however, the conversational nature\nof our data implies the existence of various rela-\ntionships that can be exploited to improve cumu-\nlative classification accuracy for speech segments\nbelonging to the same debate. Our classification\nframework, directly inspired by Blum and Chawla\n(2001), integrates both perspectives, optimizing\nits labeling of speech segments based on both in-\ndividual speech-segment classification scores and\npreferences for groups of speech segments to re-\nceive the same label. In this section, we discuss\nthe specific classification framework that we adopt\nand the set of mechanisms that we propose for\nmodeling specific types of relationships.\n"},{"#tail":"\n","@confidence":"0.961319571428572","#text":"\nLet s1, s2, . . . , sn be the sequence of speech seg-\nments within a given debate, and let Y and\nN stand for the ?yea? and ?nay? class, respec-\ntively. Assume we have a non-negative func-\ntion ind(s, C) indicating the degree of preference\nthat an individual-document classifier, such as an\nSVM, has for placing speech-segment s in class\nC. Also, assume that some pairs of speech seg-\nments have weighted links between them, where\nthe non-negative strength (weight) str(`) for a\nlink ` indicates the degree to which it is prefer-\nable that the linked speech segments receive the\nsame label. Then, any class assignment c =\nc(s1), c(s2), . . . , c(sn) can be assigned a cost\n"},{"#tail":"\n","@confidence":"0.957195285714286","#text":"\n` between s,s?\nstr(`),\nwhere c(s) is the ?opposite? class from c(s). A\nminimum-cost assignment thus represents an opti-\nmum way to classify the speech segments so that\neach one tends not to be put into the class that\nthe individual-document classifier disprefers, but\nat the same time, highly associated speech seg-\nments tend not to be put in different classes.\nAs has been previously observed and exploited\nin the NLP literature (Pang and Lee, 2004; Agar-\nwal and Bhattacharyya, 2005; Barzilay and Lap-\nata, 2005), the above optimization function, unlike\nmany others that have been proposed for graph or\nset partitioning, can be solved exactly in an prov-\nably efficient manner via methods for finding min-\nimum cuts in graphs. In our view, the contribution\nof our work is the examination of new types of\nrelationships, not the method by which such re-\nlationships are incorporated into the classification\ndecision.\n"},{"#tail":"\n","@confidence":"0.928604666666667","#text":"\nIn our experiments, we employed the well-known\nclassifier SVMlight to obtain individual-document\nclassification scores, treating Y as the positive\nclass and using plain unigrams as features.5 Fol-\nlowing standard practice in sentiment analysis\n(Pang et al, 2002), the input to SVMlight con-\nsisted of normalized presence-of-feature (rather\nthan frequency-of-feature) vectors. The ind value\n5SVMlight is available at svmlight.joachims.org. Default\nparameters were used, although experimentation with differ-\nent parameter settings is an important direction for future\nwork (Daelemans and Hoste, 2002; Munson et al, 2005).\nfor each speech segment s was based on the signed\ndistance d(s) from the vector representing s to the\ntrained SVM decision plane:\n"},{"#tail":"\n","@confidence":"0.994544666666667","#text":"\nwhere ?s is the standard deviation of d(s) over all\nspeech segments s in the debate in question, and\nind(s,N ) def= 1? ind(s,Y).\nWe now turn to the more interesting problem of\nrepresenting the preferences that speech segments\nmay have for being assigned to the same class.\n"},{"#tail":"\n","@confidence":"0.996883733333333","#text":"\nA wide range of relationships between text seg-\nments can be modeled as positive-strength links.\nHere we discuss two types of constraints that are\nconsidered in this work.\nSame-speaker constraints: In Congressional\ndebates and in general social-discourse contexts,\na single speaker may make a number of comments\nregarding a topic. It is reasonable to expect that in\nmany settings, the participants in a discussion may\nbe convinced to change their opinions midway\nthrough a debate. Hence, in the general case we\nwish to be able to express ?soft? preferences for all\nof an author?s statements to receive the same label,\nwhere the strengths of such constraints could, for\ninstance, vary according to the time elapsed be-\ntween the statements. Weighted links are an ap-\npropriate means to express such variation.\nHowever, if we assume that most speakers do\nnot change their positions in the course of a dis-\ncussion, we can conclude that all comments made\nby the same speaker must receive the same label.\nThis assumption holds by fiat for the ground-truth\nlabels in our dataset because these labels were\nderived from the single vote cast by the speaker\non the bill being discussed.6 We can implement\nthis assumption via links whose weights are essen-\ntially infinite. Although one can also implement\nthis assumption via concatenation of same-speaker\nspeech segments (see Section 4.3), we view the\nfact that our graph-based framework incorporates\n"},{"#tail":"\n","@confidence":"0.991489628571429","#text":"\nboth hard and soft constraints in a principled fash-\nion as an advantage of our approach.\nDifferent-speaker agreements In House dis-\ncourse, it is common for one speaker to make ref-\nerence to another in the context of an agreement\nor disagreement over the topic of discussion. The\nsystematic identification of instances of agreement\ncan, as we have discussed, be a powerful tool for\nthe development of intelligently selected weights\nfor links between speech segments.\nThe problem of agreement identification can be\ndecomposed into two sub-problems: identifying\nreferences and their targets, and deciding whether\neach reference represents an instance of agree-\nment. In our case, the first task is straightfor-\nward because we focused solely on by-name ref-\nerences.7 Hence, we will now concentrate on the\nsecond, more interesting task.\nWe approach the problem of classifying refer-\nences by representing each reference with a word-\npresence vector derived from a window of text\nsurrounding the reference.8 In the training set,\nwe classify each reference connecting two speak-\ners with a positive or negative label depending on\nwhether the two voted the same way on the bill un-\nder discussion9. These labels are then used to train\nan SVM classifier, the output of which is subse-\nquently used to create weights on agreement links\nin the test set as follows.\nLet d(r) denote the distance from the vector\nrepresenting reference r to the agreement-detector\nSVM?s decision plane, and let ?r be the standard\ndeviation of d(r) over all references in the debate\nin question. We then define the strength agr of the\nagreement link corresponding to the reference as:\n"},{"#tail":"\n","@confidence":"0.864037","#text":"\nThe free parameter ? specifies the relative impor-\n"},{"#tail":"\n","@confidence":"0.932930666666667","#text":"\nrepresent relationships between speech segments, we ignore\nreferences for which the target of the reference did not speak\nin the debate in which the reference was made.\n"},{"#tail":"\n","@confidence":"0.793738125","#text":"\ncent. ?Amdmts?=?speech segments containing the\nword ?amendment??. Recall that boldface indi-\ncates results for development-set-optimal settings.\ntance of the agr scores. The threshold ?agr con-\ntrols the precision of the agreement links, in that\nvalues of ?agr greater than zero mean that greater\nconfidence is required before an agreement link\ncan be added.10\n"},{"#tail":"\n","@confidence":"0.999654111111111","#text":"\nThis section presents experiments testing the util-\nity of using speech-segment relationships, evalu-\nating against a number of baselines. All reported\nresults use values for the free parameter ? derived\nvia tuning on the development set. In the tables,\nboldface indicates the development- and test-set\nresults for the development-set-optimal parameter\nsettings, as one would make algorithmic choices\nbased on development-set performance.\n"},{"#tail":"\n","@confidence":"0.949852869565217","#text":"\nRecall that to gather inter-speaker agreement in-\nformation, the strategy employed in this paper is\nto classify by-name references to other speakers\nas to whether they indicate agreement or not.\nTo train our agreement classifier, we experi-\nmented with undoing the deletion of amendment-\nrelated speech segments in the training set. Note\nthat such speech segments were never included in\nthe development or test set, since, as discussed in\nSection 2, their labels are probably noisy; how-\never, including them in the training set alows the\nclassifier to examine more instances even though\nsome of them are labeled incorrectly. As Table\n2 shows, using more, if noisy, data yields bet-\nter agreement-classification results on the devel-\nopment set, and so we use that policy in all subse-\nquent experiments.11\n10Our implementation puts a link between just one arbi-\ntrary pair of speech segments among all those uttered by a\ngiven pair of apparently agreeing speakers. The ?infinite-\nweight? same-speaker links propagate the agreement infor-\nmation to all other such pairs.\n11Unfortunately, this policy leads to inferior test-set agree-\n"},{"#tail":"\n","@confidence":"0.99231555","#text":"\nAn important observation is that precision may\nbe more important than accuracy in deciding\nwhich agreement links to add: false positives with\nrespect to agreement can cause speech segments\nto be incorrectly assigned the same label, whereas\nfalse negatives mean only that agreement-based\ninformation about other speech segments is not\nemployed. As described above, we can raise\nagreement precision by increasing the threshold\n?agr, which specifies the required confidence for\nthe addition of an agreement link. Indeed, Table\n3 shows that we can improve agreement precision\nby setting ?agr to the (positive) mean agreement\nscore ? assigned by the SVM agreement-classifier\nover all references in the given debate12. How-\never, this comes at the cost of greatly reducing\nagreement accuracy (development: 64.38%; test:\n66.18%) due to lowered recall levels. Whether\nor not better speech-segment classification is ulti-\nmately achieved is discussed in the next sections.\n"},{"#tail":"\n","@confidence":"0.902579772727273","#text":"\nclassification\nBaselines The first two data rows of Table\n4 depict baseline performance results. The\n#(?support?) ? #(?oppos?) baseline is meant\nto explore whether the speech-segment classifica-\ntion task can be reduced to simple lexical checks.\nSpecifically, this method uses the signed differ-\nence between the number of words containing the\nstem ?support? and the number of words contain-\ning the stem ?oppos? (returning the majority class\nif the difference is 0). No better than 62.67% test-\nset accuracy is obtained by either baseline.\nUsing relationship information Applying an\nSVM to classify each speech segment in isolation\nleads to clear improvements over the two base-\nline methods, as demonstrated in Table 4. When\nwe impose the constraint that all speech segments\nuttered by the same speaker receive the same la-\nbel via ?same-speaker links?, both test-set and\nment classification. Section 4.5 contains further discussion.\n12We elected not to explicitly tune the value of ?agr in or-\nder to minimize the number of free parameters to deal with.\n"},{"#tail":"\n","@confidence":"0.967697055555555","#text":"\ntion accuracy, in percent. Here, the initial SVM is\nrun on the concatenation of all of a given speaker?s\nspeech segments, but the results are computed\nover speech segments (not speakers), so that they\ncan be compared to those in Table 4.\ndevelopment-set accuracy increase even more, in\nthe latter case quite substantially so.\nThe last two lines of Table 4 show that the\nbest results are obtained by incorporating agree-\nment information as well. The highest test-set re-\nsult, 71.16%, is obtained by using a high-precision\nthreshold to determine which agreement links to\nadd. While the development-set results would in-\nduce us to utilize the standard threshold value of 0,\nwhich is sub-optimal on the test set, the ?agr = 0\nagreement-link policy still achieves noticeable im-\nprovement over not using agreement links (test set:\n70.81% vs. 67.21%).\n"},{"#tail":"\n","@confidence":"0.952506333333333","#text":"\nclassification\nWe use speech segments as the unit of classifica-\ntion because they represent natural discourse units.\nAs a consequence, we are able to exploit relation-\nships at the speech-segment level. However, it is\ninteresting to consider whether we really need to\nconsider relationships specifically between speech\nsegments themselves, or whether it suffices to sim-\nply consider relationships between the speakers\n"},{"#tail":"\n","@confidence":"0.999729625","#text":"\nof the speech segments. In particular, as an al-\nternative to using same-speaker links, we tried a\nspeaker-based approach wherein the way we de-\ntermine the initial individual-document classifica-\ntion score for each speech segment uttered by a\nperson p in a given debate is to run an SVM on the\nconcatenation of all of p?s speech segments within\nthat debate. (We also ensure that agreement-link\ninformation is propagated from speech-segment to\nspeaker pairs.)\nHow does the use of same-speaker links com-\npare to the concatenation of each speaker?s speech\nsegments? Tables 4 and 5 show that, not sur-\nprisingly, the SVM individual-document classifier\nworks better on the concatenated speech segments\nthan on the speech segments in isolation. How-\never, the effect on overall classification accuracy\nis less clear: the development set favors same-\nspeaker links over concatenation, while the test set\ndoes not.\nBut we stress that the most important obser-\nvation we can make from Table 5 is that once\nagain, the addition of agreement information leads\nto substantial improvements in accuracy.\n"},{"#tail":"\n","@confidence":"0.999291538461539","#text":"\nRecall that in in our experiments, we created\nfinite-weight agreement links, so that speech seg-\nments appearing in pairs flagged by our (imper-\nfect) agreement detector can potentially receive\ndifferent labels. We also experimented with forc-\ning such speech segments to receive the same la-\nbel, either through infinite-weight agreement links\nor through a speech-segment concatenation strat-\negy similar to that described in the previous sub-\nsection. Both strategies resulted in clear degrada-\ntion in performance on both the development and\ntest sets, a finding that validates our encoding of\nagreement information as ?soft? preferences.\n"},{"#tail":"\n","@confidence":"0.999540333333333","#text":"\nWe have seen several cases in which the method\nthat performs best on the development set does\nnot yield the best test-set performance. However,\nwe felt that it would be illegitimate to change the\ntrain/development/test sets in a post hoc fashion,\nthat is, after seeing the experimental results.\nMoreover, and crucially, it is very clear that\nusing agreement information, encoded as prefer-\nences within our graph-based approach rather than\nas hard constraints, yields substantial improve-\nments on both the development and test set; this,\nwe believe, is our most important finding.\n"},{"#tail":"\n","@confidence":"0.995143413043478","#text":"\nPolitically-oriented text Sentiment analysis has\nspecifically been proposed as a key enabling tech-\nnology in eRulemaking, allowing the automatic\nanalysis of the opinions that people submit (Shul-\nman et al, 2005; Cardie et al, 2006; Kwon et al,\n2006). There has also been work focused upon de-\ntermining the political leaning (e.g., ?liberal? vs.\n?conservative?) of a document or author, where\nmost previously-proposed methods make no di-\nrect use of relationships between the documents to\nbe classified (the ?unlabeled? texts) (Laver et al,\n2003; Efron, 2004; Mullen and Malouf, 2006). An\nexception is Grefenstette et al (2004), who exper-\nimented with determining the political orientation\nof websites essentially by classifying the concate-\nnation of all the documents found on that site.\nOthers have applied the NLP technologies of\nnear-duplicate detection and topic-based text cat-\negorization to politically oriented text (Yang and\nCallan, 2005; Purpura and Hillard, 2006).\nDetecting agreement We used a simple method\nto learn to identify cross-speaker references indi-\ncating agreement. More sophisticated approaches\nhave been proposed (Hillard et al, 2003), in-\ncluding an extension that, in an interesting re-\nversal of our problem, makes use of sentiment-\npolarity indicators within speech segments (Gal-\nley et al, 2004). Also relevant is work on the gen-\neral problems of dialog-act tagging (Stolcke et al,\n2000), citation analysis (Lehnert et al, 1990), and\ncomputational rhetorical analysis (Marcu, 2000;\nTeufel and Moens, 2002).\nWe currently do not have an efficient means\nto encode disagreement information as hard con-\nstraints; we plan to investigate incorporating such\ninformation in future work.\nRelationships between the unlabeled items\nCarvalho and Cohen (2005) consider sequential\nrelations between different types of emails (e.g.,\nbetween requests and satisfactions thereof) to clas-\nsify messages, and thus also explicitly exploit the\nstructure of conversations.\nPrevious sentiment-analysis work in different\ndomains has considered inter-document similar-\nity (Agarwal and Bhattacharyya, 2005; Pang and\nLee, 2005; Goldberg and Zhu, 2006) or explicit\n"},{"#tail":"\n","@confidence":"0.998836263157895","#text":"\ninter-document references in the form of hyper-\nlinks (Agrawal et al, 2003).\nNotable early papers on graph-based semi-\nsupervised learning include Blum and Chawla\n(2001), Bansal et al (2002), Kondor and Lafferty\n(2002), and Joachims (2003). Zhu (2005) main-\ntains a survey of this area.\nRecently, several alternative, often quite sophis-\nticated approaches to collective classification have\nbeen proposed (Neville and Jensen, 2000; Laf-\nferty et al, 2001; Getoor et al, 2002; Taskar et\nal., 2002; Taskar et al, 2003; Taskar et al, 2004;\nMcCallum and Wellner, 2004). It would be inter-\nesting to investigate the application of such meth-\nods to our problem. However, we also believe\nthat our approach has important advantages, in-\ncluding conceptual simplicity and the fact that it is\nbased on an underlying optimization problem that\nis provably and in practice easy to solve.\n"},{"#tail":"\n","@confidence":"0.985554466666667","#text":"\nIn this study, we focused on very general types\nof cross-document classification preferences, uti-\nlizing constraints based only on speaker identity\nand on direct textual references between state-\nments. We showed that the integration of even\nvery limited information regarding inter-document\nrelationships can significantly increase the accu-\nracy of support/opposition classification.\nThe simple constraints modeled in our study,\nhowever, represent just a small portion of the\nrich network of relationships that connect state-\nments and speakers across the political universe\nand in the wider realm of opinionated social dis-\ncourse. One intriguing possibility is to take ad-\nvantage of (readily identifiable) information re-\ngarding interpersonal relationships, making use of\nspeaker/author affiliations, positions within a so-\ncial hierarchy, and so on. Or, we could even at-\ntempt to model relationships between topics or\nconcepts, in a kind of extension of collaborative\nfiltering. For example, perhaps we could infer that\ntwo speakers sharing a common opinion on evo-\nlutionary biologist Richard Dawkins (a.k.a. ?Dar-\nwin?s rottweiler?) will be likely to agree in a de-\nbate centered on Intelligent Design. While such\nfunctionality is well beyond the scope of our cur-\nrent study, we are optimistic that we can develop\nmethods to exploit additional types of relation-\nships in future work.\nAcknowledgments We thank Claire Cardie, Jon\nKleinberg, Michael Macy, Andrew Myers, and the\nsix anonymous EMNLP referees for valuable dis-\ncussions and comments. We also thank Reviewer\n1 for generously providing additional post hoc\nfeedback, and the EMNLP chairs Eric Gaussier\nand Dan Jurafsky for facilitating the process (as\nwell as for allowing authors an extra proceedings\npage. . .). This paper is based upon work sup-\nported in part by the National Science Founda-\ntion under grant no. IIS-0329064. Any opinions,\nfindings, and conclusions or recommendations ex-\npressed are those of the authors and do not neces-\nsarily reflect the views or official policies, either\nexpressed or implied, of any sponsoring institu-\ntions, the U.S. government, or any other entity.\n"},{"#tail":"\n","@confidence":"0.947129","#text":"\nA. Agarwal, P. Bhattacharyya. 2005. Sentiment anal-\nysis: A new approach for effective use of linguis-\ntic knowledge and exploiting similarities in a set of\ndocuments to be classified. In Proceedings of the\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.999345","#text":"\nDepartment of Computer Science, Cornell University\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.990405","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998062","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.995212","@genericHeader":"introduction","#text":"\n2 Corpus\n"},{"#tail":"\n","@confidence":"0.995147","@genericHeader":"method","#text":"\n3 Method\n"},{"#tail":"\n","@confidence":"0.996717","@genericHeader":"method","#text":"\n4 Evaluation\n"},{"#tail":"\n","@confidence":"0.998593","@genericHeader":"method","#text":"\n5 Related work\n"},{"#tail":"\n","@confidence":"0.956304","@genericHeader":"conclusions","#text":"\n6 Conclusion and future work\n"},{"#tail":"\n","@confidence":"0.911905","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.995375","#text":"\nTable 1: Corpus statistics.\n"},{"#tail":"\n","@confidence":"0.86306","#text":"\nTable 2: Agreement-classifier accuracy, in per-\n"},{"#tail":"\n","@confidence":"0.994534","#text":"\nTable 3: Agreement-classifier precision.\n"},{"#tail":"\n","@confidence":"0.9724885","#text":"\nTable 4: Segment-based speech-segment classifi-\ncation accuracy, in percent.\n"},{"#tail":"\n","@confidence":"0.987657","#text":"\nTable 5: Speaker-based speech-segment classifica-\n"}],"page":[{"#tail":"\n","@confidence":"0.999628","#text":"\n327\n"},{"#tail":"\n","@confidence":"0.9991","#text":"\n328\n"},{"#tail":"\n","@confidence":"0.996206","#text":"\n329\n"},{"#tail":"\n","@confidence":"0.983821","#text":"\n330\n"},{"#tail":"\n","@confidence":"0.996308","#text":"\n331\n"},{"#tail":"\n","@confidence":"0.993209","#text":"\n332\n"},{"#tail":"\n","@confidence":"0.996235","#text":"\n333\n"},{"#tail":"\n","@confidence":"0.995769","#text":"\n334\n"},{"#tail":"\n","@confidence":"0.999269","#text":"\n335\n"}],"table":[{"#tail":"\n","@confidence":"0.690934","#text":"\ntotal train test development\nspeech segments 3857 2740 860 257\ndebates 53 38 10 5\naverage number of speech segments per debate 72.8 72.1 86.0 51.4\naverage number of speakers per debate 32.1 30.9 41.1 22.6\n"},{"#tail":"\n","@confidence":"0.640944","#text":"\nmajority baseline 81.51 80.26\nTrain: no amdmts; ?agr = 0 84.25 81.07\nTrain: with amdmts; ?agr = 0 86.99 80.10\n"},{"#tail":"\n","@confidence":"0.728688","#text":"\nAgreement classifier Precision (in percent):\nDevel. set Test set\n?agr = 0 86.23 82.55\n?agr = ? 89.41 88.47\n"},{"#tail":"\n","@confidence":"0.959837142857143","#text":"\nmajority baseline 54.09 58.37\n#(?support?)?#(?oppos?) 59.14 62.67\nSVM [speech segment] 70.04 66.05\nSVM + same-speaker links 79.77 67.21\nSVM + same-speaker links . . .\n+ agreement links, ?agr = 0 89.11 70.81\n+ agreement links, ?agr = ? 87.94 71.16\n"}],"email":{"#tail":"\n","@confidence":"0.971392","#text":"\nmattthomas84@gmail.com, pabo@cs.cornell.edu, llee@cs.cornell.edu\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.484752","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.916089333333333","#text":"Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327?335, Sydney, July 2006. c?2006 Association for Computational Linguistics Get out the vote: Determining support or opposition from Congressional"},"address":{"#tail":"\n","@confidence":"0.999461","#text":"Ithaca, NY 14853-7501"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.999997","#text":"Department of Computer Science, Cornell University"},"author":[{"#tail":"\n","@confidence":"0.999602","#text":"Matt Thomas"},{"#tail":"\n","@confidence":"0.999602","#text":"Bo Pang"},{"#tail":"\n","@confidence":"0.999602","#text":"Lillian Lee"}],"abstract":{"#tail":"\n","@confidence":"0.9910045625","#text":"We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation."},"title":{"#tail":"\n","@confidence":"0.75955","#text":"floor-debate transcripts"},"email":{"#tail":"\n","@confidence":"0.998409","#text":"mattthomas84@gmail.com,pabo@cs.cornell.edu,llee@cs.cornell.edu"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"A. Agarwal, P. Bhattacharyya. 2005. Sentiment analysis: A new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified. In Proceedings of the International Conference on Natural Language Processing (ICON)."},"#text":"\n","marker":{"#tail":"\n","#text":"Agarwal, Bhattacharyya, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"d Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easil","@endWordPosition":"825","@position":"5482","annotationId":"T1","@startWordPosition":"822","@citStr":"Agarwal and Bhattacharyya, 2005"},{"#tail":"\n","#text":"ents receive the same label. Then, any class assignment c = c(s1), c(s2), . . . , c(sn) can be assigned a cost ? s ind(s, c(s))+ ? s,s?: c(s) 6=c(s?) ? ` between s,s? str(`), where c(s) is the ?opposite? class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y","@endWordPosition":"2201","@position":"14213","annotationId":"T2","@startWordPosition":"2197","@citStr":"Agarwal and Bhattacharyya, 2005"},{"#tail":"\n","#text":"onal rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum an","@endWordPosition":"4809","@position":"30848","annotationId":"T3","@startWordPosition":"4806","@citStr":"Agarwal and Bhattacharyya, 2005"}]},"title":{"#tail":"\n","#text":"Sentiment analysis: A new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the International Conference on Natural Language Processing (ICON)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Agarwal"},{"#tail":"\n","#text":"P Bhattacharyya"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"R. Agrawal, S. Rajagopalan, R. Srikant, Y. Xu. 2003. Mining newsgroups using networks arising from social behavior. In Proceedings of WWW, 529?535."},"#text":"\n","pages":{"#tail":"\n","#text":"529--535"},"marker":{"#tail":"\n","#text":"Agrawal, Rajagopalan, Srikant, Xu, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ple, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data. Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants? political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (?I second that!?) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf. Agrawal et al (2003)). Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text. Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually. Intuition suggests that this is true of the data with which we experiment, for several reasons. First, U.S. congressional debates contain very rich language and cover an extremely wide variety of topics, ranging from ","@endWordPosition":"991","@position":"6565","annotationId":"T4","@startWordPosition":"988","@citStr":"Agrawal et al (2003)"},{"#tail":"\n","#text":"hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that ","@endWordPosition":"4832","@position":"30983","annotationId":"T5","@startWordPosition":"4829","@citStr":"Agrawal et al, 2003"}]},"title":{"#tail":"\n","#text":"Mining newsgroups using networks arising from social behavior."},"booktitle":{"#tail":"\n","#text":"In Proceedings of WWW,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Agrawal"},{"#tail":"\n","#text":"S Rajagopalan"},{"#tail":"\n","#text":"R Srikant"},{"#tail":"\n","#text":"Y Xu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"N. Bansal, A. Blum, S. Chawla. 2002. Correlation clustering. In Proceedings of the Symposium on Foundations of Computer Science (FOCS), 238? 247. Journal version in Machine Learning Journal, special issue on theoretical advances in data clustering, 56(1-3):89?113 (2004)."},"#text":"\n","pages":{"#tail":"\n","#text":"56--1"},"marker":{"#tail":"\n","#text":"Bansal, Blum, Chawla, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underl","@endWordPosition":"4849","@position":"31096","annotationId":"T6","@startWordPosition":"4846","@citStr":"Bansal et al (2002)"}},"title":{"#tail":"\n","#text":"Correlation clustering."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Symposium on Foundations of Computer Science (FOCS), 238? 247. Journal version in Machine Learning Journal, special issue on theoretical advances in data clustering,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"N Bansal"},{"#tail":"\n","#text":"A Blum"},{"#tail":"\n","#text":"S Chawla"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"R. Barzilay, M. Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of HLT/EMNLP, 331?338."},"#text":"\n","pages":{"#tail":"\n","#text":"331--338"},"marker":{"#tail":"\n","#text":"Barzilay, Lapata, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":", any class assignment c = c(s1), c(s2), . . . , c(sn) can be assigned a cost ? s ind(s, c(s))+ ? s,s?: c(s) 6=c(s?) ? ` between s,s? str(`), where c(s) is the ?opposite? class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and u","@endWordPosition":"2206","@position":"14241","annotationId":"T7","@startWordPosition":"2202","@citStr":"Barzilay and Lapata, 2005"}},"title":{"#tail":"\n","#text":"Collective content selection for concept-to-text generation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT/EMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Barzilay"},{"#tail":"\n","#text":"M Lapata"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"A. Blum, S. Chawla. 2001. Learning from labeled and unlabeled data using graph mincuts. In Proceedings of ICML, 19?26."},"#text":"\n","marker":{"#tail":"\n","#text":"Blum, Chawla, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"e set because our goal is to examine classification of speech segments in the context of the surrounding discussion. 3 Method The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation. As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate. Our classification framework, directly inspired by Blum and Chawla (2001), integrates both perspectives, optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label. In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships. 329 3.1 Classification framework Let s1, s2, . . . , sn be the sequence of speech segments within a given debate, and let Y and N stand for the ?yea? and ?nay? class, respectively. Assume we have a non-negative function in","@endWordPosition":"1928","@position":"12620","annotationId":"T8","@startWordPosition":"1925","@citStr":"Blum and Chawla (2001)"},{"#tail":"\n","#text":"lationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it ","@endWordPosition":"4845","@position":"31075","annotationId":"T9","@startWordPosition":"4842","@citStr":"Blum and Chawla (2001)"}]},"title":{"#tail":"\n","#text":"Learning from labeled and unlabeled data using graph mincuts."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ICML,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Blum"},{"#tail":"\n","#text":"S Chawla"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"C. Cardie, C. Farina, T. Bruce, E. Wagner. 2006. Using natural language processing to improve eRulemaking. In Proceedings of Digital Government Research (dg.o)."},"#text":"\n","marker":{"#tail":"\n","#text":"Cardie, Farina, Bruce, Wagner, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"evelopment/test sets in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of nea","@endWordPosition":"4535","@position":"28990","annotationId":"T10","@startWordPosition":"4532","@citStr":"Cardie et al, 2006"}},"title":{"#tail":"\n","#text":"Using natural language processing to improve eRulemaking."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Digital Government Research (dg.o)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"C Cardie"},{"#tail":"\n","#text":"C Farina"},{"#tail":"\n","#text":"T Bruce"},{"#tail":"\n","#text":"E Wagner"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"V. Carvalho, W. W. Cohen. 2005. On the collective classification of email ?speech acts?. In Proceedings of SIGIR, 345?352."},"#text":"\n","pages":{"#tail":"\n","#text":"345--352"},"marker":{"#tail":"\n","#text":"Carvalho, Cohen, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"llard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty ","@endWordPosition":"4767","@position":"30518","annotationId":"T11","@startWordPosition":"4764","@citStr":"Carvalho and Cohen (2005)"}},"title":{"#tail":"\n","#text":"On the collective classification of email ?speech acts?."},"booktitle":{"#tail":"\n","#text":"In Proceedings of SIGIR,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"V Carvalho"},{"#tail":"\n","#text":"W W Cohen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"W. Daelemans, V. Hoste. 2002. Evaluation of machine learning methods for natural language processing tasks. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), 755?760."},"#text":"\n","pages":{"#tail":"\n","#text":"755--760"},"marker":{"#tail":"\n","#text":"Daelemans, Hoste, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ing speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al, 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def= ? ??? ??? 1 d(s) > 2?s;( 1 + d(s)2?s ) /2 |d(s)| ? 2?s; 0 d(s) < ?2?s where ?s is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s,N ) def= 1? ind(s,Y). We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class. 3.3 Relationships between speech segments A wide range of relationships between text segme","@endWordPosition":"2357","@position":"15275","annotationId":"T12","@startWordPosition":"2354","@citStr":"Daelemans and Hoste, 2002"}},"title":{"#tail":"\n","#text":"Evaluation of machine learning methods for natural language processing tasks."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W Daelemans"},{"#tail":"\n","#text":"V Hoste"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"S. Das, M. Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA)."},"#text":"\n","marker":{"#tail":"\n","#text":"Das, Chen, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most inte","@endWordPosition":"783","@position":"5184","annotationId":"T13","@startWordPosition":"780","@citStr":"Das and Chen, 2001"}},"title":{"#tail":"\n","#text":"Yahoo! for Amazon: Extracting market sentiment from stock message boards."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Das"},{"#tail":"\n","#text":"M Chen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"K. Dave, S. Lawrence, D. M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of WWW, 519?528."},"#text":"\n","pages":{"#tail":"\n","#text":"519--528"},"marker":{"#tail":"\n","#text":"Dave, Lawrence, Pennock, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ithin the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we ","@endWordPosition":"793","@position":"5235","annotationId":"T14","@startWordPosition":"790","@citStr":"Dave et al, 2003"}},"title":{"#tail":"\n","#text":"Mining the peanut gallery: Opinion extraction and semantic classification of product reviews."},"booktitle":{"#tail":"\n","#text":"In Proceedings of WWW,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Dave"},{"#tail":"\n","#text":"S Lawrence"},{"#tail":"\n","#text":"D M Pennock"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"M. Efron. 2004. Cultural orientation: Classifying subjective documents by cociation [sic] analysis. In Proceedings of the AAAI Fall Symposium on Style and Meaning in Language, Art, Music, and Design, 41?48."},"#text":"\n","pages":{"#tail":"\n","#text":"41--48"},"marker":{"#tail":"\n","#text":"Efron, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), i","@endWordPosition":"4586","@position":"29314","annotationId":"T15","@startWordPosition":"4585","@citStr":"Efron, 2004"}},"title":{"#tail":"\n","#text":"Cultural orientation: Classifying subjective documents by cociation [sic] analysis."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the AAAI Fall Symposium on Style and Meaning in Language, Art, Music, and Design,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Efron"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"note":{"#tail":"\n","#text":"Sentiment classification bibliography. liinwww.ira.uka.de/bibliography/Misc/Sentiment.html."},"rawString":{"#tail":"\n","#text":"A. Esuli. 2006. Sentiment classification bibliography. liinwww.ira.uka.de/bibliography/Misc/Sentiment.html."},"#text":"\n","marker":{"#tail":"\n","#text":"Esuli, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005;","@endWordPosition":"740","@position":"4883","annotationId":"T16","@startWordPosition":"739","@citStr":"Esuli (2006)"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Esuli"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"M. Galley, K. McKeown, J. Hirschberg, E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use of Bayesian networks to model pragmatic dependencies. In Proceedings of the 42nd ACL, 669?676."},"#text":"\n","pages":{"#tail":"\n","#text":"669--676"},"marker":{"#tail":"\n","#text":"Galley, McKeown, Hirschberg, Shriberg, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also ","@endWordPosition":"4700","@position":"30070","annotationId":"T17","@startWordPosition":"4696","@citStr":"Galley et al, 2004"}},"title":{"#tail":"\n","#text":"Identifying agreement and disagreement in conversational speech: Use of Bayesian networks to model pragmatic dependencies."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 42nd ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Galley"},{"#tail":"\n","#text":"K McKeown"},{"#tail":"\n","#text":"J Hirschberg"},{"#tail":"\n","#text":"E Shriberg"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"L. Getoor, N. Friedman, D. Koller, B. Taskar. 2002. Learning probabilistic models of relational structure. Journal of Machine Learning Research, 3:679?707. Special issue on the Eighteenth ICML."},"journal":{"#tail":"\n","#text":"Journal of Machine Learning Research,"},"pages":{"#tail":"\n","#text":"3--679"},"#text":"\n","marker":{"#tail":"\n","#text":"Getoor, Friedman, Koller, Taskar, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual refere","@endWordPosition":"4892","@position":"31374","annotationId":"T18","@startWordPosition":"4889","@citStr":"Getoor et al, 2002"}},"title":{"#tail":"\n","#text":"Learning probabilistic models of relational structure."},"booktitle":{"#tail":"\n","#text":"Special issue on the Eighteenth ICML."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L Getoor"},{"#tail":"\n","#text":"N Friedman"},{"#tail":"\n","#text":"D Koller"},{"#tail":"\n","#text":"B Taskar"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"A. B. Goldberg, J. Zhu. 2006. Seeing stars when there aren?t many stars: Graph-based semisupervised learning for sentiment categorization. In TextGraphs: HLT/NAACL Workshop on Graphbased Algorithms for Natural Language Processing."},"#text":"\n","marker":{"#tail":"\n","#text":"Goldberg, Zhu, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"graphy). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), w","@endWordPosition":"833","@position":"5527","annotationId":"T19","@startWordPosition":"830","@citStr":"Goldberg and Zhu, 2006"},{"#tail":"\n","#text":"ns, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to ","@endWordPosition":"4817","@position":"30893","annotationId":"T20","@startWordPosition":"4814","@citStr":"Goldberg and Zhu, 2006"}]},"title":{"#tail":"\n","#text":"Seeing stars when there aren?t many stars: Graph-based semisupervised learning for sentiment categorization."},"booktitle":{"#tail":"\n","#text":"In TextGraphs: HLT/NAACL Workshop on Graphbased Algorithms for Natural Language Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A B Goldberg"},{"#tail":"\n","#text":"J Zhu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"G. Grefenstette, Y. Qu, J. G. Shanahan, D. A. Evans. 2004. Coupling niche browsers and affect analysis for an opinion mining application. In Proceedings of RIAO."},"#text":"\n","marker":{"#tail":"\n","#text":"Grefenstette, Qu, Shanahan, Evans, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ork Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem","@endWordPosition":"4597","@position":"29383","annotationId":"T21","@startWordPosition":"4594","@citStr":"Grefenstette et al (2004)"}},"title":{"#tail":"\n","#text":"Coupling niche browsers and affect analysis for an opinion mining application."},"booktitle":{"#tail":"\n","#text":"In Proceedings of RIAO."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"G Grefenstette"},{"#tail":"\n","#text":"Y Qu"},{"#tail":"\n","#text":"J G Shanahan"},{"#tail":"\n","#text":"D A Evans"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1992"},"editor":{"#tail":"\n","#text":"In P. Jacobs, ed.,"},"rawString":{"#tail":"\n","#text":"M. Hearst. 1992. Direction-based text interpretation as an information access refinement. In P. Jacobs, ed., Text-Based Intelligent Systems, 257?274. Lawrence Erlbaum Associates."},"#text":"\n","marker":{"#tail":"\n","#text":"Hearst, 1992"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the text","@endWordPosition":"732","@position":"4834","annotationId":"T22","@startWordPosition":"731","@citStr":"Hearst (1992)"}},"title":{"#tail":"\n","#text":"Direction-based text interpretation as an information access refinement."},"booktitle":{"#tail":"\n","#text":"Text-Based Intelligent Systems, 257?274. Lawrence Erlbaum Associates."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Hearst"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"D. Hillard, M. Ostendorf, E. Shriberg. 2003. Detection of agreement vs. disagreement in meetings: Training with unlabeled data. In Proceedings of HLTNAACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Hillard, Ostendorf, Shriberg, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"t al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen","@endWordPosition":"4673","@position":"29911","annotationId":"T23","@startWordPosition":"4670","@citStr":"Hillard et al, 2003"}},"title":{"#tail":"\n","#text":"Detection of agreement vs. disagreement in meetings: Training with unlabeled data."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLTNAACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Hillard"},{"#tail":"\n","#text":"M Ostendorf"},{"#tail":"\n","#text":"E Shriberg"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"T. Joachims. 2003. Transductive learning via spectral graph partitioning. In Proceedings of ICML, 290? 297."},"#text":"\n","pages":{"#tail":"\n","#text":"290--297"},"marker":{"#tail":"\n","#text":"Joachims, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"quential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in","@endWordPosition":"4856","@position":"31145","annotationId":"T24","@startWordPosition":"4855","@citStr":"Joachims (2003)"}},"title":{"#tail":"\n","#text":"Transductive learning via spectral graph partitioning."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ICML,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"T Joachims"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"R. I. Kondor, J. D. Lafferty. 2002. Diffusion kernels on graphs and other discrete input spaces. In Proceedings of ICML, 315?322."},"#text":"\n","pages":{"#tail":"\n","#text":"315--322"},"marker":{"#tail":"\n","#text":"Kondor, Lafferty, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"lho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem th","@endWordPosition":"4853","@position":"31124","annotationId":"T25","@startWordPosition":"4850","@citStr":"Kondor and Lafferty (2002)"}},"title":{"#tail":"\n","#text":"Diffusion kernels on graphs and other discrete input spaces."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ICML,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R I Kondor"},{"#tail":"\n","#text":"J D Lafferty"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"N. Kwon, S. Shulman, E. Hovy. 2006. Multidimensional text analysis for eRulemaking. In Proceedings of Digital Government Research (dg.o)."},"#text":"\n","marker":{"#tail":"\n","#text":"Kwon, Shulman, Hovy, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detecti","@endWordPosition":"4539","@position":"29009","annotationId":"T26","@startWordPosition":"4536","@citStr":"Kwon et al, 2006"}},"title":{"#tail":"\n","#text":"Multidimensional text analysis for eRulemaking."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Digital Government Research (dg.o)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"N Kwon"},{"#tail":"\n","#text":"S Shulman"},{"#tail":"\n","#text":"E Hovy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"J. Lafferty, A. McCallum, F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, 282?289."},"#text":"\n","pages":{"#tail":"\n","#text":"282--289"},"marker":{"#tail":"\n","#text":"Lafferty, McCallum, Pereira, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"iment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on d","@endWordPosition":"4888","@position":"31354","annotationId":"T27","@startWordPosition":"4884","@citStr":"Lafferty et al, 2001"}},"title":{"#tail":"\n","#text":"Conditional random fields: Probabilistic models for segmenting and labeling sequence data."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ICML,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Lafferty"},{"#tail":"\n","#text":"A McCallum"},{"#tail":"\n","#text":"F Pereira"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"M. Laver, K. Benoit, J. Garry. 2003. Extracting policy positions from political texts using words as data. American Political Science Review."},"#text":"\n","marker":{"#tail":"\n","#text":"Laver, Benoit, Garry, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et","@endWordPosition":"4584","@position":"29301","annotationId":"T28","@startWordPosition":"4581","@citStr":"Laver et al, 2003"}},"title":{"#tail":"\n","#text":"Extracting policy positions from political texts using words as data. American Political Science Review."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Laver"},{"#tail":"\n","#text":"K Benoit"},{"#tail":"\n","#text":"J Garry"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"W. Lehnert, C. Cardie, E. Riloff. 1990. Analyzing research papers using citation sentences. In Program of the Twelfth Annual Conference of the Cognitive Science Society, 511?18."},"#text":"\n","pages":{"#tail":"\n","#text":"511--18"},"marker":{"#tail":"\n","#text":"Lehnert, Cardie, Riloff, 1990"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-docume","@endWordPosition":"4722","@position":"30202","annotationId":"T29","@startWordPosition":"4719","@citStr":"Lehnert et al, 1990"}},"title":{"#tail":"\n","#text":"Analyzing research papers using citation sentences."},"booktitle":{"#tail":"\n","#text":"In Program of the Twelfth Annual Conference of the Cognitive Science Society,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W Lehnert"},{"#tail":"\n","#text":"C Cardie"},{"#tail":"\n","#text":"E Riloff"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"D. Marcu. 2000. The theory and practice of discourse parsing and summarization. MIT Press."},"#text":"\n","marker":{"#tail":"\n","#text":"Marcu, 2000"},"publisher":{"#tail":"\n","#text":"MIT Press."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tion to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang","@endWordPosition":"4728","@position":"30254","annotationId":"T30","@startWordPosition":"4727","@citStr":"Marcu, 2000"}},"title":{"#tail":"\n","#text":"The theory and practice of discourse parsing and summarization."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Marcu"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"A. McCallum, B. Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Proceedings of NIPS."},"#text":"\n","marker":{"#tail":"\n","#text":"McCallum, Wellner, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"aryya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that the integration of even very limited information r","@endWordPosition":"4908","@position":"31464","annotationId":"T31","@startWordPosition":"4905","@citStr":"McCallum and Wellner, 2004"}},"title":{"#tail":"\n","#text":"Conditional models of identity uncertainty with application to noun coreference."},"booktitle":{"#tail":"\n","#text":"In Proceedings of NIPS."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A McCallum"},{"#tail":"\n","#text":"B Wellner"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"T. Mullen, R. Malouf. 2006. A preliminary investigation into sentiment analysis of informal political discourse. In Proceedings of the AAAI Symposium on Computational Approaches to Analyzing Weblogs, 159?162."},"#text":"\n","pages":{"#tail":"\n","#text":"159--162"},"marker":{"#tail":"\n","#text":"Mullen, Malouf, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"em-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data. Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants? political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (?I second that!?) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf. Agrawal et al (2003)). Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text. Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually. Intuition suggests that this is true of the data with which we experiment, for several reasons. First, U.S. congressional debates contain very rich language and cover an extremely wide var","@endWordPosition":"985","@position":"6536","annotationId":"T32","@startWordPosition":"982","@citStr":"Mullen and Malouf (2006)"},{"#tail":"\n","#text":"is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that","@endWordPosition":"4590","@position":"29340","annotationId":"T33","@startWordPosition":"4587","@citStr":"Mullen and Malouf, 2006"}]},"title":{"#tail":"\n","#text":"A preliminary investigation into sentiment analysis of informal political discourse."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the AAAI Symposium on Computational Approaches to Analyzing Weblogs,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"T Mullen"},{"#tail":"\n","#text":"R Malouf"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"A. Munson, C. Cardie, R. Caruana. 2005. Optimizing to arbitrary NLP metrics using ensemble selection. In Proceedings of HLT-EMNLP, 539?546."},"#text":"\n","pages":{"#tail":"\n","#text":"539--546"},"marker":{"#tail":"\n","#text":"Munson, Cardie, Caruana, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al, 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def= ? ??? ??? 1 d(s) > 2?s;( 1 + d(s)2?s ) /2 |d(s)| ? 2?s; 0 d(s) < ?2?s where ?s is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s,N ) def= 1? ind(s,Y). We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class. 3.3 Relationships between speech segments A wide range of relationships between text segments can be modeled as","@endWordPosition":"2361","@position":"15296","annotationId":"T34","@startWordPosition":"2358","@citStr":"Munson et al, 2005"}},"title":{"#tail":"\n","#text":"Optimizing to arbitrary NLP metrics using ensemble selection."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT-EMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Munson"},{"#tail":"\n","#text":"C Cardie"},{"#tail":"\n","#text":"R Caruana"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"J. Neville, D. Jensen. 2000. Iterative classification in relational data. In Proceedings of the AAAI Workshop on Learning Statistical Models from Relational Data, 13?20."},"#text":"\n","pages":{"#tail":"\n","#text":"13--20"},"marker":{"#tail":"\n","#text":"Neville, Jensen, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on spe","@endWordPosition":"4883","@position":"31332","annotationId":"T35","@startWordPosition":"4880","@citStr":"Neville and Jensen, 2000"}},"title":{"#tail":"\n","#text":"Iterative classification in relational data."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the AAAI Workshop on Learning Statistical Models from Relational Data,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Neville"},{"#tail":"\n","#text":"D Jensen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"B. Pang, L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the ACL, 271?278."},"#text":"\n","pages":{"#tail":"\n","#text":"271--278"},"marker":{"#tail":"\n","#text":"Pang, Lee, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e linked speech segments receive the same label. Then, any class assignment c = c(s1), c(s2), . . . , c(sn) can be assigned a cost ? s ind(s, c(s))+ ? s,s?: c(s) 6=c(s?) ? ` between s,s? str(`), where c(s) is the ?opposite? class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document ","@endWordPosition":"2196","@position":"14180","annotationId":"T36","@startWordPosition":"2193","@citStr":"Pang and Lee, 2004"}},"title":{"#tail":"\n","#text":"A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"B Pang"},{"#tail":"\n","#text":"L Lee"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"B. Pang, L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Pang, Lee, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via t","@endWordPosition":"829","@position":"5502","annotationId":"T37","@startWordPosition":"826","@citStr":"Pang and Lee, 2005"},{"#tail":"\n","#text":"2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It","@endWordPosition":"4813","@position":"30868","annotationId":"T38","@startWordPosition":"4810","@citStr":"Pang and Lee, 2005"}]},"title":{"#tail":"\n","#text":"Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"B Pang"},{"#tail":"\n","#text":"L Lee"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP, 79?86."},"#text":"\n","pages":{"#tail":"\n","#text":"79--86"},"marker":{"#tail":"\n","#text":"Pang, Lee, Vaithyanathan, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"aker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniqu","@endWordPosition":"787","@position":"5202","annotationId":"T39","@startWordPosition":"784","@citStr":"Pang et al, 2002"},{"#tail":"\n","#text":"ph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al, 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def= ? ??? ??? 1 d(s) > 2?s;( 1 + d(s)2?s ) /2 |d(s)| ? 2?s; 0 d(s) < ?2?s where ?s is the standard deviat","@endWordPosition":"2314","@position":"14943","annotationId":"T40","@startWordPosition":"2311","@citStr":"Pang et al, 2002"}]},"title":{"#tail":"\n","#text":"Thumbs up? Sentiment classification using machine learning techniques."},"booktitle":{"#tail":"\n","#text":"In Proceedings of EMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"B Pang"},{"#tail":"\n","#text":"L Lee"},{"#tail":"\n","#text":"S Vaithyanathan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"S. Purpura, D. Hillard. 2006. Automated classification of congressional legislation. In Proceedings of Digital Government Research (dg.o)."},"#text":"\n","marker":{"#tail":"\n","#text":"Purpura, Hillard, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to","@endWordPosition":"4647","@position":"29727","annotationId":"T41","@startWordPosition":"4644","@citStr":"Purpura and Hillard, 2006"}},"title":{"#tail":"\n","#text":"Automated classification of congressional legislation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Digital Government Research (dg.o)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Purpura"},{"#tail":"\n","#text":"D Hillard"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"W. Sack. 1994. On the computation of point of view."},"#text":"\n","marker":{"#tail":"\n","#text":"Sack, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"or or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be label","@endWordPosition":"734","@position":"4847","annotationId":"T42","@startWordPosition":"733","@citStr":"Sack (1994)"}},"title":{"#tail":"\n","#text":"On the computation of point of view."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W Sack"}}},{"#tail":"\n","note":{"#tail":"\n","#text":"Student abstract."},"rawString":{"#tail":"\n","#text":"In Proceedings of AAAI, pg. 1488. Student abstract."},"#text":"\n","pages":{"#tail":"\n","#text":"1488"},"marker":{"#tail":"\n"},"booktitle":{"#tail":"\n","#text":"In Proceedings of AAAI,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"S. Shulman, D. Schlosberg. 2002. Electronic rulemaking: New frontiers in public participation. Prepared for the Annual Meeting of the American Political Science Association."},"#text":"\n","marker":{"#tail":"\n","#text":"Shulman, Schlosberg, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"abase (http://thomas.loc.gov) of congresonline accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially society-changing effect. In the United States, for example, governmental bodies are providing and soliciting political documents via the Internet, with lofty goals in mind: electronic rulemaking (eRulemaking) initiatives involving the ?electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process?, may ?[alter] the citizen-government relationship? (Shulman and Schlosberg, 2002). Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics2, or at least on political journalism3. Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process. Evaluative and persuasive documents, such as a politician?s speech regarding a bill or a blogger?s commentary on a legislative proposal, form a particularly interesting type o","@endWordPosition":"368","@position":"2507","annotationId":"T43","@startWordPosition":"365","@citStr":"Shulman and Schlosberg, 2002"}},"title":{"#tail":"\n","#text":"Electronic rulemaking: New frontiers in public participation. Prepared for the Annual Meeting of the American Political Science Association."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Shulman"},{"#tail":"\n","#text":"D Schlosberg"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"S. Shulman, J. Callan, E. Hovy, S. Zavestoski. 2005. Language processing technologies for electronic rulemaking: A project highlight. In Proceedings of Digital Government Research (dg.o), 87?88."},"#text":"\n","pages":{"#tail":"\n","#text":"87--88"},"marker":{"#tail":"\n","#text":"Shulman, Callan, Hovy, Zavestoski, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"to change the train/development/test sets in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP","@endWordPosition":"4531","@position":"28970","annotationId":"T44","@startWordPosition":"4527","@citStr":"Shulman et al, 2005"}},"title":{"#tail":"\n","#text":"Language processing technologies for electronic rulemaking: A project highlight."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Digital Government Research (dg.o),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Shulman"},{"#tail":"\n","#text":"J Callan"},{"#tail":"\n","#text":"E Hovy"},{"#tail":"\n","#text":"S Zavestoski"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"note":{"#tail":"\n","#text":"fourth edition."},"rawString":{"#tail":"\n","#text":"S. S. Smith, J. M. Roberts, R. J. Vander Wielen. 2005. The American Congress. Cambridge University Press, fourth edition."},"#text":"\n","marker":{"#tail":"\n","#text":"Smith, Roberts, Wielen, 2005"},"publisher":{"#tail":"\n","#text":"Cambridge University Press,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ant to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process. Evaluative and persuasive documents, such as a politician?s speech regarding a bill or a blogger?s commentary on a legislative proposal, form a particularly interesting type of politically oriented text. People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that (U.S.) bills often reach several hundred pages in length (Smith et al, 2005). Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist. 2E.g., ?Internet injects sweeping change into U.S. politics?, Adam Nagourney, The New York Times, April 2, 2006. 3E.g., ?The End of News??, Michael Massing, The New York Review of Books, December 1, 2005. 327 plicitly solicited in the eRulemaking scenario. In the analysis of evaluative language, it is fundamentally necessary to determine whether the author/speaker supports or disapproves of the topic of discussion. In this pap","@endWordPosition":"509","@position":"3399","annotationId":"T45","@startWordPosition":"506","@citStr":"Smith et al, 2005"}},"title":{"#tail":"\n","#text":"The American Congress."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S S Smith"},{"#tail":"\n","#text":"J M Roberts"},{"#tail":"\n","#text":"R J Vander Wielen"}]}},{"volume":{"#tail":"\n","#text":"26"},"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van EssDykema, K. Ries, E. Shriberg, D. Jurafsky, R. Martin, M. Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339?373."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Stolcke, Coccaro, Bates, Taylor, Van EssDykema, Ries, Shriberg, Jurafsky, Martin, Meteer, 2000"},"title":{"#tail":"\n","#text":"Dialogue act modeling for automatic tagging and recognition of conversational speech."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Stolcke"},{"#tail":"\n","#text":"N Coccaro"},{"#tail":"\n","#text":"R Bates"},{"#tail":"\n","#text":"P Taylor"},{"#tail":"\n","#text":"C Van EssDykema"},{"#tail":"\n","#text":"K Ries"},{"#tail":"\n","#text":"E Shriberg"},{"#tail":"\n","#text":"D Jurafsky"},{"#tail":"\n","#text":"R Martin"},{"#tail":"\n","#text":"M Meteer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"B. Taskar, P. Abbeel, D. Koller. 2002. Discriminative probabilistic models for relational data. In Proceedings of UAI, Edmonton, Canada."},"#text":"\n","marker":{"#tail":"\n","#text":"Taskar, Abbeel, Koller, 2002"},"location":{"#tail":"\n","#text":"Edmonton, Canada."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"as considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statemen","@endWordPosition":"4896","@position":"31395","annotationId":"T46","@startWordPosition":"4893","@citStr":"Taskar et al., 2002"}},"title":{"#tail":"\n","#text":"Discriminative probabilistic models for relational data."},"booktitle":{"#tail":"\n","#text":"In Proceedings of UAI,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"B Taskar"},{"#tail":"\n","#text":"P Abbeel"},{"#tail":"\n","#text":"D Koller"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"B. Taskar, C. Guestrin, D. Koller. 2003. Max-margin Markov networks. In Proceedings of NIPS."},"#text":"\n","marker":{"#tail":"\n","#text":"Taskar, Guestrin, Koller, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ocument similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that t","@endWordPosition":"4900","@position":"31415","annotationId":"T47","@startWordPosition":"4897","@citStr":"Taskar et al, 2003"}},"title":{"#tail":"\n","#text":"Max-margin Markov networks."},"booktitle":{"#tail":"\n","#text":"In Proceedings of NIPS."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"B Taskar"},{"#tail":"\n","#text":"C Guestrin"},{"#tail":"\n","#text":"D Koller"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"B. Taskar, V. Chatalbashev, D. Koller. 2004. Learning associative Markov networks. In Proceedings of ICML."},"#text":"\n","marker":{"#tail":"\n","#text":"Taskar, Chatalbashev, Koller, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that the integration of ev","@endWordPosition":"4904","@position":"31435","annotationId":"T48","@startWordPosition":"4901","@citStr":"Taskar et al, 2004"}},"title":{"#tail":"\n","#text":"Learning associative Markov networks."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ICML."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"B Taskar"},{"#tail":"\n","#text":"V Chatalbashev"},{"#tail":"\n","#text":"D Koller"}]}},{"volume":{"#tail":"\n","#text":"28"},"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"S. Teufel, M. Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409?445."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Teufel, Moens, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg ","@endWordPosition":"4732","@position":"30279","annotationId":"T49","@startWordPosition":"4729","@citStr":"Teufel and Moens, 2002"}},"title":{"#tail":"\n","#text":"Summarizing scientific articles: Experiments with relevance and rhetorical status."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Teufel"},{"#tail":"\n","#text":"M Moens"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"P. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the ACL, 417?424."},"#text":"\n","pages":{"#tail":"\n","#text":"417--424"},"marker":{"#tail":"\n","#text":"Turney, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"oposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable ","@endWordPosition":"789","@position":"5216","annotationId":"T50","@startWordPosition":"788","@citStr":"Turney, 2002"}},"title":{"#tail":"\n","#text":"Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"P Turney"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1988"},"rawString":{"#tail":"\n","#text":"J. M. Wiebe, W. J. Rapaport. 1988. A computational theory of perspective and reference in narrative. In Proceedings of the ACL, 131?138."},"#text":"\n","pages":{"#tail":"\n","#text":"131--138"},"marker":{"#tail":"\n","#text":"Wiebe, Rapaport, 1988"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"aker segment of text) represents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity b","@endWordPosition":"730","@position":"4819","annotationId":"T51","@startWordPosition":"727","@citStr":"Wiebe and Rapaport (1988)"}},"title":{"#tail":"\n","#text":"A computational theory of perspective and reference in narrative."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J M Wiebe"},{"#tail":"\n","#text":"W J Rapaport"}]}},{"volume":{"#tail":"\n","#text":"20"},"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"J. M. Wiebe. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233?287."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Wiebe, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bh","@endWordPosition":"737","@position":"4865","annotationId":"T52","@startWordPosition":"736","@citStr":"Wiebe (1994)"}},"title":{"#tail":"\n","#text":"Tracking point of view in narrative."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J M Wiebe"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"H. Yang, J. Callan. 2005. Near-duplicate detection for eRulemaking. In Proceedings of Digital Government Research (dg.o)."},"#text":"\n","marker":{"#tail":"\n","#text":"Yang, Callan, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do no","@endWordPosition":"4643","@position":"29699","annotationId":"T53","@startWordPosition":"4640","@citStr":"Yang and Callan, 2005"}},"title":{"#tail":"\n","#text":"Near-duplicate detection for eRulemaking."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Digital Government Research (dg.o)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"H Yang"},{"#tail":"\n","#text":"J Callan"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical Report TR 1530,"},"date":{"#tail":"\n","#text":"2005"},"institution":{"#tail":"\n","#text":"University of Wisconsin-Madison."},"rawString":{"#tail":"\n","#text":"J. Zhu. 2005. Semi-supervised learning literature survey. Computer Sciences Technical Report TR 1530, University of Wisconsin-Madison. Available at http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf; has been updated since the initial 2005 version."},"#text":"\n","marker":{"#tail":"\n","#text":"Zhu, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice ea","@endWordPosition":"4858","@position":"31157","annotationId":"T54","@startWordPosition":"4857","@citStr":"Zhu (2005)"}},"title":{"#tail":"\n","#text":"Semi-supervised learning literature survey. Computer Sciences"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Zhu"}}}]}}]}}
