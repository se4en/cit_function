n for the scoring. The motivation behind this approach is to get nonlocal information outside the current context (i.e. the currently considered bilingual phrase pair) into the translation process. The triplets are trained via the EM algorithm, as will be shown later in more detail. 2 Related Work In the past, a significant number of methods has been presented that try to capture long-distance dependencies, i.e. use dependencies in the data that reach beyond the local context of n-grams or phrase pairs. In language modeling, monolingual trigger approaches have been presented (Rosenfeld, 1996; Tillmann and Ney, 1997) as well as syntactical methods that parse the input and model long-range dependencies on the syntactic level by conditioning on the predecessing words and their corresponding parent nodes (Chelba and Jelinek, 2000; Roark, 2001). The latter approach was shown to reduce perplexities and improve the WER in speech recognition systems. One drawback is that the parsing process might slow down the system significantly and the approach is complicated to be integrated directly in the search process. Thus, the effect is often shown offline in reranking experiments using n-best lists. One of the simples
a source word fj given the whole tarlog (Z−1 n ¯α(fj|ei, ek))] , where Zn is defined as in Eq. 2. Using the method of Lagrangian multipliers for the normalization constraint, we take the derivative with respect to 374 ¯α(f|e, e0) and obtain: ¯α(f |e, e0) = A(f, e� e0) (4) Ef,A(f, ,e,e0) where A(f, e, e0) is a relative weight accumulator over the parallel corpus: A(f,e,e0) = δ(f, fj)Z−1 n α(f|e, e0)Cn(e, e0) (5) pall(fj|eIn 1 ) and δ(e, ei)δ(e0, ek). The function δ(·, ·) denotes the Kronecker delta. The resulting training procedure is analogous to the one presented in (Brown et al., 1993) and (Tillmann and Ney, 1997). The next section presents variants of the basic unconstrained model by putting restrictions on the valid regions of triggers (in-phrase vs. out-ofphrase) and using alignments obtained from either GIZA++ training or forced alignments in order to reduce the model size and to incorporate knowledge already obtained in previous training steps. 3.2 Model variations Based on the unconstrained triplet model presented in Section 3, we introduce additional constraints, namely the phrase-bounded and the path-aligned triplet model in the following. The former reduces the number of possible triplets by p
