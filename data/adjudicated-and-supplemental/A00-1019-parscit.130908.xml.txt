 sentences, totaling 278,127 english tokens (13,543 forms) and 292,865 french tokens (16,399 forms). 3.1 Finding Monolingual Units Finding relevant units in a text has been explored in many areas of natural language processing. Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus. For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus. This method allows the efficient retrieval of arbitrary length n-grams (Nagao and Mori, 94; Haruno et al., 96; Ikehaxa et al., 96; Shimohata et al., 1997; Russell, 1998). The literature abounds in measures that can help to decide whether words that co-occur are linguistically significant or not. In this work, the strength of association of a sequence of words wr = w1, wn is computed by two measures: a likelihood-based one p(wr) (where is the likelihood ratio given in (Dunning, 93)) and an entropy-based one e(w) (Shimohata et al., 1997). Letting T stand for the training text and m a token: argmin t(w1,74+1) iEjl,n[ e(w) = 0.5x h m Ernimw;' ET q((ww) (fir::)) EmInqmET h ffr:qeq(7:174) )) Intuitively, the first measurement accounts for the fact t
 to a special &quot;unknown&quot; word es however, these associations have a lower probability than the good ones. We also found few erratic associations (the first time/ c'etait, some hon. members/!, etc) due to distributional artifacts. It is also interesting to note that the good associations we found are not necessary compositional in nature (we must/il faut, people of canada/les canadiens, of course/evidemment, etc). 3.3 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Gaussier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags. An excerpt of the association probabilities of a unit model trained considering only the NP-s
me hon. members/!, etc) due to distributional artifacts. It is also interesting to note that the good associations we found are not necessary compositional in nature (we must/il faut, people of canada/les canadiens, of course/evidemment, etc). 3.3 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Gaussier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags. An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as .7.Np in the following) to the 39,093 english sequences still surviving after previous filters .7.1 and .F2 
