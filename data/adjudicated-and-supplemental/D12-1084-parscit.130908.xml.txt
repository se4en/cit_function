r collecting paraphrases relying on the sequential event order in the discourse, using multiple sequence alignment with a semantic similarity measure. We show that adding discourse information boosts the performance of sentence-level paraphrase acquisition, which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences. Our system beats an informed baseline by a margin of 50%. 1 Introduction It is widely agreed that identifying paraphrases is a core task for natural language processing, including applications like document summarization (Barzilay et al., 1999), Recognizing Textual Entailment (Dagan et al., 2005), natural language generation (Zhao et al., 2010; Ganitkevitch et al., 2011), and machine translation (Marton et al., 2009). As a consequence, many methods have been proposed for generating large paraphrase resources (Lin and Pantel, 2001; Szpektor et al., 2004; Dolan et al., 2004). One of the intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sente
se information boosts the performance of sentence-level paraphrase acquisition, which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences. Our system beats an informed baseline by a margin of 50%. 1 Introduction It is widely agreed that identifying paraphrases is a core task for natural language processing, including applications like document summarization (Barzilay et al., 1999), Recognizing Textual Entailment (Dagan et al., 2005), natural language generation (Zhao et al., 2010; Ganitkevitch et al., 2011), and machine translation (Marton et al., 2009). As a consequence, many methods have been proposed for generating large paraphrase resources (Lin and Pantel, 2001; Szpektor et al., 2004; Dolan et al., 2004). One of the intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning. Most approaches that extract paraphrases from parallel texts employ some type of pattern matching: sentences with the same meaning are a
cting phraselevel paraphrase fragments from matched sentences. Our system beats an informed baseline by a margin of 50%. 1 Introduction It is widely agreed that identifying paraphrases is a core task for natural language processing, including applications like document summarization (Barzilay et al., 1999), Recognizing Textual Entailment (Dagan et al., 2005), natural language generation (Zhao et al., 2010; Ganitkevitch et al., 2011), and machine translation (Marton et al., 2009). As a consequence, many methods have been proposed for generating large paraphrase resources (Lin and Pantel, 2001; Szpektor et al., 2004; Dolan et al., 2004). One of the intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning. Most approaches that extract paraphrases from parallel texts employ some type of pattern matching: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003; Callison-Burch, 2008, among others), many words in their context (Barzilay and McKeo
ds have been proposed for generating large paraphrase resources (Lin and Pantel, 2001; Szpektor et al., 2004; Dolan et al., 2004). One of the intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning. Most approaches that extract paraphrases from parallel texts employ some type of pattern matching: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003; Callison-Burch, 2008, among others), many words in their context (Barzilay and McKeown, 2001) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al., 2004). Discourse structure has only marginally been considered for this task: For example, Dolan et al. (2004) extract the first sentences from comparable articles and take them as paraphrases. Another approach (Deléger and Zweigenbaum, 2009) matches similar paragraphs in comparable texts, creating smaller comparable documents for paraphrase extraction. We believe that discourse structure delivers important information for
ektor et al., 2004; Dolan et al., 2004). One of the intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning. Most approaches that extract paraphrases from parallel texts employ some type of pattern matching: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003; Callison-Burch, 2008, among others), many words in their context (Barzilay and McKeown, 2001) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al., 2004). Discourse structure has only marginally been considered for this task: For example, Dolan et al. (2004) extract the first sentences from comparable articles and take them as paraphrases. Another approach (Deléger and Zweigenbaum, 2009) matches similar paragraphs in comparable texts, creating smaller comparable documents for paraphrase extraction. We believe that discourse structure delivers important information for the extraction of paraphrases. Sentences that play the same role in a certain discourse and ha
ective on the task and sketch our system pipeline (Sec. 3). The following two sections describe the details of the sentence matching step (Sec. 4) and the subsequent paraphrase fragment extraction (Sec. 5). We present both automatic and manual evaluation of the two system components (Sec. 6). Finally, we conclude the paper and give some hints for future work (Sec. 7). 2 Related Work Previous paraphrase extraction approaches can be roughly characterized under two aspects: 1) data source and 2) granularity of the output. Both parallel corpora and comparable corpora have been quite well studied. Barzilay and McKeown (2001) use different English translations of the same novels (i.e., monolingual parallel corpora), while others (Quirk et al., 2004) experiment on multiple sources of the same news/events, i.e., monolingual comparable corpora. Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011). Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (200
ated Work Previous paraphrase extraction approaches can be roughly characterized under two aspects: 1) data source and 2) granularity of the output. Both parallel corpora and comparable corpora have been quite well studied. Barzilay and McKeown (2001) use different English translations of the same novels (i.e., monolingual parallel corpora), while others (Quirk et al., 2004) experiment on multiple sources of the same news/events, i.e., monolingual comparable corpora. Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011). Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference 
d McKeown (2001) use different English translations of the same novels (i.e., monolingual parallel corpora), while others (Quirk et al., 2004) experiment on multiple sources of the same news/events, i.e., monolingual comparable corpora. Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011). Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphras
e machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004; Quirk et al., 2004). Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information (Regneri et al., 2010). In this earlier work, we focused on event structu
anguage as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004; Quirk et al., 2004). Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information (Regneri et al., 2010). In this earlier work, we focused on event structures and their possible realizations in natural language. The corpus used in those experiments were short crowd-sourced descriptions of everyda
ch two possible translations in the other languages as paraphrases if they share a common pivot phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004; Quirk et al., 2004). Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information (Regneri et al., 2010). In this earlier work, we focused on event structures and their possible realizations in natural language. The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bulle
ques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004; Quirk et al., 2004). Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information (Regneri et al., 2010). In this earlier work, we focused on event structures and their possible realizations in natural language. The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style. We aligned them with a hand-crafted similarity measure that was specifically designed for this text type. In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task. The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and 
2003) or inference rules (Lin and Pantel, 2001) are extracted. In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words. They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009). The research on general paraphrase fragment extraction at the sub-sentential level is mainly based 917 on phrase pair extraction techniques from the MT literature. Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. Quirk et al. (2007) extract fragments using a generative model of noisy translations. Our own work (Wang and Callison-Burch, 2011) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora. Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments. Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008)
entered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009). The research on general paraphrase fragment extraction at the sub-sentential level is mainly based 917 on phrase pair extraction techniques from the MT literature. Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. Quirk et al. (2007) extract fragments using a generative model of noisy translations. Our own work (Wang and Callison-Burch, 2011) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora. Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments. Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008). 3 Paraphrases and Discourse Previous approaches have shown that comparable texts provide a good basis for paraphrase extraction. We want to show that discourse structure is highly useful for precise and high-yield paraphrase collection from such corpora. Co
., 2011). In particular, the dependency structures of the parser’s output are used for VP2http://nlp.stanford.edu/software/ corenlp.shtml fragment extraction (Sec. 5.3). The output from the coreference resolution system is used to cluster all mentions referring to the same entity and to select one as the representative mention. If the representative mention is not a pronoun, we modify the original texts by replacing all pronoun mentions in the cluster with the syntactic head of the representative mention. Note that the coreference resolution system is applied to each recap as a whole. GIZA++ (Och and Ney, 2003) is a widely used word aligner for MT systems. We amend the input data by copying identical word pairs 10 times and adding them as additional ‘sentence’ pairs (Byrne et al., 2003), in order to emphasize the higher alignment probability between identical words. We run GIZA++ for bi-directional word alignment and obtain a lexical translation table. 5.2 Fragment Extraction As mentioned in Sec. 2, we choose to use alignmentbased approaches to this task, which allows us to use many existing MT techniques and tools. We mainly follow our previous approach (Wang and CallisonBurch, 2011), which is a mo
r MT systems. We amend the input data by copying identical word pairs 10 times and adding them as additional ‘sentence’ pairs (Byrne et al., 2003), in order to emphasize the higher alignment probability between identical words. We run GIZA++ for bi-directional word alignment and obtain a lexical translation table. 5.2 Fragment Extraction As mentioned in Sec. 2, we choose to use alignmentbased approaches to this task, which allows us to use many existing MT techniques and tools. We mainly follow our previous approach (Wang and CallisonBurch, 2011), which is a modified version of an approach by Munteanu and Marcu (2006) on translation fragment extraction. We briefly review the three-step procedure here and refer the reader to the original paper for more details: 1. Establish word-word alignment between each sentence pair using GIZA++; 2. Smooth the alignment based on lexical occurrence likelihood; 3. Extract fragment pairs using different heuristics, e.g., non-overlapping n-grams, chunk boundaries, or dependency trees. After obtaining a lexical translation table by running GIZA++, for each word pair, w1 and w2, we use both positive and negative lexical associations for the alignment, which are defined as the
ositive and negative lexical associations for the alignment, which are defined as the conditional probabilities p(w1jw2) and p(w1j:w2), respectively. The resulting alignment can be further constrained by a modified longest common substring (LCS) algorithm, which takes sequences of 920 words instead of letters as input. Smoothing (step 2) is done for each word by taking the average score of it and its four neighbor words. All the word alignments (excluding stop-words) with positive scores are selected as candidate fragment elements. Provided with the candidate fragment elements, we previously (Wang and Callison-Burch, 2011) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a (para-) phrase. We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec. 5.3). Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs. 5.3 VP-fragment Extraction To obtain more grammatical output fragments, we add another layer of linguistic information to our input sentences. Based on the dependency parses produced during preprocessing, we extract phrases containing verbs and their comple
entential paraphrase matching and paraphrase fragment extraction using manually labelled gold standards (provided in the supplementary material). We collect recaps for all 20 episodes of season 6 of House M.D., taking 8 summaries per episode (the supplementary material contains a list of all URLs). This results in 160 documents containing 14735 sentences. For evaluation, we use all episodes except no. 2, which is held out for parameter optimizations and other development purposes. 6.1 Sentential Paraphrase Evaluation To evaluate sentence matching, we adapt the baselines from our earlier work (Regneri et al., 2010) and create a new gold standard. We compute precision, recall and accuracy of our main system and suggest baselines that separately show the influence of both the MSA and the semantic scoring function. Gold-Standard We aim to create an evaluation set that contains a sufficient amount of genuine paraphrases. Finding such sentence pairs with random sampling and manual annotation is infeasible: There are more than 200, 000, 000 possible sentence pairs, and we expect less than 1% of them to be paraphrases. We thus sample pairs that either the system or the baselines recognized as paraphrases and t
ion and accuracy. It is hard to do a direct comparison with stateof-the-art paraphrase recognition systems, because most are evaluated on different corpora, e.g., the Microsoft paraphrase corpus (Dolan and Brockett, 2005, MSR). We cannot apply our system to the MSR corpus, because we take complete texts as input, while the MSR corpus solely delivers sentence pairs. While the MSR corpus is larger than our collection, the wording variations in its paraphrase pairs are usually lower than for our examples. Thus the final numbers of previous approaches might be vaguely comparable with our results: Das and Smith (2009) present two systems reaching f-scores of 0.82 and 0.83, with a precision of 0.75 and 0.80. Both precision and f-scores of our msa-based systems lie within the same range. Heilman and Smith (2010) introduce a recall-oriented system, which reaches an f-score of 0.81 by a precision of 0.76. Compared to this system, our approach results in better precision values. All further computations bases on the system using MSA and the vector space model (MSA+VEC), because it achieves the highest precision and accuracy values. 6.2 Paraphrase Fragment Evaluation We also manually evaluate precision on paraph
us (Dolan and Brockett, 2005, MSR). We cannot apply our system to the MSR corpus, because we take complete texts as input, while the MSR corpus solely delivers sentence pairs. While the MSR corpus is larger than our collection, the wording variations in its paraphrase pairs are usually lower than for our examples. Thus the final numbers of previous approaches might be vaguely comparable with our results: Das and Smith (2009) present two systems reaching f-scores of 0.82 and 0.83, with a precision of 0.75 and 0.80. Both precision and f-scores of our msa-based systems lie within the same range. Heilman and Smith (2010) introduce a recall-oriented system, which reaches an f-score of 0.81 by a precision of 0.76. Compared to this system, our approach results in better precision values. All further computations bases on the system using MSA and the vector space model (MSA+VEC), because it achieves the highest precision and accuracy values. 6.2 Paraphrase Fragment Evaluation We also manually evaluate precision on paraphrase fragments, and additionally describe the productivity of the different setups, providing some intuition about the methods’ recall. Gold-Standard We randomly collect 150 fragment pairs for eac
valuation results. We reach our best precision by using the VP-fragment heuristics, which is still more productive than the LCS method. The grammatical filter gives us a higher precision compared to the purely alignment-based approaches. Enhancing the system with coreference resolution raises the score even further. We cannot directly compare this performance to other systems, as all other approaches have different data sources. However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One state-of-theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67. We found the same number using our previous approach (Wang and Callison-Burch, 2011), which is roughly equivalent to our core module. Our approach outperforms both by 17% with similar estimated productivity. As a final comparison, we show how the performance of the sentence matching methods directly affects the fragment extraction. We use the VP-based fragment extraction system (VP), and compare the performances by using either the outputs from our main system (MSA+VP) or alternatively the baseline 
a higher precision compared to the purely alignment-based approaches. Enhancing the system with coreference resolution raises the score even further. We cannot directly compare this performance to other systems, as all other approaches have different data sources. However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One state-of-theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67. We found the same number using our previous approach (Wang and Callison-Burch, 2011), which is roughly equivalent to our core module. Our approach outperforms both by 17% with similar estimated productivity. As a final comparison, we show how the performance of the sentence matching methods directly affects the fragment extraction. We use the VP-based fragment extraction system (VP), and compare the performances by using either the outputs from our main system (MSA+VP) or alternatively the baseline that replaces MSA with a clustering algorithm (CLUSTER+VP). Both variants use the vector-based semantic similarity measure. Sentence matching Precision Productivity CLUSTER+VP 0.31
