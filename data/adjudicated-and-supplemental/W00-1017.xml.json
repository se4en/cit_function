{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","figure":[{"#tail":"\n","@confidence":"0.53507425","#text":"\nWIT: A Toolkit for Building Robust and Real-Time Spoken Dialogue\nSystems\nMikio Nakano* Noboru Miyazaki, Norihito Yasuda, Akira Sugiyama,\nJun-ichi Hirasawa, Kohji Dohsaka, Kiyoaki Aikawa\nNTT Corporation\n3-1 Morinosato-Wakamiya\nAtsugi, Kanagawa 243-0198, Japan\nE-mail: nakano@atom.brl.ntt.co.jp\n"},{"#tail":"\n","@confidence":"0.443943","#text":"\nphrases.\n4 Building Spoken Dialo~te Systems\nwith WIT\n"},{"#tail":"\n","@confidence":"0.95412725","#text":"\n(month-phrase\n(month\n(opt\n(or\nexpression-following-subject\n(admoninal-particle\n(opt\nsentence-final-particle))))))\n"},{"#tail":"\n","@confidence":"0.6956846","#text":"\n((rule name)\n(child feature structure)\n? . . (child feature structure)\n=> (mother feature structu_e)\n(priority increase) )\n((role name)\n(child feature structure)\n? . . (child feature structure)\n=> (flame operation command)\n(priority increase) )\n"}],"equation":[{"#tail":"\n","@confidence":"0.825261","#text":"\nSemantic \\[\nI ~e I\n/ specifications \\[ r 1\nI R~ae I /L..___..-- - ' - ' -~ ' - / Ph~e l\n/de~;,i~ions I /&quot; &quot;-. I de~i*~._l\nI Feature I L._.___..~.. -~-'-'-'~ , ( &quot;~ &quot;'.\nL____i----',.,'-... ~ ~ &quot;-..\n? Surface- I de .~ons \\[:- ,, l ;~ : : - - ?Y \\ l Language II Language I ~Genera f io~n_ / . I , ~ , &quot;., &quot;~&quot;----~__Z_--.--&quot; M . -. i i . ) i . . . . . . . l,,_J generaraon I ~ \\ ,, ~ ~ unaerstanding I I generation IO t procedures I TM / . . . . ~ . . . . I\n\\ I I I . . . . J\nI definitions I '\\ \\ I word I strings I hypothesea +\nI . - - I _ __~ '~seto f - &quot; -L__~I Speech i I~ ,~ L iT -s t o f - - / L / i . , i s to f\nI ~&quot; t I ~angu.~ge I- -I ~t io . I I ou~u, r ' - - ' \\ ] pre-r~o.dedr'--\\] pre-r~o~ed I\nI d~f~i~23~_l t.models .....~1 I module I I ~oam~ I~Peech m~._J , I L_ j\n"},{"#tail":"\n","@confidence":"0.5810936","#text":"\n( s ta r t -end- t imes-command\n( t ime-phrase : f rom *start)\n( t ime-phrase (:or :to nil) *end)\n=> (command (set :start *start)\n(set :end *end)))\n"},{"#tail":"\n","@confidence":"0.750809","#text":"\nt ime-phrase is a phrase category, : f rom and\n( : or : to n i l ) are case feature values, and\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.84915","#text":"\nDialogue Systems\n"},{"#tail":"\n","@confidence":"0.999793","#text":"\n3.1 Speech Recognition\n"},{"#tail":"\n","@confidence":"0.999771","#text":"\n3.2 Language Understanding\n"},{"#tail":"\n","@confidence":"0.420586","#text":"\nSignificant-utterance Sequence Search) (Nakano\n"},{"#tail":"\n","@confidence":"0.999258","#text":"\n3.3 Language Generation\n"},{"#tail":"\n","@confidence":"0.993721","#text":"\n3.4 Speech Output\n"},{"#tail":"\n","@confidence":"0.9299655","#text":"\n4.1 Domain-Dependent System\nSpecifications\n"},{"#tail":"\n","@confidence":"0.999003","#text":"\n4.2 Compiling System Specifications\n"},{"#tail":"\n","@confidence":"0.997617","#text":"\n6.1 Descriptive Power\n"},{"#tail":"\n","@confidence":"0.998578","#text":"\n6.2 Consistency\n"},{"#tail":"\n","@confidence":"0.466188","#text":"\n6-3 Avoiding Information Loss\n"},{"#tail":"\n","@confidence":"0.99254","#text":"\n6.4 Problems and Limitations\n"}],"footnote":{"#tail":"\n","@confidence":"0.934473666666667","#text":"\n3In this section, we use examples of different description\nfrom the actual ones for simplicity. Actual specifications are\nwritten in part in Japanese.\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.997094782178218","#text":"\nJames F. Allen, Bradford W. Miller, Eric K. Ringger,\nand Teresa Sikorski. 1996. A robust system for nat-\nural spoken dialogue. In Proceedings of the 34th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (A CL-96), pages 62-70.\nHarald Aust, Martin Oerder, Frank Seide, and Volker\nSteinbiss. 1995. The Philips automatic train\ntimetable information system. Speech Communi-\ncation, 17:249--262.\nJames Barnett and Mona Singh. 1997. Designing\na portable spoken language system. In Elisabeth\nMaier, Marion Mast, and Susann LuperFoy, editors,\nDialogue Processing inSpoken Language Systems,\npages 156--170. Springer-Vedag.\nDaniel G. Bobrow, Ronald M. Kaplan, Martin Kay,\nDona!d A. Norman, Henry Thompson, and Terry\nWinograd. 1977. GUS, a frame driven dialog sys-\ntem. Arnficial Intelligence, 8:155-173.\nJennifer Chu-Carroll. 1999. Fo:rrn-based reason-\ning for mixed-initiative dialogue management in\ninformation-query systems. In Proceedings of the\nSixth European Conference on Speech Communica-\ntion and Technology (Eurospeech-99) , pages 1519-\n1522.\nJunnifer Chu-Carroll. 2000. MIMIC: An adaptive\nmixed initiative spoken dialogue system for infor-\nmation queries. In Proceedings of the 6th Con-\nf~rence on Applied Natural Language Processing\n(ANLP-O0), pages 97-104.\nKohji Dohsaka nd Akira Shimazu. 1997. System ar-\nchitecture for spoken utterance production in col-\nlaborative dialogue. In Working Notes of IJCAI\n1997 Workshop on Collaboration, Cooperation and\nConflict in Dialogue Systems.\nKohji Dohsaka, Norihito Yasuda, Noboru Miyazaki,\nMikio Nakano, and Kiyoaki AJkawa. 2000. An ef-\nficient dialogue control method under system's lim-\nited knowledge. In Proceedings of the Sixth Inter-\nnational Conference on Spoken Language Process-\ning (ICSLP-O0).\nJun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano,\nand Takeshi Kawabata. 1998. Implementation\nof coordinative nodding behavior on spoken dia-\nlogue systems. In Proceedings of the Fgth Interna-\ntional Conference on Spoken Language Processing\n(1CSLP-98), pages 2347-2350.\nTetsunod Kobayashi, Shuichi Itahashi, Satoru\nHayamizu, and Toshiyuki Takezawa. 1992. Asj\ncontinuous speech corpus for research. The journal\nof th e Acoustical Society of Japan, 48(12): 888-893.\nMikio Nakano and Akira Shimazu. 1999. Pars-\ning utterances including self-repairs. In Yorick\nWilks, editor, Machine Conversations, pages 99-\n112. Kluwer Academic Publishers.\nMikio Nakano, Aldra Shimazu, and Kiyoshi Kogure.\n1994. A grammar and a parser for spontaneous\nspeech. In Proceedings of the 15th Interna-\ntional Conference on Computational Linguistics\n(COLING-94), pages 1014-1020.\nMildo Nakano, Kohji Dohsaka, Noboru Miyazald,\nInn ichi Hirasawa, Masafiami Tamoto, Masahito\nKawarnon, Akira Sugiyama, and Takeshi Kawa-\nbata. 1999a. Handling rich turn-taking in spoken\ndialogue systems. In Proceedings of the Sixth Eu-\nropean Conference on Speech Communication a d\nTechnology (Eurospeech-99), pages 1167-1170.\nMikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,\nKohji Dohsaka, and Takeshi Kawabata. 1999b.\nUnderstanding unsegmented user utterances in real-\ntime spoken dialogue systems. In Proceedings of\nthe 37th Annual Meeting of the Association for\nComputational Linguistics (ACL-99), pages 200--\n207.\nMikio Nakano. 1991. Constraint projection: An ef-\nficient treatment of disjunctive f ature descriptions.\nIn Proceedings of the 29th Annual Meeting of the\nAssociation for Computational Linguistics (ACL-\n90, pages 307-314.\nYoshiaki Noda, Yoshikazu Yamaguchi, Tomokazu\nYamada, Akihiro Imamura, Satoshi Takahashi,\nTomoko Matsui, and Kiyoaki Aikawa. 1998. The\ndevelopment of speech recognition engine REX. In\nProceedings of the 1998 1EICE General Confer-\nence D-14-9, page 220. (in Japanese).\nFernando C. N. Pereira and David H. D. Warren.\n1980. Definite clause grammars for language\nanalysis--a survey of the formalism and a compar-\nison with augmented transition etworks. Artificial\nIntelligence, 13:231-278.\nCarl J. Pollard and Ivan A. Sag. 1994. Head-Driven\nPhrase Structure Grammar. CSLI, Stanford.\nMunehiko Sasajima, Yakehide Yano, and Yasuyuki\nKono. 1999. EUROPA: A genetic framework for\ndeveloping spoken dialogue systems. In Proceed-\nings of the Sixth European Conference on Speech\nCommunication a d Technology (Eurospeech-99),\npages 1163--1166.\nStephanie Seneff, Ed Hurley, Raymond Lau, Chris-\nfine Pao, Philipp Sehmid, and Victor Zue. 1998.\nGALAXY-H: A reference architecture for conver-\nsational system development. In Proceedings of\n"},{"#tail":"\n","@confidence":"0.997341416666667","#text":"\nthe Fifth International Con l~rence on Spoken Lan-\nguage Processing (ICSLP-98).\nStephen Sutton, Ronaid A. Cole, Jacques de Villiers,\nJohan SchMkwyk, Pieter Vermeulen, Michael W.\nMacon, Yonghong Yah, Edward Kaiser, Brian Run-\ndie, K.haldoun Shobaki, Paul Hosom, Alex Kain,\nJohan Wouters, Dominic W. Massaro, and Michael\nCohen. 1998. Universal speech tools: The\nCSLU toolkit. In Proceedings of the Fifth Interna-\ntional Conference on Spoken Language Processing\n(1CSLP-98), pages 3221-3224.\nMarilyn Walker, Irene Langkilde, Jerry Wright, Allen\nGorin, and Diane Litman. 2000. Learning to pre-\ndict problematic situations in a spoken dialogue\nsystem: Experiments with how may I help you? In\nProceedings of the First Meeting of the North Amer-\nican Chapter of the Association for Computational\nLinguistics (NAA CL-O0), pages 210--217.\nVictor Zue, Stephanie Seneff, James Glass, Joseph Po-\nlifroni, Christine Pao, Timothy J. Hazen, and Lee\nHe~erington. 2000. Jupiter: A telephone-based\nconversational interface for weather information.\n1EEE Transactions on Speech and Audio Process-\ning, 8(1):85-96.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.994117818181818","#text":"\nThis paper describes WI'I; a toolkit\nfor building spoken dialogue systems.\nWIT features an incremental under-\nstanding mechanism that enables ro-\nbust utterance understanding and real-\ntime responses. WIT's ability to com-\npile domain-dependent system specifi-\ncations into internal knowledge sources\nmakes building spoken dialogue sys-\ntems much easier than :it is from\nscratch.\n"},{"#tail":"\n","@confidence":"0.996744836065574","#text":"\nThe recent great advances in speech and language\ntechnologies have made it possible to build fully\nimplemented spoken dialogue systems (Aust et\nal., 1995; Allen et al, 1996; Zue et al, 2000;\nWalker et al, 2000). One of the next research\ngoals is to make these systems task-portable, that\nis, to simplify the process of porting to another\ntask domain.\nTo this end, several toolkits for building spo-\nken dialogue systems have been developed (Bar-\nnett and Singh, 1997; Sasajima et al, 1999).\nOne is the CSLU Toolkit (Sutton et al, 1998),\nwhich enables rapid prototyping of a spoken di-\nalogue system that incorporates a finite-state dia-\nlogue model. It decreases the amount of the ef-\nfort required in building a spoken dialogue sys-\ntem in a user-defined task domain. However, it\nlimits system functions; it is not easy to employ\nthe advanced language processing techniques de-\nveloped in the realm of computational linguis-\ntics. Another is GALAXY-II (Seneffet al, 1998),\n*Mikio Nakano is currently a visiting scientist at MIT\nLaboratory for Computer Science.\nwhich enables modules in a dialogue system to\ncommunicate with each other. It consists of the\nhub and several servers, such as the speech recog-\nnition server and the natural language server, and\nthe hub communicates with these servers. Al-\nthough it requires more specifications than finite-\nstate-model-based toolkits, it places less limita-\ntions on system functions.\nOur objective is to build robust and real-time\nspoken dialogue systems in different ask do-\nmains. By robust we mean utterance understand-\ning is robust enough to capture not only utter-\nances including rammatical errors or self-repairs\nbut also utterances that are not clearly segmented\ninto sentences by pauses. Real time means the\nsystem can respond to the user in real time. The\nreason we focus on these features i that they are\ncrucial to the usability of spoken dialogue sys-\ntems as well as to the accuracy of understand-\ning and appropriateness of the content of the sys-\ntem utterance. Robust understanding allows the\nuser to speak to the system in an unrestricted\nway. Responding in real time is important be-\ncause if a system response is delayed, the user\nmight think that his/her utterance was not recog-\nnized by the system and make another utterance,\nmaking the dialogue disorderly. Systems having\nthese features hould have several modules that\nwork in parallel, and each module needs some\ndomain-dependent k owledge sources. Creat-\ning and maintaining these knowledge sources re-\nquire much effort, thus a toolkit would be help-\nful. Previous toolkits, however, do not allow us to\nachieve these features, or do not provide mecha-\nnisms that achieve these features without requir-\ning excessive fforts by the developers.\nThis paper presents WIT 1, which is a toolkit\nIWIT is an acronym of Workable spoken dialogue lnter-\n"},{"#tail":"\n","@confidence":"0.997768111111111","#text":"\nfor building spoken dialogue systems that inte-\ngrate speech recognition, language understanding\nand generation, and speech output. WIT features\nan incremental understanding method (Nakano et\nal., 1999b) that makes it possible to build a robust\nand real-time system. In addition, WIT compiles\ndomain-dependent system specifications into in-\nternal knowledge sources o that building systems\nis easier. Although WIT requires more domain-\ndependent specifications than finite-state-model-\nbased toolkits, WIT-based systems are capable\nof taking full advantage of language processing\ntechnology. WIT has been implemented and used\nto build several spoken dialogue systems.\nIn what follows, we overview WIT, explain its\narchitecture, domain-dependent system specifica-\ntions, and implementation, and then discuss its\nadvantages and problems.\n"},{"#tail":"\n","@confidence":"0.998164125","#text":"\nA WIT-based spoken dialogue system has four\nmain modules: the speech recognition module,\nthe language understanding module, the lan-\nguage generation module, and the speech out-\nput module. These modules exploit domain-\ndependent knowledge sources, which are auto-\nmatically generated from the domain-dependent\nsystem specifications. The relationship among\nthe modules, knowledge sources, and specifica-\ntions are depicted in Figure 1.\nWIT can also display and move a human-face-\nlike animated agent, which is controlled by the\nspeech output module, although this paper does\nnot go into details because it focuses only on spo-\nken dialogue. We also omit the GUI facilities pro-\nvided by WIT.\n"},{"#tail":"\n","@confidence":"0.997735666666667","#text":"\nHere we explain how the modules in WIT work\nby exploiting domain-dependent k owledge and\nhow they interact with each other.\n"},{"#tail":"\n","@confidence":"0.993500377358491","#text":"\nThe speech recognition module is a phoneme-\nHMM-based speaker-independent continuous\nspeech recognizer that incrementally outputs\nface Toolldt.\nword hypotheses. As the recogn/fion engine,\neither VoiceRex, developed by NTI&quot; (Noda et\nal., 1998), or HTK from Entropic Research can\nbe used. Acoustic models for HTK is trained\nwith the continuous peech database of the\nAcoustical Society of Japan (Kobayashi et al,\n1992). This recognizer incrementally outputs\nword hypotheses a soon as they are found in the\nbest-scored path in the forward search (Hirasawa\net al, 1998) using the ISTAR (Incremental\nStructure Transmitter And Receiver) protocol,\nwhich conveys word graph information as well as\nword hypotheses. This incremental output allows\nthe language understanding module to process\nrecognition results before the speech interval\nends, and thus real-time responses are possible.\nThis module continuously runs and outputs\nrecognition results when it detects a speech\ninterval. This enables the language generation\nmodule to react immediately touser interruptions\nwhile the system is speaking.\nThe language model for speech recognition\nis a network (regular) grammar, and it allows\neach speech interval to be an arbitrary number\nof phrases. A phrase is a sequence of words,\nwhich is to be defined in a domain-dependent\nway. Sentences can be decomposed into a cou-\nple of phrases. The reason we use a repeti-\ntion of phrases instead of a sentence grammar\nfor the language model is that the speech recog-\nnition module of a robust spoken dialogue sys-\ntem sometimes has to recognize spontaneously\nspoken utterances, which include self-repairs and\nrepetition. In Japanese, bunsetsu is appropriate\nfor defining phrases. A bunsetsu consists of one\ncontent word and a number (possibly zero) of\nfunction words. In the meeting room reservation\nsystem we have developed, examples of defined\nphrases are bunsetsu to specify the room to be re-\nserved and the time of the reservation and bun-\nsetsu to express affirmation and negation.\nWhen the speech recognition module finds a\nphrase boundary, it sends the category of the\nphrase to the language understanding module,\nand this information is used in the parsing pro-\ncess.\nIt is possible to hold multiple language mod-\nels and use any one of them when recogniz-\ning a speech interval. The language models are\n"},{"#tail":"\n","@confidence":"0.99483275","#text":"\nswitched according to the requests from the lan-\nguage understanding module. In this way, the\nspeech recognition success rate is increased by\nusing the context of the dialogue.\nAlthough the current version of WIT does not\nexploit probabilistic language models, such mod-\nels can be incorporated without changing the ba-\nsic WIT architecture.\n"},{"#tail":"\n","@confidence":"0.99162","#text":"\nThe language understanding :module receives\nword hypotheses from the speech recognition\nmodule and incrementally understands the se-\nquence of the word hypotheses to update the di-\nalogue state, in which the resnlt of understand-\ning and discourse information are represented\nby a frame (i.e., attribute-value pairs). The un-\nderstanding module utilizes ISSS (Incremental\n"},{"#tail":"\n","@confidence":"0.998522666666667","#text":"\net al, 1999b), which is an integrated parsing and\ndiscourse processing method. ISSS enables the\nincremental understanding of user utterances that\nare not segmented into sentences prior to pars-\ning by incrementally finding the most plausible\nsequence of sentences (or significant utterances\nin the ISSS terms) out of the possible sentence\nsequences for the input word sequence. ISSS\nalso makes it possible for the language generation\nmodule to respond in real time because it can out-\nput a partial result of understanding at any point\nin time.\nThe domain-dependent knowledge used in this\nmodule consists of a unification-based lexicon\nand phrase structure rules. Disjunctive feature\ndescriptions are also possible; WIT incorporates\nan efficient method for handling disjunctions\n(Nakano, 1991). When a phrase boundary is de-\ntected, the feature structure for a phrase is com-\nputed using some built-in rules from the feature\nstructure rules for the words in the phrase. The\nphrase structure rules specify what kind of phrase\nsequences can be considered as sentences, and\nthey also enable computing the semantic repre-\nsentation for found sentences. Two kinds of sen-\ntenees can be considered; domain-related ones\nthat express the user's intention about he reser-\n"},{"#tail":"\n","@confidence":"0.996672411764706","#text":"\nvafion and dialogue-related ones that express the\nuser's attitude with respect to the progress of the\ndialogue, such as confirmation and denial. Con-\nsidering the meeting room reservation system, ex-\namples of domain-related sentences are &quot;I need to\nbook Room 2 on Wednesday&quot;, I need to book\nRoom 2&quot;, and &quot;Room 2&quot; and dialogue-related\nones are &quot;yes&quot;, &quot;no&quot;, and &quot;Okay&quot;.\nThe semantic representation for a sentence is\na command for updatingthe dialogue state. The\ndialogue state is represented bya list of attribute-\nvalue pairs. For example, attributes used in the\nmeeting room reservation system include task-\nrelated attributes, such as the date and time of\nthe reservation, as well as attributes that represent\ndiscourse-related information, such as confirma-\ntion and grounding.\n"},{"#tail":"\n","@confidence":"0.965903676470589","#text":"\nHow the language generation module works\nvaries depending on whether the user or system\nhas the initiative of turn taking in the dialogue 2.\nPrecisely speaking, the participant having the ini-\ntiative is the one the system assumes has it in the\ndialogue.\nThe domain-dependent k owledge used by the\nlanguage generation module is generation proce-\ndures, which consist of a set of dialogue-phase\ndefinitions. For each dialogue phase, an initial\nfunction, an action function, a time-out function,\nand a language model are assigned. In addition,\nphase definitions designate whether the user or\nthe system has the initiative. In the phases in\nwhich the system has the initiative, only the ini-\ntial function and the language model are assigned.\nThe meeting room reservation system, for exam-\nple, has three phases: the phase in which the\nuser tells the system his/her equest, he phase in\nwhich the system confirms it, and the phase in\nwhich the system tells the user the result of the\ndatabase access. In the first two phases, the user\nholds the initiative, and in the last phase, the sys-\ntern holds the initiative.\nFunctions defined here decide what string\nshould be spoken and send that string to the\nspeech output module based on the current di-\nalogue state. They can also shift the dialogue\n2The notion of the initiative inthis paper isdifferent from\nthat of the dialogue initiative of Chu-Carroll (2000).\nphase and change the holder of the initiative as\nwell as change the dialogue state. When the dia-\nlogue phase shifts, the language model foi&quot; speech\nrecognition is changed to get better speech recog-\nnition performance. Typically, the language gen-\neration module is responsible for database access.\nThe language generation module works as fol-\nlows. It first checks which dialogue participant\nhas the initiative. If the initiative is held by the\nuser, it waits until the user's speech interval ends\nor a duration of silence after the end of a system\nutterance is detected. The action function in the\ndialogue phase at that point in time is executed in\nthe former case; the time-out function is executed\nin the latter case. Then it goes back to the initial\nstage. If the system holds the initiative, the mod-\nule executes the initial function of the phase. In\ntypical question-answer systems, the user has the\ninitiative when asking questions and the system\nhas it when answering.\nSince the language generation module works in\nparallel with the language understanding module,\nutterance generation is possible even while the\nsystem is listening to user utterances and that ut-\nterance understanding is possible even while it is\nspeaking (Nakano et al, 1999a). Thus the system\ncan respond immediately after user pauses when\nthe user has the initiative. When the system holds\nthe initiative, it can immediately react to an in-\nterruption by the user because user utterances are\nunderstood in an incremental way (Dohsaka nd\nShimazu, 1997).\nThe time-out function is effective in moving\nthe dialogue forward when the dialogue gets\nstuck for some reason. For example, the system\nmay be able to repeat the same question with an-\nother expression and may also be able to ask the\nuser a more specific question.\n"},{"#tail":"\n","@confidence":"0.997986333333333","#text":"\nThe speech output module produces peech ac-\ncording to the requests from the language gener-\nation module by using the correspondence table\nbetween strings and pre-recorded speech data. It\nalso notifies the language generation module that\nspeech output has finished so that the language\ngeneration module can take into account the tim-\ning of the end of system utterance. The meeting\nroom reservation system uses speech files of short\n"},{"#tail":"\n","@confidence":"0.994687315789474","#text":"\nSpoken dialogue systems can be built with WIT\nby preparing several domain-dependent specifica-\ntions. Below we explain the specifications.\nFeature Definitions: Feature definitions pec-\nify the set of features used in the grammar for lan-\nguage understanding. They also specify whether\neach feature is a head feature or a foot feature\n(Pollard and Sag, 1994). This information isused\nwhen constructing feature structures for phrases\nin a built-in process.\nThe following is an example of a feature defini-\ntion. Here we use examples from the specification\nof the meeting room reservation system.\n(case head)\nIt means that the case feature is used and it is a\nhead feature 3.\nLexieal Descriptions: Lexical descriptions\nspecify both pronunciations and grammatical\nfeatures for words. Below is an example lexical\nitem for the word 1-gatsu (January).\n(l-gatsu ichigatsu month nil i)\nThe first three elements are the identifier, the pro-\nnunciation, and the grammatical category of the\nword. The remaining two elements are the case\nand semantic feature values.\nPhrase Definitions: Phrase definitions pecify\nwhat kind of word sequence can be recognized\nas a phrase. Each definition is a pair compris-\ning a phrase category name and a network of\nword categories. In the example below, month-\nphrase is the phrase category name and the re-\nmaining part is the network of word categories.\nopt means an option and or means a disjunc-\ntion. For instance, a word sequence that con-\nsists of a word in the month category, such as 1-\ngatsu (January), and a word in the adraon ina l -\npar t i c le category, such as no (of), forms a\nphrase in the month-phrase category.\n"},{"#tail":"\n","@confidence":"0.951130181818182","#text":"\nNetwork Definitions: Network definitions\nspecify what kind of phrases can be included in\neach language model. Each definition is a pair\ncomprising a network name and a set of phrase\ncategory names.\nSemantic-Frame Specifications: The result of\nunderstanding and dialogue history can be stored\nin the dialogue state, which is represented by a\nflat frame structure, i.e., a set of attribute-value\npairs. Semantic-frame specifications define the\nattributes used in the frame. The meeting room\nreservation system uses task-related attributes.\nTwo are s tar t and end, which represent the\nuser's intention about the start and end times of\nthe reservation for some meeting room. It also\nhas attributes that represent discourse informa-\ntion. One is conf i rmed, whose value indicates\nwhether if the system has already made an utter-\nance to confirm the content of the task-related at-\ntributes.\nRule Definitions: Each rule has one of the fol-\nlowing two forms.\n"},{"#tail":"\n","@confidence":"0.991680285714286","#text":"\nThese roles are similar to DCG (Pereira nd War-\nren, 1980) rules; they can include logical vari-\nables and these variables can be bound when\nthese rules are applied. It is possible to add to the\nrules constraints that stipulate relationships that\nmust hold among variables (Nakano, 199 I), but\nwe do not explain these constraints indetail in this\n"},{"#tail":"\n","@confidence":"0.957708916666667","#text":"\npaper. The priorities are used for disambiguat-\ning interpretation i the incremental understand-\ning method (Nakano et al, 1999b).\nWhen the command on the right-hand side of\nthe arrow is a frame operation command, phrases\nto which this rule can be applied can be consid-\nered a sentence, and the sentence's semantic rep-\nresentation is the command for updating the dia-\nlogue state. The command is one of the follow-\ning:\n? A command to set the value of an attribute\nof the frame,\n? A command to increase the priority,\nConditional commands (If-then-else type\ncommand, the condition being whether the\nvalue of an attribute of the flame is or is not\nequal to a specified value, or a conjunction\nor disjunction of the above condition), or\n? A list of commands to be sequentially exe-\ncuted.\nThanks to conditional commands, it is possible\nto represent the semantics of sentences context-\ndependently.\nThe following rule is an example.\n"},{"#tail":"\n","@confidence":"0.959912","#text":"\nThe name of this rule is s ta r t -end- t imes-\ncommand. The second and third elements\nare child feature structures. In these elements,\n"},{"#tail":"\n","@confidence":"0.985399266666667","#text":"\n*s tar t and *end are semantic feature val-\nues. Here :or means a disjunction, and sym-\nbols starting with an asterisk are variables. The\nright-hand side of the arrow is a command to up-\ndate the frame. The second element of the com-\nmand, (set :start *start), changes the\n: s ta r t atttribute value of the frame to the in-\nstance of *s tar t , which should be bound when\napplying this rule to the child feature structures.\nPhase Definitions: Each phase definition con-\nsists of a phase name, a network name, an ini-\ntiative holder specification, an initial function, an\naction function, a maximum silence duration, and\na time-out function. The network name is the\nidentifier of the language model for the speech\nrecognition. The maximum silence duration spec-\nifies how long the generation module should wait\nuntil the time-out function is invoked.\nBelow is an example of a phase definition.\nThe first element request is the name of this\nphase, &quot; f ra r_ request&quot; is the name of the\nnetwork, and move- to - reques t -phase and\nrequest -phase-act ion are the names of\nthe initial and action functions. In this phase,\nthe maximum silence duration is ten seconds and\nthe name of the time-out function is request -\nphas e- t imeou t.\n(request &quot; fmr_request&quot;\nmove- to - reques t -phase\nrequest -phase-act ion\n"},{"#tail":"\n","@confidence":"0.93337068","#text":"\nrequest -phase- t imeout )\nFor the definitions of these functions, WIT pro-\nvides functions for accessing the dialogue state,\nsending a request o speak to the speech out-\nput module, generating strings to be spoken us-\ning surface generation templates, hifting the di-\nalogue phase, taking and releasing the initiative,\nand so on. Functions are defined in terms of the\nCommon Lisp program.\nSurface-generation Templates: Surface-\ngeneration templates are used by the surface\ngeneration library function, which converts\na list-structured semantic representation to a\nsequence of strings. Each string can be spoken,\ni.e., it is in the list of pre-recorded speech files.\nFor example, let us consider the conversion\nof the semantic representation (date (date -\nexpress ion 3 15) ) to strings using the fol-\nlowing template.\n( (date\n(date -express ion *month *day))\n( (*month gatsu) (*day nichi) ) )\nThe surface generation library function matches\nthe input semantic representation with the first el-\nement of the template and checks if a sequences\n"},{"#tail":"\n","@confidence":"0.9887978","#text":"\nof strings appear in the speech file list. It re-\nturns ( ' '3gagsu l5n ich i ' ' ) (March 15th)\nif the string &quot;3gatsul5nichi&quot; s in the list of\npre-recorded speech files, and otherwise, returns\n( ' ' 3gatsu . . . . 15n ich i ' ' ) when these\nstrings are in the list.\nList of Pre-recorded Speech Files: The list of\npre-recorded speech files should show the corre-\nspondence between strings and speech files to be\nplayed by the speech output module.\n"},{"#tail":"\n","@confidence":"0.999840888888889","#text":"\nFrom the specifications explained above, domain-\ndependent knowledge sources are created as indi-\ncated by the dashed arrows in Figure 1. When cre-\nating the knowledge sources, WIT checks for sev-\neral kinds of consistency. For example, the set of\nword categories appearing in the lexicon and the\nset of word categories appearing in phrase deft-\nnifions are compared. This makes it easy to find\nerrors in the domain specifications.\n"},{"#tail":"\n","@confidence":"0.999782476190476","#text":"\nWIT has been implemented in Common Lisp and\nC on UNIX, and we have built several experi-\nmental and demonstration dialogue systems using\nit, including a meeting room reservation system\n(Nakano et al, 1999b), a video-recording pro-\ngramming system, a schedule management sys-\ntem (Nakano et al, 1999a), and a weather in-\nformation system (Dohsaka et al, 2000). The\nmeeting room reservation system has vocabulary\nof about 140 words, around 40 phrase structure\nrules, nine attributes in the semantic frame, and\naround 100 speech files. A sample dialogue be-\ntween this system and a naive user is shown\nin Figure 2. This system employs HTK as the\nspeech recognition engine. The weather informa-\ntion system can answer the user's questions about\nweather forecasts in Japan. The vocabulary size\nis around 500, and the number of phrase structure\nrules is 31. The number of attributes in the se-\nmantic flame is 11, and the number of the files of\nthe pre-recorded speech is about 13,000.\n"},{"#tail":"\n","@confidence":"0.997972833333333","#text":"\nAs explained above, the architecture of WIT al-\nlows us to develop a system that can use utter-\nances that are not clearly segmented into sen-\ntences by pauses and respond in real time. Below\nwe discuss other advantages and remaining prob-\nlems.\n"},{"#tail":"\n","@confidence":"0.996369","#text":"\nWhereas previous finite-state-model-based tool-\nkits place many severe restrictions on domain de-\nscriptions, WIT has enough descriptive power to\nbuild a variety of dialogue systems. Although the\ndialogue state is represented bya simple attribute-\nvalue matrix, since there is no limitation on the\nnumber of attributes, it can hold more compli-\ncated information. For example, it is possible to\nrepresent a discourse stack whose depth is lim-\nited. Recording some dialogue history is also\npossible. Since the language understanding mod-\nule utilizes unification, a wide variety of lin-\nguistic phenomena can be covered. For exam-\nple, speech repairs, particle omission, and fillers\ncan be dealt with in the framework of unifica-\ntion grammar (Nakano et al, 1994; Nakano and\nShimazu, 1999). The language generation mod-\nule features Common Lisp functions, so there is\nno limitation on the description. Some of the\nsystems we have developed feature a generation\nmethod based on hierarchical planning (Dohsaka\nand Shirnazu, 1997). It is also possible to build a\nsimple finite-state-model-based dialogue system\nusing WIT. States can be represented bydialogue\nphases in WIT.\n"},{"#tail":"\n","@confidence":"0.999512272727273","#text":"\nIn an agglutinative language such as Japanese,\nthere is no established definition of words, so dia-\nlogue system developers must define words. This\nsometimes causes a problem in that the defini-\ntion of word, that is, the word boundaries, in the\nspeech recognition module are different from that\nin the language understanding module. In WIT,\nhowever, since the common lexicon is used in\nboth the speech recognition module and language\nunderstanding module, the consistency between\nthem is maintained.\n"},{"#tail":"\n","@confidence":"0.999649333333333","#text":"\nIn ordinary spoken language systems, the speech\nrecognition module sends just a word hypoth-\nesis to the language processing module, which\n"},{"#tail":"\n","@confidence":"0.702436652173913","#text":"\ndonoy6na goy6ken desh6 ka (how may I help you?)\nkaigishitsu oyoyaku shitai ndesu ga (I'd like to make a reserva-\ntion for a meeting room)\nhai (uh-huh)\nsan-gatsujfini-nichi (on March 12th)\nhal (uh-huh)\njayo-ji kara (from 14:00)\nhai (uh-huh)\njashichi-ji sanjup-pun made (to 17:30)\nhai (uh-huh)\ndai-kaigishitsu (the large meeting room)\nsan-gatsu jani-nichi, j~yo-ji kara, jashichi-ji sanjup-pun made,\ndai-kaigishitsu toyfi koto de yoroshf deshrka (on March 12th,\nfrom 14:00 to 17:30, the large meeting room, is that right?) &quot;\nhai (yes)\nkashikomarimashitd (allright)\nAn example dialogue of an example system\nmust disambiguate word meaning and find phrase\nboundaries by parsing. In contrast, the speech\nrecognition module in WIT sends not only words\nbut also word categories, phrase boundaries, and\nphrase categories. This leads to less expensive\nand better language understanding.\n"},{"#tail":"\n","@confidence":"0.999884785714286","#text":"\nSeveral problems remain with WIT. One of the\nmost significant is that he system developer must\nwrite language generation functions. If the gen-\neration functions employ sophisticated dialogue\nstrategies, the system can perform complicated\ndialogues that are not just question answering.\nWIT, however, does not provide task-independent\nfacilities that make it easier to employ such dia-\nlogue strategies.\nThere have been several efforts aimed at de-\nveloping a domain-independent me hod for gen-\nerating responses from a frame representation f\nuser requests (Bobrow et al, 1977; Chu-CarroU,\n1999). Incorporating such techniques would deo\ncrease the system developer workload. However,\nthere has been no work on domain-independent\nresponse generation for robust spoken dialogue\nsystems that can deal with utterances that might\ninclude pauses in the middle of a sentence, which\nWIT handles well. Therefore incorporating those\ntechniques remains as a future work.\nAnother limitation is that WIT cannot deal with\nmultiple speech recognition candidates such as\nthose in an N-best list. Extending WIT to deal\nwith multiple recognition results would improve\nthe performance of the whole system. The ISSS\npreference mechanism is expected to play a role\nin choosing the best recognition result.\n"},{"#tail":"\n","@confidence":"0.999488857142857","#text":"\nThis paper described WIT, a toolkit for build-\ning spoken dialogue systems. Although it re-\nquires more system specifications than previous\nfinite-state-model-based toolkits, it enables one\nto easily construct real-time, robust spoken dia-\nlogue systems that incorporates advanced compu-\ntational linguistics technologies.\n"},{"#tail":"\n","@confidence":"0.993907875","#text":"\nThe authors thank Drs. Ken'ichiro Ishii, Nori-\nhiro Hagita, and Takeshi Kawabata for their sup-\nport of this research. Thanks also go to Tetsuya\nKubota, Ryoko Kima, and the members of the\nDialogue Understanding Research Group. We\nused the speech recognition engine VoiceRex de-\nveloped by NTT Cyber Space Laboratories and\nthank those who helped us use it. Comments by\n"},{"#tail":"\n","@confidence":"0.989877","#text":"\nthe anonymous reviewers were of' great help.\n"}],"#text":"\n","sectionHeader":[{"#tail":"\n","@confidence":"0.960954","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998146","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.99776","@genericHeader":"introduction","#text":"\n2 Overview\n"},{"#tail":"\n","@confidence":"0.836323","@genericHeader":"method","#text":"\n3 Architecture of WIT-Based Spoken\n"},{"#tail":"\n","@confidence":"0.994054","@genericHeader":"method","#text":"\n5 Implementation\n"},{"#tail":"\n","@confidence":"0.995103","@genericHeader":"method","#text":"\n6 Discussion\n"},{"#tail":"\n","@confidence":"0.997231","@genericHeader":"conclusions","#text":"\n7 Conclusion\n"},{"#tail":"\n","@confidence":"0.944119","@genericHeader":"acknowledgments","#text":"\nAcknowledgements\n"},{"#tail":"\n","@confidence":"0.983455","@genericHeader":"references","#text":"\nReferences\n"}],"page":[{"#tail":"\n","@confidence":"0.991817","#text":"\n150\n"},{"#tail":"\n","@confidence":"0.991844","#text":"\n151\n"},{"#tail":"\n","@confidence":"0.997889","#text":"\n152\n"},{"#tail":"\n","@confidence":"0.993801","#text":"\n153\n"},{"#tail":"\n","@confidence":"0.998634","#text":"\n154\n"},{"#tail":"\n","@confidence":"0.976089","#text":"\n10.0\n"},{"#tail":"\n","@confidence":"0.996458","#text":"\n155\n"},{"#tail":"\n","@confidence":"0.9993","#text":"\n156\n"},{"#tail":"\n","@confidence":"0.988098","#text":"\n157\n"},{"#tail":"\n","@confidence":"0.978065","#text":"\n158\n"},{"#tail":"\n","@confidence":"0.999551","#text":"\n159\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.56354725","#text":"\nuser utterance system utterance\ndomain-dependent:\nspecification knowledge source module\nFigure 1: Architecture of WIT\n"},{"#tail":"\n","@confidence":"0.522311","#text":"\nFigure 2:\n"}],"table":{"#tail":"\n","@confidence":"0.976993133333333","#text":"\nspeaker start end utterance\ntime (s) time (s)\nsystem: 614.53 615.93\nuser: 616.38 618.29\nsystem: 619.97 620.13\nuser: 622.65 624.08\nsystem: 625.68 625.91\nuser: 626.65 627.78\nsystem: 629.25 629.55\nuser: 629.91 631.67\nsystem: 633.29 633.57\nuser: 634.95 636.00\nsystem: 637.50 645.43\nuser: 645.74 646.04\nsystem: 647.05 648.20\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.350975","#tail":"\n","@no":"0","address":{"#tail":"\n","@confidence":"0.9172155","#text":"3-1 Morinosato-Wakamiya Atsugi, Kanagawa 243-0198, Japan"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.743639","#text":"Jun-ichi Hirasawa, Kohji Dohsaka, Kiyoaki Aikawa NTT Corporation"},"author":[{"#tail":"\n","@confidence":"0.959288","#text":"Mikio Nakano Noboru Miyazaki"},{"#tail":"\n","@confidence":"0.959288","#text":"Norihito Yasuda"},{"#tail":"\n","@confidence":"0.959288","#text":"Akira Sugiyama"}],"abstract":{"#tail":"\n","@confidence":"0.98853425","#text":"This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT's ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than :it is from scratch."},"title":{"#tail":"\n","@confidence":"0.999612","#text":"WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogue Systems"},"email":{"#tail":"\n","@confidence":"0.998223","#text":"E-mail:nakano@atom.brl.ntt.co.jp"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"James F. Allen, Bradford W. Miller, Eric K. Ringger, and Teresa Sikorski. 1996. A robust system for natural spoken dialogue. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (A CL-96), pages 62-70."},"#text":"\n","pages":{"#tail":"\n","#text":"62--70"},"marker":{"#tail":"\n","#text":"Allen, Miller, Ringger, Sikorski, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" Japan E-mail: nakano@atom.brl.ntt.co.jp Abstract This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental under- standing mechanism that enables ro- bust utterance understanding and real- time responses. WIT's ability to com- pile domain-dependent system specifi- cations into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defined task domain. How","@endWordPosition":"119","@position":"869","annotationId":"T1","@startWordPosition":"116","@citStr":"Allen et al, 1996"}},"title":{"#tail":"\n","#text":"A robust system for natural spoken dialogue."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (A CL-96),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"James F Allen"},{"#tail":"\n","#text":"Bradford W Miller"},{"#tail":"\n","#text":"Eric K Ringger"},{"#tail":"\n","#text":"Teresa Sikorski"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Harald Aust, Martin Oerder, Frank Seide, and Volker Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication, 17:249--262."},"journal":{"#tail":"\n","#text":"Speech Communication,"},"#text":"\n","pages":{"#tail":"\n","#text":"17--249"},"marker":{"#tail":"\n","#text":"Aust, Oerder, Seide, Steinbiss, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" Kanagawa 243-0198, Japan E-mail: nakano@atom.brl.ntt.co.jp Abstract This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental under- standing mechanism that enables ro- bust utterance understanding and real- time responses. WIT's ability to com- pile domain-dependent system specifi- cations into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defin","@endWordPosition":"115","@position":"850","annotationId":"T2","@startWordPosition":"112","@citStr":"Aust et al., 1995"}},"title":{"#tail":"\n","#text":"The Philips automatic train timetable information system."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Harald Aust"},{"#tail":"\n","#text":"Martin Oerder"},{"#tail":"\n","#text":"Frank Seide"},{"#tail":"\n","#text":"Volker Steinbiss"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"editor":{"#tail":"\n","#text":"In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editors,"},"rawString":{"#tail":"\n","#text":"James Barnett and Mona Singh. 1997. Designing a portable spoken language system. In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editors, Dialogue Processing inSpoken Language Systems, pages 156--170. Springer-Vedag."},"#text":"\n","pages":{"#tail":"\n","#text":"156--170"},"marker":{"#tail":"\n","#text":"Barnett, Singh, 1997"},"publisher":{"#tail":"\n","#text":"Springer-Vedag."},"title":{"#tail":"\n","#text":"Designing a portable spoken language system."},"booktitle":{"#tail":"\n","#text":"Dialogue Processing inSpoken Language Systems,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"James Barnett"},{"#tail":"\n","#text":"Mona Singh"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1977"},"rawString":{"#tail":"\n","#text":"Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, Dona!d A. Norman, Henry Thompson, and Terry Winograd. 1977. GUS, a frame driven dialog system. Arnficial Intelligence, 8:155-173."},"journal":{"#tail":"\n","#text":"Arnficial Intelligence,"},"#text":"\n","pages":{"#tail":"\n","#text":"8--155"},"marker":{"#tail":"\n","#text":"Bobrow, Kaplan, Kay, Norman, Thompson, Winograd, 1977"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"derstanding. 6.4 Problems and Limitations Several problems remain with WIT. One of the most significant is that he system developer must write language generation functions. If the gen- eration functions employ sophisticated dialogue strategies, the system can perform complicated dialogues that are not just question answering. WIT, however, does not provide task-independent facilities that make it easier to employ such dia- logue strategies. There have been several efforts aimed at de- veloping a domain-independent me hod for gen- erating responses from a frame representation f user requests (Bobrow et al, 1977; Chu-CarroU, 1999). Incorporating such techniques would deo crease the system developer workload. However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well. Therefore incorporating those techniques remains as a future work. Another limitation is that WIT cannot deal with multiple speech recognition candidates such as those in an N-best list. Extending WIT to deal with multiple recognition results would improve the performance of the who","@endWordPosition":"4575","@position":"28175","annotationId":"T3","@startWordPosition":"4572","@citStr":"Bobrow et al, 1977"}},"title":{"#tail":"\n","#text":"GUS, a frame driven dialog system."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Daniel G Bobrow"},{"#tail":"\n","#text":"Ronald M Kaplan"},{"#tail":"\n","#text":"Martin Kay"},{"#tail":"\n","#text":"Donad A Norman"},{"#tail":"\n","#text":"Henry Thompson"},{"#tail":"\n","#text":"Terry Winograd"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Jennifer Chu-Carroll. 1999. Fo:rrn-based reasoning for mixed-initiative dialogue management in information-query systems. In Proceedings of the Sixth European Conference on Speech Communication and Technology (Eurospeech-99) , pages 1519-1522."},"#text":"\n","pages":{"#tail":"\n","#text":"1519--1522"},"marker":{"#tail":"\n","#text":"Chu-Carroll, 1999"},"title":{"#tail":"\n","#text":"Fo:rrn-based reasoning for mixed-initiative dialogue management in information-query systems."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixth European Conference on Speech Communication and Technology (Eurospeech-99) ,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Jennifer Chu-Carroll"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Junnifer Chu-Carroll. 2000. MIMIC: An adaptive mixed initiative spoken dialogue system for information queries. In Proceedings of the 6th Conf~rence on Applied Natural Language Processing (ANLP-O0), pages 97-104."},"#text":"\n","pages":{"#tail":"\n","#text":"97--104"},"marker":{"#tail":"\n","#text":"Chu-Carroll, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ple, has three phases: the phase in which the user tells the system his/her equest, he phase in which the system confirms it, and the phase in which the system tells the user the result of the database access. In the first two phases, the user holds the initiative, and in the last phase, the sys- tern holds the initiative. Functions defined here decide what string should be spoken and send that string to the speech output module based on the current di- alogue state. They can also shift the dialogue 2The notion of the initiative inthis paper isdifferent from that of the dialogue initiative of Chu-Carroll (2000). phase and change the holder of the initiative as well as change the dialogue state. When the dia- logue phase shifts, the language model foi&quot; speech recognition is changed to get better speech recog- nition performance. Typically, the language gen- eration module is responsible for database access. The language generation module works as fol- lows. It first checks which dialogue participant has the initiative. If the initiative is held by the user, it waits until the user's speech interval ends or a duration of silence after the end of a system utterance is detected. The action function in t","@endWordPosition":"2070","@position":"12816","annotationId":"T4","@startWordPosition":"2069","@citStr":"Chu-Carroll (2000)"}},"title":{"#tail":"\n","#text":"MIMIC: An adaptive mixed initiative spoken dialogue system for information queries."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 6th Conf~rence on Applied Natural Language Processing (ANLP-O0),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Junnifer Chu-Carroll"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Kohji Dohsaka nd Akira Shimazu. 1997. System architecture for spoken utterance production in collaborative dialogue. In Working Notes of IJCAI 1997 Workshop on Collaboration, Cooperation and Conflict in Dialogue Systems."},"#text":"\n","marker":{"#tail":"\n","#text":"1997"},"title":{"#tail":"\n","#text":"Kohji Dohsaka nd Akira Shimazu."},"booktitle":{"#tail":"\n","#text":"In Working Notes of IJCAI"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Kohji Dohsaka, Norihito Yasuda, Noboru Miyazaki, Mikio Nakano, and Kiyoaki AJkawa. 2000. An efficient dialogue control method under system's limited knowledge. In Proceedings of the Sixth International Conference on Spoken Language Processing (ICSLP-O0)."},"#text":"\n","marker":{"#tail":"\n","#text":"Dohsaka, Yasuda, Miyazaki, Nakano, AJkawa, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" for sev- eral kinds of consistency. For example, the set of word categories appearing in the lexicon and the set of word categories appearing in phrase deft- nifions are compared. This makes it easy to find errors in the domain specifications. 5 Implementation WIT has been implemented in Common Lisp and C on UNIX, and we have built several experi- mental and demonstration dialogue systems using it, including a meeting room reservation system (Nakano et al, 1999b), a video-recording pro- gramming system, a schedule management sys- tem (Nakano et al, 1999a), and a weather in- formation system (Dohsaka et al, 2000). The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue be- tween this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather informa- tion system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the se- mantic flame is 11, and the number of the files of the pre-recorded sp","@endWordPosition":"3868","@position":"23600","annotationId":"T5","@startWordPosition":"3865","@citStr":"Dohsaka et al, 2000"}},"title":{"#tail":"\n","#text":"An efficient dialogue control method under system's limited knowledge."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixth International Conference on Spoken Language Processing (ICSLP-O0)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kohji Dohsaka"},{"#tail":"\n","#text":"Norihito Yasuda"},{"#tail":"\n","#text":"Noboru Miyazaki"},{"#tail":"\n","#text":"Mikio Nakano"},{"#tail":"\n","#text":"Kiyoaki AJkawa"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Jun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano, and Takeshi Kawabata. 1998. Implementation of coordinative nodding behavior on spoken dialogue systems. In Proceedings of the Fgth International Conference on Spoken Language Processing (1CSLP-98), pages 2347-2350."},"#text":"\n","pages":{"#tail":"\n","#text":"2347--2350"},"marker":{"#tail":"\n","#text":"Hirasawa, Miyazaki, Nakano, Kawabata, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ow they interact with each other. 3.1 Speech Recognition The speech recognition module is a phoneme- HMM-based speaker-independent continuous speech recognizer that incrementally outputs face Toolldt. word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI&quot; (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous peech database of the Acoustical Society of Japan (Kobayashi et al, 1992). This recognizer incrementally outputs word hypotheses a soon as they are found in the best-scored path in the forward search (Hirasawa et al, 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation module to react immediately touser interruptions while the system is speaking. The language model for speech recognition is a network (regular) grammar, ","@endWordPosition":"896","@position":"5872","annotationId":"T6","@startWordPosition":"893","@citStr":"Hirasawa et al, 1998"}},"title":{"#tail":"\n","#text":"Implementation of coordinative nodding behavior on spoken dialogue systems."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fgth International Conference on Spoken Language Processing (1CSLP-98),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jun-ichi Hirasawa"},{"#tail":"\n","#text":"Noboru Miyazaki"},{"#tail":"\n","#text":"Mikio Nakano"},{"#tail":"\n","#text":"Takeshi Kawabata"}]}},{"date":{"#tail":"\n","#text":"1992"},"issue":{"#tail":"\n","#text":"12"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"by WIT. 3 Architecture of WIT-Based Spoken Dialogue Systems Here we explain how the modules in WIT work by exploiting domain-dependent k owledge and how they interact with each other. 3.1 Speech Recognition The speech recognition module is a phoneme- HMM-based speaker-independent continuous speech recognizer that incrementally outputs face Toolldt. word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI&quot; (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous peech database of the Acoustical Society of Japan (Kobayashi et al, 1992). This recognizer incrementally outputs word hypotheses a soon as they are found in the best-scored path in the forward search (Hirasawa et al, 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation modu","@endWordPosition":"872","@position":"5723","annotationId":"T7","@startWordPosition":"869","@citStr":"Kobayashi et al, 1992"}},"title":{"#tail":"\n","#text":"Asj continuous speech corpus for research."},"volume":{"#tail":"\n","#text":"48"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Tetsunod Kobayashi, Shuichi Itahashi, Satoru Hayamizu, and Toshiyuki Takezawa. 1992. Asj continuous speech corpus for research. The journal of th e Acoustical Society of Japan, 48(12): 888-893."},"journal":{"#tail":"\n","#text":"The journal of th e Acoustical Society of Japan,"},"#text":"\n","pages":{"#tail":"\n","#text":"888--893"},"marker":{"#tail":"\n","#text":"Kobayashi, Itahashi, Hayamizu, Takezawa, 1992"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Tetsunod Kobayashi"},{"#tail":"\n","#text":"Shuichi Itahashi"},{"#tail":"\n","#text":"Satoru Hayamizu"},{"#tail":"\n","#text":"Toshiyuki Takezawa"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"editor":{"#tail":"\n","#text":"In Yorick Wilks, editor,"},"rawString":{"#tail":"\n","#text":"Mikio Nakano and Akira Shimazu. 1999. Parsing utterances including self-repairs. In Yorick Wilks, editor, Machine Conversations, pages 99-112. Kluwer Academic Publishers."},"#text":"\n","pages":{"#tail":"\n","#text":"99--112"},"marker":{"#tail":"\n","#text":"1999"},"publisher":{"#tail":"\n","#text":"Kluwer Academic Publishers."},"title":{"#tail":"\n","#text":"Mikio Nakano and Akira Shimazu."},"booktitle":{"#tail":"\n","#text":"Machine Conversations,"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Mikio Nakano, Aldra Shimazu, and Kiyoshi Kogure. 1994. A grammar and a parser for spontaneous speech. In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94), pages 1014-1020."},"#text":"\n","pages":{"#tail":"\n","#text":"1014--1020"},"marker":{"#tail":"\n","#text":"Nakano, Shimazu, Kogure, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"to build a variety of dialogue systems. Although the dialogue state is represented bya simple attribute- value matrix, since there is no limitation on the number of attributes, it can hold more compli- cated information. For example, it is possible to represent a discourse stack whose depth is lim- ited. Recording some dialogue history is also possible. Since the language understanding mod- ule utilizes unification, a wide variety of lin- guistic phenomena can be covered. For exam- ple, speech repairs, particle omission, and fillers can be dealt with in the framework of unifica- tion grammar (Nakano et al, 1994; Nakano and Shimazu, 1999). The language generation mod- ule features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented bydialogue phases in WIT. 6.2 Consistency In an agglutinative language such as Japanese, there is no established definition of words, so dia- logue system developers must define words. This sometimes causes a problem ","@endWordPosition":"4139","@position":"25264","annotationId":"T8","@startWordPosition":"4136","@citStr":"Nakano et al, 1994"}},"title":{"#tail":"\n","#text":"A grammar and a parser for spontaneous speech."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mikio Nakano"},{"#tail":"\n","#text":"Aldra Shimazu"},{"#tail":"\n","#text":"Kiyoshi Kogure"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"Mildo Nakano, Kohji Dohsaka, Noboru Miyazald, Inn ichi Hirasawa, Masafiami Tamoto, Masahito Kawarnon, Akira Sugiyama, and Takeshi Kawabata. 1999a. Handling rich turn-taking in spoken dialogue systems. In Proceedings of the Sixth European Conference on Speech Communication a d Technology (Eurospeech-99), pages 1167-1170."},"#text":"\n","pages":{"#tail":"\n","#text":"1167--1170"},"marker":{"#tail":"\n","#text":"Nakano, Dohsaka, "},"title":{"#tail":"\n","#text":"Noboru Miyazald, Inn ichi Hirasawa, Masafiami Tamoto, Masahito Kawarnon, Akira Sugiyama, and Takeshi Kawabata. 1999a. Handling rich turn-taking in spoken dialogue systems."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixth European Conference on Speech Communication a d Technology (Eurospeech-99),"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mildo Nakano"},{"#tail":"\n","#text":"Kohji Dohsaka"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa, Kohji Dohsaka, and Takeshi Kawabata. 1999b. Understanding unsegmented user utterances in realtime spoken dialogue systems. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99), pages 200--207."},"#text":"\n","pages":{"#tail":"\n","#text":"200--207"},"marker":{"#tail":"\n","#text":"Nakano, Miyazaki, Hirasawa, Dohsaka, Kawabata, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"k owledge sources. Creat- ing and maintaining these knowledge sources re- quire much effort, thus a toolkit would be help- ful. Previous toolkits, however, do not allow us to achieve these features, or do not provide mecha- nisms that achieve these features without requir- ing excessive fforts by the developers. This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter- 150 for building spoken dialogue systems that inte- grate speech recognition, language understanding and generation, and speech output. WIT features an incremental understanding method (Nakano et al., 1999b) that makes it possible to build a robust and real-time system. In addition, WIT compiles domain-dependent system specifications into in- ternal knowledge sources o that building systems is easier. Although WIT requires more domain- dependent specifications than finite-state-model- based toolkits, WIT-based systems are capable of taking full advantage of language processing technology. WIT has been implemented and used to build several spoken dialogue systems. In what follows, we overview WIT, explain its architecture, domain-dependent system specifica- tions, and implementation, and then di","@endWordPosition":"587","@position":"3771","annotationId":"T9","@startWordPosition":"584","@citStr":"Nakano et al., 1999"},{"#tail":"\n","#text":"ough the current version of WIT does not exploit probabilistic language models, such mod- els can be incorporated without changing the ba- sic WIT architecture. 3.2 Language Understanding The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the se- quence of the word hypotheses to update the di- alogue state, in which the resnlt of understand- ing and discourse information are represented by a frame (i.e., attribute-value pairs). The un- derstanding module utilizes ISSS (Incremental Significant-utterance Sequence Search) (Nakano et al, 1999b), which is an integrated parsing and discourse processing method. ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars- ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence. ISSS also makes it possible for the language generation module to respond in real time because it can out- put a partial result of understanding at any point in time. The domain-dependent knowledge used in this module consists of a u","@endWordPosition":"1519","@position":"9350","annotationId":"T10","@startWordPosition":"1516","@citStr":"Nakano et al, 1999"},{"#tail":"\n","#text":" is executed in the former case; the time-out function is executed in the latter case. Then it goes back to the initial stage. If the system holds the initiative, the mod- ule executes the initial function of the phase. In typical question-answer systems, the user has the initiative when asking questions and the system has it when answering. Since the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that ut- terance understanding is possible even while it is speaking (Nakano et al, 1999a). Thus the system can respond immediately after user pauses when the user has the initiative. When the system holds the initiative, it can immediately react to an in- terruption by the user because user utterances are understood in an incremental way (Dohsaka nd Shimazu, 1997). The time-out function is effective in moving the dialogue forward when the dialogue gets stuck for some reason. For example, the system may be able to repeat the same question with an- other expression and may also be able to ask the user a more specific question. 3.4 Speech Output The speech output module produces pe","@endWordPosition":"2276","@position":"14073","annotationId":"T11","@startWordPosition":"2273","@citStr":"Nakano et al, 1999"},{"#tail":"\n","#text":"ority increase) ) ((role name) (child feature structure) ? . . (child feature structure) => (flame operation command) (priority increase) ) These roles are similar to DCG (Pereira nd War- ren, 1980) rules; they can include logical vari- ables and these variables can be bound when these rules are applied. It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano, 199 I), but we do not explain these constraints indetail in this 154 paper. The priorities are used for disambiguat- ing interpretation i the incremental understand- ing method (Nakano et al, 1999b). When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be consid- ered a sentence, and the sentence's semantic rep- resentation is the command for updating the dia- logue state. The command is one of the follow- ing: ? A command to set the value of an attribute of the frame, ? A command to increase the priority, Conditional commands (If-then-else type command, the condition being whether the value of an attribute of the flame is or is not equal to a specified value, or a conjunction or disjunction of the above condit","@endWordPosition":"3026","@position":"18767","annotationId":"T12","@startWordPosition":"3023","@citStr":"Nakano et al, 1999"},{"#tail":"\n","#text":"bove, domain- dependent knowledge sources are created as indi- cated by the dashed arrows in Figure 1. When cre- ating the knowledge sources, WIT checks for sev- eral kinds of consistency. For example, the set of word categories appearing in the lexicon and the set of word categories appearing in phrase deft- nifions are compared. This makes it easy to find errors in the domain specifications. 5 Implementation WIT has been implemented in Common Lisp and C on UNIX, and we have built several experi- mental and demonstration dialogue systems using it, including a meeting room reservation system (Nakano et al, 1999b), a video-recording pro- gramming system, a schedule management sys- tem (Nakano et al, 1999a), and a weather in- formation system (Dohsaka et al, 2000). The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue be- tween this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather informa- tion system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, ","@endWordPosition":"3844","@position":"23446","annotationId":"T13","@startWordPosition":"3841","@citStr":"Nakano et al, 1999"}]},"title":{"#tail":"\n","#text":"Understanding unsegmented user utterances in realtime spoken dialogue systems."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mikio Nakano"},{"#tail":"\n","#text":"Noboru Miyazaki"},{"#tail":"\n","#text":"Jun-ichi Hirasawa"},{"#tail":"\n","#text":"Kohji Dohsaka"},{"#tail":"\n","#text":"Takeshi Kawabata"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Mikio Nakano. 1991. Constraint projection: An efficient treatment of disjunctive f ature descriptions. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL90, pages 307-314."},"#text":"\n","pages":{"#tail":"\n","#text":"307--314"},"marker":{"#tail":"\n","#text":"Nakano, 1991"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rior to pars- ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence. ISSS also makes it possible for the language generation module to respond in real time because it can out- put a partial result of understanding at any point in time. The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules. Disjunctive feature descriptions are also possible; WIT incorporates an efficient method for handling disjunctions (Nakano, 1991). When a phrase boundary is de- tected, the feature structure for a phrase is com- puted using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic repre- sentation for found sentences. Two kinds of sen- tenees can be considered; domain-related ones that express the user's intention about he reser- 152 vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confir","@endWordPosition":"1635","@position":"10132","annotationId":"T14","@startWordPosition":"1634","@citStr":"Nakano, 1991"}},"title":{"#tail":"\n","#text":"Constraint projection: An efficient treatment of disjunctive f ature descriptions."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL90,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Mikio Nakano"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"note":{"#tail":"\n","#text":"(in Japanese)."},"rawString":{"#tail":"\n","#text":"Yoshiaki Noda, Yoshikazu Yamaguchi, Tomokazu Yamada, Akihiro Imamura, Satoshi Takahashi, Tomoko Matsui, and Kiyoaki Aikawa. 1998. The development of speech recognition engine REX. In Proceedings of the 1998 1EICE General Conference D-14-9, page 220. (in Japanese)."},"#text":"\n","pages":{"#tail":"\n","#text":"14--9"},"marker":{"#tail":"\n","#text":"Noda, Yamaguchi, 1998"},"title":{"#tail":"\n","#text":"Tomokazu Yamada, Akihiro Imamura, Satoshi Takahashi, Tomoko Matsui, and Kiyoaki Aikawa."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 1998 1EICE General Conference"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yoshiaki Noda"},{"#tail":"\n","#text":"Yoshikazu Yamaguchi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1980"},"rawString":{"#tail":"\n","#text":"Fernando C. N. Pereira and David H. D. Warren. 1980. Definite clause grammars for language analysis--a survey of the formalism and a comparison with augmented transition etworks. Artificial Intelligence, 13:231-278."},"journal":{"#tail":"\n","#text":"Artificial Intelligence,"},"#text":"\n","pages":{"#tail":"\n","#text":"13--231"},"marker":{"#tail":"\n","#text":"Pereira, Warren, 1980"},"title":{"#tail":"\n","#text":"Definite clause grammars for language analysis--a survey of the formalism and a comparison with augmented transition etworks."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Fernando C N Pereira"},{"#tail":"\n","#text":"David H D Warren"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. CSLI, Stanford."},"#text":"\n","marker":{"#tail":"\n","#text":"Pollard, Sag, 1994"},"location":{"#tail":"\n","#text":"Stanford."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"hat the language generation module can take into account the tim- ing of the end of system utterance. The meeting room reservation system uses speech files of short 153 phrases. 4 Building Spoken Dialo~te Systems with WIT 4.1 Domain-Dependent System Specifications Spoken dialogue systems can be built with WIT by preparing several domain-dependent specifica- tions. Below we explain the specifications. Feature Definitions: Feature definitions pec- ify the set of features used in the grammar for lan- guage understanding. They also specify whether each feature is a head feature or a foot feature (Pollard and Sag, 1994). This information isused when constructing feature structures for phrases in a built-in process. The following is an example of a feature defini- tion. Here we use examples from the specification of the meeting room reservation system. (case head) It means that the case feature is used and it is a head feature 3. Lexieal Descriptions: Lexical descriptions specify both pronunciations and grammatical features for words. Below is an example lexical item for the word 1-gatsu (January). (l-gatsu ichigatsu month nil i) The first three elements are the identifier, the pro- nunciation, and the gramma","@endWordPosition":"2508","@position":"15529","annotationId":"T15","@startWordPosition":"2505","@citStr":"Pollard and Sag, 1994"}},"booktitle":{"#tail":"\n","#text":"Head-Driven Phrase Structure Grammar. CSLI,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Carl J Pollard"},{"#tail":"\n","#text":"Ivan A Sag"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Munehiko Sasajima, Yakehide Yano, and Yasuyuki Kono. 1999. EUROPA: A genetic framework for developing spoken dialogue systems. In Proceedings of the Sixth European Conference on Speech Communication a d Technology (Eurospeech-99), pages 1163--1166."},"#text":"\n","pages":{"#tail":"\n","#text":"1163--1166"},"marker":{"#tail":"\n","#text":"Sasajima, Yano, Kono, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tions into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defined task domain. However, it limits system functions; it is not easy to employ the advanced language processing techniques de- veloped in the realm of computational linguis- tics. Another is GALAXY-II (Seneffet al, 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a ","@endWordPosition":"174","@position":"1186","annotationId":"T16","@startWordPosition":"171","@citStr":"Sasajima et al, 1999"}},"title":{"#tail":"\n","#text":"EUROPA: A genetic framework for developing spoken dialogue systems."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixth European Conference on Speech Communication a d Technology (Eurospeech-99),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Munehiko Sasajima"},{"#tail":"\n","#text":"Yakehide Yano"},{"#tail":"\n","#text":"Yasuyuki Kono"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Stephanie Seneff, Ed Hurley, Raymond Lau, Chrisfine Pao, Philipp Sehmid, and Victor Zue. 1998. GALAXY-H: A reference architecture for conversational system development. In Proceedings of the Fifth International Con l~rence on Spoken Language Processing (ICSLP-98). Stephen Sutton, Ronaid A. Cole, Jacques de Villiers, Johan SchMkwyk, Pieter Vermeulen, Michael W."},"#text":"\n","marker":{"#tail":"\n","#text":"Seneff, Hurley, Lau, Pao, Sehmid, Zue, 1998"},"title":{"#tail":"\n","#text":"GALAXY-H: A reference architecture for conversational system development."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth International Con l~rence on Spoken Language Processing (ICSLP-98). Stephen"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Stephanie Seneff"},{"#tail":"\n","#text":"Ed Hurley"},{"#tail":"\n","#text":"Raymond Lau"},{"#tail":"\n","#text":"Chrisfine Pao"},{"#tail":"\n","#text":"Philipp Sehmid"},{"#tail":"\n","#text":"Victor Zue"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Macon, Yonghong Yah, Edward Kaiser, Brian Rundie, K.haldoun Shobaki, Paul Hosom, Alex Kain, Johan Wouters, Dominic W. Massaro, and Michael Cohen. 1998. Universal speech tools: The CSLU toolkit. In Proceedings of the Fifth International Conference on Spoken Language Processing (1CSLP-98), pages 3221-3224."},"#text":"\n","pages":{"#tail":"\n","#text":"3221--3224"},"marker":{"#tail":"\n","#text":"Macon, Kaiser, Rundie, Shobaki, Hosom, Kain, Wouters, Massaro, Cohen, 1998"},"title":{"#tail":"\n","#text":"Universal speech tools: The CSLU toolkit."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth International Conference on Spoken Language Processing (1CSLP-98),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yonghong Yah Macon"},{"#tail":"\n","#text":"Edward Kaiser"},{"#tail":"\n","#text":"Brian Rundie"},{"#tail":"\n","#text":"K haldoun Shobaki"},{"#tail":"\n","#text":"Paul Hosom"},{"#tail":"\n","#text":"Alex Kain"},{"#tail":"\n","#text":"Johan Wouters"},{"#tail":"\n","#text":"Dominic W Massaro"},{"#tail":"\n","#text":"Michael Cohen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Marilyn Walker, Irene Langkilde, Jerry Wright, Allen Gorin, and Diane Litman. 2000. Learning to predict problematic situations in a spoken dialogue system: Experiments with how may I help you? In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAA CL-O0), pages 210--217."},"#text":"\n","pages":{"#tail":"\n","#text":"210--217"},"marker":{"#tail":"\n","#text":"Walker, Langkilde, Wright, Gorin, Litman, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"o.jp Abstract This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental under- standing mechanism that enables ro- bust utterance understanding and real- time responses. WIT's ability to com- pile domain-dependent system specifi- cations into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defined task domain. However, it limits system functions; it i","@endWordPosition":"127","@position":"907","annotationId":"T17","@startWordPosition":"124","@citStr":"Walker et al, 2000"}},"title":{"#tail":"\n","#text":"Learning to predict problematic situations in a spoken dialogue system: Experiments with how may I help you?"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAA CL-O0),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Marilyn Walker"},{"#tail":"\n","#text":"Irene Langkilde"},{"#tail":"\n","#text":"Jerry Wright"},{"#tail":"\n","#text":"Allen Gorin"},{"#tail":"\n","#text":"Diane Litman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Victor Zue, Stephanie Seneff, James Glass, Joseph Polifroni, Christine Pao, Timothy J. Hazen, and Lee He~erington. 2000. Jupiter: A telephone-based conversational interface for weather information. 1EEE Transactions on Speech and Audio Processing, 8(1):85-96."},"#text":"\n","pages":{"#tail":"\n","#text":"8--1"},"marker":{"#tail":"\n","#text":"Zue, Seneff, Glass, Polifroni, Pao, Hazen, Heerington, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"no@atom.brl.ntt.co.jp Abstract This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental under- standing mechanism that enables ro- bust utterance understanding and real- time responses. WIT's ability to com- pile domain-dependent system specifi- cations into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defined task domain. However, it limits s","@endWordPosition":"123","@position":"886","annotationId":"T18","@startWordPosition":"120","@citStr":"Zue et al, 2000"}},"title":{"#tail":"\n","#text":"Jupiter: A telephone-based conversational interface for weather information."},"booktitle":{"#tail":"\n","#text":"1EEE Transactions on Speech and Audio Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Victor Zue"},{"#tail":"\n","#text":"Stephanie Seneff"},{"#tail":"\n","#text":"James Glass"},{"#tail":"\n","#text":"Joseph Polifroni"},{"#tail":"\n","#text":"Christine Pao"},{"#tail":"\n","#text":"Timothy J Hazen"},{"#tail":"\n","#text":"Lee Heerington"}]}}]}}]}}
