{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd ACL, Ann Arbor, MI."},"#text":"\n","marker":{"#tail":"\n","#text":"Chiang, 2005"},"location":{"#tail":"\n","#text":"Ann Arbor, MI."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" many syntax-based models where translation rules directly encode linguistic knowledge. Typically, these models extract rules using parse trees from both or either side(s) of the bitext. The former case, with trees on both sides, is often called tree-to-tree models; while the latter case, with trees on either source or target side, include both treeto-string and string-to-tree models (see Table 1). Leveraging from structural and linguistic information from parse trees, these models are believed to be better than their phrase-based counterparts in tree-to-string string-to-tree string-to-string Chiang (2005) Table 1: A classification of syntax-based MT. The first three use linguistic syntax, while the last one only formal syntax. Our experiments cover the second type using a packed forest in place of the tree for rule-extraction. handling non-local reorderings, and have achieved promising translation results.1 However, these systems suffer from a major limitation, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors. To make things worse, modern statistical parsers are often trained on domains quite different from those used in","@endWordPosition":"297","@position":"2041","annotationId":"T1","@startWordPosition":"296","@citStr":"Chiang (2005)"},{"#tail":"\n","#text":"re 5, with only PP and VPB swapped from the English word order. 3Admissible set (Wang et al., 2007) is also known as \u201cfrontier set\u201d (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. IP0,6 NPB0,1 VP1,6 B`ushi PP1,3 VPB3,6 P1,2 NPB2,3 jˇuxing le huit´an yˇu Sh¯al´ong 209 These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6, as shown in Figure 4. Such a forest has a structure of a hypergraph (Huang and Chiang, 2005), where items like NP0, 3 are called nodes, whose indices denote the source span, and combinations like e1 : IP0, 6 → NPB0, 3 VP3, 6 we call hyperedges. We denote head(e) and tails(e) to be the consequent and antecedant items of hyperedge e, respectively. For example, head(e1) = IP0, 6, tails(e1) = {NPB0, 3,VP3, 6}. We also denote BS(v) to be the set of incoming hyperedges of node v, being different ways of deriving it. For example, in Figure 4, BS(IP0, 6) = {e1, e2}. 3.2 Forest-based Rule Extraction Algorithm Like in tree-based extraction, we extract rules from a packed forest F in two steps:","@endWordPosition":"2248","@position":"13511","annotationId":"T2","@startWordPosition":"2247","@citStr":"Chiang, 2005"},{"#tail":"\n","#text":"o 0.2738 Table 4: BLEU score results trained on large data. during both rule extraction and decoding phases. Since the data scale is larger than the small data, we are forced to use harsher pruning thresholds, with pe = 5 for extraction and pd = 10 for decoding. The final BLEU score results are shown in Table 4. With both tree-based and forest-based decoding, rules extracted from forests significantly outperform those extracted from 1-best trees (p < 0.01). The final result with both forest-based extraction and forest-based decoding reaches a BLEU score of 0.2816, outperforming that of Hiero (Chiang, 2005), one of the best performing systems to date. These results confirm that our novel forest-based rule extraction approach is a promising direction for syntaxbased machine translation. 6 Conclusion and Future Work In this paper, we have presented a novel approach that extracts translation rules from a packed forest encoding exponentially many trees, rather than from 1-best or k-best parses. Experiments on a state-ofthe-art tree-to-string system show that this method improves BLEU score significantly, with reasonable extraction speed. When combined with our previous work on forest-based decoding,","@endWordPosition":"4558","@position":"26941","annotationId":"T3","@startWordPosition":"4557","@citStr":"Chiang, 2005"}]},"title":{"#tail":"\n","#text":"A hierarchical phrase-based model for statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 43rd ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David Chiang"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"rm better than these linguistically sophisticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1For example, in recent NIST Evaluations, some of these models (Galley et al., 2006; Quirk et al., 2005; Liu et al., 2006) ranked among top 10. See http://www.nist.gov/speech/tests/mt/. Liu et al. (2006); Huang et al. (2006) Galley et al. (2006) 206 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206\u2013214, Honolulu, October 2008.c�2008 Association for Computational Linguistics IP → x1 x3 with x2 yˇu Figure 1: Example translation rule r1. The Chinese conjunction yˇu \u201cand\u201d is translated into English prep. \u201cwith\u201d. also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k2 simil","@endWordPosition":"506","@position":"3367","annotationId":"T4","@startWordPosition":"503","@citStr":"Galley et al., 2006"},{"#tail":"\n","#text":"n improves BLEU score by over 1 point on a state-of-the-art tree-to-string system (Liu et al., 2006; Mi et al., 2008), which is also 0.5 points better than (and twice as fast as) extracting on 30-best parses. When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date. Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-to-tree models. r2 ⇓ r3 ⇓ Bush held with NPB NPB hu`ıt´an Sh¯al´ong r� ⇓ r5 ⇓ Bush held a meeting with Sharon r2 NPB(B`ush´ı) → Bush r3 VPB(VV(jˇux´ıng) AS(le) x1:NPB) → held x1 r4 NPB(Sh¯al´ong) → Sharon r5 NPB(hu`ıt´an) → a meeting Figure 2: Example derivation of tree-to-string translation, with rules used. Each shaded region denotes a tree fragment that is pattern-matched with the rule being applied. B`ush´ı le hu`ıt´an yˇu Sh¯al´ong hu`ıt´an le (a) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an ⇓ 1-","@endWordPosition":"736","@position":"4835","annotationId":"T5","@startWordPosition":"733","@citStr":"Galley et al., 2006"},{"#tail":"\n","#text":"elided. in the figure), which serve as potential cut-points for rule extraction.3 With the admissible set computed, rule extraction is as simple as a depth-first traversal from the root: we \u201ccut\u201d the tree at all admissible nodes to form tree fragments and extract a rule for each fragment, with variables matching the admissible descendant nodes. For example, the tree in Figure 3 is cut into 6 pieces, each of which corresponds to a rule on the right. These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig. 1) (Galley et al., 2006). Our experiments use composed rules. 3 Forest-based Rule Extraction We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu:","@endWordPosition":"2021","@position":"12189","annotationId":"T6","@startWordPosition":"2018","@citStr":"Galley et al., 2006"},{"#tail":"\n","#text":"f rule r is simply αβ(lhs(r)) c(r) = (3) αβ(TOP) where TOP denotes the root node of the forest. Like in the M-step in EM algorithm, we now extend the maximum likelihood estimation to fractional counts for three conditional probabilities regarding a rule, which will be used in the experiments: c(r) P(r |lhs(r)) = Er\u2032:lhs(r\u2032)=lhs(r) c(r\u2032), (4) Pr rhs r)) = c(r) ( I ( Er�:rhs(r')=rhs(r) c(r\u2032), (5) P(r |root(lhs(r))) c(r) (6) = Er\u2032:root(lhs(r\u2032))=root(lhs(r)) c(r\u2032). 4 Related Work The concept of packed forest has been previously used in translation rule extraction, for example in rule composition (Galley et al., 2006) and tree binarization (Wang et al., 2007). However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest. Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial. The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). The first direct application of parse ","@endWordPosition":"3348","@position":"19740","annotationId":"T7","@startWordPosition":"3345","@citStr":"Galley et al., 2006"}]},"title":{"#tail":"\n","#text":"Scalable inference and training of context-rich syntactic translation models."},"booktitle":{"#tail":"\n","#text":"In Proceedings of COLING-ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michel Galley"},{"#tail":"\n","#text":"Jonathan Graehl"},{"#tail":"\n","#text":"Kevin Knight"},{"#tail":"\n","#text":"Daniel Marcu"},{"#tail":"\n","#text":"Steve DeNeefe"},{"#tail":"\n","#text":"Wei Wang"},{"#tail":"\n","#text":"Ignacio Thayer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL: HLT, Columbus, OH."},"#text":"\n","marker":{"#tail":"\n","#text":"Mi, Huang, Liu, 2008"},"location":{"#tail":"\n","#text":"Columbus, OH."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" x3 with x2 yˇu Figure 1: Example translation rule r1. The Chinese conjunction yˇu \u201cand\u201d is translated into English prep. \u201cwith\u201d. also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k2 similar tree-pairs in tree-to-tree models). We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists. Experiments (Section 5) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system (Liu et al., 2006; Mi et al., 2008), which is also 0.5 points better than (and twice as fast as) extracting on 30-best parses. When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date. Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-t","@endWordPosition":"656","@position":"4332","annotationId":"T8","@startWordPosition":"653","@citStr":"Mi et al., 2008"},{"#tail":"\n","#text":" English: \u201cBush held a meeting2 with Sharon1\u201d Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into a parse tree (b), which will be converted into an English string in 5 steps. First, at the root node, we apply rule r1 shown in Figure 1, which translates the Chinese coordination construction (\u201c... and ...\u201d) into an English prepositional phrase. Then, from step (c) we continue applying rules to untranslated Chinese subtrees, until we get the complete English translation in (e).2 2We swap the 1-best and 2-best parses of the example sentence from our earlier paper (Mi et al., 2008), since the current 1-best parse is easier to illustrate the rule extraction algorithm. 207 IP \u201cBush .. Sharon\u201d → x1 x4 x2 x3 CC (yˇu) → with NPB (B`ush´ı) → Bush NPB (Sh¯al´ong) → Sharon VPB (VV(jˇux´ıng) AS(le) x1:NPB) → held x1 NPB (hu`ıt´an) → a meeting NP \u201cBush ⊔ with Sharon\u201d NPB \u201cBush\u201d B`ush´ı NPB \u201cSharon\u201d Sh¯al´ong VV \u201cheld\u201d jˇux´ıng NPB \u201ca meeting\u201d hu`ıt´an AS \u201cheld\u201d le Bush held a meeting with Sharon VPB \u201cheld .. meeting\u201d CC \u201cwith\u201d yˇu (minimal) rules extracted IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) Figure 3: Tree-based rule extraction (Galley et al., 2004). Each non-leaf node in the tre","@endWordPosition":"1079","@position":"6862","annotationId":"T9","@startWordPosition":"1076","@citStr":"Mi et al., 2008"},{"#tail":"\n","#text":"tree-based extraction, we extract rules from a packed forest F in two steps: (1) admissible set computation (where to cut), and (2) fragmentation (how to cut). It turns out that the exact formulation developed for admissible set in the tree-based case can be applied to a forest without any change. The fragmentation step, however, becomes much more involved since we now face a choice of multiple parse hyperedges at each node. In other words, it becomes nondeterministic how to \u201ccut\u201d a forest into tree fragments, which is analogous to the non-deterministic pattern-match in forest-based decoding (Mi et al., 2008). For example there are two parse hyperedges e1 and e2 at the root node in Figure 4. When we follow one of them to grow a fragment, there again will be multiple choices at each of its tail nodes. Like in tree-based case, a fragment is said to be complete if all its leaf nodes are admissible. Otherwise, an incomplete fragment can grow at any non-admissible frontier node v, where following each parse hyperedge at v will split off a new fragment. For example, following e2 at the root node will immediately lead us to two admissible nodes, NPB0, 1 and VP1, 6 (we will highlight admissible nodes by g","@endWordPosition":"2447","@position":"14651","annotationId":"T10","@startWordPosition":"2444","@citStr":"Mi et al., 2008"},{"#tail":"\n","#text":"However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest. Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial. The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below). This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding. 11 · e E fra�g7 11 · v E yield(frag) P(e) (2) β(v) 211 BLEU score Our experiments will use both default 1-best decoding and forest-based decoding. As we will see in the next section, the best result comes when we combine the merits of both, i.e., using forests in both rule extraction and decoding. There is also a parallel work on extracting rules from k-best pars","@endWordPosition":"3455","@position":"20400","annotationId":"T11","@startWordPosition":"3452","@citStr":"Mi et al., 2008"},{"#tail":"\n","#text":"and translation length penalties, respectively. The conditional probability P(d |T) decomposes into the product of rule probabilities: P(d |T) = 11 P(r). (8) r∈d Each P(r) is in turn a product of five probabilities: P(r) =P(r |lhs(r))A' · P(r |rhs(r))A' · P(r |root(lhs(r)))As · Pllm(lhs(r) |rhs(r))A' · Pllm(rhs(r) |lhs(r))A' where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.3, and the last two are lexical probabilities. These parameters Al ... As are tuned by minimum error rate training (Och, 2003) on the dev sets. We refer readers to Mi et al. (2008) for details of the decoding algorithm. 0 1 2 3 4 5 6 average extracting time (secs/1000 sentences) Figure 6: Comparison of extraction time and BLEU score: forest-based vs.1-best and 30-best. rules from... extraction decoding BLEU 1-best trees 0.24 1.74 0.2430 30-best trees 5.56 3.31 0.2488 forest: pe=8 2.36 3.40 0.2533 Pharaoh - - 0.2297 Table 2: Results with different rule extraction methods. Extraction and decoding columns are running times in secs per 1000 sentences and per sentence, respectively. We use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Foll","@endWordPosition":"3783","@position":"22302","annotationId":"T12","@startWordPosition":"3780","@citStr":"Mi et al. (2008)"}]},"title":{"#tail":"\n","#text":"Forestbased translation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL: HLT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Haitao Mi"},{"#tail":"\n","#text":"Liang Huang"},{"#tail":"\n","#text":"Qun Liu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL: HLT, Columbus, OH."},"#text":"\n","marker":{"#tail":"\n","#text":"Mi, Huang, Liu, 2008"},"location":{"#tail":"\n","#text":"Columbus, OH."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" x3 with x2 yˇu Figure 1: Example translation rule r1. The Chinese conjunction yˇu \u201cand\u201d is translated into English prep. \u201cwith\u201d. also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k2 similar tree-pairs in tree-to-tree models). We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists. Experiments (Section 5) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system (Liu et al., 2006; Mi et al., 2008), which is also 0.5 points better than (and twice as fast as) extracting on 30-best parses. When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date. Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-t","@endWordPosition":"656","@position":"4332","annotationId":"T13","@startWordPosition":"653","@citStr":"Mi et al., 2008"},{"#tail":"\n","#text":" English: \u201cBush held a meeting2 with Sharon1\u201d Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into a parse tree (b), which will be converted into an English string in 5 steps. First, at the root node, we apply rule r1 shown in Figure 1, which translates the Chinese coordination construction (\u201c... and ...\u201d) into an English prepositional phrase. Then, from step (c) we continue applying rules to untranslated Chinese subtrees, until we get the complete English translation in (e).2 2We swap the 1-best and 2-best parses of the example sentence from our earlier paper (Mi et al., 2008), since the current 1-best parse is easier to illustrate the rule extraction algorithm. 207 IP \u201cBush .. Sharon\u201d → x1 x4 x2 x3 CC (yˇu) → with NPB (B`ush´ı) → Bush NPB (Sh¯al´ong) → Sharon VPB (VV(jˇux´ıng) AS(le) x1:NPB) → held x1 NPB (hu`ıt´an) → a meeting NP \u201cBush ⊔ with Sharon\u201d NPB \u201cBush\u201d B`ush´ı NPB \u201cSharon\u201d Sh¯al´ong VV \u201cheld\u201d jˇux´ıng NPB \u201ca meeting\u201d hu`ıt´an AS \u201cheld\u201d le Bush held a meeting with Sharon VPB \u201cheld .. meeting\u201d CC \u201cwith\u201d yˇu (minimal) rules extracted IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) Figure 3: Tree-based rule extraction (Galley et al., 2004). Each non-leaf node in the tre","@endWordPosition":"1079","@position":"6862","annotationId":"T14","@startWordPosition":"1076","@citStr":"Mi et al., 2008"},{"#tail":"\n","#text":"tree-based extraction, we extract rules from a packed forest F in two steps: (1) admissible set computation (where to cut), and (2) fragmentation (how to cut). It turns out that the exact formulation developed for admissible set in the tree-based case can be applied to a forest without any change. The fragmentation step, however, becomes much more involved since we now face a choice of multiple parse hyperedges at each node. In other words, it becomes nondeterministic how to \u201ccut\u201d a forest into tree fragments, which is analogous to the non-deterministic pattern-match in forest-based decoding (Mi et al., 2008). For example there are two parse hyperedges e1 and e2 at the root node in Figure 4. When we follow one of them to grow a fragment, there again will be multiple choices at each of its tail nodes. Like in tree-based case, a fragment is said to be complete if all its leaf nodes are admissible. Otherwise, an incomplete fragment can grow at any non-admissible frontier node v, where following each parse hyperedge at v will split off a new fragment. For example, following e2 at the root node will immediately lead us to two admissible nodes, NPB0, 1 and VP1, 6 (we will highlight admissible nodes by g","@endWordPosition":"2447","@position":"14651","annotationId":"T15","@startWordPosition":"2444","@citStr":"Mi et al., 2008"},{"#tail":"\n","#text":"However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest. Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial. The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below). This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding. 11 · e E fra�g7 11 · v E yield(frag) P(e) (2) β(v) 211 BLEU score Our experiments will use both default 1-best decoding and forest-based decoding. As we will see in the next section, the best result comes when we combine the merits of both, i.e., using forests in both rule extraction and decoding. There is also a parallel work on extracting rules from k-best pars","@endWordPosition":"3455","@position":"20400","annotationId":"T16","@startWordPosition":"3452","@citStr":"Mi et al., 2008"},{"#tail":"\n","#text":"and translation length penalties, respectively. The conditional probability P(d |T) decomposes into the product of rule probabilities: P(d |T) = 11 P(r). (8) r∈d Each P(r) is in turn a product of five probabilities: P(r) =P(r |lhs(r))A' · P(r |rhs(r))A' · P(r |root(lhs(r)))As · Pllm(lhs(r) |rhs(r))A' · Pllm(rhs(r) |lhs(r))A' where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.3, and the last two are lexical probabilities. These parameters Al ... As are tuned by minimum error rate training (Och, 2003) on the dev sets. We refer readers to Mi et al. (2008) for details of the decoding algorithm. 0 1 2 3 4 5 6 average extracting time (secs/1000 sentences) Figure 6: Comparison of extraction time and BLEU score: forest-based vs.1-best and 30-best. rules from... extraction decoding BLEU 1-best trees 0.24 1.74 0.2430 30-best trees 5.56 3.31 0.2488 forest: pe=8 2.36 3.40 0.2533 Pharaoh - - 0.2297 Table 2: Results with different rule extraction methods. Extraction and decoding columns are running times in secs per 1000 sentences and per sentence, respectively. We use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Foll","@endWordPosition":"3783","@position":"22302","annotationId":"T17","@startWordPosition":"3780","@citStr":"Mi et al. (2008)"}]},"title":{"#tail":"\n","#text":"Forestbased translation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL: HLT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Haitao Mi"},{"#tail":"\n","#text":"Liang Huang"},{"#tail":"\n","#text":"Qun Liu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proceedings ofEMNLP, Prague, Czech Rep., July."},"#text":"\n","marker":{"#tail":"\n","#text":"Wang, Knight, Marcu, 2007"},"location":{"#tail":"\n","#text":"Prague, Czech Rep.,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"tially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu: it can be either a conjunction (CC \u201cand\u201d) as shown in Figure 3, or a preposition (P \u201cwith\u201d) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3Admissible set (Wang et al., 2007) is also known as \u201cfrontier set\u201d (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. IP0,6 NPB0,1 VP1,6 B`ushi PP1,3 VPB3,6 P1,2 NPB2,3 jˇuxing le huit´an yˇu Sh¯al´ong 209 These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6, as shown in Figure 4. Such a forest has a structure of a hypergraph (Huang and Chiang, 2005), where items like NP0, 3 are called nodes, whose indices denote the source span, and ","@endWordPosition":"2156","@position":"12997","annotationId":"T18","@startWordPosition":"2153","@citStr":"Wang et al., 2007"},{"#tail":"\n","#text":"TOP) where TOP denotes the root node of the forest. Like in the M-step in EM algorithm, we now extend the maximum likelihood estimation to fractional counts for three conditional probabilities regarding a rule, which will be used in the experiments: c(r) P(r |lhs(r)) = Er\u2032:lhs(r\u2032)=lhs(r) c(r\u2032), (4) Pr rhs r)) = c(r) ( I ( Er�:rhs(r')=rhs(r) c(r\u2032), (5) P(r |root(lhs(r))) c(r) (6) = Er\u2032:root(lhs(r\u2032))=root(lhs(r)) c(r\u2032). 4 Related Work The concept of packed forest has been previously used in translation rule extraction, for example in rule composition (Galley et al., 2006) and tree binarization (Wang et al., 2007). However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest. Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial. The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). The first direct application of parse forest in translation is our previous work","@endWordPosition":"3356","@position":"19782","annotationId":"T19","@startWordPosition":"3353","@citStr":"Wang et al., 2007"}]},"title":{"#tail":"\n","#text":"Binarizing syntax trees to improve syntax-based machine translation accuracy."},"booktitle":{"#tail":"\n","#text":"In Proceedings ofEMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Wei Wang"},{"#tail":"\n","#text":"Kevin Knight"},{"#tail":"\n","#text":"Daniel Marcu"}]}}]}}}}
