{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-72, March."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"22--1"},"marker":{"#tail":"\n","#text":"Berger, Pietra, Pietra, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" Question Inversion you did say the eighteenth? Verb Treatment you_did say the eighteenth? Catalan Sentence has dit el divuit ? Spanish Sentence i, has dicho el dieciocho ? ing operation. This makes it impossible to translate the verb itself, because it is then unknown to the system. The same holds for combinations of pronouns and verbs that are unseen in training, e. g. the training corpus contains the bigram 'I went', but not the one 'she went'. In order to overcome this problem, we train our lexicon model using maximum entropy. 5.1 The Maximum Entropy Approach The maximum entropy approach (Berger et al., 1996) presents a powerful framework for the combination of several knowledge sources. This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy. The distribution is required to satisfy constraints, which represent facts known from the data. These constraints are expressed on the basis of feature functions hu,(s,t), where (s, t) is a pair of source and target word. The lexicon probability of a source word given the target word has the following functional form 1 t) Z(t) exp [Y\u2018 with the normalization factor Z(t) = Eexp [E ","@endWordPosition":"2091","@position":"12584","annotationId":"T1","@startWordPosition":"2088","@citStr":"Berger et al., 1996"},{"#tail":"\n","#text":"m. The features we use in our model are \u2022 a lexical feature (for the entries of the transformed vocabulary): 128, (s, t) = (5(s, s') \u2022 6(t, t') P(s 350 \u2022 the verb contained in a transformed lexicon entry (e.g. 'go' for 'you_go' or 'you_will_go): hs, ,v(s ,t) = S(s. s') \u2022 V erb(t, v) , where 1, if t contains the verb v V erb(t, v) = 0, otherwise This enables us to translate the verb alone even if it occurs in the training corpus only as a spliced entry. For an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance (Berger et al., 1996) or (Ratnaparkhi, 1997). 5.2 Training We performed the following training steps: \u2022 transform the English (= source language) part of the corpus as described in Sections 4.1 and 4.2 \u2022 train the statistical translation system using this modified source language corpus 1 \u2022 with the resulting alignment, train the lexicon model using maximum entropy with the features described in Section 5.1 This training can be performed using converging iterative training procedures like described by (Darroch and Ratcliff, 1972) or (Della Pietra et al., 1997) 2. The basic training procedures for the translation s","@endWordPosition":"2328","@position":"13912","annotationId":"T2","@startWordPosition":"2325","@citStr":"Berger et al., 1996"}]},"title":{"#tail":"\n","#text":"A maximum entropy approach to natural language processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A L Berger"},{"#tail":"\n","#text":"S A Della Pietra"},{"#tail":"\n","#text":"V J Della Pietra"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-72, March."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"22--1"},"marker":{"#tail":"\n","#text":"Berger, Pietra, Pietra, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" Question Inversion you did say the eighteenth? Verb Treatment you_did say the eighteenth? Catalan Sentence has dit el divuit ? Spanish Sentence i, has dicho el dieciocho ? ing operation. This makes it impossible to translate the verb itself, because it is then unknown to the system. The same holds for combinations of pronouns and verbs that are unseen in training, e. g. the training corpus contains the bigram 'I went', but not the one 'she went'. In order to overcome this problem, we train our lexicon model using maximum entropy. 5.1 The Maximum Entropy Approach The maximum entropy approach (Berger et al., 1996) presents a powerful framework for the combination of several knowledge sources. This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy. The distribution is required to satisfy constraints, which represent facts known from the data. These constraints are expressed on the basis of feature functions hu,(s,t), where (s, t) is a pair of source and target word. The lexicon probability of a source word given the target word has the following functional form 1 t) Z(t) exp [Y\u2018 with the normalization factor Z(t) = Eexp [E ","@endWordPosition":"2091","@position":"12584","annotationId":"T3","@startWordPosition":"2088","@citStr":"Berger et al., 1996"},{"#tail":"\n","#text":"m. The features we use in our model are \u2022 a lexical feature (for the entries of the transformed vocabulary): 128, (s, t) = (5(s, s') \u2022 6(t, t') P(s 350 \u2022 the verb contained in a transformed lexicon entry (e.g. 'go' for 'you_go' or 'you_will_go): hs, ,v(s ,t) = S(s. s') \u2022 V erb(t, v) , where 1, if t contains the verb v V erb(t, v) = 0, otherwise This enables us to translate the verb alone even if it occurs in the training corpus only as a spliced entry. For an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance (Berger et al., 1996) or (Ratnaparkhi, 1997). 5.2 Training We performed the following training steps: \u2022 transform the English (= source language) part of the corpus as described in Sections 4.1 and 4.2 \u2022 train the statistical translation system using this modified source language corpus 1 \u2022 with the resulting alignment, train the lexicon model using maximum entropy with the features described in Section 5.1 This training can be performed using converging iterative training procedures like described by (Darroch and Ratcliff, 1972) or (Della Pietra et al., 1997) 2. The basic training procedures for the translation s","@endWordPosition":"2328","@position":"13912","annotationId":"T4","@startWordPosition":"2325","@citStr":"Berger et al., 1996"}]},"title":{"#tail":"\n","#text":"A maximum entropy approach to natural language processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A L Berger"},{"#tail":"\n","#text":"S A Della Pietra"},{"#tail":"\n","#text":"V J Della Pietra"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311 ."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"19--2"},"marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Mercer, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tes. These numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy: there, we have an average of 4.20 for Catalan and 4.46 for Spanish. Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4. This shows that our method was successful in producing a more focused lexicon probability distribution. We performed translation experiments with an implementation of the IBM-4 translation model (Brown et al., 1993). A description of the system can be found in (Tillmann and Ney, 2002). Table 5 presents an assessment of translation quality for both the language pairs English\u2014Catalan and English\u2014Spanish. We see that there is a significant decrease in error rate for the translation into Catalan. This change is consistent across both error rates, the WER and 100\u2014BLEU. For translations from English into Spanish, the improvement is less substantial. A reason for this might be that the Spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher: 1.57 for Spanish versus 1.53 ","@endWordPosition":"3140","@position":"19098","annotationId":"T5","@startWordPosition":"3137","@citStr":"Brown et al., 1993"}},"title":{"#tail":"\n","#text":"The mathematics of statistical machine translation: Parameter estimation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P F Brown"},{"#tail":"\n","#text":"S A Della Pietra"},{"#tail":"\n","#text":"V J Della Pietra"},{"#tail":"\n","#text":"R L Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proc. 39th Annual Meeting of the Assoc. for Computational Linguistics -joint with EACL, pages 228-235, Toulouse, France, July."},"#text":"\n","pages":{"#tail":"\n","#text":"228--235"},"marker":{"#tail":"\n","#text":"Germann, Jahr, Knight, Marcu, Yamada, 2001"},"location":{"#tail":"\n","#text":"Toulouse, France,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"y the arg max operation in Eq. 1, i.e. it explores the space of all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on","@endWordPosition":"923","@position":"5782","annotationId":"T6","@startWordPosition":"920","@citStr":"Germann et al., 2001"}},"title":{"#tail":"\n","#text":"Fast decoding and optimal decoding for machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. 39th Annual Meeting of the Assoc. for Computational Linguistics -joint with EACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"U Germann"},{"#tail":"\n","#text":"M Jahr"},{"#tail":"\n","#text":"K Knight"},{"#tail":"\n","#text":"D Marcu"},{"#tail":"\n","#text":"K Yamada"}]}},{"date":{"#tail":"\n","#text":"1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"n in Eq. 1, i.e. it explores the space of all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on the correct infle","@endWordPosition":"927","@position":"5800","annotationId":"T7","@startWordPosition":"924","@citStr":"Och et al., 1999"}},"title":{"#tail":"\n","#text":"Improved alignment models for statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"University of Maryland,"},"rawString":{"#tail":"\n","#text":"F.J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proc. Joint SIGDAT Conf on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20-28, University of Maryland, College Park, MD, June."},"#text":"\n","pages":{"#tail":"\n","#text":"20--28"},"marker":{"#tail":"\n","#text":"Och, Tillmann, Ney, 1999"},"location":{"#tail":"\n","#text":"College Park, MD,"},"booktitle":{"#tail":"\n","#text":"In Proc. Joint SIGDAT Conf on Empirical Methods in Natural Language Processing and Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"F J Och"},{"#tail":"\n","#text":"C Tillmann"},{"#tail":"\n","#text":"H Ney"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th Annual Meeting of the Assoc. for Computational Linguistics, pages 311-318, Philadelphia, PA, July."},"#text":"\n","pages":{"#tail":"\n","#text":"311--318"},"marker":{"#tail":"\n","#text":"Papineni, Roukos, Ward, Zhu, 2002"},"location":{"#tail":"\n","#text":"Philadelphia, PA,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ollowing criteria are used: \u2022 WER (word error rate): The word error rate is based on the Levenshtein distance. It is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the reference string. Since some sentences in the develop and test set occur several times with different reference translations (which holds especially for short sentences like 'okay, good-bye'), we calculate the minimal distance to this set of references as proposed in (NieBen et al., 2000). \u2022 BLEU (bilingual evaluation understudy): (Papineni et al., 2002) have proposed a 3 The Brill tagger can be downloaded from http://www.research.microsoft.com/users/brill/ 351 Table 4: Statistics of the training, develop and test set of the English-Spanish-Catalan LC-STAR corpus (*number of words without punctuation marks) English Spanish Catalan Original Transformed Training Sentences 13 352 Words Words&quot; 123 454 114 099 118 534 118 137 101 738 92 383 96 997 96 503 Vocabulary Size 2 154 2 776 3 933 3 572 Singletons 790 (37%) 1 165 (42%) 1 844 (47%) 1 658 (47%) Develop Sentences 272 Words Unknown Words 2 267 2 096 2217 2211 21 22 34 34 Test Sentences 262 Word","@endWordPosition":"2793","@position":"16920","annotationId":"T8","@startWordPosition":"2790","@citStr":"Papineni et al., 2002"}},"title":{"#tail":"\n","#text":"BLEU: a method for automatic evaluation of machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. 40th Annual Meeting of the Assoc. for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Papineni"},{"#tail":"\n","#text":"S Roukos"},{"#tail":"\n","#text":"T Ward"},{"#tail":"\n","#text":"W J Zhu"}]}}]}}}}
