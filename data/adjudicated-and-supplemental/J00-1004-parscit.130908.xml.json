{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Alshawi, Hiyan and Fei Xia. 1997. English-to-Mandarin speech translation with head transducers. In Proceedings of the Workshop on Spoken Language Translation, Madrid, Spain."},"#text":"\n","marker":{"#tail":"\n","#text":"Alshawi, Xia, 1997"},"location":{"#tail":"\n","#text":"Madrid,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tion. We can now define the string-to-string transduction function for a head transducer to be the function that maps an input string to the output string produced by the lowest-cost valid derivation taken over all initial states and initial symbols. (Formally, the function is partial in that it is not defined on an input when there are no derivations or when there are multiple outputs with the same minimal cost.) In the transducers produced by the training method described in this paper, the source and target positions are in the set {-1, 0,1}, though we have also used handcoded transducers (Alshawi and Xia 1997) and automatically trained transducers (Alshawi and Douglas 2000) with a larger range of positions. 2.2 Relationship to Standard FSTs The operation of a traditional left-to-right transducer can be simulated by a head transducer by starting at the leftmost input symbol and setting the positions of the first transition taken to a = 0 and /3 = 0, and the positions for subsequent transitions to a = 1 and )3 = 1. However, we can illustrate the fact that head transducers are more 47 Computational Linguistics Volume 26, Number 1 a:a 0:0 Figure 2 Head transducer to reverse an input string of arbitrary","@endWordPosition":"1377","@position":"8332","annotationId":"T1","@startWordPosition":"1374","@citStr":"Alshawi and Xia 1997"}},"title":{"#tail":"\n","#text":"English-to-Mandarin speech translation with head transducers."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Workshop on Spoken Language Translation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hiyan Alshawi"},{"#tail":"\n","#text":"Fei Xia"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"Brown, P. J., J. Cocke, S. A. Della Pietra, V. J. Della Pietra, J. Lafferty, R. L. Mercer, and P. Rossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79-85."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"16--2"},"marker":{"#tail":"\n","#text":"Brown, Cocke, Pietra, Pietra, Lafferty, Mercer, Rossin, 1990"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"n Hutchins and Somers [1992]). The reduction of effort results, in large part, from being able to do without artificial intermediate representations of meaning; we do not require the development of semantic mapping rules (or indeed any rules) or the creation of a corpus including semantic annotations. Compared with left-to-right transduction, middle-out transduction also aids robustness because, when complete derivations are not available, partial derivations tend to have meaningful headwords. At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al. 1993) for training translation systems automatically. One advantage is that our method attempts to model the natural decomposition of sentences into phrases. Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model. In particular, our search algorithm finds optimal transductions of test sentences in less than &quot;real time&quot; on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important conside","@endWordPosition":"6373","@position":"39203","annotationId":"T2","@startWordPosition":"6370","@citStr":"Brown et al. 1990"}},"title":{"#tail":"\n","#text":"A statistical approach to machine translation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P J Brown"},{"#tail":"\n","#text":"J Cocke"},{"#tail":"\n","#text":"S A Della Pietra"},{"#tail":"\n","#text":"V J Della Pietra"},{"#tail":"\n","#text":"J Lafferty"},{"#tail":"\n","#text":"R L Mercer"},{"#tail":"\n","#text":"P Rossin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Dorr, B. J. 1994. Machine translation divergences: A formal description and proposed solution. Computational Linguistics, 20(4):597-634."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"20--4"},"marker":{"#tail":"\n","#text":"Dorr, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching. This contrasts with one of the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the translation problem, i.e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language. The training method has four stages: (i) Compute co-occurrence statistics from the training data. (ii) Search for an optimal synchronized hierarchical alignment for each bitext. (iii) Construct a set of head transducers that can generate these alignments with transition weights derived from maximum likelihood estimation. 4.1 Computing Pairing Costs For each sourc","@endWordPosition":"3142","@position":"19074","annotationId":"T3","@startWordPosition":"3141","@citStr":"Dorr 1994"}},"title":{"#tail":"\n","#text":"Machine translation divergences: A formal description and proposed solution."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"B J Dorr"}}}]}}}}
