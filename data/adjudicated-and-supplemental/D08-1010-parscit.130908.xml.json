{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, pages 609\u2013616."},"#text":"\n","pages":{"#tail":"\n","#text":"609--616"},"marker":{"#tail":"\n","#text":"Liu, Liu, Lin, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ERS) model for syntax-based statistical machine translation (SMT). The MERS model combines local contextual information around rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in p","@endWordPosition":"151","@position":"1098","annotationId":"T1","@startWordPosition":"148","@citStr":"Liu et al., 2006"},{"#tail":"\n","#text":" no structures; \u2022 Our approach can help the decoder perform both lexical selection and phrase reorderings, while WSD can help the decoder only perform lexical selection; \u2022 Our method takes WSD as a special case, since a rule may only consists of terminals. In our previous work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: \u2022 The MERS model here combines rich information of source syntactic tree as features since the translation model is linguistically syntaxbased. He et al. (2008) did not use this information. \u2022 In this paper, we build MERS models for all ambiguous source-sides, including lexicalized (source-side which only contains terminals), partially lexicalized (source-side which contains both terminals and nonterminals), and unlexicalized (source-side which only contains nonterminals). He et al. (2008) only built MERS models for partially lexicalized sourcesides. In the TAT mod","@endWordPosition":"807","@position":"5219","annotationId":"T2","@startWordPosition":"804","@citStr":"Liu et al., 2006"},{"#tail":"\n","#text":" source syntactic tree and target string. TAT can capture linguistically motivated reorderings at short or long distance. Experiments show that by incorporating MERS model, the baseline system achieves statistically significant improvement. This paper is organized as follows: Section 2 reviews the TAT model; Section 3 introduces the MERS model and describes feature definitions; Section 4 demonstrates a method to incorporate the MERS model into the translation model; Section 5 reports and analyzes experimental results; Section 6 gives conclusions. 2 Baseline System Our baseline system is Lynx (Liu et al., 2006), which is a linguistically syntax-based SMT system. For translating a source sentence f1 = f1...fï¿½...fi, Lynx firstly employs a parser to produce a source syntactic tree T(f1 ), and then uses the source syntactic tree as the input to search translations: DNP NPB DNP NPB industrial products manufacturing X, :NP DEG X, :NN NN levels X, :NP DEG overall standard X,:NN NN of the match 90 (1) !eI1 = argmaxe,,1Pr(eI1|fJ1 ) = argmaxe,,1Pr(T(fJ1 )|fJ1 )Pr(eI 1|T(fJ1 )) In doing this, Lynx uses tree-to-string alignment template to build relationship between source syntactic tree and target string. A TA","@endWordPosition":"1003","@position":"6524","annotationId":"T3","@startWordPosition":"1000","@citStr":"Liu et al., 2006"},{"#tail":"\n","#text":"rce tree has one sibling node NPB. Those features make use of rich information around a rule, including the contextual information of a rule and the information of sub-trees covered by nonterminals. They are never used in Liu\u2019s TAT model. Figure 5 shows features for a partially lexicalized source tree. Furthermore, we also build MERS models for lexicalized and unlexicalized source trees. Note that for lexicalized tree, features do not include the information of sub-trees since there is no nonterminals. The features can be easily obtained by modifying the TAT extraction algorithm described in (Liu et al., 2006). When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately. We set the iteration number to 100 and Gaussian prior to 1. 4 Integrating the MERS Models into the Translation Model We integrate the MERS models into the TAT model during the translation of each source sentence. Thus the MERS models can help the decoder perform context-dependent rule selection during decoding. For int","@endWordPosition":"2258","@position":"13807","annotationId":"T4","@startWordPosition":"2255","@citStr":"Liu et al., 2006"},{"#tail":"\n","#text":"ets are used as the test sets. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n = 4. 5.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing \u201cgrow-diag-final\u201d method (Koehn et al., 2003). We use a Chinese parser developed by Deyi Xiong (Xiong et al., 2005) to parse the Chinese sentences of the training corpus. Our TAT extraction algorithm is similar to Liu et al. (2006), except that we make some tiny modifications to extract contextual features for MERS models. To extract TAT, we set the maximum height of the source sub-tree to h = 3, the maximum number of direct descendants of a node of sub-tree to c = 5. See (Liu et al., 2006) for specific definitions of these parameters. Table 1 shows statistical information of TATs which are filtered by the two test sets. For each type (lexicalized, partially lexicalized, unlexicalized) of TATs, a great portion of the source trees are ambiguous. The number of ambiguous source trees accounts for 78.34% of the total source","@endWordPosition":"2862","@position":"17426","annotationId":"T5","@startWordPosition":"2859","@citStr":"Liu et al. (2006)"}]},"title":{"#tail":"\n","#text":"Tree-tostring alignment template for statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yang Liu"},{"#tail":"\n","#text":"Qun Liu"},{"#tail":"\n","#text":"Shouxun Lin"}]}}}}}}
