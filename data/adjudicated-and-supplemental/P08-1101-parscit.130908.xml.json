{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the EMNLP conference, pages 1\u20138, Philadelphia, PA."},"#text":"\n","pages":{"#tail":"\n","#text":"1--8"},"marker":{"#tail":"\n","#text":"Collins, 2002"},"location":{"#tail":"\n","#text":"Philadelphia, PA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"cted by reranking: POS information is used to improve segmentation only for the N segmentor outputs. In this paper, we propose a novel joint model for Chinese word segmentation and POS tagging, which does not limiting the interaction between segmentation and POS information in reducing the combined search space. Instead, a novel multiple beam search algorithm is used to do decoding efficiently. Candidate ranking is based on a discriminative joint model, with features being extracted from segmented words and POS tags simultaneously. The training is performed by a single generalized perceptron (Collins, 2002). In experiments with the Chinese Treebank data, the joint model gave an error reduction of 14.6% in segmentation accuracy and 12.2% in the overall segmentation and tagging accuracy, compared to the traditional pipeline approach. In addition, the overall results are comparable to the best systems in the literature, which exploit knowledge outside the training data, even though our system is fully data-driven. Different methods have been proposed to reduce error propagation between pipelined tasks, both in general (Sutton et al., 2004; Daum´e III and Marcu, 2005; Finkel et al., 2006) and for sp","@endWordPosition":"769","@position":"4843","annotationId":"T1","@startWordPosition":"768","@citStr":"Collins, 2002"},{"#tail":"\n","#text":"previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1. The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2. The word segmentation features are extracted from word bigrams, capturing word, word length and character information in the context. The word length features are normalized, with those more than 15 being treated as 15. The POS tagging features are based on contextual information from the tag trigram, as well as the neighboring three-word window. To reduce overfitting and increase the decoding speed, templates 4, 5, 6 and 7 only include words with less than 3 characters. Like the baseline segmentor, the baseline tagger also normalizes word le","@endWordPosition":"1047","@position":"6569","annotationId":"T2","@startWordPosition":"1046","@citStr":"Collins (2002)"},{"#tail":"\n","#text":"tagged output. Given an input sentence x, the output F(x) satisfies: F(x) = arg max Score(y) yEGEN(x) where GEN(x) represents the set of possible outputs for x. Score(y) is computed by a feature-based linear model. Denoting the global feature vector for the tagged sentence y with 4b(y), we have: Score(y) = -b(y) · w� where w� is the parameter vector in the model. Each element in w� gives a weight to its corresponding element in 4b(y), which is the count of a particular feature over the whole sentence y. We calculate the w� value by supervised learning, using the averaged perceptron algorithm (Collins, 2002), given in Figure 1. 1 We take the union of feature templates from the baseline segmentor (Table 1) and POS tagger (Table 2) as the feature templates for the joint system. All features are treated equally and processed together according to the linear model, regardless of whether they are from the baseline segmentor or tagger. In fact, most features from the baseline POS tagger, when used in the joint model, represent segmentation patterns as well. For example, the aforementioned pattern \u201cnumber word\u201d + \u201c^\u201d, which is 1In order to provide a comparison for the perceptron algorithm we also tried ","@endWordPosition":"1663","@position":"10058","annotationId":"T3","@startWordPosition":"1662","@citStr":"Collins, 2002"}]},"title":{"#tail":"\n","#text":"Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the EMNLP conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Jenny Rose Finkel, Christopher D. Manning, and Andrew Y. Ng. 2006. Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. In Proceedings of the EMNLP Conference, pages 618\u2013626, Sydney, Australia."},"#text":"\n","pages":{"#tail":"\n","#text":"618--626"},"marker":{"#tail":"\n","#text":"Finkel, Manning, Ng, 2006"},"location":{"#tail":"\n","#text":"Sydney, Australia."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ized perceptron (Collins, 2002). In experiments with the Chinese Treebank data, the joint model gave an error reduction of 14.6% in segmentation accuracy and 12.2% in the overall segmentation and tagging accuracy, compared to the traditional pipeline approach. In addition, the overall results are comparable to the best systems in the literature, which exploit knowledge outside the training data, even though our system is fully data-driven. Different methods have been proposed to reduce error propagation between pipelined tasks, both in general (Sutton et al., 2004; Daum´e III and Marcu, 2005; Finkel et al., 2006) and for specific problems such as language modeling and utterance classification (Saraclar and Roark, 2005) and labeling and chunking (Shimizu and Haas, 2006). Though our model is built specifically for Chinese word segmentation and POS tagging, the idea of using the perceptron model to solve multiple tasks simultaneously can be generalized to other tasks. Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model fr","@endWordPosition":"863","@position":"5432","annotationId":"T4","@startWordPosition":"860","@citStr":"Finkel et al., 2006"}},"title":{"#tail":"\n","#text":"Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the EMNLP Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jenny Rose Finkel"},{"#tail":"\n","#text":"Christopher D Manning"},{"#tail":"\n","#text":"Andrew Y Ng"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hybrid approach to word segmentation and pos tagging. In Proceedings of ACL Demo and Poster Session, pages 217\u2013220, Prague, Czech Republic."},"#text":"\n","pages":{"#tail":"\n","#text":"217--220"},"marker":{"#tail":"\n","#text":"Nakagawa, Uchimoto, 2007"},"location":{"#tail":"\n","#text":"Prague, Czech Republic."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ance was negligible. 892 the large search space by imposing strong restrictions on the form of search candidates. In particular, Ng and Low (2004) used character-based POS tagging, which prevents some important POS tagging features such as word + POS tag; Shi and Wang (2007) used an N-best reranking approach, which limits the influence of POS tagging on segmentation to the N-best list. In comparison, our joint model does not impose any hard limitations on the interaction between segmentation and POS information.4 Fast decoding speed is achieved by using a novel multiple-beam search algorithm. Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach. Word information is used to process known-words, and character information is used for unknown words in a similar way to Ng and Low (2004). In comparison, our model handles character and word information simultaneously in a single perceptron model. 5 Experiments The Chinese Treebank (CTB) 4 is used for the experiments. It is separated into two parts: CTB 3 (420K characters in 150K words / 10364 sentences) is used for the final 10-fold cross validation, and the rest (240K characters in 150K words / 4798 ","@endWordPosition":"3455","@position":"20372","annotationId":"T5","@startWordPosition":"3452","@citStr":"Nakagawa and Uchimoto (2007)"}},"title":{"#tail":"\n","#text":"A hybrid approach to word segmentation and pos tagging."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL Demo and Poster Session,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Tetsuji Nakagawa"},{"#tail":"\n","#text":"Kiyotaka Uchimoto"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-speech tagging: One-at-a-time or all-at-once? Word-based or character-based? In Proceedings of the EMNLP Conference, pages 277\u2013284, Barcelona, Spain."},"#text":"\n","pages":{"#tail":"\n","#text":"277--284"},"marker":{"#tail":"\n","#text":"Ng, Low, 2004"},"location":{"#tail":"\n","#text":"Barcelona,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"d chooses the overall best output. A major challenge for such a joint system is the large search space faced by the decoder. For a sentence with n characters, the number of possible output sequences is O(2n−1 · Tn), where T is the size of the tag set. Due to the nature of the combined candidate items, decoding can be inefficient even with dynamic programming. Recent research on Chinese POS tagging has started to investigate joint segmentation and tagging, reporting accuracy improvements over the pipeline approach. Various decoding approaches have been used to reduce the combined search space. Ng and Low (2004) mapped the joint segmentation and POS tagging task into a single character sequence tagging problem. Two types of tags are assigned to each character to represent its segmentation and POS. For example, the tag \u201cb NN\u201d indicates a character at the beginning of a noun. Using this method, POS features are allowed to interact with segmentation. 888 Proceedings ofACL-08: HLT, pages 888\u2013896, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Since tagging is restricted to characters, the search space is reduced to O((4T)'), and beam search decoding is effective with a s","@endWordPosition":"476","@position":"2975","annotationId":"T6","@startWordPosition":"473","@citStr":"Ng and Low (2004)"},{"#tail":"\n","#text":" maximum word lengths and character categories is straightforward. For the online updating of the tag dictionary, however, the decision for frequent words must be made dynamically because the word frequencies keep changing. This is done by caching the number of occurrences of the current most frequent word M, and taking all words currently above the threshold M/5000 + 5 as frequent words. 5000 is a rough figure to control the number of frequent words, set according to Zipf\u2019s law. The parameter 5 is used to force all tags to be enumerated before a word is seen more than 5 times. 4 Related Work Ng and Low (2004) and Shi and Wang (2007) were described in the Introduction. Both models reduced 3We took this approach because we wanted the whole training process to be online. However, for comparison purposes, we also tried precomputing the above information before training and the difference in performance was negligible. 892 the large search space by imposing strong restrictions on the form of search candidates. In particular, Ng and Low (2004) used character-based POS tagging, which prevents some important POS tagging features such as word + POS tag; Shi and Wang (2007) used an N-best reranking approach","@endWordPosition":"3307","@position":"19453","annotationId":"T7","@startWordPosition":"3304","@citStr":"Ng and Low (2004)"},{"#tail":"\n","#text":"0364 sentences) is used for the final 10-fold cross validation, and the rest (240K characters in 150K words / 4798 sentences) is used as training and test data for development. The standard F-scores are used to measure both the word segmentation accuracy and the overall segmentation and tagging accuracy, where the overall accuracy is TF = 2pr/(p + r), with the precision p being the percentage of correctly segmented and tagged words in the decoder output, and the recall r being the percentage of gold-standard tagged words that are correctly identified by the decoder. For direct comparison with Ng and Low (2004), the POS tagging accuracy is also calculated by the percentage of correct tags on each character. 5.1 Development experiments The learning curves of the baseline and joint models are shown in Figure 3, Figure 4 and Figure 5, respectively. These curves are used to show the conver4Apart from the beam search algorithm, we do impose some minor limitations on the search space by methods such as the tag dictionary, but these can be seen as optional pruning methods for optimization. 1 2 3 4 5 6 7 8 9 10 Number of training iterations Figure 3: The learning curve of the baseline segmentor 0.9 0.89 F-s","@endWordPosition":"3642","@position":"21474","annotationId":"T8","@startWordPosition":"3639","@citStr":"Ng and Low (2004)"},{"#tail":"\n","#text":"TF \u2013 overall F-score, TA \u2013 tagging accuracy by character. quent. These three types of errors significantly outnumber the rest, together contributing 14.92% of all the errors. Moreover, the most commonly mistaken tags are NN and VV, while among the most frequent tags in the corpus, PU, DEG and M had comparatively less errors. Lastly, segmentation errors contribute around half (51.47%) of all the errors. 5.2 Test results 10-fold cross validation is performed to test the accuracy of the joint word segmentor and POS tagger, and to make comparisons with existing models in the literature. Following Ng and Low (2004), we partition the sentences in CTB 3, ordered by sentence ID, into 10 groups evenly. In the nth test, the nth group is used as the testing data. Table 4 shows the detailed results for the cross validation tests, each row representing one test. As can be seen from the table, the joint model outperforms the baseline system in each test. Table 5 shows the overall accuracies of the baseline and joint systems, and compares them to the relevant models in the literature. The accuracy of each model is shown in a row, where \u201cNg\u201d represents the models from Ng and Low (2004) and \u201cShi\u201d represents the mod","@endWordPosition":"4446","@position":"25950","annotationId":"T9","@startWordPosition":"4443","@citStr":"Ng and Low (2004)"},{"#tail":"\n","#text":"he joint model of Shi and Wang (2007). Despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy. One likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven. However, the comparison is indirect because our partitions of the CTB corpus are different. Shi and Wang (2007) also chunked the sentences before doing 10-fold cross validation, but used an uneven split. We chose to follow Ng and Low (2004) and split the sentences evenly to facilitate further comparison. Compared with Ng and Low (2004), our baseline model gave slightly better accuracy, consistent with our previous observations about the word segmentors (Zhang and Clark, 2007). Due to the large accuracy gain from the baseline, our joint model performed much better. In summary, when compared with existing joint word segmentation and POS tagging systems in the literature, our proposed model achieved the best accuracy boost from the cascaded baseline, and competent overall accuracy. 6 Conclusion and Future Work We proposed a joint C","@endWordPosition":"4794","@position":"27982","annotationId":"T10","@startWordPosition":"4791","@citStr":"Ng and Low (2004)"}]},"title":{"#tail":"\n","#text":"Chinese part-of-speech tagging: One-at-a-time or all-at-once? Word-based or character-based?"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the EMNLP Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hwee Tou Ng"},{"#tail":"\n","#text":"Jin Kiat Low"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Nobuyuki Shimizu and Andrew Haas. 2006. Exact decoding for jointly labeling and chunking sequences. In Proceedings of the COLING/ACL Conference, Poster Sessions, Sydney, Australia."},"#text":"\n","marker":{"#tail":"\n","#text":"Shimizu, Haas, 2006"},"location":{"#tail":"\n","#text":"Poster Sessions, Sydney, Australia."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"d 12.2% in the overall segmentation and tagging accuracy, compared to the traditional pipeline approach. In addition, the overall results are comparable to the best systems in the literature, which exploit knowledge outside the training data, even though our system is fully data-driven. Different methods have been proposed to reduce error propagation between pipelined tasks, both in general (Sutton et al., 2004; Daum´e III and Marcu, 2005; Finkel et al., 2006) and for specific problems such as language modeling and utterance classification (Saraclar and Roark, 2005) and labeling and chunking (Shimizu and Haas, 2006). Though our model is built specifically for Chinese word segmentation and POS tagging, the idea of using the perceptron model to solve multiple tasks simultaneously can be generalized to other tasks. Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation","@endWordPosition":"887","@position":"5591","annotationId":"T11","@startWordPosition":"884","@citStr":"Shimizu and Haas, 2006"}},"title":{"#tail":"\n","#text":"Exact decoding for jointly labeling and chunking sequences."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the COLING/ACL Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nobuyuki Shimizu"},{"#tail":"\n","#text":"Andrew Haas"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of the ACL Conference, pages 840\u2013847, Prague, Czech Republic."},"#text":"\n","pages":{"#tail":"\n","#text":"840--847"},"marker":{"#tail":"\n","#text":"Zhang, Clark, 2007"},"location":{"#tail":"\n","#text":"Prague, Czech Republic."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"et al., 2004; Daum´e III and Marcu, 2005; Finkel et al., 2006) and for specific problems such as language modeling and utterance classification (Saraclar and Roark, 2005) and labeling and chunking (Shimizu and Haas, 2006). Though our model is built specifically for Chinese word segmentation and POS tagging, the idea of using the perceptron model to solve multiple tasks simultaneously can be generalized to other tasks. Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1. The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Ch","@endWordPosition":"952","@position":"5992","annotationId":"T12","@startWordPosition":"949","@citStr":"Zhang and Clark, 2007"},{"#tail":"\n","#text":"er word\u201d in the baseline tagger, is also an effective indicator of the segmentation of the two words (especially \u201c^\u201d) in the joint model. 3.2 The decoding algorithm One of the main challenges for the joint segmentation and POS tagging system is the decoding algorithm. The speed and accuracy of the decoder is important for the perceptron learning algorithm, but the system faces a very large search space of combined candidates. Given the linear model and feature templates, exact inference is very hard even with dynamic programming. Experiments with the standard beam-search decoder described in (Zhang and Clark, 2007) resulted in low accuracy. This beam search algorithm processes an input sentence incrementally. At each stage, the incoming character is combined with existing partial candidates in all possible ways to generate new partial candidates. An agenda is used to control the search space, keeping only the B best partial candidates ending with the current character. The algorithm is simple and efficient, with a linear time complexity of O(BTn), where n is the size of input sentence, and T is the size of the tag set (T = 1 for pure word segmentation). It worked well for word segmentation alone (Zhang ","@endWordPosition":"2098","@position":"12349","annotationId":"T13","@startWordPosition":"2095","@citStr":"Zhang and Clark, 2007"},{"#tail":"\n","#text":"racters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven. However, the comparison is indirect because our partitions of the CTB corpus are different. Shi and Wang (2007) also chunked the sentences before doing 10-fold cross validation, but used an uneven split. We chose to follow Ng and Low (2004) and split the sentences evenly to facilitate further comparison. Compared with Ng and Low (2004), our baseline model gave slightly better accuracy, consistent with our previous observations about the word segmentors (Zhang and Clark, 2007). Due to the large accuracy gain from the baseline, our joint model performed much better. In summary, when compared with existing joint word segmentation and POS tagging systems in the literature, our proposed model achieved the best accuracy boost from the cascaded baseline, and competent overall accuracy. 6 Conclusion and Future Work We proposed a joint Chinese word segmentation and POS tagging model, which achieved a considerable reduction in error rate compared to a baseline twostage system. We used a single linear model for combined word segmentation and POS tagging, and chose the genera","@endWordPosition":"4830","@position":"28222","annotationId":"T14","@startWordPosition":"4827","@citStr":"Zhang and Clark, 2007"}]},"title":{"#tail":"\n","#text":"Chinese segmentation with a word-based perceptron algorithm."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yue Zhang"},{"#tail":"\n","#text":"Stephen Clark"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of the ACL Conference, pages 840\u2013847, Prague, Czech Republic."},"#text":"\n","pages":{"#tail":"\n","#text":"840--847"},"marker":{"#tail":"\n","#text":"Zhang, Clark, 2007"},"location":{"#tail":"\n","#text":"Prague, Czech Republic."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"et al., 2004; Daum´e III and Marcu, 2005; Finkel et al., 2006) and for specific problems such as language modeling and utterance classification (Saraclar and Roark, 2005) and labeling and chunking (Shimizu and Haas, 2006). Though our model is built specifically for Chinese word segmentation and POS tagging, the idea of using the perceptron model to solve multiple tasks simultaneously can be generalized to other tasks. Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1. The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Ch","@endWordPosition":"952","@position":"5992","annotationId":"T15","@startWordPosition":"949","@citStr":"Zhang and Clark, 2007"},{"#tail":"\n","#text":"er word\u201d in the baseline tagger, is also an effective indicator of the segmentation of the two words (especially \u201c^\u201d) in the joint model. 3.2 The decoding algorithm One of the main challenges for the joint segmentation and POS tagging system is the decoding algorithm. The speed and accuracy of the decoder is important for the perceptron learning algorithm, but the system faces a very large search space of combined candidates. Given the linear model and feature templates, exact inference is very hard even with dynamic programming. Experiments with the standard beam-search decoder described in (Zhang and Clark, 2007) resulted in low accuracy. This beam search algorithm processes an input sentence incrementally. At each stage, the incoming character is combined with existing partial candidates in all possible ways to generate new partial candidates. An agenda is used to control the search space, keeping only the B best partial candidates ending with the current character. The algorithm is simple and efficient, with a linear time complexity of O(BTn), where n is the size of input sentence, and T is the size of the tag set (T = 1 for pure word segmentation). It worked well for word segmentation alone (Zhang ","@endWordPosition":"2098","@position":"12349","annotationId":"T16","@startWordPosition":"2095","@citStr":"Zhang and Clark, 2007"},{"#tail":"\n","#text":"racters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven. However, the comparison is indirect because our partitions of the CTB corpus are different. Shi and Wang (2007) also chunked the sentences before doing 10-fold cross validation, but used an uneven split. We chose to follow Ng and Low (2004) and split the sentences evenly to facilitate further comparison. Compared with Ng and Low (2004), our baseline model gave slightly better accuracy, consistent with our previous observations about the word segmentors (Zhang and Clark, 2007). Due to the large accuracy gain from the baseline, our joint model performed much better. In summary, when compared with existing joint word segmentation and POS tagging systems in the literature, our proposed model achieved the best accuracy boost from the cascaded baseline, and competent overall accuracy. 6 Conclusion and Future Work We proposed a joint Chinese word segmentation and POS tagging model, which achieved a considerable reduction in error rate compared to a baseline twostage system. We used a single linear model for combined word segmentation and POS tagging, and chose the genera","@endWordPosition":"4830","@position":"28222","annotationId":"T17","@startWordPosition":"4827","@citStr":"Zhang and Clark, 2007"}]},"title":{"#tail":"\n","#text":"Chinese segmentation with a word-based perceptron algorithm."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yue Zhang"},{"#tail":"\n","#text":"Stephen Clark"}]}}]}}}}
