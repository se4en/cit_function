<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005809">
<title confidence="0.983527">
Developing Methodology for Korean Particle Error Detection
</title>
<author confidence="0.994794">
Markus Dickinson Ross Israel Sun-Hee Lee
</author>
<affiliation confidence="0.999948">
Indiana University Indiana University Wellesley College
</affiliation>
<email confidence="0.998554">
md7@indiana.edu raisrael@indiana.edu slee6@wellesley.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996177">
We further work on detecting errors in post-
positional particle usage by learners of Korean
by improving the training data and develop-
ing a complete pipeline of particle selection.
We improve the data by filtering non-Korean
data and sampling instances to better match
the particle distribution. Our evaluation shows
that, while the data selection is effective, there
is much work to be done with preprocessing
and system optimization.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997595">
A growing area of research in analyzing learner lan-
guage is to detect errors in function words, namely
categories such as prepositions and articles (see Lea-
cock et al., 2010, and references therein). This work
has mostly been for English, and there are issues,
such as greater morphological complexity, in mov-
ing to other languages (see, e.g., de Ilarraza et al.,
2008; Dickinson et al., 2010). Our goal is to build a
machine learning system for detecting errors in post-
positional particles in Korean, a significant source of
learner errors (Ko et al., 2004; Lee et al., 2009b).
Korean postpositional particles are morphemes
that attach to a preceding nominal to indicate a range
of linguistic functions, including grammatical func-
tions, e.g., subject and object; semantic roles; and
discourse functions. In (1), for instance, ka marks
the subject (function) and agent (semantic role).1
Similar to English prepositions, particles can also
have modifier functions, adding meanings of time,
location, instrument, possession, and so forth.
</bodyText>
<footnote confidence="0.860069">
1We use the Yale Romanization scheme for writing Korean.
</footnote>
<page confidence="0.981109">
81
</page>
<equation confidence="0.24385">
(1) Sumi-ka
Sumi-SBJ
sikan-ul kitaly-ess-ta.
hours-OBJ wait-PAST-END
</equation>
<bodyText confidence="0.983061444444444">
‘Sumi waited for John for (the whole) two hours in
his house.’
We treat the task of particle error detection as
one of particle selection, and we use machine learn-
ing because it has proven effective in similar tasks
for other languages (e.g., Chodorow et al., 2007;
Oyama, 2010). Training on a corpus of well-formed
Korean, we predict which particle should appear af-
ter a given nominal; if this is different from the
learner’s, we have detected an error. Using a ma-
chine learner has the advantage of being able to per-
form well without a researcher having to specify
rules, especially with the complex set of linguistic
relationships motivating particle selection.2
We build from Dickinson et al. (2010) in two
main ways: first, we implement a presence-selection
pipeline that has proven effective for English prepo-
sition error detection (cf. Gamon et al., 2008). As
the task is understudied, the work is preliminary, but
it nonetheless is able to highlight the primary ar-
eas of focus for future work. Secondly, we improve
upon the training data, in particular doing a better
job of selecting relevant instances for the machine
learner. Obtaining better-quality training data is a
major issue for machine learning applied to learner
language, as the domain of writing is different from
news-heavy training domains (Gamon, 2010).
</bodyText>
<footnote confidence="0.9808785">
2See Dickinson and Lee (2009); de Ilarraza et al. (2008);
Oyama (2010) for related work in other languages.
</footnote>
<figure confidence="0.924987125">
John-uy
John-GEN
cip-eyse
house-LOC
ku-lul
he-OBJ
twu
two
</figure>
<note confidence="0.39185">
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81–86,
Portland, Oregon, 24 June 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.812271" genericHeader="method">
2 Particle error detection
</sectionHeader>
<subsectionHeader confidence="0.99912">
2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999993666666667">
Korean is an agglutinative language: Korean words
(referred to as ecels) are usually composed of a
root with a number of functional affixes. We thus
first segment and POS tag the text, for both train-
ing and testing, using a hybrid (trigram + rule-
based) morphological tagger for Korean (Han and
Palmer, 2004). The tagger is designed for native
language and is not optimized to make guesses for
ill-formed input. While the POS tags assigned to the
learner corpus are thus often incorrect (see Lee et al.,
2009a), there is the more primary problem of seg-
mentation, as discussed in more detail in section 4.
</bodyText>
<subsectionHeader confidence="0.996966">
2.2 Machine learning
</subsectionHeader>
<bodyText confidence="0.999966861111111">
We use the Maximum Entropy Toolkit (Le, 2004)
for machine learning. Training on a corpus of well-
formed Korean, we predict which particle should ap-
pear after a given nominal; if this is different from
what the learner used, we have detected an error. It
is important that the data represent the relationships
between specific lexical items: in the comparable
English case, for example, interest is usually found
with in: interest in/*with learning.
Treating the ends of nominal elements as possible
particle slots, we break classification into two steps:
1) Is there a particle? (Yes/No); and 2) What is the
exact particle? Using two steps eases the task of ac-
tual particle prediction: with a successful classifica-
tion of negative and positive instances, there is no
need to handle nominals that have no particle in step
2. To evaluate our parameters for obtaining the most
relevant instances, we keep the task simple and per-
form only step 1, as this step provides information
about the usability of the training data. For actual
system performance, we evaluate both steps.
In selecting features for Korean, we have to ac-
count for relatively free word order (Chung et al.,
2010). We follow our previous work (Dickinson
et al., 2010) in our feature choices, using a five-
word window that includes the target stem and two
words on either side for context (see also Tetreault
and Chodorow, 2008). Each word is broken down
into: stem, affixes, stem POS, and affixes POS. We
also have features for the preceding and following
noun and verb, thereby approximating relevant se-
lectional properties. Although these are relatively
shallow features, they provide enough lexical and
grammatical context to help select better or worse
training data (section 3) and to provide a basis for a
preliminary system (section 4).
</bodyText>
<sectionHeader confidence="0.832694" genericHeader="method">
3 Obtaining the most relevant instances
</sectionHeader>
<bodyText confidence="0.999896375">
We need well-formed Korean data in order to train
a machine learner. To acquire this, we use web-
based corpora, as this allows us to find data similar
to learner language, and using web as corpus (WaC)
tools allows us to adjust parameters for new data
(Dickinson et al., 2010). However, the methodology
outlined in Dickinson et al. (2010) can be improved
in at least three ways, outlined next.
</bodyText>
<subsectionHeader confidence="0.999631">
3.1 Using sub-corpora
</subsectionHeader>
<bodyText confidence="0.999989416666666">
Web corpora can be built by searching for a set of
seed terms, extracting documents with those terms
(Baroni and Bernardini, 2004). One way to improve
such corpora is to use better seeds, namely, those
which are: 1) domain-appropriate (e.g., about trav-
eling), and 2) of an appropriate level. In Dickinson
et al. (2010), we show that basic terms result in poor
quality Korean, but slightly more advanced terms on
the same topics result in better-formed data.
Rather than use all of the seed terms to create a
single corpus, we divide the seed terms into 13 sep-
arate sets, based on the individual topics from our
learner corpus. The sub-corpora are then combined
to create a cohesive corpus covering all the topics.
For example, we use 10 Travel words to build a
subcorpus, 10 Learning Korean words for a differ-
ent subcorpus, and so forth. This means that terms
appropriate for one topic are not mixed with terms
for a different topic, ensuring more coherent web
documents. Otherwise, we might obtain a Health
Management word, such as pyengwen (‘hospital’),
mixed with a Generation Gap word, such as kaltung
(‘conflict’)—in this case, leading to webpages on
war, a topic not represented in our learner corpus.
</bodyText>
<subsectionHeader confidence="0.999185">
3.2 Filtering
</subsectionHeader>
<bodyText confidence="0.9999352">
One difficulty with our web corpora is that some of
them have large amounts of other languages along
with Korean. The keywords are in the corpora, but
there is additional text, often in Chinese, English, or
Japanese. These types of pages are unreliable for
</bodyText>
<page confidence="0.992297">
82
</page>
<bodyText confidence="0.9997142">
our purposes, as they may not exhibit natural Ko-
rean. By using a simple filter, we check whether a
majority of the characters in a webpage are indeed
from the Korean writing system, and remove pages
beneath a certain threshold.
</bodyText>
<subsectionHeader confidence="0.998821">
3.3 Instance sampling
</subsectionHeader>
<bodyText confidence="0.999960176470588">
Particles are often dropped in colloquial and even
written Korean, whereas learners are more often
required to use them. It is not always the case
that the web pages contain the same ratio of par-
ticles as learners are expected to use. To alleviate
this over-weighting of having no particle attached
to a noun, we propose to downsample our corpora
for the machine learning experiments, by remov-
ing a randomly-selected proportion of (negative) in-
stances. Instance sampling has been effective for
other NLP tasks, e.g., anaphora resolution (Wunsch
et al., 2009), when the number of negative instances
is much greater than the positive ones. In our web
corpora, nouns have a greater than 50% chance of
having no particle; in section 3.4, we thus downsam-
ple to varying amounts of negative instances from
about 45% to as little as 10% of the total corpus.
</bodyText>
<subsectionHeader confidence="0.996204">
3.4 Training data selection
</subsectionHeader>
<bodyText confidence="0.999894952380952">
In Dickinson et al. (2010), we used a Korean learner
data set from Lee et al. (2009b) for development. It
contains 3198 ecels, 1842 of which are nominals,
and 1271 (P70%) of those have particles. We use
this same corpus for development, to evaluate filter-
ing and down-sampling. Evaluating on (yes/no) par-
ticle presence, in tables 1 and 2, recall is the percent-
age of positive instances we correctly find and pre-
cision is the percentage of instances that we classify
as positive that actually are. A baseline of always
guessing a particle gives 100% recall, 69% preci-
sion, and 81.7% F-score.
Table 1 shows the results of the MaxEnt system
for step 1, using training data built for the topics in
the data with filter thresholds of 50%, 70%, 90%,
and 100%—i.e., requiring that percentage of Korean
characters—as well as the unfiltered corpus. The
best F-score is with the filter set at 90%, despite the
size of the filtered corpus being smaller than the full
corpus. Accordingly, we use the 90% filter on our
training corpus for the experiments described below.
</bodyText>
<table confidence="0.999870285714286">
Threshold 100% 90% 70% 50% Full
Ecel 67k 9.6m 10.3m 11.1m 12.7m
Instances 37k 5.8m 6.3m 7.1m 8.4m
Accuracy 74.75 81.11 74.64 80.29 80.46
Precision 80.03 86.14 79.65 85.41 85.56
Recall 84.50 86.55 84.97 86.15 86.23
F-score 82.20 86.34 82.22 85.78 85.89
</table>
<tableCaption confidence="0.999926">
Table 1: Step 1 (particle presence) results with filters
</tableCaption>
<bodyText confidence="0.989834428571428">
The results for instance sampling are given in ta-
ble 2. We experiment with positive to negative sam-
pling ratios of 1.3/1 (P43% negative instances), 2/1
(P33%), 4/1 (P20%), and 10/1 (P10%). We select
the 90% filter, 1.3/1 downsampling settings and ap-
ply them to the training corpus (section 3.1) for all
experiments below.
</bodyText>
<table confidence="0.999862833333333">
P/N ratio 10/1 4/1 2/1 1.3/1 1/1.05
Instances 3.1m 3.5m 4.3m 5m 5.8m
Accuracy 74.75 77.85 80.23 81.59 81.11
Precision 73.38 76.72 80.75 84.26 86.14
Recall 99.53 97.48 93.71 90.17 86.55
F-score 84.47 85.86 86.74 87.12 86.34
</table>
<tableCaption confidence="0.999208">
Table 2: Step 1 (presence) results with instance sampling
</tableCaption>
<bodyText confidence="0.9997234">
One goal has been to improve the web as corpus
corpus methodology for training a machine learning
system. The results in tables 1 and 2 reinforce our
earlier finding that size is not necessarily the most
important variable in determining the usefulness or
overall quality of data collected from the web for
NLP tasks (Dickinson et al., 2010). Indeed, the cor-
pus producing best results (90% filter, 1.3:1 down-
sampling) is more than 3 million instances smaller
than the unfiltered, unsampled corpus.
</bodyText>
<sectionHeader confidence="0.993899" genericHeader="method">
4 Initial system evaluation
</sectionHeader>
<bodyText confidence="0.999955125">
We have obtained an annotated corpus of 25 essays
from heritage intermediate learners,3 with 299 sen-
tences and 2515 ecels (2676 ecels after correcting
spacing errors). There are 1138 nominals, with 93
particle errors (5 added particles, 35 omissions, 53
substitutions)—in other words, less than 10% of par-
ticles are errors. There are 979 particles after cor-
rection. We focus on 38 particles that intermediate
</bodyText>
<footnote confidence="0.965143">
3Heritage learners have had exposure to Korean at a young
age, such as growing up with Korean spoken at home.
</footnote>
<page confidence="0.999362">
83
</page>
<bodyText confidence="0.999389444444445">
students can be reasonably expected to use. A parti-
cle is one of three types (cf. Nam and Ko, 2005): 1)
case markers, 2) adverbials (cf. prepositions), and
3) auxiliary particles.4
Table 3 gives the results for the entire system on
the test corpus, with separate results for each cat-
egory of particle, (Case, Adv., and Aux.) as well
as the concatenation of the three (All). The ac-
curacy presented here is in terms of only the par-
ticle in question, as opposed to the full form of
root+particle(s). Step 2 is presented in 2 ways: Clas-
sified, meaning that all of the instances classified as
needing a particle by step 1 are processed, or Gold,
in which we rely on the annotation to determine par-
ticle presence. It is not surprising, then, that Gold
experiments are more accurate than Classified ex-
periments, due to step 1 errors and also preprocess-
ing issues, discussed next.
</bodyText>
<table confidence="0.9996305">
Data # Step 1 Step 2
Classified Gold
Case 504 95.83% 71.23% 72.22%
Adv. 205 82.43% 30.24% 32.68%
Aux. 207 89.37% 31.41% 35.74%
All 916 91.37% 53.05% 55.13%
</table>
<tableCaption confidence="0.9522875">
Table 3: Accuracy for step 1 (particle presence) &amp; step 2
(particle selection), with number (#) of instances
</tableCaption>
<bodyText confidence="0.99983775">
Preprocessing For the particles we examine, there
are 135 mis-segmented nominals. The problem is
more conspicuous if we look at the entire corpus:
the tagger identifies 1547 nominal roots, but there
are only 1138. Some are errors in segmentation, i.e.,
mis-identifying the proper root of the ecel, and some
are problems with tagging the root, e.g., a nominal
mistagged as a verb. Table 4 provides results divided
by cases with only correctly pre-processed ecels and
where the target ecel has been mis-handled by the
tagger. This checks whether the system particle is
correct, ignoring whether the whole form is correct;
if full-form accuracy is considered, we have no way
to get the 135 inaccurate cases correct.
Error detection While our goal now is to estab-
lish a starting point, the ultimate, on-going goal of
</bodyText>
<footnote confidence="0.877161">
4Full corpus details will be made available at: http://
cl.indiana.edu/˜particles/.
</footnote>
<table confidence="0.9973245">
Step 1 Step 2
Data # Classified Gold
Accurate 781 94.24% 55.95% 58.13%
Inaccurate 135 74.81% 36.29% 38.51%
</table>
<tableCaption confidence="0.974972">
Table 4: Overall accuracy divided by accurate and inac-
curate preprocessing
</tableCaption>
<table confidence="0.999484">
Case Adv. Aux. All
Precision 28.82% 7.69% 5.51% 15.45%
Recall 87.50% 100% 77.78% 88.00%
</table>
<tableCaption confidence="0.999849">
Table 5: Error detection (using Gold step 1)
</tableCaption>
<bodyText confidence="0.999850285714286">
this work is to develop a robust system for automati-
cally detecting errors in learner data. Thus, it is nec-
essary to measure our performance at actually find-
ing the erroneous instances extracted from our test
corpus. Table 5 provides results for step 2 in terms
of our ability to detect erroneous instances. We re-
port precision and recall, calculated as in figure 1.
</bodyText>
<table confidence="0.831137833333333">
From the set of erroneous instances:
True Positive (TP) ML class 7� student class
False Negative (FN) ML class = student class
From the set of correct instances:
False Positive (FP) ML class 7� student class
True Negative (TN) ML class = student class
TP
Precision
(P) TP+FP
TP
Recall
(R) TP+FN
</table>
<figureCaption confidence="0.998221">
Figure 1: Precision and recall for error detection
</figureCaption>
<subsectionHeader confidence="0.993003">
4.1 Discussion and Outlook
</subsectionHeader>
<bodyText confidence="0.999165">
One striking aspect about the results in table 3 is the
gap in accuracy between case particles and the other
two categories, particularly in step 2. This points at
a need to develop independent systems for each type
of particle, each relying on different types of linguis-
tic information. Auxiliary particles, for example, in-
clude topic particles which—similar to English arti-
cles (Han et al., 2006)—require discourse informa-
tion to get correct. Still, as case particles comprise
more than half of all particles in our corpus, the sys-
tem is already potentially useful to learners.
Comparing the rows in table 4, the dramatic drop
in accuracy when moving to inaccurately-processed
</bodyText>
<page confidence="0.995923">
84
</page>
<bodyText confidence="0.999943302325581">
cases shows a clear need for preprocessing adapted
to learner data. While it is disconcerting that nearly
15% (135/916) of the cases have no chance of re-
sulting in a correct full form, the results indicate that
we can obtain reliable accuracy (cf. 94.24%) for pre-
dicting particle presence across all types of particles,
assuming good morphological tagging.
From table 5, it is apparent that we are overguess-
ing errors; recall that only 10% of particles are er-
roneous, whereas we more often guess a different
particle. While this tendency results in high recall,
a tool for learners should have higher precision, so
that correct usage is not flagged. However, this is
a first attempt at error detection, and simply know-
ing that precision is low means we can take steps
to solve this deficiency. Our training data may have
too many possible classes in it, and we have not yet
accounted for phonological alternations; e.g. if the
system guesses ul when lul is correct, we count a
miss, even though they are different realizations of
the same morpheme.
To try and alleviate the over-prediction of errors,
we have begun to explore implementing a confi-
dence filter. As a first pass, we use a simple fil-
ter that compares the probability of the best parti-
cle to the probability of the particle the learner pro-
vided; the absolute difference in probabilities must
be above a certain threshold. Table 6 provides the er-
ror detection results for each type of particle, incor-
porating confidence filters of 10%, 20%, 30%, 40%,
50%, and 60%. The results show that increasing the
threshold at which we accept the classifier’s answer
can significantly increase precision, at the cost of re-
call. As noted above, higher precision is desirable,
so we plan on further developing this confidence fil-
ter. We may also include heuristic-based filters, such
as the ones implemented in Criterion (see Leacock
et al., 2010), as well as a language model approach
(Gamon et al., 2008).
Finally, we are currently working on improving
the POS tagger, testing other taggers in the pro-
cess, and developing optimal feature sets for differ-
ent kinds of particles.
</bodyText>
<equation confidence="0.892753166666667">
R P
R P
R P
R P
R P
R P
</equation>
<sectionHeader confidence="0.998229" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9889545">
We would like to thank the IU CL discussion group
and Joel Tetreault for feedback at various points.
</bodyText>
<table confidence="0.999611615384615">
Adv Aux Case All
10% 10.0% 6.3% 29.9% 16.3%
100% 77.8% 67.8% 73.3%
20% 13.5% 7.8% 32.6% 18.0%
100% 77.8% 50.0% 60.0%
30% 20.0% 8.3% 36.1% 20.8%
100% 66.7% 39.3% 50.7%
40% 19.4% 14.3% 48.6% 26.9%
60.0% 66.7% 30.4% 38.7%
50% 23.1% 16.7% 57.9% 32.1%
30.0% 44.4% 19.6% 24.0%
60% 40.0% 26.7% 72.3% 45.2%
20.0% 44.4% 14.3% 18.7%
</table>
<tableCaption confidence="0.998844">
Table 6: Error detection with confidence filters
</tableCaption>
<sectionHeader confidence="0.995881" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995962357142857">
Marco Baroni and Silvia Bernardini. 2004. Bootcat:
Bootstrapping corpora and terms from the web. In
Proceedings of LREC 2004, pages 1313–1316.
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, pages 25–30.
Prague.
Tagyoung Chung, Matt Post, and Daniel Gildea.
2010. Factors affecting the accuracy of korean
parsing. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 49–57.
Los Angeles, CA, USA.
Arantza Diaz de Ilarraza, Koldo Gojenola, and
Maite Oronoz. 2008. Detecting erroneous uses
of complex postpositions in an agglutinative lan-
guage. In Proceedings of COLING-08. Manch-
ester.
Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2010. Building a korean web corpus for analyz-
ing learner language. In Proceedings of the 6th
Workshop on the Web as Corpus (WAC-6). Los
Angeles.
Markus Dickinson and Chong Min Lee. 2009. Mod-
ifying corpus annotation to support the analysis of
learner language. CALICO Journal, 26(3).
Michael Gamon. 2010. Using mostly native data
</reference>
<page confidence="0.993589">
85
</page>
<reference confidence="0.997867547169811">
to correct errors in learners’ writing. In Human
Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the
Association for Computational Linguistics, pages
163–171. Los Angeles, California.
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using
contextual speller techniques and language mod-
eling for esl error correction. In Proceedings of
IJCNLP. Hyderabad, India.
Chung-Hye Han and Martha Palmer. 2004. A mor-
phological tagger for korean: Statistical tagging
combined with corpus-based morphological rule
application. Machine Translation, 18(4):275–
297.
Na-Rae Han, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting errors in english article us-
age by non-native speakers. Natural Language
Engineering, 12(2).
S. Ko, M. Kim, J. Kim, S. Seo, H. Chung, and
S. Han. 2004. An analysis of Korean learner cor-
pora and errors. Hanguk Publishing Co.
Zhang Le. 2004. Maximum Entropy Mod-
eling Toolkit for Python and C++. URL
http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html.
Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel Tetreault. 2010. Automated Gram-
matical Error Detection for Language Learners.
Synthesis Lectures on Human Language Tech-
nologies. Morgan &amp; Claypool.
Chong Min Lee, Soojeong Eom, and Markus Dick-
inson. 2009a. Towards analyzing korean learner
particles. Talk given at CALICO ’09 Pre-
Conference Workshop on Automatic Analysis of
Learner Language. Tempe, AZ.
Sun-Hee Lee, Seok Bae Jang, and Sang kyu Seo.
2009b. Annotation of korean learner corpora for
particle error detection. CALICO Journal, 26(3).
Ki-shim Nam and Yong-kun Ko. 2005. Korean
Grammar (phyocwun kwuke mwunpeplon). Top
Publisher, Seoul.
Hiromi Oyama. 2010. Automatic error detection
method for japanese particles. Polyglossia, 18.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in esl
writing. In Proceedings of COLING-08. Manch-
ester.
Holger Wunsch, Sandra K¨ubler, and Rachael
Cantrell. 2009. Instance sampling methods for
pronoun resolution. In Proceedings of RANLP
2009. Borovets, Bulgaria.
</reference>
<page confidence="0.998564">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989398">
<title confidence="0.999902">Developing Methodology for Korean Particle Error Detection</title>
<author confidence="0.994047">Markus Dickinson Ross Israel Sun-Hee Lee</author>
<affiliation confidence="0.999994">Indiana University Indiana University Wellesley</affiliation>
<email confidence="0.998813">md7@indiana.eduraisrael@indiana.eduslee6@wellesley.edu</email>
<abstract confidence="0.999684545454545">We further work on detecting errors in postpositional particle usage by learners of Korean by improving the training data and developing a complete pipeline of particle selection. We improve the data by filtering non-Korean data and sampling instances to better match the particle distribution. Our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Bootcat: Bootstrapping corpora and terms from the web.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>1313--1316</pages>
<contexts>
<context position="6587" citStr="Baroni and Bernardini, 2004" startWordPosition="1051" endWordPosition="1054">asis for a preliminary system (section 4). 3 Obtaining the most relevant instances We need well-formed Korean data in order to train a machine learner. To acquire this, we use webbased corpora, as this allows us to find data similar to learner language, and using web as corpus (WaC) tools allows us to adjust parameters for new data (Dickinson et al., 2010). However, the methodology outlined in Dickinson et al. (2010) can be improved in at least three ways, outlined next. 3.1 Using sub-corpora Web corpora can be built by searching for a set of seed terms, extracting documents with those terms (Baroni and Bernardini, 2004). One way to improve such corpora is to use better seeds, namely, those which are: 1) domain-appropriate (e.g., about traveling), and 2) of an appropriate level. In Dickinson et al. (2010), we show that basic terms result in poor quality Korean, but slightly more advanced terms on the same topics result in better-formed data. Rather than use all of the seed terms to create a single corpus, we divide the seed terms into 13 separate sets, based on the individual topics from our learner corpus. The sub-corpora are then combined to create a cohesive corpus covering all the topics. For example, we </context>
</contexts>
<marker>Baroni, Bernardini, 2004</marker>
<rawString>Marco Baroni and Silvia Bernardini. 2004. Bootcat: Bootstrapping corpora and terms from the web. In Proceedings of LREC 2004, pages 1313–1316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Joel Tetreault</author>
<author>Na-Rae Han</author>
</authors>
<title>Detection of grammatical errors involving prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th ACLSIGSEM Workshop on Prepositions,</booktitle>
<pages>25--30</pages>
<location>Prague.</location>
<contexts>
<context position="2102" citStr="Chodorow et al., 2007" startWordPosition="315" endWordPosition="318">1), for instance, ka marks the subject (function) and agent (semantic role).1 Similar to English prepositions, particles can also have modifier functions, adding meanings of time, location, instrument, possession, and so forth. 1We use the Yale Romanization scheme for writing Korean. 81 (1) Sumi-ka Sumi-SBJ sikan-ul kitaly-ess-ta. hours-OBJ wait-PAST-END ‘Sumi waited for John for (the whole) two hours in his house.’ We treat the task of particle error detection as one of particle selection, and we use machine learning because it has proven effective in similar tasks for other languages (e.g., Chodorow et al., 2007; Oyama, 2010). Training on a corpus of well-formed Korean, we predict which particle should appear after a given nominal; if this is different from the learner’s, we have detected an error. Using a machine learner has the advantage of being able to perform well without a researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.2 We build from Dickinson et al. (2010) in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). A</context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the 4th ACLSIGSEM Workshop on Prepositions, pages 25–30. Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Factors affecting the accuracy of korean parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>49--57</pages>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="5372" citStr="Chung et al., 2010" startWordPosition="849" endWordPosition="852">s there a particle? (Yes/No); and 2) What is the exact particle? Using two steps eases the task of actual particle prediction: with a successful classification of negative and positive instances, there is no need to handle nominals that have no particle in step 2. To evaluate our parameters for obtaining the most relevant instances, we keep the task simple and perform only step 1, as this step provides information about the usability of the training data. For actual system performance, we evaluate both steps. In selecting features for Korean, we have to account for relatively free word order (Chung et al., 2010). We follow our previous work (Dickinson et al., 2010) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also Tetreault and Chodorow, 2008). Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties. Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a pr</context>
</contexts>
<marker>Chung, Post, Gildea, 2010</marker>
<rawString>Tagyoung Chung, Matt Post, and Daniel Gildea. 2010. Factors affecting the accuracy of korean parsing. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 49–57. Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arantza Diaz de Ilarraza</author>
<author>Koldo Gojenola</author>
<author>Maite Oronoz</author>
</authors>
<title>Detecting erroneous uses of complex postpositions in an agglutinative language.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING-08.</booktitle>
<location>Manchester.</location>
<marker>de Ilarraza, Gojenola, Oronoz, 2008</marker>
<rawString>Arantza Diaz de Ilarraza, Koldo Gojenola, and Maite Oronoz. 2008. Detecting erroneous uses of complex postpositions in an agglutinative language. In Proceedings of COLING-08. Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>Ross Israel</author>
<author>Sun-Hee Lee</author>
</authors>
<title>Building a korean web corpus for analyzing learner language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th Workshop on the Web as Corpus (WAC-6).</booktitle>
<location>Los Angeles.</location>
<contexts>
<context position="1065" citStr="Dickinson et al., 2010" startWordPosition="155" endWordPosition="158">ean data and sampling instances to better match the particle distribution. Our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization. 1 Introduction A growing area of research in analyzing learner language is to detect errors in function words, namely categories such as prepositions and articles (see Leacock et al., 2010, and references therein). This work has mostly been for English, and there are issues, such as greater morphological complexity, in moving to other languages (see, e.g., de Ilarraza et al., 2008; Dickinson et al., 2010). Our goal is to build a machine learning system for detecting errors in postpositional particles in Korean, a significant source of learner errors (Ko et al., 2004; Lee et al., 2009b). Korean postpositional particles are morphemes that attach to a preceding nominal to indicate a range of linguistic functions, including grammatical functions, e.g., subject and object; semantic roles; and discourse functions. In (1), for instance, ka marks the subject (function) and agent (semantic role).1 Similar to English prepositions, particles can also have modifier functions, adding meanings of time, loca</context>
<context position="2540" citStr="Dickinson et al. (2010)" startWordPosition="387" endWordPosition="390">k of particle error detection as one of particle selection, and we use machine learning because it has proven effective in similar tasks for other languages (e.g., Chodorow et al., 2007; Oyama, 2010). Training on a corpus of well-formed Korean, we predict which particle should appear after a given nominal; if this is different from the learner’s, we have detected an error. Using a machine learner has the advantage of being able to perform well without a researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.2 We build from Dickinson et al. (2010) in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). As the task is understudied, the work is preliminary, but it nonetheless is able to highlight the primary areas of focus for future work. Secondly, we improve upon the training data, in particular doing a better job of selecting relevant instances for the machine learner. Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy traini</context>
<context position="5426" citStr="Dickinson et al., 2010" startWordPosition="858" endWordPosition="861">xact particle? Using two steps eases the task of actual particle prediction: with a successful classification of negative and positive instances, there is no need to handle nominals that have no particle in step 2. To evaluate our parameters for obtaining the most relevant instances, we keep the task simple and perform only step 1, as this step provides information about the usability of the training data. For actual system performance, we evaluate both steps. In selecting features for Korean, we have to account for relatively free word order (Chung et al., 2010). We follow our previous work (Dickinson et al., 2010) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also Tetreault and Chodorow, 2008). Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties. Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4). 3 Obtaining the most rel</context>
<context position="6775" citStr="Dickinson et al. (2010)" startWordPosition="1083" endWordPosition="1086"> as this allows us to find data similar to learner language, and using web as corpus (WaC) tools allows us to adjust parameters for new data (Dickinson et al., 2010). However, the methodology outlined in Dickinson et al. (2010) can be improved in at least three ways, outlined next. 3.1 Using sub-corpora Web corpora can be built by searching for a set of seed terms, extracting documents with those terms (Baroni and Bernardini, 2004). One way to improve such corpora is to use better seeds, namely, those which are: 1) domain-appropriate (e.g., about traveling), and 2) of an appropriate level. In Dickinson et al. (2010), we show that basic terms result in poor quality Korean, but slightly more advanced terms on the same topics result in better-formed data. Rather than use all of the seed terms to create a single corpus, we divide the seed terms into 13 separate sets, based on the individual topics from our learner corpus. The sub-corpora are then combined to create a cohesive corpus covering all the topics. For example, we use 10 Travel words to build a subcorpus, 10 Learning Korean words for a different subcorpus, and so forth. This means that terms appropriate for one topic are not mixed with terms for a d</context>
<context position="9093" citStr="Dickinson et al. (2010)" startWordPosition="1477" endWordPosition="1480">particle attached to a noun, we propose to downsample our corpora for the machine learning experiments, by removing a randomly-selected proportion of (negative) instances. Instance sampling has been effective for other NLP tasks, e.g., anaphora resolution (Wunsch et al., 2009), when the number of negative instances is much greater than the positive ones. In our web corpora, nouns have a greater than 50% chance of having no particle; in section 3.4, we thus downsample to varying amounts of negative instances from about 45% to as little as 10% of the total corpus. 3.4 Training data selection In Dickinson et al. (2010), we used a Korean learner data set from Lee et al. (2009b) for development. It contains 3198 ecels, 1842 of which are nominals, and 1271 (P70%) of those have particles. We use this same corpus for development, to evaluate filtering and down-sampling. Evaluating on (yes/no) particle presence, in tables 1 and 2, recall is the percentage of positive instances we correctly find and precision is the percentage of instances that we classify as positive that actually are. A baseline of always guessing a particle gives 100% recall, 69% precision, and 81.7% F-score. Table 1 shows the results of the Ma</context>
<context position="11379" citStr="Dickinson et al., 2010" startWordPosition="1863" endWordPosition="1866"> P/N ratio 10/1 4/1 2/1 1.3/1 1/1.05 Instances 3.1m 3.5m 4.3m 5m 5.8m Accuracy 74.75 77.85 80.23 81.59 81.11 Precision 73.38 76.72 80.75 84.26 86.14 Recall 99.53 97.48 93.71 90.17 86.55 F-score 84.47 85.86 86.74 87.12 86.34 Table 2: Step 1 (presence) results with instance sampling One goal has been to improve the web as corpus corpus methodology for training a machine learning system. The results in tables 1 and 2 reinforce our earlier finding that size is not necessarily the most important variable in determining the usefulness or overall quality of data collected from the web for NLP tasks (Dickinson et al., 2010). Indeed, the corpus producing best results (90% filter, 1.3:1 downsampling) is more than 3 million instances smaller than the unfiltered, unsampled corpus. 4 Initial system evaluation We have obtained an annotated corpus of 25 essays from heritage intermediate learners,3 with 299 sentences and 2515 ecels (2676 ecels after correcting spacing errors). There are 1138 nominals, with 93 particle errors (5 added particles, 35 omissions, 53 substitutions)—in other words, less than 10% of particles are errors. There are 979 particles after correction. We focus on 38 particles that intermediate 3Herit</context>
</contexts>
<marker>Dickinson, Israel, Lee, 2010</marker>
<rawString>Markus Dickinson, Ross Israel, and Sun-Hee Lee. 2010. Building a korean web corpus for analyzing learner language. In Proceedings of the 6th Workshop on the Web as Corpus (WAC-6). Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>Chong Min Lee</author>
</authors>
<title>Modifying corpus annotation to support the analysis of learner language.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="3195" citStr="Dickinson and Lee (2009)" startWordPosition="491" endWordPosition="494">plement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). As the task is understudied, the work is preliminary, but it nonetheless is able to highlight the primary areas of focus for future work. Secondly, we improve upon the training data, in particular doing a better job of selecting relevant instances for the machine learner. Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). 2See Dickinson and Lee (2009); de Ilarraza et al. (2008); Oyama (2010) for related work in other languages. John-uy John-GEN cip-eyse house-LOC ku-lul he-OBJ twu two Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81–86, Portland, Oregon, 24 June 2011. c�2011 Association for Computational Linguistics 2 Particle error detection 2.1 Pre-processing Korean is an agglutinative language: Korean words (referred to as ecels) are usually composed of a root with a number of functional affixes. We thus first segment and POS tag the text, for both training and testing, using a h</context>
</contexts>
<marker>Dickinson, Lee, 2009</marker>
<rawString>Markus Dickinson and Chong Min Lee. 2009. Modifying corpus annotation to support the analysis of learner language. CALICO Journal, 26(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>163--171</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="3164" citStr="Gamon, 2010" startWordPosition="488" endWordPosition="489"> ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). As the task is understudied, the work is preliminary, but it nonetheless is able to highlight the primary areas of focus for future work. Secondly, we improve upon the training data, in particular doing a better job of selecting relevant instances for the machine learner. Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). 2See Dickinson and Lee (2009); de Ilarraza et al. (2008); Oyama (2010) for related work in other languages. John-uy John-GEN cip-eyse house-LOC ku-lul he-OBJ twu two Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81–86, Portland, Oregon, 24 June 2011. c�2011 Association for Computational Linguistics 2 Particle error detection 2.1 Pre-processing Korean is an agglutinative language: Korean words (referred to as ecels) are usually composed of a root with a number of functional affixes. We thus first segment and POS tag the text, for both </context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>Michael Gamon. 2010. Using mostly native data to correct errors in learners’ writing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 163–171. Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Jianfeng Gao</author>
<author>Chris Brockett</author>
<author>Alexander Klementiev</author>
<author>William Dolan</author>
<author>Dmitriy Belenko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for esl error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="2699" citStr="Gamon et al., 2008" startWordPosition="412" endWordPosition="415"> Chodorow et al., 2007; Oyama, 2010). Training on a corpus of well-formed Korean, we predict which particle should appear after a given nominal; if this is different from the learner’s, we have detected an error. Using a machine learner has the advantage of being able to perform well without a researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.2 We build from Dickinson et al. (2010) in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). As the task is understudied, the work is preliminary, but it nonetheless is able to highlight the primary areas of focus for future work. Secondly, we improve upon the training data, in particular doing a better job of selecting relevant instances for the machine learner. Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). 2See Dickinson and Lee (2009); de Ilarraza et al. (2008); Oyama (2010) for related work in other languages. John-uy John-GEN cip-eyse</context>
<context position="17811" citStr="Gamon et al., 2008" startWordPosition="2944" endWordPosition="2947"> probabilities must be above a certain threshold. Table 6 provides the error detection results for each type of particle, incorporating confidence filters of 10%, 20%, 30%, 40%, 50%, and 60%. The results show that increasing the threshold at which we accept the classifier’s answer can significantly increase precision, at the cost of recall. As noted above, higher precision is desirable, so we plan on further developing this confidence filter. We may also include heuristic-based filters, such as the ones implemented in Criterion (see Leacock et al., 2010), as well as a language model approach (Gamon et al., 2008). Finally, we are currently working on improving the POS tagger, testing other taggers in the process, and developing optimal feature sets for different kinds of particles. R P R P R P R P R P R P Acknowledgments We would like to thank the IU CL discussion group and Joel Tetreault for feedback at various points. Adv Aux Case All 10% 10.0% 6.3% 29.9% 16.3% 100% 77.8% 67.8% 73.3% 20% 13.5% 7.8% 32.6% 18.0% 100% 77.8% 50.0% 60.0% 30% 20.0% 8.3% 36.1% 20.8% 100% 66.7% 39.3% 50.7% 40% 19.4% 14.3% 48.6% 26.9% 60.0% 66.7% 30.4% 38.7% 50% 23.1% 16.7% 57.9% 32.1% 30.0% 44.4% 19.6% 24.0% 60% 40.0% 26.7%</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Michael Gamon, Jianfeng Gao, Chris Brockett, Alexander Klementiev, William Dolan, Dmitriy Belenko, and Lucy Vanderwende. 2008. Using contextual speller techniques and language modeling for esl error correction. In Proceedings of IJCNLP. Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-Hye Han</author>
<author>Martha Palmer</author>
</authors>
<title>A morphological tagger for korean: Statistical tagging combined with corpus-based morphological rule application.</title>
<date>2004</date>
<journal>Machine Translation,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>297</pages>
<contexts>
<context position="3877" citStr="Han and Palmer, 2004" startWordPosition="597" endWordPosition="600"> other languages. John-uy John-GEN cip-eyse house-LOC ku-lul he-OBJ twu two Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81–86, Portland, Oregon, 24 June 2011. c�2011 Association for Computational Linguistics 2 Particle error detection 2.1 Pre-processing Korean is an agglutinative language: Korean words (referred to as ecels) are usually composed of a root with a number of functional affixes. We thus first segment and POS tag the text, for both training and testing, using a hybrid (trigram + rulebased) morphological tagger for Korean (Han and Palmer, 2004). The tagger is designed for native language and is not optimized to make guesses for ill-formed input. While the POS tags assigned to the learner corpus are thus often incorrect (see Lee et al., 2009a), there is the more primary problem of segmentation, as discussed in more detail in section 4. 2.2 Machine learning We use the Maximum Entropy Toolkit (Le, 2004) for machine learning. Training on a corpus of wellformed Korean, we predict which particle should appear after a given nominal; if this is different from what the learner used, we have detected an error. It is important that the data re</context>
</contexts>
<marker>Han, Palmer, 2004</marker>
<rawString>Chung-Hye Han and Martha Palmer. 2004. A morphological tagger for korean: Statistical tagging combined with corpus-based morphological rule application. Machine Translation, 18(4):275– 297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in english article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="15574" citStr="Han et al., 2006" startWordPosition="2567" endWordPosition="2570">tances: False Positive (FP) ML class 7� student class True Negative (TN) ML class = student class TP Precision (P) TP+FP TP Recall (R) TP+FN Figure 1: Precision and recall for error detection 4.1 Discussion and Outlook One striking aspect about the results in table 3 is the gap in accuracy between case particles and the other two categories, particularly in step 2. This points at a need to develop independent systems for each type of particle, each relying on different types of linguistic information. Auxiliary particles, for example, include topic particles which—similar to English articles (Han et al., 2006)—require discourse information to get correct. Still, as case particles comprise more than half of all particles in our corpus, the system is already potentially useful to learners. Comparing the rows in table 4, the dramatic drop in accuracy when moving to inaccurately-processed 84 cases shows a clear need for preprocessing adapted to learner data. While it is disconcerting that nearly 15% (135/916) of the cases have no chance of resulting in a correct full form, the results indicate that we can obtain reliable accuracy (cf. 94.24%) for predicting particle presence across all types of particl</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in english article usage by non-native speakers. Natural Language Engineering, 12(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ko</author>
<author>M Kim</author>
<author>J Kim</author>
<author>S Seo</author>
<author>H Chung</author>
<author>S Han</author>
</authors>
<title>An analysis of Korean learner corpora and errors.</title>
<date>2004</date>
<publisher>Hanguk Publishing Co.</publisher>
<contexts>
<context position="1229" citStr="Ko et al., 2004" startWordPosition="183" endWordPosition="186">ith preprocessing and system optimization. 1 Introduction A growing area of research in analyzing learner language is to detect errors in function words, namely categories such as prepositions and articles (see Leacock et al., 2010, and references therein). This work has mostly been for English, and there are issues, such as greater morphological complexity, in moving to other languages (see, e.g., de Ilarraza et al., 2008; Dickinson et al., 2010). Our goal is to build a machine learning system for detecting errors in postpositional particles in Korean, a significant source of learner errors (Ko et al., 2004; Lee et al., 2009b). Korean postpositional particles are morphemes that attach to a preceding nominal to indicate a range of linguistic functions, including grammatical functions, e.g., subject and object; semantic roles; and discourse functions. In (1), for instance, ka marks the subject (function) and agent (semantic role).1 Similar to English prepositions, particles can also have modifier functions, adding meanings of time, location, instrument, possession, and so forth. 1We use the Yale Romanization scheme for writing Korean. 81 (1) Sumi-ka Sumi-SBJ sikan-ul kitaly-ess-ta. hours-OBJ wait-</context>
</contexts>
<marker>Ko, Kim, Kim, Seo, Chung, Han, 2004</marker>
<rawString>S. Ko, M. Kim, J. Kim, S. Seo, H. Chung, and S. Han. 2004. An analysis of Korean learner corpora and errors. Hanguk Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Le</author>
</authors>
<date>2004</date>
<booktitle>Maximum Entropy Modeling Toolkit for Python and C++. URL http://homepages.inf.ed.ac.uk/ s0450736/maxent_toolkit.html.</booktitle>
<contexts>
<context position="4240" citStr="Le, 2004" startWordPosition="662" endWordPosition="663">o as ecels) are usually composed of a root with a number of functional affixes. We thus first segment and POS tag the text, for both training and testing, using a hybrid (trigram + rulebased) morphological tagger for Korean (Han and Palmer, 2004). The tagger is designed for native language and is not optimized to make guesses for ill-formed input. While the POS tags assigned to the learner corpus are thus often incorrect (see Lee et al., 2009a), there is the more primary problem of segmentation, as discussed in more detail in section 4. 2.2 Machine learning We use the Maximum Entropy Toolkit (Le, 2004) for machine learning. Training on a corpus of wellformed Korean, we predict which particle should appear after a given nominal; if this is different from what the learner used, we have detected an error. It is important that the data represent the relationships between specific lexical items: in the comparable English case, for example, interest is usually found with in: interest in/*with learning. Treating the ends of nominal elements as possible particle slots, we break classification into two steps: 1) Is there a particle? (Yes/No); and 2) What is the exact particle? Using two steps eases </context>
</contexts>
<marker>Le, 2004</marker>
<rawString>Zhang Le. 2004. Maximum Entropy Modeling Toolkit for Python and C++. URL http://homepages.inf.ed.ac.uk/ s0450736/maxent_toolkit.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="845" citStr="Leacock et al., 2010" startWordPosition="118" endWordPosition="122">further work on detecting errors in postpositional particle usage by learners of Korean by improving the training data and developing a complete pipeline of particle selection. We improve the data by filtering non-Korean data and sampling instances to better match the particle distribution. Our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization. 1 Introduction A growing area of research in analyzing learner language is to detect errors in function words, namely categories such as prepositions and articles (see Leacock et al., 2010, and references therein). This work has mostly been for English, and there are issues, such as greater morphological complexity, in moving to other languages (see, e.g., de Ilarraza et al., 2008; Dickinson et al., 2010). Our goal is to build a machine learning system for detecting errors in postpositional particles in Korean, a significant source of learner errors (Ko et al., 2004; Lee et al., 2009b). Korean postpositional particles are morphemes that attach to a preceding nominal to indicate a range of linguistic functions, including grammatical functions, e.g., subject and object; semantic </context>
<context position="17752" citStr="Leacock et al., 2010" startWordPosition="2933" endWordPosition="2936">the particle the learner provided; the absolute difference in probabilities must be above a certain threshold. Table 6 provides the error detection results for each type of particle, incorporating confidence filters of 10%, 20%, 30%, 40%, 50%, and 60%. The results show that increasing the threshold at which we accept the classifier’s answer can significantly increase precision, at the cost of recall. As noted above, higher precision is desirable, so we plan on further developing this confidence filter. We may also include heuristic-based filters, such as the ones implemented in Criterion (see Leacock et al., 2010), as well as a language model approach (Gamon et al., 2008). Finally, we are currently working on improving the POS tagger, testing other taggers in the process, and developing optimal feature sets for different kinds of particles. R P R P R P R P R P R P Acknowledgments We would like to thank the IU CL discussion group and Joel Tetreault for feedback at various points. Adv Aux Case All 10% 10.0% 6.3% 29.9% 16.3% 100% 77.8% 67.8% 73.3% 20% 13.5% 7.8% 32.6% 18.0% 100% 77.8% 50.0% 60.0% 30% 20.0% 8.3% 36.1% 20.8% 100% 66.7% 39.3% 50.7% 40% 19.4% 14.3% 48.6% 26.9% 60.0% 66.7% 30.4% 38.7% 50% 23.1</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Min Lee</author>
<author>Soojeong Eom</author>
<author>Markus Dickinson</author>
</authors>
<title>Towards analyzing korean learner particles. Talk given at CALICO ’09 PreConference Workshop on Automatic Analysis of Learner Language.</title>
<date>2009</date>
<location>Tempe, AZ.</location>
<contexts>
<context position="1247" citStr="Lee et al., 2009" startWordPosition="187" endWordPosition="190"> and system optimization. 1 Introduction A growing area of research in analyzing learner language is to detect errors in function words, namely categories such as prepositions and articles (see Leacock et al., 2010, and references therein). This work has mostly been for English, and there are issues, such as greater morphological complexity, in moving to other languages (see, e.g., de Ilarraza et al., 2008; Dickinson et al., 2010). Our goal is to build a machine learning system for detecting errors in postpositional particles in Korean, a significant source of learner errors (Ko et al., 2004; Lee et al., 2009b). Korean postpositional particles are morphemes that attach to a preceding nominal to indicate a range of linguistic functions, including grammatical functions, e.g., subject and object; semantic roles; and discourse functions. In (1), for instance, ka marks the subject (function) and agent (semantic role).1 Similar to English prepositions, particles can also have modifier functions, adding meanings of time, location, instrument, possession, and so forth. 1We use the Yale Romanization scheme for writing Korean. 81 (1) Sumi-ka Sumi-SBJ sikan-ul kitaly-ess-ta. hours-OBJ wait-PAST-END ‘Sumi wai</context>
<context position="4077" citStr="Lee et al., 2009" startWordPosition="632" endWordPosition="635">24 June 2011. c�2011 Association for Computational Linguistics 2 Particle error detection 2.1 Pre-processing Korean is an agglutinative language: Korean words (referred to as ecels) are usually composed of a root with a number of functional affixes. We thus first segment and POS tag the text, for both training and testing, using a hybrid (trigram + rulebased) morphological tagger for Korean (Han and Palmer, 2004). The tagger is designed for native language and is not optimized to make guesses for ill-formed input. While the POS tags assigned to the learner corpus are thus often incorrect (see Lee et al., 2009a), there is the more primary problem of segmentation, as discussed in more detail in section 4. 2.2 Machine learning We use the Maximum Entropy Toolkit (Le, 2004) for machine learning. Training on a corpus of wellformed Korean, we predict which particle should appear after a given nominal; if this is different from what the learner used, we have detected an error. It is important that the data represent the relationships between specific lexical items: in the comparable English case, for example, interest is usually found with in: interest in/*with learning. Treating the ends of nominal eleme</context>
<context position="9150" citStr="Lee et al. (2009" startWordPosition="1489" endWordPosition="1492">a for the machine learning experiments, by removing a randomly-selected proportion of (negative) instances. Instance sampling has been effective for other NLP tasks, e.g., anaphora resolution (Wunsch et al., 2009), when the number of negative instances is much greater than the positive ones. In our web corpora, nouns have a greater than 50% chance of having no particle; in section 3.4, we thus downsample to varying amounts of negative instances from about 45% to as little as 10% of the total corpus. 3.4 Training data selection In Dickinson et al. (2010), we used a Korean learner data set from Lee et al. (2009b) for development. It contains 3198 ecels, 1842 of which are nominals, and 1271 (P70%) of those have particles. We use this same corpus for development, to evaluate filtering and down-sampling. Evaluating on (yes/no) particle presence, in tables 1 and 2, recall is the percentage of positive instances we correctly find and precision is the percentage of instances that we classify as positive that actually are. A baseline of always guessing a particle gives 100% recall, 69% precision, and 81.7% F-score. Table 1 shows the results of the MaxEnt system for step 1, using training data built for the</context>
</contexts>
<marker>Lee, Eom, Dickinson, 2009</marker>
<rawString>Chong Min Lee, Soojeong Eom, and Markus Dickinson. 2009a. Towards analyzing korean learner particles. Talk given at CALICO ’09 PreConference Workshop on Automatic Analysis of Learner Language. Tempe, AZ.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sun-Hee Lee</author>
</authors>
<title>Seok Bae Jang, and Sang kyu Seo. 2009b. Annotation of korean learner corpora for particle error detection.</title>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Lee, </marker>
<rawString>Sun-Hee Lee, Seok Bae Jang, and Sang kyu Seo. 2009b. Annotation of korean learner corpora for particle error detection. CALICO Journal, 26(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ki-shim Nam</author>
<author>Yong-kun Ko</author>
</authors>
<title>Korean Grammar (phyocwun kwuke mwunpeplon).</title>
<date>2005</date>
<publisher>Top Publisher, Seoul.</publisher>
<contexts>
<context position="12185" citStr="Nam and Ko, 2005" startWordPosition="1997" endWordPosition="2000">ave obtained an annotated corpus of 25 essays from heritage intermediate learners,3 with 299 sentences and 2515 ecels (2676 ecels after correcting spacing errors). There are 1138 nominals, with 93 particle errors (5 added particles, 35 omissions, 53 substitutions)—in other words, less than 10% of particles are errors. There are 979 particles after correction. We focus on 38 particles that intermediate 3Heritage learners have had exposure to Korean at a young age, such as growing up with Korean spoken at home. 83 students can be reasonably expected to use. A particle is one of three types (cf. Nam and Ko, 2005): 1) case markers, 2) adverbials (cf. prepositions), and 3) auxiliary particles.4 Table 3 gives the results for the entire system on the test corpus, with separate results for each category of particle, (Case, Adv., and Aux.) as well as the concatenation of the three (All). The accuracy presented here is in terms of only the particle in question, as opposed to the full form of root+particle(s). Step 2 is presented in 2 ways: Classified, meaning that all of the instances classified as needing a particle by step 1 are processed, or Gold, in which we rely on the annotation to determine particle p</context>
</contexts>
<marker>Nam, Ko, 2005</marker>
<rawString>Ki-shim Nam and Yong-kun Ko. 2005. Korean Grammar (phyocwun kwuke mwunpeplon). Top Publisher, Seoul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiromi Oyama</author>
</authors>
<title>Automatic error detection method for japanese particles.</title>
<date>2010</date>
<journal>Polyglossia,</journal>
<volume>18</volume>
<contexts>
<context position="2116" citStr="Oyama, 2010" startWordPosition="319" endWordPosition="320">rks the subject (function) and agent (semantic role).1 Similar to English prepositions, particles can also have modifier functions, adding meanings of time, location, instrument, possession, and so forth. 1We use the Yale Romanization scheme for writing Korean. 81 (1) Sumi-ka Sumi-SBJ sikan-ul kitaly-ess-ta. hours-OBJ wait-PAST-END ‘Sumi waited for John for (the whole) two hours in his house.’ We treat the task of particle error detection as one of particle selection, and we use machine learning because it has proven effective in similar tasks for other languages (e.g., Chodorow et al., 2007; Oyama, 2010). Training on a corpus of well-formed Korean, we predict which particle should appear after a given nominal; if this is different from the learner’s, we have detected an error. Using a machine learner has the advantage of being able to perform well without a researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.2 We build from Dickinson et al. (2010) in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). As the task is </context>
</contexts>
<marker>Oyama, 2010</marker>
<rawString>Hiromi Oyama. 2010. Automatic error detection method for japanese particles. Polyglossia, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in esl writing.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING-08.</booktitle>
<location>Manchester.</location>
<contexts>
<context position="5585" citStr="Tetreault and Chodorow, 2008" startWordPosition="886" endWordPosition="889">s no need to handle nominals that have no particle in step 2. To evaluate our parameters for obtaining the most relevant instances, we keep the task simple and perform only step 1, as this step provides information about the usability of the training data. For actual system performance, we evaluate both steps. In selecting features for Korean, we have to account for relatively free word order (Chung et al., 2010). We follow our previous work (Dickinson et al., 2010) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also Tetreault and Chodorow, 2008). Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties. Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4). 3 Obtaining the most relevant instances We need well-formed Korean data in order to train a machine learner. To acquire this, we use webbased corpora, as this allows us to find data s</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel Tetreault and Martin Chodorow. 2008. The ups and downs of preposition error detection in esl writing. In Proceedings of COLING-08. Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Wunsch</author>
<author>Sandra K¨ubler</author>
<author>Rachael Cantrell</author>
</authors>
<title>Instance sampling methods for pronoun resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of RANLP 2009. Borovets,</booktitle>
<marker>Wunsch, K¨ubler, Cantrell, 2009</marker>
<rawString>Holger Wunsch, Sandra K¨ubler, and Rachael Cantrell. 2009. Instance sampling methods for pronoun resolution. In Proceedings of RANLP 2009. Borovets, Bulgaria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>