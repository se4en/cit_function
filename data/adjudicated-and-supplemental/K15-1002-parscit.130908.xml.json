{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"E. Bengtson and D. Roth. 2008. Understanding the value of features for coreference resolution. In EMNLP."},"#text":"\n","marker":{"#tail":"\n","#text":"Bengtson, Roth, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" mention is inside the boundary of another mention), but mention heads never overlap. This property also simplifies the problem of mention head candidate generation. In the example above, the first \u201cthey\u201d refers to \u201cMultinational companies investing in China\u201d and the second \u201cThey\u201d refers to \u201cDomestic manufacturers, who are also suffering\u201d. In both cases, the mention heads are sufficient to support the decisions: \u201dthey\u201d refers to \u201dcompanies\u201d, and \u201dThey\u201d refers to \u201dmanufacturers\u201d. In fact, most of the features3 implemented in existing coreference resolution systems rely solely on mention heads (Bengtson and Roth, 2008). Furthermore, consider the possible mention candidate \u201cleague\u201d (italic in the text). It is not chosen as a mention because the surrounding context is not focused on \u201canti-piracy league\u201d. So, mention the CoNLL-2012 dataset is built from OntoNotes-5.0 corpus. 2This example is chosen from the ACE-2004 corpus. 3All features except for those that rely on modifiers. Figure 1: Comparison between a traditional pipelined system and our proposed system. We split up mention detection into two steps: mention head candidate generation and (an optional) mention boundary detection. We feed mention heads rat","@endWordPosition":"967","@position":"6288","annotationId":"T1","@startWordPosition":"964","@citStr":"Bengtson and Roth, 2008"},{"#tail":"\n","#text":"learn a leftlinking scoring function fu,v = w2 φ(u,v), where φ(u,v) is a pairwise feature vector and w2 is the weight vector. The inference algorithm is inspired by the best-left-link approach (Chang et al., 2011), where they solve the following ILP problem: argmax ∑ y u<v,u,vEM s.t. ∑ yu,v G 1, bv E M, (1) u<v yu,v E {0,1} bu,v E M. Here, yu,v = 1 iff mentions u,v are directly linked. Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred. For this mention-pair coreference model φ(u,v), we use the same set of features used in Bengtson and Roth (2008). fu,vyu,v, 14 2.3 Joint Inference Framework We extend expression (1) to facilitate joint inference on mention heads and coreference as follows: argmax ∑ fu,vyu,v + ∑ gmym, y u<v,u,v∈M m∈M s.t. ∑ yu,v ≤ 1, ∀v ∈ M0, u<v ∑ yu,v ≤ yv, ∀v ∈ M0, u<v yu,v ∈ {0,1}, ym ∈ {0,1} ∀u,v,m ∈ M0. Here, M0 is the set of all mention head candidates. ym is the decision variable for mention head candidate m. ym = 1 if and only if the mention head m is chosen. To consider coreference decisions and mention head decisions together, we add the constraint ∑u<v yu,v ≤ yv, which ensures that if a candidate mention head","@endWordPosition":"2106","@position":"13377","annotationId":"T2","@startWordPosition":"2103","@citStr":"Bengtson and Roth (2008)"},{"#tail":"\n","#text":"andard coreference resolution datasets, ACE-2004 (NIST, 2004) and OntoNotes-5.0 (Hovy et al., 2006). Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets. 4.1 Experimental Setup Datasets The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007; Bengtson and Roth, 2008). The OntoNotes-5.0 dataset, which is released for the CoNLL-2012 Shared Task (Pradhan et al., 2012), contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. 17 MUC B3 CEAFe AVG GoldM/H 78.17 81.64 78.45 79.42 StanfordM 63.89 70.33 70.21 68.14 PredictedM 64.28 70.37 70.16 68.27 H-M-CorefM 65.81 71.97 71.14 69.64 H-Joint-MM 67.28 73.06 73.25 71.20 StanfordH 70.28 73.93 73.04 72.42 PredictedH 71.35 75.33 74.02 73.57 H-M-CorefH 71.81 75.69 ","@endWordPosition":"4198","@position":"26006","annotationId":"T3","@startWordPosition":"4195","@citStr":"Bengtson and Roth, 2008"},{"#tail":"\n","#text":"6.28 HeadH 34.00 7.01 JointH 19.22 15.21 H-Joint-MH 53.22 22.22 CoNLL-2012 MDER PGR(AVG) Headx 25.04 12.16 Jointx 10.45 10.44 H-Joint-Mx 35.49 22.60 HeadH 22.40 10.58 JointH 11.81 13.75 H-Joint-MH 34.21 24.33 Table 5: Analysis of performance improvement in terms of Mention Detection Error Reduction (MDER) and Performance Gap Reduction (PGR) for coreference resolution on the ACE-2004 and CoNLL-2012 datasets. \u201cHead\u201d represents the mention head candidate generation module, \u201cJoint\u201d represents the joint learning and inference framework, and \u201cH-JointM\u201d indicates the end-to-end system. Cardie, 2002; Bengtson and Roth, 2008; Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too (Chang et al., 2011; Denis and Baldridge, 2007). In this paper, we use the Constrained Latent Left-Linking Model (CL3M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardi","@endWordPosition":"5816","@position":"36460","annotationId":"T4","@startWordPosition":"5813","@citStr":"Bengtson and Roth, 2008"}]},"title":{"#tail":"\n","#text":"Understanding the value of features for coreference resolution."},"booktitle":{"#tail":"\n","#text":"In EMNLP."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E Bengtson"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"E. Bengtson and D. Roth. 2008. Understanding the value of features for coreference resolution. In EMNLP."},"#text":"\n","marker":{"#tail":"\n","#text":"Bengtson, Roth, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" mention is inside the boundary of another mention), but mention heads never overlap. This property also simplifies the problem of mention head candidate generation. In the example above, the first \u201cthey\u201d refers to \u201cMultinational companies investing in China\u201d and the second \u201cThey\u201d refers to \u201cDomestic manufacturers, who are also suffering\u201d. In both cases, the mention heads are sufficient to support the decisions: \u201dthey\u201d refers to \u201dcompanies\u201d, and \u201dThey\u201d refers to \u201dmanufacturers\u201d. In fact, most of the features3 implemented in existing coreference resolution systems rely solely on mention heads (Bengtson and Roth, 2008). Furthermore, consider the possible mention candidate \u201cleague\u201d (italic in the text). It is not chosen as a mention because the surrounding context is not focused on \u201canti-piracy league\u201d. So, mention the CoNLL-2012 dataset is built from OntoNotes-5.0 corpus. 2This example is chosen from the ACE-2004 corpus. 3All features except for those that rely on modifiers. Figure 1: Comparison between a traditional pipelined system and our proposed system. We split up mention detection into two steps: mention head candidate generation and (an optional) mention boundary detection. We feed mention heads rat","@endWordPosition":"967","@position":"6288","annotationId":"T5","@startWordPosition":"964","@citStr":"Bengtson and Roth, 2008"},{"#tail":"\n","#text":"learn a leftlinking scoring function fu,v = w2 φ(u,v), where φ(u,v) is a pairwise feature vector and w2 is the weight vector. The inference algorithm is inspired by the best-left-link approach (Chang et al., 2011), where they solve the following ILP problem: argmax ∑ y u<v,u,vEM s.t. ∑ yu,v G 1, bv E M, (1) u<v yu,v E {0,1} bu,v E M. Here, yu,v = 1 iff mentions u,v are directly linked. Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred. For this mention-pair coreference model φ(u,v), we use the same set of features used in Bengtson and Roth (2008). fu,vyu,v, 14 2.3 Joint Inference Framework We extend expression (1) to facilitate joint inference on mention heads and coreference as follows: argmax ∑ fu,vyu,v + ∑ gmym, y u<v,u,v∈M m∈M s.t. ∑ yu,v ≤ 1, ∀v ∈ M0, u<v ∑ yu,v ≤ yv, ∀v ∈ M0, u<v yu,v ∈ {0,1}, ym ∈ {0,1} ∀u,v,m ∈ M0. Here, M0 is the set of all mention head candidates. ym is the decision variable for mention head candidate m. ym = 1 if and only if the mention head m is chosen. To consider coreference decisions and mention head decisions together, we add the constraint ∑u<v yu,v ≤ yv, which ensures that if a candidate mention head","@endWordPosition":"2106","@position":"13377","annotationId":"T6","@startWordPosition":"2103","@citStr":"Bengtson and Roth (2008)"},{"#tail":"\n","#text":"andard coreference resolution datasets, ACE-2004 (NIST, 2004) and OntoNotes-5.0 (Hovy et al., 2006). Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets. 4.1 Experimental Setup Datasets The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007; Bengtson and Roth, 2008). The OntoNotes-5.0 dataset, which is released for the CoNLL-2012 Shared Task (Pradhan et al., 2012), contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. 17 MUC B3 CEAFe AVG GoldM/H 78.17 81.64 78.45 79.42 StanfordM 63.89 70.33 70.21 68.14 PredictedM 64.28 70.37 70.16 68.27 H-M-CorefM 65.81 71.97 71.14 69.64 H-Joint-MM 67.28 73.06 73.25 71.20 StanfordH 70.28 73.93 73.04 72.42 PredictedH 71.35 75.33 74.02 73.57 H-M-CorefH 71.81 75.69 ","@endWordPosition":"4198","@position":"26006","annotationId":"T7","@startWordPosition":"4195","@citStr":"Bengtson and Roth, 2008"},{"#tail":"\n","#text":"6.28 HeadH 34.00 7.01 JointH 19.22 15.21 H-Joint-MH 53.22 22.22 CoNLL-2012 MDER PGR(AVG) Headx 25.04 12.16 Jointx 10.45 10.44 H-Joint-Mx 35.49 22.60 HeadH 22.40 10.58 JointH 11.81 13.75 H-Joint-MH 34.21 24.33 Table 5: Analysis of performance improvement in terms of Mention Detection Error Reduction (MDER) and Performance Gap Reduction (PGR) for coreference resolution on the ACE-2004 and CoNLL-2012 datasets. \u201cHead\u201d represents the mention head candidate generation module, \u201cJoint\u201d represents the joint learning and inference framework, and \u201cH-JointM\u201d indicates the end-to-end system. Cardie, 2002; Bengtson and Roth, 2008; Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too (Chang et al., 2011; Denis and Baldridge, 2007). In this paper, we use the Constrained Latent Left-Linking Model (CL3M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardi","@endWordPosition":"5816","@position":"36460","annotationId":"T8","@startWordPosition":"5813","@citStr":"Bengtson and Roth, 2008"}]},"title":{"#tail":"\n","#text":"Understanding the value of features for coreference resolution."},"booktitle":{"#tail":"\n","#text":"In EMNLP."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E Bengtson"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"K.-W. Chang, R. Samdani, and D. Roth. 2013. A constrained latent variable model for coreference resolution. In EMNLP."},"#text":"\n","marker":{"#tail":"\n","#text":"Chang, Samdani, Roth, 2013"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"raw text, outperforming existing state-ofart systems on both the ACE-2004 and the CoNLL-2012 datasets. At the same time, our joint approach is shown to improve mention detection by close to 15% F1. One key insight underlying our approach is that identifying and co-referring mention heads is not only sufficient but is more robust than working with complete mentions. 1 Introduction Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception). Most coreference resolution work simply mentions it in passing as a module in the pipelined system (Chang et al., 2013; Durrett and Klein, 2013; Lee et al., 2011; Bj¨orkelund and Kuhn, 2014). However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty. Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1. Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions. These performance gaps are worrisome, since the real goal of NLP systems is to process raw data. System Dataset Gold Predict Gap Illinois CoNLL-12 77.05 60.00 17.05 Illinois CoNLL-11","@endWordPosition":"213","@position":"1406","annotationId":"T9","@startWordPosition":"210","@citStr":"Chang et al., 2013"},{"#tail":"\n","#text":"d mentions and reduce the performance gap compared to using gold mentions. The rest of the paper is organized as follows. We explain the joint head-coreference learning and inference framework in Sec. 2. Our mention head candidate generation module and mention boundary detection module are described in Sec. 3. We report our experimental results in Sec. 4, review related work in Sec. 5 and conclude in Sec. 6. 2 A Joint Head-Coreference Framework This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in Chang et al. (2013) and the ILP formulation from Chang et al. (2011). The joint learning and inference model takes as input mention head candidates 4Available at http://cogcomp.cs.illinois. edu/page/software_view/Coref (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly","@endWordPosition":"1697","@position":"10904","annotationId":"T10","@startWordPosition":"1694","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"Ctrans,m(w1) = (w> 1 ϕ(u0) − Ω(u0))ϕ(u0)Ω(m). The partial subgradient w.r.t. mention head m for the coreference weight vector w2 is given by ∇w2,mC(w1,w2) = λ2w2+ ⎨⎪ ⎧ ⎪⎩ 0 if Ω(m) = 0 and Ω(u0) = 0. φ(u0,m) − φ(u00,m) if Ω(m) = 1, φ(u0,m) if Ω(m) = 0 and Ω(u0) = 1, (3) Here λ1 and λ2 are regularization coefficients which are tuned on the development set. To learn the mention head detection model, we consider two different parts of the gradient in expression (2). ∇Clocal,m(w1) is exactly the local gradient of mention head m while we add ∇Ctrans,m(w1) to represent 5More details can be found in Chang et al. (2013). The difference here is that we also consider the validity of mention heads using Ω(u),Ω(m) 15 the gradient for mention head u', the mention head chosen by the current best-left-linking model for m. This serves to maximize the margin between valid mention heads and invalid ones. As invalid mention heads will not be linked to any other mention head, ∇trans is zero when m is invalid. When training the mention-pair coreference model, we only consider gradients when at least one of the two mention heads m,u' is valid, as shown in expression (3). When mention head m is valid (n(m) = 1), the gradie","@endWordPosition":"2708","@position":"16822","annotationId":"T11","@startWordPosition":"2705","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"amed entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj¨orkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL3M) as our mention-pair coreference model in the joint framework10. When the CL3M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref9No parsing information is needed at evaluation time. 10We ","@endWordPosition":"4523","@position":"28106","annotationId":"T12","@startWordPosition":"4520","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"ent in terms of Mention Detection Error Reduction (MDER) and Performance Gap Reduction (PGR) for coreference resolution on the ACE-2004 and CoNLL-2012 datasets. \u201cHead\u201d represents the mention head candidate generation module, \u201cJoint\u201d represents the joint learning and inference framework, and \u201cH-JointM\u201d indicates the end-to-end system. Cardie, 2002; Bengtson and Roth, 2008; Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too (Chang et al., 2011; Denis and Baldridge, 2007). In this paper, we use the Constrained Latent Left-Linking Model (CL3M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. Ratinov and Roth (2009) present detailed studies on t","@endWordPosition":"5857","@position":"36706","annotationId":"T13","@startWordPosition":"5854","@citStr":"Chang et al. (2013)"}]},"title":{"#tail":"\n","#text":"A constrained latent variable model for coreference resolution."},"booktitle":{"#tail":"\n","#text":"In EMNLP."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K-W Chang"},{"#tail":"\n","#text":"R Samdani"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"K.-W. Chang, R. Samdani, and D. Roth. 2013. A constrained latent variable model for coreference resolution. In EMNLP."},"#text":"\n","marker":{"#tail":"\n","#text":"Chang, Samdani, Roth, 2013"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"raw text, outperforming existing state-ofart systems on both the ACE-2004 and the CoNLL-2012 datasets. At the same time, our joint approach is shown to improve mention detection by close to 15% F1. One key insight underlying our approach is that identifying and co-referring mention heads is not only sufficient but is more robust than working with complete mentions. 1 Introduction Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception). Most coreference resolution work simply mentions it in passing as a module in the pipelined system (Chang et al., 2013; Durrett and Klein, 2013; Lee et al., 2011; Bj¨orkelund and Kuhn, 2014). However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty. Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1. Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions. These performance gaps are worrisome, since the real goal of NLP systems is to process raw data. System Dataset Gold Predict Gap Illinois CoNLL-12 77.05 60.00 17.05 Illinois CoNLL-11","@endWordPosition":"213","@position":"1406","annotationId":"T14","@startWordPosition":"210","@citStr":"Chang et al., 2013"},{"#tail":"\n","#text":"d mentions and reduce the performance gap compared to using gold mentions. The rest of the paper is organized as follows. We explain the joint head-coreference learning and inference framework in Sec. 2. Our mention head candidate generation module and mention boundary detection module are described in Sec. 3. We report our experimental results in Sec. 4, review related work in Sec. 5 and conclude in Sec. 6. 2 A Joint Head-Coreference Framework This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in Chang et al. (2013) and the ILP formulation from Chang et al. (2011). The joint learning and inference model takes as input mention head candidates 4Available at http://cogcomp.cs.illinois. edu/page/software_view/Coref (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly","@endWordPosition":"1697","@position":"10904","annotationId":"T15","@startWordPosition":"1694","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"Ctrans,m(w1) = (w> 1 ϕ(u0) − Ω(u0))ϕ(u0)Ω(m). The partial subgradient w.r.t. mention head m for the coreference weight vector w2 is given by ∇w2,mC(w1,w2) = λ2w2+ ⎨⎪ ⎧ ⎪⎩ 0 if Ω(m) = 0 and Ω(u0) = 0. φ(u0,m) − φ(u00,m) if Ω(m) = 1, φ(u0,m) if Ω(m) = 0 and Ω(u0) = 1, (3) Here λ1 and λ2 are regularization coefficients which are tuned on the development set. To learn the mention head detection model, we consider two different parts of the gradient in expression (2). ∇Clocal,m(w1) is exactly the local gradient of mention head m while we add ∇Ctrans,m(w1) to represent 5More details can be found in Chang et al. (2013). The difference here is that we also consider the validity of mention heads using Ω(u),Ω(m) 15 the gradient for mention head u', the mention head chosen by the current best-left-linking model for m. This serves to maximize the margin between valid mention heads and invalid ones. As invalid mention heads will not be linked to any other mention head, ∇trans is zero when m is invalid. When training the mention-pair coreference model, we only consider gradients when at least one of the two mention heads m,u' is valid, as shown in expression (3). When mention head m is valid (n(m) = 1), the gradie","@endWordPosition":"2708","@position":"16822","annotationId":"T16","@startWordPosition":"2705","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"amed entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj¨orkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL3M) as our mention-pair coreference model in the joint framework10. When the CL3M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref9No parsing information is needed at evaluation time. 10We ","@endWordPosition":"4523","@position":"28106","annotationId":"T17","@startWordPosition":"4520","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"ent in terms of Mention Detection Error Reduction (MDER) and Performance Gap Reduction (PGR) for coreference resolution on the ACE-2004 and CoNLL-2012 datasets. \u201cHead\u201d represents the mention head candidate generation module, \u201cJoint\u201d represents the joint learning and inference framework, and \u201cH-JointM\u201d indicates the end-to-end system. Cardie, 2002; Bengtson and Roth, 2008; Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too (Chang et al., 2011; Denis and Baldridge, 2007). In this paper, we use the Constrained Latent Left-Linking Model (CL3M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. Ratinov and Roth (2009) present detailed studies on t","@endWordPosition":"5857","@position":"36706","annotationId":"T18","@startWordPosition":"5854","@citStr":"Chang et al. (2013)"}]},"title":{"#tail":"\n","#text":"A constrained latent variable model for coreference resolution."},"booktitle":{"#tail":"\n","#text":"In EMNLP."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K-W Chang"},{"#tail":"\n","#text":"R Samdani"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"K.-W. Chang, R. Samdani, and D. Roth. 2013. A constrained latent variable model for coreference resolution. In EMNLP."},"#text":"\n","marker":{"#tail":"\n","#text":"Chang, Samdani, Roth, 2013"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"raw text, outperforming existing state-ofart systems on both the ACE-2004 and the CoNLL-2012 datasets. At the same time, our joint approach is shown to improve mention detection by close to 15% F1. One key insight underlying our approach is that identifying and co-referring mention heads is not only sufficient but is more robust than working with complete mentions. 1 Introduction Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception). Most coreference resolution work simply mentions it in passing as a module in the pipelined system (Chang et al., 2013; Durrett and Klein, 2013; Lee et al., 2011; Bj¨orkelund and Kuhn, 2014). However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty. Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1. Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions. These performance gaps are worrisome, since the real goal of NLP systems is to process raw data. System Dataset Gold Predict Gap Illinois CoNLL-12 77.05 60.00 17.05 Illinois CoNLL-11","@endWordPosition":"213","@position":"1406","annotationId":"T19","@startWordPosition":"210","@citStr":"Chang et al., 2013"},{"#tail":"\n","#text":"d mentions and reduce the performance gap compared to using gold mentions. The rest of the paper is organized as follows. We explain the joint head-coreference learning and inference framework in Sec. 2. Our mention head candidate generation module and mention boundary detection module are described in Sec. 3. We report our experimental results in Sec. 4, review related work in Sec. 5 and conclude in Sec. 6. 2 A Joint Head-Coreference Framework This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in Chang et al. (2013) and the ILP formulation from Chang et al. (2011). The joint learning and inference model takes as input mention head candidates 4Available at http://cogcomp.cs.illinois. edu/page/software_view/Coref (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly","@endWordPosition":"1697","@position":"10904","annotationId":"T20","@startWordPosition":"1694","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"Ctrans,m(w1) = (w> 1 ϕ(u0) − Ω(u0))ϕ(u0)Ω(m). The partial subgradient w.r.t. mention head m for the coreference weight vector w2 is given by ∇w2,mC(w1,w2) = λ2w2+ ⎨⎪ ⎧ ⎪⎩ 0 if Ω(m) = 0 and Ω(u0) = 0. φ(u0,m) − φ(u00,m) if Ω(m) = 1, φ(u0,m) if Ω(m) = 0 and Ω(u0) = 1, (3) Here λ1 and λ2 are regularization coefficients which are tuned on the development set. To learn the mention head detection model, we consider two different parts of the gradient in expression (2). ∇Clocal,m(w1) is exactly the local gradient of mention head m while we add ∇Ctrans,m(w1) to represent 5More details can be found in Chang et al. (2013). The difference here is that we also consider the validity of mention heads using Ω(u),Ω(m) 15 the gradient for mention head u', the mention head chosen by the current best-left-linking model for m. This serves to maximize the margin between valid mention heads and invalid ones. As invalid mention heads will not be linked to any other mention head, ∇trans is zero when m is invalid. When training the mention-pair coreference model, we only consider gradients when at least one of the two mention heads m,u' is valid, as shown in expression (3). When mention head m is valid (n(m) = 1), the gradie","@endWordPosition":"2708","@position":"16822","annotationId":"T21","@startWordPosition":"2705","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"amed entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj¨orkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL3M) as our mention-pair coreference model in the joint framework10. When the CL3M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref9No parsing information is needed at evaluation time. 10We ","@endWordPosition":"4523","@position":"28106","annotationId":"T22","@startWordPosition":"4520","@citStr":"Chang et al. (2013)"},{"#tail":"\n","#text":"ent in terms of Mention Detection Error Reduction (MDER) and Performance Gap Reduction (PGR) for coreference resolution on the ACE-2004 and CoNLL-2012 datasets. \u201cHead\u201d represents the mention head candidate generation module, \u201cJoint\u201d represents the joint learning and inference framework, and \u201cH-JointM\u201d indicates the end-to-end system. Cardie, 2002; Bengtson and Roth, 2008; Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too (Chang et al., 2011; Denis and Baldridge, 2007). In this paper, we use the Constrained Latent Left-Linking Model (CL3M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. Ratinov and Roth (2009) present detailed studies on t","@endWordPosition":"5857","@position":"36706","annotationId":"T23","@startWordPosition":"5854","@citStr":"Chang et al. (2013)"}]},"title":{"#tail":"\n","#text":"A constrained latent variable model for coreference resolution."},"booktitle":{"#tail":"\n","#text":"In EMNLP."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K-W Chang"},{"#tail":"\n","#text":"R Samdani"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"Computer Science Department, University of Pennsylvenia."},"rawString":{"#tail":"\n","#text":"M. Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Computer Science Department, University of Pennsylvenia."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ntion heads from valid ones, and thus has the ability to preserve valid singleton heads. Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples. We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples. Specifically, after mention head candidate generation (described in Sec. 3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker (Punyakanok and Roth, 2001)6 to extract more noun phrases from the text and employ Collins head rules (Collins, 1999) to identify their heads. When these extracted heads do not overlap with gold mention heads, we treat them as negative examples. We note that the aforementioned joint framework can take as input either complete mention candidates or mention head candidates. However, in this paper we only feed mention heads into it. Our experimental results support our intuition that this provides better results. 6http://cogcomp.cs.illinois.edu/page/ software_view/Chunker 3 Mention Detection Modules This section describes the module that generates our mention head candidates, and then how the mention heads are ","@endWordPosition":"3061","@position":"18905","annotationId":"T24","@startWordPosition":"3060","@citStr":"Collins, 1999"},{"#tail":"\n","#text":"erence resolution for all systems on the ACE-2004 dataset. Subscripts (M, H) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as a = 0.9, fl = 0.8, �1 = 0.2 and I2 = 0.3. The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes5.0 dataset only has mention annotations. Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information. The parsing information9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: ","@endWordPosition":"4400","@position":"27296","annotationId":"T25","@startWordPosition":"4399","@citStr":"Collins, 1999"}]},"title":{"#tail":"\n","#text":"Head-driven Statistical Models for Natural Language Parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"Computer Science Department, University of Pennsylvenia."},"rawString":{"#tail":"\n","#text":"M. Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Computer Science Department, University of Pennsylvenia."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ntion heads from valid ones, and thus has the ability to preserve valid singleton heads. Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples. We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples. Specifically, after mention head candidate generation (described in Sec. 3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker (Punyakanok and Roth, 2001)6 to extract more noun phrases from the text and employ Collins head rules (Collins, 1999) to identify their heads. When these extracted heads do not overlap with gold mention heads, we treat them as negative examples. We note that the aforementioned joint framework can take as input either complete mention candidates or mention head candidates. However, in this paper we only feed mention heads into it. Our experimental results support our intuition that this provides better results. 6http://cogcomp.cs.illinois.edu/page/ software_view/Chunker 3 Mention Detection Modules This section describes the module that generates our mention head candidates, and then how the mention heads are ","@endWordPosition":"3061","@position":"18905","annotationId":"T26","@startWordPosition":"3060","@citStr":"Collins, 1999"},{"#tail":"\n","#text":"erence resolution for all systems on the ACE-2004 dataset. Subscripts (M, H) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as a = 0.9, fl = 0.8, �1 = 0.2 and I2 = 0.3. The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes5.0 dataset only has mention annotations. Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information. The parsing information9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: ","@endWordPosition":"4400","@position":"27296","annotationId":"T27","@startWordPosition":"4399","@citStr":"Collins, 1999"}]},"title":{"#tail":"\n","#text":"Head-driven Statistical Models for Natural Language Parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"A. Culotta, M. Wick, and A. McCallum. 2007. Firstorder probabilistic models for coreference resolution. In NAACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Culotta, Wick, McCallum, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"eriments on the two standard coreference resolution datasets, ACE-2004 (NIST, 2004) and OntoNotes-5.0 (Hovy et al., 2006). Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets. 4.1 Experimental Setup Datasets The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007; Bengtson and Roth, 2008). The OntoNotes-5.0 dataset, which is released for the CoNLL-2012 Shared Task (Pradhan et al., 2012), contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. 17 MUC B3 CEAFe AVG GoldM/H 78.17 81.64 78.45 79.42 StanfordM 63.89 70.33 70.21 68.14 PredictedM 64.28 70.37 70.16 68.27 H-M-CorefM 65.81 71.97 71.14 69.64 H-Joint-MM 67.28 73.06 73.25 71.20 StanfordH 70.28 73.93 73.04 72.42 PredictedH 71.35 75.33 74.02 73.","@endWordPosition":"4194","@position":"25980","annotationId":"T28","@startWordPosition":"4191","@citStr":"Culotta et al., 2007"}},"title":{"#tail":"\n","#text":"Firstorder probabilistic models for coreference resolution."},"booktitle":{"#tail":"\n","#text":"In NAACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Culotta"},{"#tail":"\n","#text":"M Wick"},{"#tail":"\n","#text":"A McCallum"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2014"},"rawString":{"#tail":"\n","#text":"G. Durrett and D. Klein. 2014. A joint model for entity analysis: Coreference, typing, and linking."},"#text":"\n","marker":{"#tail":"\n","#text":"Durrett, Klein, 2014"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"named entity information. The parsing information9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj¨orkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL3M) as our mention-pair coreference model in the joint framework10. When the CL3M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can fe","@endWordPosition":"4501","@position":"27973","annotationId":"T29","@startWordPosition":"4498","@citStr":"Durrett and Klein, 2014"},{"#tail":"\n","#text":"04 and CoNLL-2012 datasets is shown in Table 2 and Table 3 respectively.12 These results show that our developed system H-Joint-M 11The latest scorer is version v8.01, but MUC, B3, CEAFe and CoNLL average scores are not changed. For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input. Results for HOTCoref are slightly different from the results reported in Bj¨orkelund and Kuhn (2014). For Berkeley system, we use the reported results from Durrett and Klein (2014). 18 shows significant improvement on all metrics for both datasets. Existing systems only report results on mentions. Here, we also show their performance evaluated on mention heads. When evaluated on mention heads rather than mentions13, we can always expect a performance increase for all systems on both datasets. Even though evaluating on mentions is more common in the literature, it is often enough to identify just mention heads in coreference chains (as shown in the example from Sec. 1). H-M-Coref can already bring substantial performance improvement, which indicates that it is helpful fo","@endWordPosition":"5010","@position":"31221","annotationId":"T30","@startWordPosition":"5007","@citStr":"Durrett and Klein (2014)"},{"#tail":"\n","#text":"d entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities. The most relevant study on mentions in the context of coreference was done in Recasens et al. (2013); this work studies distinguishing single mentions from coreferent mentions. Our joint framework provides similar insights, where the added mention decision variable partly reflects if the mention is singleton or not. Several recent works suggest studying coreference jointly with other tasks. Lee et al. (2012) model entity coreference and event coreference jointly; Durrett and Klein (2014) consider joint coreference and entity-linking. The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework. While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different. 6 Conclusion This paper proposes a joint inference approach to the end-to-end coreference resolution problem. By moving to identify mention heads rather than mentions, and by developing an ILP-based, joint, online learning and inference approach, we close a significant fracti","@endWordPosition":"6136","@position":"38541","annotationId":"T31","@startWordPosition":"6133","@citStr":"Durrett and Klein (2014)"}]},"title":{"#tail":"\n","#text":"A joint model for entity analysis: Coreference, typing, and linking."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"G Durrett"},{"#tail":"\n","#text":"D Klein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2014"},"rawString":{"#tail":"\n","#text":"G. Durrett and D. Klein. 2014. A joint model for entity analysis: Coreference, typing, and linking."},"#text":"\n","marker":{"#tail":"\n","#text":"Durrett, Klein, 2014"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"named entity information. The parsing information9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj¨orkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL3M) as our mention-pair coreference model in the joint framework10. When the CL3M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can fe","@endWordPosition":"4501","@position":"27973","annotationId":"T32","@startWordPosition":"4498","@citStr":"Durrett and Klein, 2014"},{"#tail":"\n","#text":"04 and CoNLL-2012 datasets is shown in Table 2 and Table 3 respectively.12 These results show that our developed system H-Joint-M 11The latest scorer is version v8.01, but MUC, B3, CEAFe and CoNLL average scores are not changed. For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input. Results for HOTCoref are slightly different from the results reported in Bj¨orkelund and Kuhn (2014). For Berkeley system, we use the reported results from Durrett and Klein (2014). 18 shows significant improvement on all metrics for both datasets. Existing systems only report results on mentions. Here, we also show their performance evaluated on mention heads. When evaluated on mention heads rather than mentions13, we can always expect a performance increase for all systems on both datasets. Even though evaluating on mentions is more common in the literature, it is often enough to identify just mention heads in coreference chains (as shown in the example from Sec. 1). H-M-Coref can already bring substantial performance improvement, which indicates that it is helpful fo","@endWordPosition":"5010","@position":"31221","annotationId":"T33","@startWordPosition":"5007","@citStr":"Durrett and Klein (2014)"},{"#tail":"\n","#text":"d entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities. The most relevant study on mentions in the context of coreference was done in Recasens et al. (2013); this work studies distinguishing single mentions from coreferent mentions. Our joint framework provides similar insights, where the added mention decision variable partly reflects if the mention is singleton or not. Several recent works suggest studying coreference jointly with other tasks. Lee et al. (2012) model entity coreference and event coreference jointly; Durrett and Klein (2014) consider joint coreference and entity-linking. The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework. While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different. 6 Conclusion This paper proposes a joint inference approach to the end-to-end coreference resolution problem. By moving to identify mention heads rather than mentions, and by developing an ILP-based, joint, online learning and inference approach, we close a significant fracti","@endWordPosition":"6136","@position":"38541","annotationId":"T34","@startWordPosition":"6133","@citStr":"Durrett and Klein (2014)"}]},"title":{"#tail":"\n","#text":"A joint model for entity analysis: Coreference, typing, and linking."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"G Durrett"},{"#tail":"\n","#text":"D Klein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of HLT/NAACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"We construct negative examples as (oi_1,ha,b,L) and (oj+1,ha,b,R). Once trained, the binary classifier takes in the head, a token and the direction of the token relative to the head, and decides whether the token is inside or outside the mention corresponding to the head. At test time, this classifier is used around each confirmed head to determine the mention boundaries. The features used here are similar to the mention head detection model described in Sec. 2.1. 4 Experiments We present experiments on the two standard coreference resolution datasets, ACE-2004 (NIST, 2004) and OntoNotes-5.0 (Hovy et al., 2006). Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets. 4.1 Experimental Setup Datasets The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007; Bengtson and Roth, 2008). The OntoNotes-5.0 dataset, which is released for the CoNLL-2012 Shared Ta","@endWordPosition":"4118","@position":"25481","annotationId":"T35","@startWordPosition":"4115","@citStr":"Hovy et al., 2006"}},"title":{"#tail":"\n","#text":"Ontonotes: The 90% solution."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT/NAACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E Hovy"},{"#tail":"\n","#text":"M Marcus"},{"#tail":"\n","#text":"M Palmer"},{"#tail":"\n","#text":"L Ramshaw"},{"#tail":"\n","#text":"R Weischedel"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and Y. Zhang. 2012. CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In CoNLL."},"#text":"\n","marker":{"#tail":"\n","#text":"Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"r approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets. 4.1 Experimental Setup Datasets The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007; Bengtson and Roth, 2008). The OntoNotes-5.0 dataset, which is released for the CoNLL-2012 Shared Task (Pradhan et al., 2012), contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. 17 MUC B3 CEAFe AVG GoldM/H 78.17 81.64 78.45 79.42 StanfordM 63.89 70.33 70.21 68.14 PredictedM 64.28 70.37 70.16 68.27 H-M-CorefM 65.81 71.97 71.14 69.64 H-Joint-MM 67.28 73.06 73.25 71.20 StanfordH 70.28 73.93 73.04 72.42 PredictedH 71.35 75.33 74.02 73.57 H-M-CorefH 71.81 75.69 74.45 73.98 H-Joint-MH 72.74 76.69 75.18 74.87 Table 2: Performance of coreference resolution for al","@endWordPosition":"4214","@position":"26106","annotationId":"T36","@startWordPosition":"4210","@citStr":"Pradhan et al., 2012"}},"title":{"#tail":"\n","#text":"CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes."},"booktitle":{"#tail":"\n","#text":"In CoNLL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Pradhan"},{"#tail":"\n","#text":"A Moschitti"},{"#tail":"\n","#text":"N Xue"},{"#tail":"\n","#text":"O Uryupina"},{"#tail":"\n","#text":"Y Zhang"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL)."},"#text":"\n","marker":{"#tail":"\n","#text":"Ratinov, Roth, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"nt mentions. The sequence labeling component builds on the following assumption: Assumption Different mentions have different heads, and heads do not overlap with each other. That is, for each mi,j, we have a corresponding head ha,b where i < a < b < j. Moreover, for another head ha',b', we have the satisfying condition a − b' > 0 or b − a' < 0 bha,b,ha',b'. Based on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation, as shown, e.g. in Ratinov and Roth (2009). The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks. The problem is then transformed into a simple, but constrained, 5-class classification problem. The BILOU-classifier shares all features with the mention head detection model described in Sec. 2.1 except for two: length of mention heads and NPMI over head boundary. For each instance, the feature 16 vector is sparse and we use sparse perceptron (Jackson and Craven, 1996) for supervised training. We also apply a two layer prediction aggr","@endWordPosition":"3425","@position":"21166","annotationId":"T37","@startWordPosition":"3422","@citStr":"Ratinov and Roth (2009)"},{"#tail":"\n","#text":"Linking Model (CL3M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. Ratinov and Roth (2009) present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non-local features, and integration of external knowledge. NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004). The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and","@endWordPosition":"5944","@position":"37276","annotationId":"T38","@startWordPosition":"5941","@citStr":"Ratinov and Roth (2009)"}]},"title":{"#tail":"\n","#text":"Design challenges and misconceptions in named entity recognition."},"booktitle":{"#tail":"\n","#text":"In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L Ratinov"},{"#tail":"\n","#text":"D Roth"}]}}]}}}}
