es underspecify the relative order for many pairs of adjectives and are often difficult to apply in practice. In this paper, we will discuss a number of statistical and machine learning approaches to automatically extracting from large corpora the constraints on the order of prenominal adjectives in English. 2 Word bigram model The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system. One approach to this more general problem, taken by the ‘Nitrogen’ generator (Langkilde and Knight, 1998a; Langkilde and Knight, 1998b), takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model. Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word. It also should be straightforwardly applicable to the more specific problem we are addressing here. To determine the correct order for a sequence of prenominal adjectives, we can
 pair types occur only once, and 49% of the adjective types only occur once. Second, we get no useful information about the syntagmatic context in which a pair appears. The lefthand context is almost always a determiner, and including information about the modified head noun would only make the data even sparser. This lack of context makes this problem different from other problems, such as part-of-speech tagging and grapheme-to-phoneme conversion, for which statistical and machine learning solutions have been proposed. 3.2 Direct evidence The simplest strategy for ordering adjectives is what Shaw and Hatzivassiloglou (1999) call the direct evidence method. To order the pair {a,b}, count how many times the ordered sequences (a,b) and (b,a) appear in the training data and output the pair in the order which occurred more often. This method has the advantage of being conceptually very simple, easy to implement, and highly accurate for pairs of adjectives which actually appear in the training data. Applying this method to the adjectives sequences taken from the BNC yields better than 98% accuracy for pairs that occurred in the training data. However, since as we have seen, the majority of pairs occur only once, the o
he sparseness inherent to this kind of data, we need a method which can generalize from the pairs which occur in the training data to unseen pairs. 3.3 Transitivity One way to think of the direct evidence method is to see that it defines a relation � on the set of English adjectives. Given two adjectives, if the ordered pair (a,b) appears in the training data more often then the pair (b,a), then a � b. If the reverse is true, and (b,a) is found more often than (a,b), then b � a. If neither order appears in the training data, then neither a � b nor b � a and an order must be randomly assigned. Shaw and Hatzivassiloglou (1999) propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation �. That is, if a � c and c � b, we can conclude that a � b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs (large,new) and (new,green) occur fairly frequently. Therefore, in the face of this evidence we can assign this pair the order (large,green), which not coincidently is the correct E
culty with applying the transitive closure method to any large dataset is that there often will be evidence for both orders of any given pair. For instance, alongside the evidence supporting the order (large,green), we also find the pairs (green,byzantine), (byzantine,decorative), and (decorative,new), which suggest the order (green,large). Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second. The first ordered pairs are more frequent, as are the individual adjectives involved. To quantify the relative strengths of these transitive inferences, Shaw and Hatzivassiloglou (1999) propose to assign a weight to each link. Say the order (a,b) occurs m times and the pair {a,b} occurs n times in total. Then the weight of the pair a —* b is: n 1 n n ∑ k 2 k=m This weight decreases as the probability that the observed order did not occur strictly by chance increases. This way, the problem of finding the order best supported by the evidence can be stated as a general shortest path problem: to find the preferred order for {a,b}, find the sum of the weights of the pairs in the lowest-weighted path from a to b and from b to a and choose whichever is lower. Using this method, Sha
r improving the results. First, while semantic information is not available for all adjectives, it is clearly available for some. Furthermore, any realistic dialog system would make use of some limited vocabulary Direct evidence 78.28% Adjective bigrams 88.02% MBL (morphological) 89.34% (*) Positional probabilities 89.73% (*) MBL (combined) 91.85% Table 1: Summary of results. With the exception of the starred values, all differences are statistically significant (p < 0.005) for which semantic information would be available. More generally, distributional clustering techniques (Sch¨utze, 1992; Pereira et al., 1993) could be applied to extract semantic classes from the corpus itself. Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results. The second area where the methods described here could be improved is in the way that multiple information sources are integrated. The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data. It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techn
ing in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results. The second area where the methods described here could be improved is in the way that multiple information sources are integrated. The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data. It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997). In particular, boosting (Schapire, 1999; Abney et al., 1999) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly. 5 Conclusion In this paper, we have presented the results of applying a number of statistical and machine learning techniques to the problem of predicting the order of prenominal adjectives in English. The scores for each of the methods are summarized in table 1. The best methods yield around 90% accuracy, better than the best previously published methods when applied to the broad domain data of the British National Corpus. Note that McNemar’s test (Dietterich, 1998) con
