chine interface using speech require an &quot;understanding&quot; of the intended message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current ad- vances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural anguage workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Repre- sentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). Spoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139 ~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; 
ing&quot; of the intended message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current ad- vances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural anguage workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Repre- sentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). Spoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139 ~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; interchangeably. (~) 1992 Association for Computa
d message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current ad- vances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural anguage workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Repre- sentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). Spoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139 ~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; interchangeably. (~) 1992 Association for Computational Linguisti
to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current ad- vances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural anguage workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Repre- sentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). Spoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139 ~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; interchangeably. (~) 1992 Association for Computational Linguistics Computational L
ould I be careful?&quot;). 11 Secondly, it greatly simplifies the implementation, because rules do not have to be ex- plicitly monitored uring the parse. Given a particular parent and a particular child, the system can generate the allowable right siblings without having to note who the left siblings (beyond the immediate one) were. Finally, and perhaps most importantly, probabilities are established on arcs connecting sibling pairs regardless of which rule is under construction. In this sense the arc probabilities behave like the familiar word- level bigrams of simple recognition language models (Jelinek 1976), except hat they apply to siblings at multiple levels of the hierarchy. This makes the probabilities mean- ingful as a product of conditional probabilities as the parse advances to deeper levels of the parse tree and also as it returns to higher levels of the parse tree. This approach implies an independence assumption that claims that what can follow depends only on the left sibling and the parent. One negative aspect of the cross-pollination is that the system can potentially generalize to include forms that are agrammatical. For instance, the forms &quot;Pick the box up&quot; and &quot;Pick up the box,&quot; 
 rules producing forms such as &quot;Pick up the box up&quot; and &quot;Pick up the box up the box!&quot; This problem can be overcome ither by giving the two structures different LHS names or by grouping &quot;up the box&quot; and &quot;the box up&quot; into distinct parent nodes, adding another layer to the hierarchy on the RHS. Another solution is to use a trace mechanism to link the two positions for the object, thus preventing it from occurring in both places. A final alternative is to include a PARTICLE bit among 10 Some modification of this scheme is necessary when the input stream is not deterministic. For the A* algorithm (Hart et al 1968) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion. Unless some kind of normalization is done, the short theories have an unfair advantage, s imply because fewer probability scores have been multiplied. With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found. 11 The auxiliary verb sets the mode of the main verb to be root or past participle as appropriate. 67 Computational Linguistics Volume 18, Number 1 the features which, once set, cannot be reset. In fact, tlhere were
il we have a better handle on control strategies in the best-first search algorithm, it is impossible to predict he computational load for a spoken-input mode. 2.5 Constraints and Gaps This section describes how TINA handles everal issues that are often considered to be part of the task of a parser. These include agreement constraints, emantic restrictions, 68 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in &quot;(which article)/do you think I should read (ti)?&quot;) (Chomsky 1977). The gap mechanism resembles the Hold register idea of ATNs (Woods 1970) and the treatment of bounded omination metavariables in lexical functional grammars (LFGs) (Bresnan 1982, p. 235 ft.), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent odes. Our approach to the design of a constraint mechanism is to establish a framework general enough to handle syntactic, semantic, and, ultimately, phonological constraints using identical functional procedures applied at the node level. The intent was to
lgorithm, it is impossible to predict he computational load for a spoken-input mode. 2.5 Constraints and Gaps This section describes how TINA handles everal issues that are often considered to be part of the task of a parser. These include agreement constraints, emantic restrictions, 68 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in &quot;(which article)/do you think I should read (ti)?&quot;) (Chomsky 1977). The gap mechanism resembles the Hold register idea of ATNs (Woods 1970) and the treatment of bounded omination metavariables in lexical functional grammars (LFGs) (Bresnan 1982, p. 235 ft.), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent odes. Our approach to the design of a constraint mechanism is to establish a framework general enough to handle syntactic, semantic, and, ultimately, phonological constraints using identical functional procedures applied at the node level. The intent was to de- sign a grammar for which the rules would be kept completely free of 
 into the FLOAT-OBJECT position, for its children, during the top-down cycle. It also requires that the FLOAT-OBJECT be absorbed somewhere among its descen- dants by a designated absorber node, a condition that is checked uring the bottom-up cycle. The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree. That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an \[end\] node is encountered. To a first approximation, a CURRENT-FOCUS reaches only nodes that are c-commanded (Chomsky 1977) by its generator. Finally, certain blocker nodes block the transfer of the FLOAT-OBJECT to their children. A simple example will help explain how this works. For the sentence &quot;(How many pies)/ did Mike buy (ti)?&quot; as illustrated by the parse tree in Figure 3, the \[q- subject\] &quot;how many pies&quot; is a generator, so it fills the CURRENT-FOCUS with its subparse. The \[do-question\] is an activator; it moves the CURRENT-FOCUS into the FLOAT-OBJECT position. Finally, the object of '~ouy,&quot; an absorber, takes the \[q-subject\] as its subparse. The \[do-question\] refuses to accept any solutions from it
 is reached, the word &quot;Jane&quot; is in the CURRENT-FOCUS slot, and the phrase &quot;which hospital&quot; is still in the FLOAT-OBJECT slot. The \[participial-phrase\] for &quot;taken \[object\]&quot; activates &quot;Jane,&quot; but only for its children. This word is ultimately absorbed by the \[object\] node within the verb phrase. Meanwhile, the \[participial-phrase\] passes along the original FLOAT- OBJECT (&quot;which hospital&quot;) to its right sibling, the adverbial prepositional phrase, &quot;to \[object\].&quot; The phrase &quot;which hospital&quot; is finally absorbed by the preposition's object. The example used to illustrate the power of ATNs (Woods 1986), &quot;John was be- lieved to have been shot,&quot; also parses correctly, because the \[object\] node following the verb &quot;believed&quot; acts as both an absorber and a (re)generator. Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexist- ing FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)? 71 Computational Linguistics Volume 1
 a current-focus i passed only to siblings and children, but not to parents). Thus, the CURRENT-FOCUS &quot;I&quot; is passed on from the predicate to the adjunct, and eventually to the verb &quot;stay.&quot; Finally, in the case of passive voice, the CURRENT-FOCUS slot is empty at the time the verb is proposed, because the CURRENT-FOCUS which was the surface-form subject has been moved to the float-object position. In this case, the verb has no information concerning its subject, and so it identifies it as an unbound pronoun. Semantic filters can also be used to prevent multiple versions of the same case frame (Fillmore 1968) showing up as complements. For instance, the set of comple- ments \[from-place\], \[to-place\], and \[at-time\] are freely ordered following a movement verb such as &quot;leave.&quot; Thus a flight can &quot;leave for Chicago from Boston at nine,&quot; or, equivalently, &quot;leave at nine for Chicago from Boston.&quot; If these complements are each allowed to follow the other, then in TINA an infinite sequence of \[from-place\]s, \[to-place\]s and \[at-time\]s is possible. This is of course unacceptable, but it is straight- forward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in t
il mode, i.e., if the semantic restrictions fail, the node dies. This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic onditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept he proposals. This approach resembles the work by Grishman et al (1986) and Hirschman et al (1975) on selectional restrictions. The semantic onditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area. 3. Evaluat ion Measures This section addresses some performance measures for a grammar, including coverage, portability, perplexity, and trainability. Perplexity, roughly defined as the geometric mean of the number of alternative word hypotheses that may follow each word in the sentence, is of particular concern in spoken la
ntic restrictions fail, the node dies. This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic onditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept he proposals. This approach resembles the work by Grishman et al (1986) and Hirschman et al (1975) on selectional restrictions. The semantic onditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area. 3. Evaluat ion Measures This section addresses some performance measures for a grammar, including coverage, portability, perplexity, and trainability. Perplexity, roughly defined as the geometric mean of the number of alternative word hypotheses that may follow each word in the sentence, is of particular concern in spoken language tasks. Portability a
native word hypotheses that may follow each word in the sentence, is of particular concern in spoken language tasks. Portability and trainability concern the ease with which an existing rammar can be ported to a new task, as well as the amount of training data necessary before the grammar is able to generalize well to unseen data. 74 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recog- nizer and with a functioning database back-end (Zue et al 1990). The VOYAGER system can answer a number of different ypes of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which 
 concern in spoken language tasks. Portability and trainability concern the ease with which an existing rammar can be ported to a new task, as well as the amount of training data necessary before the grammar is able to generalize well to unseen data. 74 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recog- nizer and with a functioning database back-end (Zue et al 1990). The VOYAGER system can answer a number of different ypes of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community. 3
e grammar is able to generalize well to unseen data. 74 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recog- nizer and with a functioning database back-end (Zue et al 1990). The VOYAGER system can answer a number of different ypes of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community. 3.1 Portability We tested ease of portability for TINA by beginning with a grammar built from the 450 TIMIT sentences and then deriving a grammar for the RM task. These two tasks represent very differe
any rules that were particular to the original domain. It required less than one person-month o convert he grammar from TIMIT to the RM task. 3.2 Perplexity and Coverage in RM Task A set of 791 sentences within the RM task have been designated as training sentences, and a separate set of 200 sentences as the test set. We built a subset grammar from the 791 parsed training sentences, and then used this grammar to test coverage and perplexity on the unseen test sentences. The grammar could parse 100% of the training sentences and 84% of the test sentences. A formula for the test set perplexity (Lee 1989) is: 13 N _1 ~log2P(wi \] wi-1,...Wl). N Perplexity = 2 i=1 where the wi are the sequence of all words in all sentences, N is the total number of words, including an &quot;end&quot; word after each sentence, and P(wi I Wi--I~'''Wl) is the probability of the ith word given all preceding wordsJ 4 If all words are assumed equally likely, then P(wi \] wi-1,.., wl) can be determined by counting all the words that could follow each word in the sentence, along all workable partial theories. If the grammar contains probability estimates, then these can be used in place of the equally 13 The appendix includes an
ed in a simulation mode in which the speech recognition component was 76 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Table 3 Perplexity and coverage data for test and training samples within the VOYAGER domain. Data set: Test Test Training System: initial expanded expanded No Prob: 20.6 27.1 25.8 Prob: 7.1 8.3 8.1 Coverage: 69% 76% 78% excluded. Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. Subsequent processing by the natural anguage and response generation components was done automatically by the computer (Zue et al 1989). We were able to'collect a total, of nearly 5000 utterances in this fashion. The speech material was then used to train the recognizer component, and the text material was used to train the natural language and back-end components. We designated a subset of 3312 sentences as the training set, and augmented the original rules so as to cover a number of sentences that appeared to stay within the domain of the back-end. We did not try to expand the rules to cover sentences that the back-end could not deal with, because we wanted to keep the natural anguage component tightly restricted to sentenc
 speech or through text input. In both of these systems, TINA provides the interface between the recognizer and the application back-end. In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area. In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system (Zue et al 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing. The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar c
briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system (Zue et al 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing. The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse (Zue et al 1991) To produce these &quot;N-best&quot; alte
 is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse (Zue et al 1991) To produce these &quot;N-best&quot; alternatives, we make use of a standard A* search algorithm (Hart 1968, Jelinek 1976). Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold. We have thus far developed two versions of the control strategy, a &quot;loosely cou- pled&quot; system and a &quot;tightly coupled&quot; system. Both versions begin with a 
lem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse (Zue et al 1991) To produce these &quot;N-best&quot; alternatives, we make use of a standard A* search algorithm (Hart 1968, Jelinek 1976). Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold. We have thus far developed two versions of the control strategy, a &quot;loosely cou- pled&quot; system and a &quot;tightly coupled&quot; system. Both versions begin with a Viterbi search all the way to the end of the sentence, resulting in not only the first candidate solution but al
In our case, we can use the Viterbi path to the end as the estimate of the future score. This path is guaranteed to be the best way to get to the end; however, it may not parse. Hence it is a tight upper bound on the true score for the rest of the sentence. The recognizer can continue to propose hypotheses until one 78 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications successfully parses, or until a quitting criterion is reached, such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing. Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA'S probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguis- tic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al 1991). Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but i
 the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing. Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA'S probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguis- tic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al 1991). Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model. Once a parser has produced an analysis of a particular sentence, the next step is to convert it to a meaning representation form that can be used to perform what- ever operations the user intended by speaking the sentence. We currently achieve this translation step in a second-pass treewalk through the completed parse tree. Although the generation of semantic frames co
iformly in strictly hierarchical structures in the parse tree. Thus the parse tree contains nodes such as \[subject\] and \[dir-object\] that represent structural roles, as well as nodes such as \[on-street\] and \[a-school\] representing specific semantic categories. There are no separate semantic rules off to the side; rather, the semantic information is encoded irectly as names attached to nodes in the tree. Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However, the method we are currently using in the ATIS domain (Seneff et al 1991) represents our most promising approach to this problem. We have decided to limit semantic frame types to a small set of choices uch as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree. Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to spec- ifications based on its broad-class identity (as on
er of different application domains, and gave some perfor- mance statistics in terms of perplexity/coverage/overgeneralization within some of these domains. The most interesting result was obtained within the VOYAGER domain (see Sections 3.3 and 3.4). The perplexity (average number of words that can follow a given word) decreased from 70 to 28 to 8 when the grammar changed from word- pair (derived from the same grammar) to parser without probabilities to parser with probabilities. We_currently have two application domains that can carry on a spoken dialog with a user. One, the VOYAGER domain (Zue et al 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS (Seneff et al 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues. As of this writing, we have a fully integrated version of the VOYAGER system, using an A* search algorithm (Goodine t al. 1991). The pars
The most interesting result was obtained within the VOYAGER domain (see Sections 3.3 and 3.4). The perplexity (average number of words that can follow a given word) decreased from 70 to 28 to 8 when the grammar changed from word- pair (derived from the same grammar) to parser without probabilities to parser with probabilities. We_currently have two application domains that can carry on a spoken dialog with a user. One, the VOYAGER domain (Zue et al 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS (Seneff et al 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues. As of this writing, we have a fully integrated version of the VOYAGER system, using an A* search algorithm (Goodine t al. 1991). The parser produces a set of next-word candi- dates dynamically for each partial theory. We have not yet incorporated probabilities from TINA into the search, but they 
e training sentences, and an illustration of both parsing and perplexity computation for a test sentence. Since there are only five training sentences, a number of the arcs of the original grammar are lost after training. This is a problem to be aware of in building grammars from example sentences. In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out. Unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an N-gram back-off model (Katz 1987). The Grammar: (parentheses indicate optional elements) number = hundreds-p lace (tens-place) ones-place number = tens-place number = (tens-place) ones-place hundreds-p lace = digits (hundred) hundreds-p lace = a hundred (and) tens-place = tens tens-place = teens (this overgeneral izes a bit) tens-place = oh (as in &quot;four oh five&quot;) ones-place = digits tens = \[twenty thirty forty ...\] (a terminal node with eight individual words) digits = \[zero one two three four .... \] teens = \[ten eleven twelve... \] oh = \[oh\] hundred = \[hundred\] and = \[and\] The training sentences: (with spoken form
