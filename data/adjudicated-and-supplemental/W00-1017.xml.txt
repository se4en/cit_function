 Kanagawa 243-0198, Japan E-mail: nakano@atom.brl.ntt.co.jp Abstract This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental under- standing mechanism that enables ro- bust utterance understanding and real- time responses. WIT's ability to com- pile domain-dependent system specifi- cations into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defin
 Japan E-mail: nakano@atom.brl.ntt.co.jp Abstract This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental under- standing mechanism that enables ro- bust utterance understanding and real- time responses. WIT's ability to com- pile domain-dependent system specifi- cations into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defined task domain. How
no@atom.brl.ntt.co.jp Abstract This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental under- standing mechanism that enables ro- bust utterance understanding and real- time responses. WIT's ability to com- pile domain-dependent system specifi- cations into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defined task domain. However, it limits s
o.jp Abstract This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental under- standing mechanism that enables ro- bust utterance understanding and real- time responses. WIT's ability to com- pile domain-dependent system specifi- cations into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defined task domain. However, it limits system functions; it i
tions into internal knowledge sources makes building spoken dialogue sys- tems much easier than :it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al, 1996; Zue et al, 2000; Walker et al, 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spo- ken dialogue systems have been developed (Bar- nett and Singh, 1997; Sasajima et al, 1999). One is the CSLU Toolkit (Sutton et al, 1998), which enables rapid prototyping of a spoken di- alogue system that incorporates a finite-state dia- logue model. It decreases the amount of the ef- fort required in building a spoken dialogue sys- tem in a user-defined task domain. However, it limits system functions; it is not easy to employ the advanced language processing techniques de- veloped in the realm of computational linguis- tics. Another is GALAXY-II (Seneffet al, 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a 
k owledge sources. Creat- ing and maintaining these knowledge sources re- quire much effort, thus a toolkit would be help- ful. Previous toolkits, however, do not allow us to achieve these features, or do not provide mecha- nisms that achieve these features without requir- ing excessive fforts by the developers. This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter- 150 for building spoken dialogue systems that inte- grate speech recognition, language understanding and generation, and speech output. WIT features an incremental understanding method (Nakano et al., 1999b) that makes it possible to build a robust and real-time system. In addition, WIT compiles domain-dependent system specifications into in- ternal knowledge sources o that building systems is easier. Although WIT requires more domain- dependent specifications than finite-state-model- based toolkits, WIT-based systems are capable of taking full advantage of language processing technology. WIT has been implemented and used to build several spoken dialogue systems. In what follows, we overview WIT, explain its architecture, domain-dependent system specifica- tions, and implementation, and then di
by WIT. 3 Architecture of WIT-Based Spoken Dialogue Systems Here we explain how the modules in WIT work by exploiting domain-dependent k owledge and how they interact with each other. 3.1 Speech Recognition The speech recognition module is a phoneme- HMM-based speaker-independent continuous speech recognizer that incrementally outputs face Toolldt. word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI&quot; (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous peech database of the Acoustical Society of Japan (Kobayashi et al, 1992). This recognizer incrementally outputs word hypotheses a soon as they are found in the best-scored path in the forward search (Hirasawa et al, 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation modu
ow they interact with each other. 3.1 Speech Recognition The speech recognition module is a phoneme- HMM-based speaker-independent continuous speech recognizer that incrementally outputs face Toolldt. word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI&quot; (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous peech database of the Acoustical Society of Japan (Kobayashi et al, 1992). This recognizer incrementally outputs word hypotheses a soon as they are found in the best-scored path in the forward search (Hirasawa et al, 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation module to react immediately touser interruptions while the system is speaking. The language model for speech recognition is a network (regular) grammar, 
ough the current version of WIT does not exploit probabilistic language models, such mod- els can be incorporated without changing the ba- sic WIT architecture. 3.2 Language Understanding The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the se- quence of the word hypotheses to update the di- alogue state, in which the resnlt of understand- ing and discourse information are represented by a frame (i.e., attribute-value pairs). The un- derstanding module utilizes ISSS (Incremental Significant-utterance Sequence Search) (Nakano et al, 1999b), which is an integrated parsing and discourse processing method. ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars- ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence. ISSS also makes it possible for the language generation module to respond in real time because it can out- put a partial result of understanding at any point in time. The domain-dependent knowledge used in this module consists of a u
rior to pars- ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence. ISSS also makes it possible for the language generation module to respond in real time because it can out- put a partial result of understanding at any point in time. The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules. Disjunctive feature descriptions are also possible; WIT incorporates an efficient method for handling disjunctions (Nakano, 1991). When a phrase boundary is de- tected, the feature structure for a phrase is com- puted using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic repre- sentation for found sentences. Two kinds of sen- tenees can be considered; domain-related ones that express the user's intention about he reser- 152 vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confir
ple, has three phases: the phase in which the user tells the system his/her equest, he phase in which the system confirms it, and the phase in which the system tells the user the result of the database access. In the first two phases, the user holds the initiative, and in the last phase, the sys- tern holds the initiative. Functions defined here decide what string should be spoken and send that string to the speech output module based on the current di- alogue state. They can also shift the dialogue 2The notion of the initiative inthis paper isdifferent from that of the dialogue initiative of Chu-Carroll (2000). phase and change the holder of the initiative as well as change the dialogue state. When the dia- logue phase shifts, the language model foi&quot; speech recognition is changed to get better speech recog- nition performance. Typically, the language gen- eration module is responsible for database access. The language generation module works as fol- lows. It first checks which dialogue participant has the initiative. If the initiative is held by the user, it waits until the user's speech interval ends or a duration of silence after the end of a system utterance is detected. The action function in t
 is executed in the former case; the time-out function is executed in the latter case. Then it goes back to the initial stage. If the system holds the initiative, the mod- ule executes the initial function of the phase. In typical question-answer systems, the user has the initiative when asking questions and the system has it when answering. Since the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that ut- terance understanding is possible even while it is speaking (Nakano et al, 1999a). Thus the system can respond immediately after user pauses when the user has the initiative. When the system holds the initiative, it can immediately react to an in- terruption by the user because user utterances are understood in an incremental way (Dohsaka nd Shimazu, 1997). The time-out function is effective in moving the dialogue forward when the dialogue gets stuck for some reason. For example, the system may be able to repeat the same question with an- other expression and may also be able to ask the user a more specific question. 3.4 Speech Output The speech output module produces pe
hat the language generation module can take into account the tim- ing of the end of system utterance. The meeting room reservation system uses speech files of short 153 phrases. 4 Building Spoken Dialo~te Systems with WIT 4.1 Domain-Dependent System Specifications Spoken dialogue systems can be built with WIT by preparing several domain-dependent specifica- tions. Below we explain the specifications. Feature Definitions: Feature definitions pec- ify the set of features used in the grammar for lan- guage understanding. They also specify whether each feature is a head feature or a foot feature (Pollard and Sag, 1994). This information isused when constructing feature structures for phrases in a built-in process. The following is an example of a feature defini- tion. Here we use examples from the specification of the meeting room reservation system. (case head) It means that the case feature is used and it is a head feature 3. Lexieal Descriptions: Lexical descriptions specify both pronunciations and grammatical features for words. Below is an example lexical item for the word 1-gatsu (January). (l-gatsu ichigatsu month nil i) The first three elements are the identifier, the pro- nunciation, and the gramma
ority increase) ) ((role name) (child feature structure) ? . . (child feature structure) => (flame operation command) (priority increase) ) These roles are similar to DCG (Pereira nd War- ren, 1980) rules; they can include logical vari- ables and these variables can be bound when these rules are applied. It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano, 199 I), but we do not explain these constraints indetail in this 154 paper. The priorities are used for disambiguat- ing interpretation i the incremental understand- ing method (Nakano et al, 1999b). When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be consid- ered a sentence, and the sentence's semantic rep- resentation is the command for updating the dia- logue state. The command is one of the follow- ing: ? A command to set the value of an attribute of the frame, ? A command to increase the priority, Conditional commands (If-then-else type command, the condition being whether the value of an attribute of the flame is or is not equal to a specified value, or a conjunction or disjunction of the above condit
bove, domain- dependent knowledge sources are created as indi- cated by the dashed arrows in Figure 1. When cre- ating the knowledge sources, WIT checks for sev- eral kinds of consistency. For example, the set of word categories appearing in the lexicon and the set of word categories appearing in phrase deft- nifions are compared. This makes it easy to find errors in the domain specifications. 5 Implementation WIT has been implemented in Common Lisp and C on UNIX, and we have built several experi- mental and demonstration dialogue systems using it, including a meeting room reservation system (Nakano et al, 1999b), a video-recording pro- gramming system, a schedule management sys- tem (Nakano et al, 1999a), and a weather in- formation system (Dohsaka et al, 2000). The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue be- tween this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather informa- tion system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, 
 for sev- eral kinds of consistency. For example, the set of word categories appearing in the lexicon and the set of word categories appearing in phrase deft- nifions are compared. This makes it easy to find errors in the domain specifications. 5 Implementation WIT has been implemented in Common Lisp and C on UNIX, and we have built several experi- mental and demonstration dialogue systems using it, including a meeting room reservation system (Nakano et al, 1999b), a video-recording pro- gramming system, a schedule management sys- tem (Nakano et al, 1999a), and a weather in- formation system (Dohsaka et al, 2000). The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue be- tween this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather informa- tion system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the se- mantic flame is 11, and the number of the files of the pre-recorded sp
to build a variety of dialogue systems. Although the dialogue state is represented bya simple attribute- value matrix, since there is no limitation on the number of attributes, it can hold more compli- cated information. For example, it is possible to represent a discourse stack whose depth is lim- ited. Recording some dialogue history is also possible. Since the language understanding mod- ule utilizes unification, a wide variety of lin- guistic phenomena can be covered. For exam- ple, speech repairs, particle omission, and fillers can be dealt with in the framework of unifica- tion grammar (Nakano et al, 1994; Nakano and Shimazu, 1999). The language generation mod- ule features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented bydialogue phases in WIT. 6.2 Consistency In an agglutinative language such as Japanese, there is no established definition of words, so dia- logue system developers must define words. This sometimes causes a problem 
derstanding. 6.4 Problems and Limitations Several problems remain with WIT. One of the most significant is that he system developer must write language generation functions. If the gen- eration functions employ sophisticated dialogue strategies, the system can perform complicated dialogues that are not just question answering. WIT, however, does not provide task-independent facilities that make it easier to employ such dia- logue strategies. There have been several efforts aimed at de- veloping a domain-independent me hod for gen- erating responses from a frame representation f user requests (Bobrow et al, 1977; Chu-CarroU, 1999). Incorporating such techniques would deo crease the system developer workload. However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well. Therefore incorporating those techniques remains as a future work. Another limitation is that WIT cannot deal with multiple speech recognition candidates such as those in an N-best list. Extending WIT to deal with multiple recognition results would improve the performance of the who
