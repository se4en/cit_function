cy table, based on counts obtained from a subset of the BNC, is given in Table 2. One column contains estimates of counts arising from concepts in appearing in the subject position of run: f run, subj). A second column contains estimates of counts arising from concepts in appearing in the subject position of a verb other than run. The figures in brackets are the expected values, given that the null hypothesis is true. There is a choice of which statistic to use in conjunction with the test. The usual statistic encountered in text books is the Pearson chisquared statistic, denoted X2. However, Dunning (1993) claims that the log-likelihood chisquared statistic (G2) is more appropriate for corpus-based NLP. In Section 6, we compare the two statistics in a task-based evaluation. For Table 2, the value of G2 is 3.8 and the value of X2 is 2.5. Assuming a level of significance of a = 0.05, the critical value is 12.6 (for 6 degrees of freedom). Thus, for this a value, the null hypothesis would not be rejected for either statistic, and the conclusion would be that there is no reason to suppose p(runl (canine), subj) is not a reasonable approximation of p(rtml (dog), subj). A key question is how to select
uared test are not met. One condition that is likely to be violated is the requirement that expected values in the contingency table should not be too small. (A rule of thumb often found in text books is that the expected values should be greater than 5.) One response to this problem is to apply some kind of thresholding, and either ignore counts below the threshold, or only apply the test to tables that do not contain low counts. The problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important (Collins and Brooks, 1995). Another approach would be to use Fisher's exact test, which can be applied to tables regardless of the size of the counts. The main problem with this test is that it is computationally expensive, especially for large contingency tables (and it only applies to tables with whole number counts). What we have found in practice is that applying the chi-squared test to tables with low counts tends to produce an insignificant result, and the null hypothesis is not rejected. The consequences of this for the generalisation procedure are that low count tables tend to result in the procedure moving up 
 required for Li and Abe's application of MDL. This did create a problem, in that many of the cuts returned by MDL were over-generalising at the (entity) node. The reason is that person), which is close to (entity), and dominated by (entity), has two parents: (iif e_f orm) and (causal_agent). This DAG-like property was responsible for the over-generalisation, and so we removed the link between / per son) and causal_agent). This appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in Li and Abe (1998). 6 Pseudo Disambiguation Experiments The task we used to compare different generalisation techniques is similar to that used by Pereira et al. (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and vi, is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatised. This resulted in a data set of around 1.
entity) node. The reason is that person), which is close to (entity), and dominated by (entity), has two parents: (iif e_f orm) and (causal_agent). This DAG-like property was responsible for the over-generalisation, and so we removed the link between / per son) and causal_agent). This appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in Li and Abe (1998). 6 Pseudo Disambiguation Experiments The task we used to compare different generalisation techniques is similar to that used by Pereira et al. (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and vi, is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatised. This resulted in a data set of around 1.3 million (v, n) pairs. To form a test set, 3,000 of these pairs were randomly selected, such that each selected pair contained a fairly frequent verb
s that person), which is close to (entity), and dominated by (entity), has two parents: (iif e_f orm) and (causal_agent). This DAG-like property was responsible for the over-generalisation, and so we removed the link between / per son) and causal_agent). This appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in Li and Abe (1998). 6 Pseudo Disambiguation Experiments The task we used to compare different generalisation techniques is similar to that used by Pereira et al. (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and vi, is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatised. This resulted in a data set of around 1.3 million (v, n) pairs. To form a test set, 3,000 of these pairs were randomly selected, such that each selected pair contained a fairly frequent verb. (Following Pereira et 
n, the high score is not too surprising. As a final experiment, we compared the task performance using the X2, rather than G2, statistic in the chi-squared test. The results are given in Table 6 for the complete data set. The a value % correct - G2 % correct - X2 0.0005 73.5 (3.3) 73.9 (3.0) 0.05 73.2 (2.8) 73.7 (2.5) 0.3 72.5 (2.4) 73.6 (2.2) 0.75 73.5 (1.9) 73.8 (1.8) 0.995 72.8 (1.2) 72.3 (1.2) Table 6: Disambiguation results for G2 and X2 figures in brackets give the average number of generalised levels. The X2 statistic is performing at least as well as G2, throwing doubt on the claim by Dunning (1993) that the G2 statistic is better suited for use in corpus-based NLP. The results show clearly that the average level of generalisation is slightly higher for G2 than X2. This suggests a possible explanation for the results presented here, and those in Dunning (1993), which is that the X2 statistic provides a less conservative test when counts in the contingency table are low. A less conservative test is better suited to the pseudo disambiguation task, since this results in a low level of generalisation, on the whole, which is good for this task. In contrast, the task that Dunning considers, th
