cs.biu.ac.il Abstract In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recogniz
ract In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recognizing subjectverb 
er, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recognizing subjectverb (SV) and verb-object
ctic patterns, and may therefore require less training information. That would not be the case when evaluating a full parser on selected target patterns, because its training material would still include full parse-trees labeled with other patterns as well. The approach presented here, of trainable partial parsing, attempts to reduce the gap between shallow and full parsing. It is an extension of shallow parsing towards handling composite and multiple patterns, while maintaining the local nature of the task, and simplicity of training material. One approach to partial parsing was presented by Buchholz et al. (1999), who extended a shallow-parsing technique to partial parsing. The output of NP and VP chunking was used as an input to grammatical relation inference. The inferences process is cascaded, and a clear improvement was obtained by passing results across cascades. Another approach for partial parsing was presented by Skut and Brants (1998). Their method is an extension of that of Church (1988) for finding NP's, achieved by extending the feature space to include structural information. Processing goes simultaneously for structures at all levels, from left to right. Since there are no cascades, the 
he gap between shallow and full parsing. It is an extension of shallow parsing towards handling composite and multiple patterns, while maintaining the local nature of the task, and simplicity of training material. One approach to partial parsing was presented by Buchholz et al. (1999), who extended a shallow-parsing technique to partial parsing. The output of NP and VP chunking was used as an input to grammatical relation inference. The inferences process is cascaded, and a clear improvement was obtained by passing results across cascades. Another approach for partial parsing was presented by Skut and Brants (1998). Their method is an extension of that of Church (1988) for finding NP's, achieved by extending the feature space to include structural information. Processing goes simultaneously for structures at all levels, from left to right. Since there are no cascades, the structural level of the output is limited by that of the feature set. This paper presents an extension of the algorithm of Argamon et al. (1998, 1999, hereafter MBSL), which handles and exploits compositional structures. MBSL is a memory-based algorithm that uses raw-data segments for learning chunks. It works with POS tags, and combin
with covers containing fewer tiles, with larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedd
ining fewer tiles, with larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedded instances, ne
ith larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedded instances, nemb in a tile. A limit of
67.1 51.7 all NP 79.3 88.5 83.7 base NP 1 93.2 93.5 93.3 composite 71.4 49.0 58.1 all NP 87.2 77.7 82.2 NP (TKS99) 76.1 91.3 83.0 Table 3: NP Results, OT = 0.6, tile length< 5. Rows 5-10 refer to experiments where baseNPs were distinguished from composite ones. There are currently no other partial parsers on these tasks to compare the combined VP and NP results to. Tjong Kim Sang (1999) presented result for composite NP, obtained by repeated cascading, similar to our results with seperate base and composite NPs and no internal structure. Our results are lower than those of full parsers, e.g., Collins (1997) - as might be expected since much less structural data, and no lexical data are being used. 4 Discussion We have presented a memory-based learning method for partial parsing which can handle and exploit compositional information. Like other shallow-parsing systems, it is most useful when the number of target patterns is small. In particular, the method does not require fully-parsed sentences as training, unlike trainable full parsing methods. The training material has to contain only bracketing of the target patterns, implying much simpler training material when the parsing task is limited. T
 generalizations, while APP uses rules derived from complete instances. • The grammar rules of APP do not include context, which is taken into account when generating the non-terminal S. In MBSL, the context is consulted for each instance candidate. • APP, like DOP, uses a probabilistic model. The probability of a grammar rule X Y is Freq(X y)/Freq(X). Analogously, the denominator in MBSL would be Freq(Y). The presented method concerns primarily with phrases, which can be represented by a tree structure. It is not aimed at handling dependencies, which require heavy use of lexical information (Hindle and Rooth, 1993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buc
rom complete instances. • The grammar rules of APP do not include context, which is taken into account when generating the non-terminal S. In MBSL, the context is consulted for each instance candidate. • APP, like DOP, uses a probabilistic model. The probability of a grammar rule X Y is Freq(X y)/Freq(X). Analogously, the denominator in MBSL would be Freq(Y). The presented method concerns primarily with phrases, which can be represented by a tree structure. It is not aimed at handling dependencies, which require heavy use of lexical information (Hindle and Rooth, 1993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buchholz et al. (1999), the method extends an existi
mation (Hindle and Rooth, 1993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buchholz et al. (1999), the method extends an existing flat shallow-parsing method to handle composite structures. It yields a significant improvement over the flat method, especially for long and more complex structures. As can be expected, the performance of the partial method is still lower than that of full parsers, which exploit (and require) much richer information. The results of this line of research enrich the space of alternative parsing approaches, aiming to reduce the gap between shallow and full parsing. Acknowledgements Y. K. thanks Jorn Veenstra, Sabine Buchholz, and Khalil
993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buchholz et al. (1999), the method extends an existing flat shallow-parsing method to handle composite structures. It yields a significant improvement over the flat method, especially for long and more complex structures. As can be expected, the performance of the partial method is still lower than that of full parsers, which exploit (and require) much richer information. The results of this line of research enrich the space of alternative parsing approaches, aiming to reduce the gap between shallow and full parsing. Acknowledgements Y. K. thanks Jorn Veenstra, Sabine Buchholz, and Khalil Sima'an for thorough and h
