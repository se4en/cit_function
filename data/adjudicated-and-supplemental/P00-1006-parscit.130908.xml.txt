p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost e
,$) = Z(h, s) where q(w I h, s) is a reference distribution, f (w ,h, s) maps (w, h, s) into an ndimensional feature vector, is a corresponding vector of feature weights (the parameters of the model), and Z(h, s) = Ew q(w I h, s) exp(. f(w, h)) is a normalizing factor. 2Rosenfeld (1996) reports a greater perplexity reduction (23% versus 10%) over a baseline trigram language model due the use of ME versus linear word triggers. However, since the models tested apparently differed in other aspects, it is hard to determine how much of this gain can be attributed to the use of ME. It can be shown (Berger et al., 1996) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values.3 There is no requirement that the components of f represent disjoint or statistically independent events. This result motivates the use of MEMD models, but it offers only weak guidance on how to select q or f. In practice, q is usually chosen on the basis of efficiency considerations (when the information it ca
es this feature and one which does not: 1 Pst (T IS) G8t = ITI log P(TIS) where the training corpus (5, T) consists of a set of (statistically independent) sentence pairs (s, t), and pst is the model which includes ht. Since MEMD models are trained by finding the set of feature weights which maximizes the likelihood of the training corpus, it is natural to rate features according to how much they contribute to this likelihood. A powerful strategy for using gains is to build a model iteratively by adding at each step the feature which gives the highest gain with respect to those already added. Berger et al (1996) describe an efficient algorithm for accomplishing this in which approximations to Pst (TIS) are computed in parallel for all (new) features ft by holding all weights in the existing model fixed and optimizing only over a8t. However, this method requires many expensive passes over the corpus to optimize the weights for the set of features under consideration at each step, and it adds only one feature per step, so it is not practical for constructing models containing thousands of features or more. In a recent paper (Printz, 1998), Printz argues that it is usually sufficient to perform the iter
