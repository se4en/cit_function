{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"note":{"#tail":"\n","@confidence":"0.526318","#text":"\nProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146?1157,\n"},"listItem":[{"#tail":"\n","@confidence":"0.955999166666667","#text":"\nlowing generative process:\n1. A document-specific topic distribution, ?d ?\nDir(?) is drawn.\n2. For the ith word in the document,\n(a) A topic assignment zi ? ?d is drawn,\n(b) and a word wi ? ?zi is drawn and ob-\n"},{"#tail":"\n","@confidence":"0.999591142857143","#text":"\n1. A document-specific topic distribution, ?d ?\nDir(?) is drawn.\n2. For the ith (word, feature) pair in the document,\n(a) A topic assignment zi ? ?d is drawn;\n(b) a word wi ? ?zi is drawn;\n(c) a feature fi ? ?zi is drawn;\n(d) the pair (wi, fi) is observed.\n"}],"@no":"0","figure":{"#tail":"\n","@confidence":"0.986367769230769","#text":"\nMost Probable Words Translations Prototypical Images\nWasser water\nSchiff ship\nSee lake\nMeer sea\nMeter meter\nFlu? river\n@card@ (number)\nUhr clock\nFreitag Friday\nSonntag Sunday\nSamstag Saturday\nMontag Monday\n"},"address":{"#tail":"\n","@confidence":"0.638876","#text":"\nSeattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics\n"},"author":[{"#tail":"\n","@confidence":"0.997185","#text":"\nStephen Roller\n"},{"#tail":"\n","@confidence":"0.885947","#text":"\nSabine Schulte im Walde\n"}],"equation":[{"#tail":"\n","@confidence":"0.9781435","#text":"\np(wi, ?d) =\n?\nk\np(wi|?k)p(zi = k|?d).\n"},{"#tail":"\n","@confidence":"0.969798","#text":"\np(wi, fi, ?d) =\n?\nk\np(wi|?k)p(fi|?k)p(zi = k|?d).\n"},{"#tail":"\n","@confidence":"0.95284","#text":"\np(wi, fi, f\n?\ni , . . . , ?d) =\n?\nk\np(wi|?i)p(fi|?k)p(f\n?\ni |?\n?\ni) ? ? ? p(zi = k|?d)\n"},{"#tail":"\n","@confidence":"0.938128714285714","#text":"\ntwo distributions,\n?FN&Sj,w =\n{\n?FNj,w 1 ? j ? K\nFN ,\n?Sj?KFN ,w K\nFN < j ? KFN +KS ,\n"},{"#tail":"\n","@confidence":"0.972691181818182","#text":"\nsKL(w1||w2) = KL(w1||w2) +KL(w2||w1),\nand KL divergence is defined as\nKL(w1||w2) =\n?\nk\nln\n(\np(t = k|w1)\np(t = k|w2)\n)\np(t = k|w1).\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.999383","#text":"\n3.1 Textual Modality\n"},{"#tail":"\n","@confidence":"0.988696","#text":"\n3.2 Cognitive Modalities\n"},{"#tail":"\n","@confidence":"0.995891","#text":"\n3.3 Visual Modalities\n"},{"#tail":"\n","@confidence":"0.848216","#text":"\n4.1 Latent Dirichlet Allocation\nLatent Dirichlet Allocation (Blei et al, 2003), or\n"},{"#tail":"\n","@confidence":"0.878302","#text":"\n4.2 Multimodal LDA\n"},{"#tail":"\n","@confidence":"0.981414","#text":"\n4.3 3D Multimodal LDA\n"},{"#tail":"\n","@confidence":"0.976376","#text":"\n4.4 Hybrid Multimodal LDA\n"},{"#tail":"\n","@confidence":"0.846375","#text":"\n4.5 Inference\n"},{"#tail":"\n","@confidence":"0.976579","#text":"\n5.1 Generating Multimodal Corpora\n"},{"#tail":"\n","@confidence":"0.995285","#text":"\n5.2 Evaluation\n"},{"#tail":"\n","@confidence":"0.8968745","#text":"\n5.3 Model Selection and Hyperparameter\nOptimization\n"},{"#tail":"\n","@confidence":"0.999842","#text":"\n6.1 Predicting Compositionality Ratings\n"},{"#tail":"\n","@confidence":"0.999771","#text":"\n6.2 Predicting Association Norms\n"}],"subsubsectionHeader":{"#tail":"\n","@confidence":"0.966909","#text":"\n3.3.1 Image Processing\n"},"footnote":[{"#tail":"\n","@confidence":"0.987183","#text":"\n1http://stephenroller.com/research/\n"},{"#tail":"\n","@confidence":"0.532369666666667","#text":"\nmon object or clear visual attribute, and words are\nexpress in terms of these visual commonalities.\n3http://simplecv.org\n"},{"#tail":"\n","@confidence":"0.642155","#text":"\n3D Multimodal LDA assumes that all modalities\n"}],"title":{"#tail":"\n","@confidence":"0.8472605","#text":"\nA Multimodal LDA Model Integrating\nTextual, Cognitive and Visual Modalities\n"},"reference":[{"#tail":"\n","@confidence":"0.9247584","#text":"\nMark Andrews, Gabriella Vigliocco, and David Vinson.\n2009. Integrating experiential and distributional data\nto learn semantic representations. Psychological Re-\nview, 116(3):463.\nYoav Artzi and Luke Zettlemoyer. 2013. Weakly super-\nvised learning of semantic parsers for mapping instruc-\ntions to actions. In Transactions of the Association for\nComputational Linguistics, volume 1, pages 49?62.\nLisa Aziz-Zadeh, Stephen M. Wilson, Giacomo Rizzo-\nlatti, and Marco Iacoboni. 2006. Congruent embodied\nrepresentations for visually presented actions and lin-\nguistic phrases describing actions. Current Biology,\n16(18):1818?1823.\nMarco Baroni, Silvia Bernardini, Adriano Ferraresi, and\nEros Zanchetta. 2009. The wacky wide web: a\n"},{"#tail":"\n","@confidence":"0.982028622641509","#text":"\ncollection of very large linguistically processed web-\ncrawled corpora. Language Resources and Evalua-\ntion, 43(3):209?226.\nHerbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van\nGool. 2008. Surf: Speeded up robust features. Com-\nputer Vision and Image Understanding, 110(3):346?\n359, June.\nDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.\n2003. Latent dirichlet alocation. The Journal of Ma-\nchine Learning Research, 3:993?1022.\nElia Bruni, Giang Binh Tran, and Marco Baroni. 2011.\nDistributional semantics from text and images. Pro-\nceedings of the EMNLP 2011 Geometrical Models for\nNatural Language Semantics, pages 22?32.\nElia Bruni, Gemma Boleda, Marco Baroni, and Nam-\nKhanh Tran. 2012a. Distributional semantics in tech-\nnicolor. In Proceedings of the 50th Annual Meeting of\nthe Association for Computational Linguistics, pages\n136?145.\nElia Bruni, Jasper Uijlings, Marco Baroni, and Nicu\nSebe. 2012b. Distributional semantics with eyes: Us-\ning image analysis to improve computational represen-\ntations of word meaning. In Proceedings of the 20th\nACM International Conference on Multimedia, pages\n1219?1228.\nDavid L. Chen and Raymond J. Mooney. 2011. Learn-\ning to interpret natural language navigation instruc-\ntions from observations. Proceedings of the 25th AAAI\nConference on Artificial Intelligence, pages 859?865,\nAugust.\nDavid L. Chen, Joohyun Kim, and Raymond J. Mooney.\n2010. Training a multilingual sportscaster: Using per-\nceptual context to learn language. Journal of Artificial\nIntelligence Research, 37(1):397?436.\nScott Deerwester, Susan T. Dumais, George W. Furnas,\nThomas K. Landauer, and Richard Harshman. 1990.\nIndexing by latent semantic analysis. Journal of the\nAmerican Society for Information Science, 41(6):391?\n407.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In Computer Vision and Pat-\ntern Recognition, 2009. CVPR 2009. IEEE Conference\non, pages 248?255. IEEE.\nThomas Deselaers and Vittorio Ferrari. 2011. Visual and\nsemantic similarity in imagenet. In IEEE Conference\non Computer Vision and Pattern Recognition, pages\n1777?1784.\nMatthijs Douze, Herve? Je?gou, Harsimrat Sandhawalia,\nLaurent Amsaleg, and Cordelia Schmid. 2009. Eval-\nuation of gist descriptors for web-scale image search.\nIn Proceedings of the ACM International Conference\non Image and Video Retrieval, pages 19:1?19:8.\nJacob Eisenstein, Brendan O?Connor, Noah A. Smith,\nand Eric P. Xing. 2010. A latent variable model\nfor geographic lexical variation. In Proceedings of\nthe 2010 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1277?1287.\nAli Farhadi, Ian Endres, Derek Hoiem, and David\nForsyth. 2009. Describing objects by their attributes.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1778?1785.\nYansong Feng and Mirella Lapata. 2010a. How many\nwords is a picture worth? Automatic caption gener-\nation for news images. In Proceedings of the 48th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1239?1249.\nYansong Feng and Mirella Lapata. 2010b. Visual in-\nformation in semantic representation. In Human Lan-\nguage Technologies: the 2010 Annual Conference of\nthe North American Chapter of the Association for\nComputational Linguistics, pages 91?99.\nThomas L. Griffiths, Mark Steyvers, and Joshua B.\nTenenbaum. 2007. Topics in semantic representation.\nPsychological Review, 114(2):211.\nMatthew Hoffman, David M. Blei, and Francis Bach.\n2010. Online learning for latent dirichlet alocation.\nAdvances in Neural Information Processing Systems,\n23:856?864.\nMatthew Hoffman, David M. Blei, Chong Wang, and\nJohn Paisley. 2012. Stochastic variational inference.\nArXiv e-prints, June.\nBrendan T. Johns and Michael N. Jones. 2012. Percep-\ntual inference through global lexical similarity. Topics\nin Cognitive Science, 4(1):103?120.\nDhiraj Joshi, James Z. Wang, and Jia Li. 2006. The story\npicturing engine?a system for automatic text illustra-\ntion. ACM Transactions on Multimedia Computing,\nCommunications, and Applications, 2(1):68?89.\nRohit J. Kate and Raymond J. Mooney. 2007. Learning\nlanguage semantics from ambiguous supervision. In\nProceedings of the 22nd Conference on Artificial In-\ntelligence, volume 7, pages 895?900.\nDavid G. Lowe. 2004. Distinctive image features from\nscale-invariant keypoints. International Journal of\nComputer Vision, 60(2):91?110.\nStefan Mathe, Afsaneh Fazly, Sven Dickinson, and\nSuzanne Stevenson. 2008. Learning the abstract mo-\ntion semantics of verbs from captioned videos. In\nIEEE Computer Society Conference on Computer Vi-\nsion and Pattern Recognition Workshops, pages 1?8.\nCynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and\nDieter Fox. 2012. Learning to parse natural language\ncommands to a robot control system. In Proceedings\nof the 13th International Symposium on Experimental\nRobotics.\n"},{"#tail":"\n","@confidence":"0.99955378125","#text":"\nKen McRae, George S. Cree, Mark S. Seidenberg, and\nChris McNorgan. 2005. Semantic feature production\nnorms for a large set of living and nonliving things.\nBehavior Research Methods, 37(4):547?559.\nTanvi S. Motwani and Raymond J. Mooney. 2012. Im-\nproving video activity recognition using object recog-\nnition and text mining. In ECAI, pages 600?605.\nDouglas L. Nelson, Cathy L. McEvoy, and Thomas A.\nSchreiber. 2004. The University of South Florida free\nassociation, rhyme, and word fragment norms. Be-\nhavior Research Methods, Instruments, & Computers,\n36(3):402?407.\nAude Oliva and Antonio Torralba. 2001. Modeling the\nshape of the scene: A holistic representation of the\nspatial envelope. International Journal of Computer\nVision, 42(3):145?175.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. In Advances in Neural Infor-\nmation Processing Systems, pages 1143?1151.\nDevi Parikh and Kristen Grauman. 2011. Relative at-\ntributes. In International Conference on Computer Vi-\nsion, pages 503?510. IEEE.\nFriedemann Pulvermu?ller, Olaf Hauk, Vadim V. Nikulin,\nand Risto J Ilmoniemi. 2005. Functional links be-\ntween motor and language systems. European Journal\nof Neuroscience, 21(3):793?797.\nMichaela Regneri, Marcus Rohrbach, Dominikus Wet-\nzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.\n2013. Grounding action descriptions in videos. In\nTransactions of the Association for Computational\nLinguistics, volume 1, pages 25?36.\nMarcus Rohrbach, Michael Stark, Gyo?rgy Szarvas, Iryna\nGurevych, and Bernt Schiele. 2010. What helps\nwhere?and why? Semantic relatedness for knowledge\ntransfer. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 910?917.\nStephen Roller, Michael Speriosu, Sarat Rallapalli, Ben-\njamin Wing, and Jason Baldridge. 2012. Super-\nvised text-based geolocation using language models\non an adaptive grid. In Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning, pages 1500?1510.\nSabine Schulte im Walde, Susanne Borgwaldt, and\nRonny Jauch. 2012. Association norms of german\nnoun compounds. In Proceedings of the 8th Interna-\ntional Conference on Language Resources and Evalu-\nation, pages 632?639, Istanbul, Turkey.\nD. Sculley. 2010. Web-scale k-means clustering. In\nProceedings of the 19th International Conference on\nWorld Wide Web, pages 1177?1178.\nCarina Silberer and Mirella Lapata. 2012. Grounded\nmodels of semantic representation. In Proceedings\nof the 2012 Joint Conference on Empirical Methods\nin Natural Language Processing and Computational\nNatural Language Learning, pages 1423?1433, Jeju\nIsland, Korea, July.\nCarina Silberer, Vittorio Ferrari, and Mirella Lapata.\n2013. Models of semantic representation with visual\nattributes. In Proceedings of the 51th Annual Meet-\ning of the Association for Computational Linguistics,\nSofia, Bulgaria, August.\nRichard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert\nBastani, Christopher D. Manning, and Andrew Y. Ng.\n2013. Zero-shot learning through cross-modal trans-\nfer. International Conference on Learning Represen-\ntations.\nMark Steyvers. 2010. Combining feature norms and\ntext data with topic models. Acta Psychologica,\n133(3):234?243.\nStefanie Tellex, Thomas Kollar, Steven Dickerson,\nMatthew R. Walter, Ashis Gopal Banerjee, Seth J.\nTeller, and Nicholas Roy. 2011. Understanding nat-\nural language commands for robotic navigation and\nmobile manipulation. Proceedings of the 25th AAAI\nConference on Artificial Intelligence.\nMarco Tettamanti, Giovanni Buccino, Maria Cristina\nSaccuman, Vittorio Gallese, Massimo Danna, Paola\nScifo, Ferruccio Fazio, Giacomo Rizzolatti, Stefano F.\nCappa, and Daniela Perani. 2005. Listening to action-\nrelated sentences activates fronto-parietal motor cir-\ncuits. Journal of Cognitive Neuroscience, 17(2):273?\n281.\nLuis Von Ahn. 2006. Games with a purpose. Computer,\n39(6):92?94.\nClaudia von der Heide and Susanne Borgwaldt. 2009.\nAssoziationen zu Unter-, Basis- und Oberbegriffen.\nEine explorative Studie. In Proceedings of the 9th\nNorddeutsches Linguistisches Kolloquium, pages 51?\n74.\nBenjamin Wing and Jason Baldridge. 2011. Simple su-\npervised document geolocation with geodesic grids.\nIn Proceedings of the 49th Annual Meeting of the As-\nsociation for Computational Linguistics: Human Lan-\nguage Technologies, volume 11, pages 955?964.\n"}],"#tail":"\n","@confidence":"0.000000","bodyText":[{"#tail":"\n","@confidence":"0.998138086956522","#text":"\nRecent investigations into grounded models of\nlanguage have shown that holistic views of\nlanguage and perception can provide higher\nperformance than independent views. In this\nwork, we improve a two-dimensional multi-\nmodal version of Latent Dirichlet Allocation\n(Andrews et al, 2009) in various ways. (1) We\noutperform text-only models in two different\nevaluations, and demonstrate that low-level\nvisual features are directly compatible with\nthe existing model. (2) We present a novel\nway to integrate visual features into the LDA\nmodel using unsupervised clusters of images.\nThe clusters are directly interpretable and im-\nprove on our evaluation tasks. (3) We provide\ntwo novel ways to extend the bimodal mod-\nels to support three or more modalities. We\nfind that the three-, four-, and five-dimensional\nmodels significantly outperform models using\nonly one or two modalities, and that nontex-\ntual modalities each provide separate, disjoint\nknowledge that cannot be forced into a shared,\nlatent structure.\n"},{"#tail":"\n","@confidence":"0.999588744186046","#text":"\nIn recent years, an increasing body of work has been\ndevoted to multimodal or ?grounded? models of lan-\nguage where semantic representations of words are\nextended to include perceptual information. The un-\nderlying hypothesis is that the meanings of words\nare explicitly tied to our perception and understand-\ning of the world around us, and textual-information\nalone is insufficient for a complete understanding of\nlanguage.\nThe language grounding problem has come in\nmany different flavors with just as many different ap-\nproaches. Some approaches apply semantic parsing,\nwhere words and sentences are mapped to logical\nstructure meaning (Kate and Mooney, 2007). Oth-\ners provide automatic mappings of natural language\ninstructions to executable actions, such as interpret-\ning navigation directions (Chen and Mooney, 2011)\nor robot commands (Tellex et al, 2011; Matuszek et\nal., 2012). Some efforts have tackled tasks such as\nautomatic image caption generation (Feng and La-\npata, 2010a; Ordonez et al, 2011), text illustration\n(Joshi et al, 2006), or automatic location identifica-\ntion of Twitter users (Eisenstein et al, 2010; Wing\nand Baldridge, 2011; Roller et al, 2012).\nAnother line of research approaches grounded\nlanguage knowledge by augmenting distributional\napproaches of word meaning with perceptual infor-\nmation (Andrews et al, 2009; Steyvers, 2010; Feng\nand Lapata, 2010b; Bruni et al, 2011; Silberer and\nLapata, 2012; Johns and Jones, 2012; Bruni et al,\n2012a; Bruni et al, 2012b; Silberer et al, 2013).\nAlthough these approaches have differed in model\ndefinition, the general goal in this line of research\nhas been to enhance word meaning with perceptual\ninformation in order to address one of the most com-\nmon criticisms of distributional semantics: that the\n?meaning of words is entirely given by other words?\n(Bruni et al, 2012b).\nIn this paper, we explore various ways to integrate\nnew perceptual information through novel computa-\ntional modeling of this grounded knowledge into a\nmultimodal distributional model of word meaning.\nThe model we rely on was originally developed by\n"},{"#tail":"\n","@confidence":"0.986230772727273","#text":"\nAndrews et al (2009) and is based on a general-\nization of Latent Dirichlet Allocation. This model\nhas previously been shown to provide excellent per-\nformance on multiple tasks, including prediction of\nassociation norms, word substitution errors, seman-\ntic inferences, and word similarity (Andrews et al,\n2009; Silberer and Lapata, 2012). While prior work\nhas used the model only with feature norms and vi-\nsual attributes, we show that low-level image fea-\ntures are directly compatible with the model and\nprovide improved representations of word meaning.\nWe also show how simple, unsupervised clusters of\nimages can act as a semantically useful and qualita-\ntively interesting set of features. Finally, we describe\ntwo ways to extend the model by incorporating three\nor more modalities. We find that each modality pro-\nvides useful but disjoint information for describing\nword meaning, and that a hybrid integration of mul-\ntiple modalities provides significant improvements\nin the representations of word meaning. We release\nboth our code and data to the community for future\nresearch.1\n"},{"#tail":"\n","@confidence":"0.998636047619048","#text":"\nThe language grounding problem has received sig-\nnificant attention in recent years, owed in part to the\nwide availability of data sets (e.g. Flickr, Von Ahn\n(2006)), computing power, improved computer vi-\nsion models (Oliva and Torralba, 2001; Lowe, 2004;\nFarhadi et al, 2009; Parikh and Grauman, 2011)\nand neurological evidence of ties between the lan-\nguage, perceptual and motor systems in the brain\n(Pulvermu?ller et al, 2005; Tettamanti et al, 2005;\nAziz-Zadeh et al, 2006).\nMany approaches to multimodal research have\nsucceeded by abstracting away raw perceptual in-\nformation and using high-level representations in-\nstead. Some works abstract perception via the us-\nage of symbolic logic representations (Chen et al,\n2010; Chen and Mooney, 2011; Matuszek et al,\n2012; Artzi and Zettlemoyer, 2013), while others\nchoose to employ concepts elicited from psycholin-\nguistic and cognition studies. Within the latter cat-\negory, the two most common representations have\nbeen association norms, where subjects are given a\n"},{"#tail":"\n","@confidence":"0.9878315625","#text":"\ncue word and name the first (or several) associated\nwords that come to mind (e.g., Nelson et al (2004)),\nand feature norms, where subjects are given a cue\nword and asked to describe typical properties of the\ncue concept (e.g., McRae et al (2005)).\nGriffiths et al (2007) helped pave the path for\ncognitive-linguistic multimodal research, showing\nthat Latent Dirichlet Allocation outperformed La-\ntent Semantic Analysis (Deerwester et al, 1990) in\nthe prediction of association norms. Andrews et al\n(2009) furthered this work by showing that a bi-\nmodal topic model, consisting of both text and fea-\nture norms, outperformed models using only one\nmodality on the prediction of association norms,\nword substitution errors, and semantic interference\ntasks. In a similar vein, Steyvers (2010) showed that\na different feature-topic model improved predictions\non a fill-in-the-blank task. Johns and Jones (2012)\ntake an entirely different approach by showing that\none can successfully infer held out feature norms\nfrom weighted mixtures based on textual similarity.\nSilberer and Lapata (2012) introduce a new method\nof multimodal integration based on Canonical Cor-\nrelation Analysis, and performs a systematic com-\nparison between their CCA-based model and others\non association norm prediction, held out feature pre-\ndiction, and word similarity.\nAs computer vision techniques have improved\nover the past decade, other research has begun di-\nrectly using visual information in place of feature\nnorms. The first work to do this with topic models is\nFeng and Lapata (2010b). They use a Bag of Visual\nWords (BoVW) model (Lowe, 2004) to create a bi-\nmodal vocabulary describing documents. The topic\nmodel using the bimodal vocabulary outperforms a\npurely textual based model in word association and\nword similarity prediction. Bruni et al (2012a) show\nhow a BoVW model may be easily combined with\na distributional vector space model of language us-\ning only vector concatenation. Bruni et al (2012b)\nshow that the contextual visual words (i.e. the visual\nfeatures around an object, rather than of the object\nitself) are even more useful at times, suggesting the\nplausibility of a sort of distributional hypothesis for\nimages. More recently, Silberer et al (2013) show\nthat visual attribute classifiers, which have been im-\nmensely successful in object recognition (Farhadi\net al, 2009), act as excellent substitutes for feature\n"},{"#tail":"\n","@confidence":"0.997561846153846","#text":"\nnorms. Other work on modeling the meanings of\nverbs using video recognition has also begun show-\ning great promise (Mathe et al, 2008; Regneri et al,\n2013).\nThe Computer Vision community has also bene-\nfited greatly from efforts to unify the two modalities.\nTo name a few examples, Rohrbach et al (2010)\nand Socher et al (2013) show how semantic infor-\nmation from text can be used to improve zero-shot\nclassification (i.e., classifying never-before-seen ob-\njects), and Motwani and Mooney (2012) show that\nverb clusters can be used to improve activity recog-\nnition in videos.\n"},{"#tail":"\n","@confidence":"0.986801833333333","#text":"\nOur experiments use several existing and new data\nsets for each of our modalities. We employ a large\nweb corpus and a large set of association norms. We\nalso introduce two new overlapping data sets: a col-\nlection of feature norms and a collection of images\nfor a number of German nouns.\n"},{"#tail":"\n","@confidence":"0.9986831","#text":"\nFor our Text modality, we use deWaC, a large Ger-\nman web corpus created by the WaCKy group (Ba-\nroni et al, 2009) containing approximately 1.7B\nword tokens. We filtered the corpus by: removing\nwords with unprintable characters or encoding trou-\nbles; removing all stopwords; removing word types\nwith a total frequency of less than 500; and remov-\ning documents with a length shorter than 100. The\nresulting corpus has 1,038,883 documents consist-\ning of 75,678 word types and 466M word tokens.\n"},{"#tail":"\n","@confidence":"0.999717258064516","#text":"\nAssociation Norms (AN) is a collection of asso-\nciation norms collected by Schulte im Walde et al\n(2012). In association norm experiments, subjects\nare presented with a cue word and asked to list the\nfirst few words that come to mind. With enough sub-\njects and responses, association norms can provide a\ncommon and detailed view of the meaning compo-\nnents of cue words. After removing responses given\nonly once in the entire study, the data set contains\na total of 95,214 cue-response pairs for 1,012 nouns\nand 5,716 response types.\nFeature Norms (FN) is our new collection of fea-\nture norms for a group of 569 German nouns. We\npresent subjects on Amazon Mechanical Turk with\na cue noun and ask them to give between 4 and 8\ntypical descriptive features of the noun. Subjects\nare given ten example responses; one such exam-\nple is a cue of Tisch ?table? and a response of hat\nBeine ?has legs?. After collection, subjects who\nare obvious spammers or did not follow instructions\nare manually filtered. Responses are manually cor-\nrected for spelling mistakes and semantically nor-\nmalized.2 Finally, responses which are only given\nonce in the study are removed. The final data set\ncontains 11,714 cue-response pairs for 569 nouns\nand 2,589 response types.\nNote that the difference between association\nnorms and feature norms is subtle, but important. In\nAN collection, subjects simply name related words\nas fast as possible, while in FN collection, subjects\nmust carefully describe the cue.\n"},{"#tail":"\n","@confidence":"0.950629142857143","#text":"\nBilderNetle (?little ImageNet? in Swabian German)\nis our new data set of German noun-to-ImageNet\nsynset mappings. ImageNet is a large-scale and\nwidely used image database, built on top of Word-\nNet, which maps words into groups of images,\ncalled synsets (Deng et al, 2009). Multiple synsets\nexist for each meaning of a word. For example, Im-\nageNet contains two different synsets for the word\nmouse: one contains images of the animal, while\nthe other contains images of the computer periph-\neral. This BilderNetle data set provides mappings\nfrom German noun types to images of the nouns via\nImageNet.\nStarting with a set of noun compounds and their\nnominal constituents von der Heide and Borgwaldt\n(2009), five native German speakers and one native\nEnglish speaker (including the authors of this paper)\nwork together to map German nouns to ImageNet\nsynsets. With the assistance of a German-English\ndictionary, the participants annotate each word with\nall its possible meanings. After discussing the an-\nnotations with the German speakers, the English\nspeaker manually map the word meanings to synset\nsenses in ImageNet. Finally, the German speakers\nreview samples of the images for each word to en-\n2For brevity, we include the full details of the spammer iden-\ntification, cleansing process and normalization techniques in the\nSupplementary Materials.\n"},{"#tail":"\n","@confidence":"0.999130916666667","#text":"\nsure the pictures accurately reflect the original noun\nin question. Not all words or meanings are mapped\nto ImageNet, as there are a number of words with-\nout entries in ImageNet, but the resulting data set\ncontains a considerable amount of polysemy. The fi-\nnal data set contains 2022 word-synset mappings for\njust 309 words. All but three of these words overlap\nwith our data set of feature norms. After extract-\ning sections of images using bounding boxes when\navailable by ImageNet (and using the entire image\nwhen bounding boxes are unavailable), the data set\ncontains 1,305,602 images.\n"},{"#tail":"\n","@confidence":"0.998303483870968","#text":"\nAfter the collection of all the images, we extracted\nsimple, low-level computer vision features to use as\nmodalities in our experiments.\nFirst, we compute a simple Bag of Visual Words\n(BoVW) model for our images using SURF key-\npoints (Bay et al, 2008). SURF is a method\nfor selecting points-of-interest within an image. It\nis faster and more forgiving than the commonly\nknown SIFT algorithm. We compute SURF key-\npoints for every image in our data set using Sim-\npleCV3 and randomly sample 1% of the keypoints.\nThe keypoints are clustered into 5,000 visual code-\nwords (centroids) using k-means clustering (Sculley,\n2010), and images are then quantized over the 5,000\ncodewords. All images for a given word are summed\ntogether to provide an average representation for the\nword. We refer to this representation as the SURF\nmodality.\nWhile this is a standard, basic BoVW model,\neach individual codeword on its own may not pro-\nvide a large degree of semantic information; typi-\ncally a BoVW representation acts predominantly as\na feature space for a classifier, and objects can only\nbe recognize using collections of codewords. To\ntest that similar concepts should share similar vi-\nsual codewords, we cluster the BoVW representa-\ntions for all our images into 500 clusters with k-\nmeans clustering, and represent each word as mem-\nbership over the image clusters, forming the SURF\nClusters modality. The number of clusters is chosen\narbitrarily. Ideally, each cluster should have a com-\n"},{"#tail":"\n","@confidence":"0.990578833333333","#text":"\nWe also compute GIST vectors (Oliva and Tor-\nralba, 2001) for every image using LearGIST\n(Douze et al, 2009). Unlike SURF descriptors,\nGIST produces a single vector representation for an\nimage. The vector does not find points of interest\nin the image, but rather attempts to provide a rep-\nresentation for the overall ?gist? of the whole im-\nage. It is frequently used in tasks like scene iden-\ntification, and Deselaers and Ferrari (2011) shows\nthat distance in GIST space correlates well with se-\nmantic distance in WordNet. After computing the\nGIST vectors, each textual word is represented as\nthe centroid GIST vector of all its images, forming\nthe GIST modality.\nFinally, as with the SURF features, we clustered\nthe GIST representations for our images into 500\nclusters, and represented words as membership in\nthe clusters, forming the GIST Clusters modality.\n"},{"#tail":"\n","@confidence":"0.99867975","#text":"\nOur experiments are based on the multimodal ex-\ntension of Latent Dirichlet Allocation developed by\nAndrews et al (2009). Previously LDA has been\nsuccessfully used to infer unsupervised joint topic\ndistributions over words and feature norms together\n(Andrews et al, 2009; Silberer and Lapata, 2012).\nIt has also been shown to be useful in joint infer-\nence of text with visual attributes obtained using vi-\nsual classifiers (Silberer et al, 2013). These mul-\ntimodal LDA models (hereafter, mLDA) have been\nshown to be qualitatively sensible and highly pre-\ndictive of several psycholinguistic tasks (Andrews et\nal., 2009). However, prior work using mLDA is lim-\nited to two modalities at a time. In this section, we\ndescribe bimodal mLDA and define two methods for\nextending it to three or more modalities.\n"},{"#tail":"\n","@confidence":"0.998345375","#text":"\nLDA, is an unsupervised Bayesian probabilistic\nmodel of text documents. It assumes that all docu-\nments are probabilistically generated from a shared\nset ofK common topics, where each topic is a multi-\nnomial distribution over the vocabulary (notated as\n?), and documents are modeled as mixtures of these\nshared topics (notated as ?). LDA assumes every\ndocument in the corpus is generated using the fol-\n"},{"#tail":"\n","@confidence":"0.897249571428571","#text":"\nserved.\nThe task of Latent Dirichlet Allocation is then to\nautomatically infer the latent document distribution\n?d for each document d ? D, and the topic distri-\nbution ?k for each of the k = {1, . . . ,K} topics,\ngiven the data. The probability that the ith word of\ndocument d is\n"},{"#tail":"\n","@confidence":"0.9857673","#text":"\nAndrews et al (2009) extend LDA to allow for the\ninference of document and topic distributions in a\nmultimodal corpus. In their model, a document con-\nsists of a set of (word, feature) pairs,4 rather than just\nwords, and documents are still modeled as mixtures\nof shared topics. Topics consist of multinomial dis-\ntributions over words, ?k, but are extended to also\ninclude multinomial distributions over features, ?k.\nThe generative process is amended to include these\nfeature distributions:\n"},{"#tail":"\n","@confidence":"0.8957775","#text":"\nThe conditional probability of the ith pair (wi, fi)\nis updated appropriately:\n"},{"#tail":"\n","@confidence":"0.999362444444445","#text":"\nThe key aspect to notice is that the observed\nword wi and feature fi are conditionally indepen-\ndent given the topic selection, zi. This powerful ex-\ntension allows for joint inference over both words\n4Here, and elsewhere, feature and f simply refer to a token\nfrom a nontextual modality and should not be confused with the\nmachine learning sense of feature.\nand features, and topics become the key link be-\ntween the text and feature modalities.\n"},{"#tail":"\n","@confidence":"0.996652857142857","#text":"\nWe can easily extend the bimodal LDA model to in-\ncorporate three or more modalities by simply per-\nforming inference over n-tuples instead of pairs, and\nstill mandating that each modality is conditionally\nindependent given the topic. We consider the ith tu-\nple (wi, fi, f ?i , . . .) in document d to have a condi-\ntional probability of:\n"},{"#tail":"\n","@confidence":"0.994410777777778","#text":"\nThat is, we simply take the original mLDA model\nof Andrews et al (2009) and generalize it in the\nsame way they generalize LDA. At first glance, it\nseems that the inference task should become more\ndifficult as the number of modalities increases and\nobserved tuples become sparser, but the task remains\nroughly the same difficulty, as all of the observed\nelements of a tuple are conditionally independent\ngiven the topic assignment zi.\n"},{"#tail":"\n","@confidence":"0.990491333333333","#text":"\nshare the same latent topic structure, ?d. It is pos-\nsible, however, that all modalities do not share some\nlatent structure, but the modalities can still combine\nin order to enhance word meaning. The intuition\nhere is that language usage is guided by all informa-\ntion gained in all modalities, but knowledge gained\nfrom one modality may not always relate to another\nmodality. For example, the color red and the feature\n?is sweet? both enhance our understanding of straw-\nberries. However, one cannot see that strawberries\nare sweet, so one should not correlate the color red\nwith the feature ?is sweet.?\nTo this end, we define Hybrid Multimodal LDA.\nIn this setting, we perform separate, bimodal mLDA\ninference according to Section 4.2 for each of the\ndifferent modalities, and then concatenate the topic\ndistributions for the words. In this way, Hybrid\nmLDA assumes that every modality shares some la-\ntent structure with the text in the corpus, but the la-\ntent structures are not shared between non-textual\nmodalities.\n"},{"#tail":"\n","@confidence":"0.95072","#text":"\nFor example, to generate a hybrid model for text,\nfeature norms and SURF, we separately perform bi-\nmodal mLDA for the text/feature norms modalities\nand the text/SURF modalities. This provides us with\ntwo topic-word distributions: ?FNk,w and ?\nS\nk?,w, and\nthe hybrid model is simply the concatenation of the\n"},{"#tail":"\n","@confidence":"0.991651","#text":"\nwhere KFN indicates the number of topics for the\nFeature Norm modality, and likewise for KS .\n"},{"#tail":"\n","@confidence":"0.995208285714286","#text":"\nAnalytical inference of the posterior distribution of\nmLDA is intractable, and must be approximated.\nPrior work using mLDA has used Gibbs Sampling to\napproximate the posterior, but we found this method\ndid not scale with larger values of K, especially\nwhen applied to the relatively large deWaC corpus.\nTo solve these scaling issues, we implement On-\nline Variational Bayesian Inference (Hoffman et al,\n2010; Hoffman et al, 2012) for our models. In\nVariational Bayesian Inference (VBI), one approx-\nimates the true posterior using simpler distributions\nwith free variables. The free variables are then op-\ntimized in an EM-like algorithm to minimize differ-\nence between the true and approximate posteriors.\nOnline VBI differs from normal VBI by using ran-\ndomly sampled minibatches in each EM step rather\nthan the entire data set. Online VBI easily scales\nand quickly converges in all of our experiments. A\nlisting of the inference algorithm may be found in\nthe Supplementary Materials and the source code is\navailable as open source.\n"},{"#tail":"\n","@confidence":"0.99896171875","#text":"\nIn order to evaluate our algorithms, we first need to\ngenerate multimodal corpora for each of our non-\ntextual modalities. We use the same method as An-\ndrews et al (2009) for generating our multimodal\ncorpora: for each word token in the text corpus,\na feature is selected stochastically from the word?s\nfeature distribution, creating a word-feature pair.\nWords without grounded features are all given the\nsame placeholder feature, also resulting in a word-\nfeature pair.5 That is, for the feature norm modal-\nity, we generate (word, feature norm) pairs; for\nthe SURF modality, we generate (word, codeword)\npairs, etc. The resulting stochastically generated\ncorpus is used in its corresponding experiments.\nThe 3D text-feature-association norm corpus is\ngenerated slightly differently: for each word in\nthe original text corpus, we check the existence\nof multimodal features in either modality. If a\nword had no features, it is represented as a triple\n(word, placeholderFN , placeholderAN ). If the\nword had only feature norms, but no associations,\nit is generated as (word, feature, placeholderAN ),\nand similarly for association norms without feature\nnorms. In the case of words with presence in both\nmodalities, we generate two triples: (word, feature,\nplaceholderAN ) and (word, placeholderFN , associ-\nation). This allows association norms and feature\nnorms to influence each other via the document mix-\ntures ?, but avoids falsely labeling explicit relation-\nships between randomly selected feature norms and\nassociations.6 Other 3D corpora are generated using\nthe same general procedure.\n"},{"#tail":"\n","@confidence":"0.87103","#text":"\nWe evaluate each of our models with two data sets: a\nset of compositionality ratings for a number of Ger-\nman noun-noun compounds, and the same associa-\ntion norm data set used as one of our training modal-\nities in some settings.\nCompositionality Ratings is a data set of com-\npositionality ratings originally collected by von der\nHeide and Borgwaldt (2009). The data set con-\nsists of 450 concrete, depictable German noun com-\npounds along with compositionality ratings with re-\ngard to their constituents. For each compound, 30\nnative German speakers are asked to rate how re-\nlated the meaning of the compound is to each of\nits constituents on a scale from 1 (highly opaque;\nentirely noncompositional) to 7 (highly transparent;\nvery compositional). The mean of the 30 judgments\n5Placeholder features must be hardcoded to have equal prob-\nability over all topics to prevent all placeholder pairs from ag-\ngregating into a single topic.\n6We did try generating the random triples without placehold-\ners, but the generated explicit relationships are overwhelmingly\ndetrimental in the settings we attempted.\n"},{"#tail":"\n","@confidence":"0.996746142857142","#text":"\nis taken as the gold compositionality rating for each\nof the compound-constituent pairs. For example,\nAhornblatt ?maple leaf? is rated highly transparent\nwith respect to its constituents, Ahorn ?maple? and\nBlatt ?leaf?, but Lo?wenzahn ?dandelion? is rated non-\ncompositional with respect to its constituents, Lo?we\n?lion? and Zahn ?tooth?.\nWe use a subset of the original data, compris-\ning of all two-part noun-noun compounds and their\nconstituents. This data set consists of 488 com-\npositionality ratings (244 compound-head and 244\ncompound-modifier ratings) for 571 words. 309 of\nthe targets have images (the entire image data set);\n563 have feature norms; and all 571 of have associ-\nation norms.\nIn order to predict compositional-\nity, for each compound-constituent pair\n(wcompound, wconstituent), we compute nega-\ntive symmetric KL divergence between the two\nwords? topic distributions, where symmetric KL\ndivergence is defined as\n"},{"#tail":"\n","@confidence":"0.998934148148148","#text":"\nThe values of ?sKL for all compound-\nconstituent word pairs are correlated with the human\njudgments of compositionality using Spearman?s ?,\na rank-order correlation coefficient. Note that, since\nKL divergence is a measure of dissimilarity, we\nuse negative symmetric KL divergence so that our\n? correlation coefficient is positive. For exam-\nple, we compute both ?sKL(Ahornblatt, Ahorn)\nand ?sKL(Ahornblatt, Blatt), and so on for all\n488 compound-constituent pairs, and then correlate\nthese values with the human judgments.\nAdditionally, we also evaluate using the Associa-\ntion Norms data set described in Section 3. Since\nit is not sensible to evaluate association norm pre-\ndiction when they are also used as training data,\nwe omit this evaluation for this modality. Follow-\ning Andrews et al (2009), we measure association\nnorm prediction as an average of percentile ranks.\nFor all possible pairs of words in our vocabulary,\nwe compute the negative symmetric KL divergence\nbetween the two words. We then compute the per-\ncentile ranks of similarity for each word pair, e.g.,\n?cat? is more similar to ?dog? than 97.3% of the\nrest of the vocabulary. We report the weighted mean\npercentile ranks for all cue-association pairs, i.e.,\nif a cue-association is given more than once, it is\ncounted more than once.\n"},{"#tail":"\n","@confidence":"0.999791384615385","#text":"\nIn all settings, we fix all Dirichlet priors at 0.1, use\na learning rate 0.7, and use minibatch sizes of 1024\ndocuments. We do not optimize these hyperparame-\nters or vary them over time. The high Dirichlet pri-\nors are chosen to prevent sparsity in topic distribu-\ntions, while the other parameters are selected as the\nbest from Hoffman et al (2010).\nIn order to optimize the number of topics K, we\nrun five trials of each modality for 2000 iterations\nfor K = {50, 100, 150, 200, 250} (a total of 25\nruns per setup). We select the value or K for each\nmodel which minimizes the average perplexity esti-\nmate over the five trials.\n"},{"#tail":"\n","@confidence":"0.996875380952381","#text":"\nTable 1 shows our results for each of our selected\nmodels with our compositionality evaluation. The\n2D models employing feature norms and associa-\ntion norms do significantly better than the text-only\nmodel (two-tailed t-test). This result is consistent\nwith other works using this model with these fea-\ntures (Andrews et al, 2009; Silberer and Lapata,\n2012).\nWe also see that the SURF visual words are able\nto provide notable, albeit not significant, improve-\nments over the text-only modality. This confirms\nthat the low-level BoVW features do carry semantic\ninformation, and are useful to consider individually.\nThe GIST vectors, on the other hand, perform al-\nmost exactly the same as the text-only model. These\nfeatures, which are usually more useful for compar-\ning overall image likeness than object likeness, do\nnot individually contain semantic information useful\nfor compositionality prediction.\nThe performance of the visual modalities reverses\nwhen we look at our cluster-based models. Text\n"},{"#tail":"\n","@confidence":"0.931713625","#text":"\n?sKL(wcompound, wconstituent) and our Composi-\ntionality gold standard. The Hybrid models are the\nconcatenation of the corresponding Bimodal mLDA\nmodels. Stars indicate statistical significance compared\nto the text-only setting at the .05, .01 and .001 levels\nusing a two-tailed t-test.\ncombined with SURF clusters is our worst perform-\ning system, indicating our clusters of images with\ncommon visual words are actively working against\nus. The clusters based on GIST, on the other hand,\nprovide a minor improvement in compositionality\nprediction.\nAll of our 3D models are better than the text-only\nmodel, but they show a performance drop relative\nto one or both of their comparable bimodal models.\nThe model combining text, feature norms, and as-\nsociation norms is especially surprising: despite the\nexcellent performance of each of the bimodal mod-\nels, the 3D model performs significantly worse than\neither of its components (p < .05). This indicates\nthat these modalities provide new insight into word\nmeaning, but cannot be forced into the same latent\nstructure.\nThe hybrid models show massive performance in-\n"},{"#tail":"\n","@confidence":"0.97162325","#text":"\nwords and their associates. Stars indicate statistical sig-\nnificance compared to the text-only modality, with gray\nstars indicating the model is statistically worse than the\ntext model. The Hybrid models are the concatenation of\nthe corresponding Bimodal mLDA models.\ncreases across the board. Indeed, our 5 modality\nhybrid model obtains a performance nearly twice\nthat of the text-only model. Not only do all 6 hy-\nbrid models do significantly better than the text-only\nmodels, they show a highly significant improvement\nover their individual components (p < .001 for all\n16 comparisons). Furthermore, improvements gen-\nerally continue to grow significantly with each addi-\ntional modality we incorporate into the hybrid model\n(p < .001 for all but the .404 to .406 compari-\nson, which is not significant). Clearly, there is a\ngreat deal to learn from combining three, four and\neven five modalities, but the modalities are learn-\ning disjoint knowledge which cannot be forced into\na shared, latent structure.\n"},{"#tail":"\n","@confidence":"0.999513142857143","#text":"\nTable 2 shows the average weighted predicted rank\nsimilarity between all cue words and associates and\ntrials. Here we see that feature norms do not seem to\nbe improving performance on the association norms.\nThis is slightly unexpected, but consistent with the\nresult that feature norms seem to provide helpful, but\ndisjoint semantic information as association norms.\n"},{"#tail":"\n","@confidence":"0.999274942857143","#text":"\nWe see that the image modalities are much more\nuseful than they are in compositionality prediction.\nThe SURF modality does extremely well in partic-\nular, but the GIST features also provide statistically\nsignificant improvements over the text-only model.\nSince the SURF and GIST image features tend to\ncapture object-likeness and scene-likeness respec-\ntively, it is possible that words which share asso-\nciates are likely related through common settings\nand objects that appear with them. This seems to\nprovide additional evidence of Bruni et al (2012b)?s\nsuggestion that something like a distributional hy-\npothesis of images is plausible.\nOnce again, the clusters of images using SURF\ncauses a dramatic drop in performance. Combined\nwith the evidence from the compositionality assess-\nment, this shows that the SURF clusters are actively\nconfusing the models and not providing semantic in-\nformation. GIST clusters, on the other hand, are pro-\nviding a marginal improvement over the text-only\nmodel, but the result is not significant. We take a\nqualitative look into the GIST clusters in the next\nsection.\nOnce again, we see that the 3D models are inef-\nfective compared to their bimodal components, but\nthe hybrid models provide at least as much informa-\ntion as their components. The Feature Norms and\nGIST Clusters hybrid model significantly improves\nover both components.7 The final four-modality hy-\nbrid significantly outperforms all comparable mod-\nels. As with the compositionality evaluation, we\nconclude that the image and and feature norm mod-\nels are providing disjoint semantic information that\ncannot be forced into a shared latent structure, but\nstill augment each other when combined.\n"},{"#tail":"\n","@confidence":"0.989300960784314","#text":"\nIn all research connecting word meaning with per-\nceptual information, it is desirable that the inferred\nrepresentations be directly interpretable. One nice\nproperty of the cluster-based modalities is that we\nmay represent each cluster as its prototypical im-\nages, and examine whether the prototypes are re-\nlated to the topics.\nWe chose to limit our analysis to the GIST clus-\n7The gain is smaller than compared to SURF Hybrid, but\nthere is much less variance in the trials.\nters for two primary reasons: first, the SURF clusters\ndid not perform well in our evaluations, and sec-\nond, preliminary investigation into the SURF clus-\nters show that the majority of SURF clusters are\nnearly identical. This indicates our SURF clusters\nare likely hindered by poor initialization or param-\neter selection, and may partially explain their poor\nperformance in evaluations.\nWe select our single best Text + GIST Clusters\ntrial from the Compositionality evaluation and look\nat the topic distributions for words and image clus-\nters. For each topic, we select the three clusters with\nthe highest weight for the topic, p(c|?k). We extract\nthe five images closest to the cluster centroids, and\nselect two topics whose prototypical images are the\nmost interesting and informative. Figure 1 shows\nthese selected topics.\nThe first example topic contains almost exclu-\nsively water-related terms. The first image, extracted\nfrom the most probable cluster, does not at first seem\nrelated to water. Upon further inspection, we find\nthat many of the water-related pictures are scenic\nviews of lakes and mountains, often containing a\ncloudy sky. It seems that the GIST cluster does\nnot tend to group images of water, but rather nature\nscenes that may contain water. This relationship is\nmore obvious in the second picture, especially when\none considers the water itself contains reflections of\nthe trees and mountain.\nThe second topic contains time-related terms. The\n?@card@? term is a special token for all non-zero\nand non-one numbers. The second word, ?Uhr?, is\npolysemous: it can mean clock, an object which tells\nthe time, or o?clock, as in We meet at 2 o?clock (?Wir\ntreffen uns um 2 Uhr.?) The three prototypical pic-\ntures are not pictures of clocks, but round, detailed\nobjects similar to clocks. We see GIST has a prefer-\nence toward clustering images based on the predom-\ninant shape of the image. Here we see the clusters\nof GIST images are not providing a definite seman-\ntic relationship, but an overwhelming visual one.\n"},{"#tail":"\n","@confidence":"0.997158","#text":"\nIn this paper, we evaluated the role of low-level im-\nage features, SURF and GIST, for their compatibil-\nity with the multimodal Latent Dirichlet Allocation\nmodel of Andrews et al (2009). We found both fea-\n"},{"#tail":"\n","@confidence":"0.998494428571428","#text":"\nrelated words, as well scenes which often appear with water. The second shows clock-like objects, but not clocks.\nture sets were directly compatible with multimodal\nLDA and provided significant gains in their ability to\npredict association norms over traditional text-only\nLDA. SURF features also provided significant gains\nover text-only LDA in predicting the compositional-\nity of noun compounds.\nWe also showed that words may be represented\nin terms of membership of image clusters based on\nthe low-level image features. Image clusters based\non GIST features were qualitatively interesting, and\nwere able to give improvements over the text-only\nmodel.\nFinally, we showed two methods for extending\nmultimodal LDA to three or more modalities: the\nfirst as a 3D model with a shared latent structure\nbetween all modalities, and the second where latent\nstructures were inferred separately for each modal-\nity and joined together into a hybrid model. Al-\nthough the 3D model was unable to compete with\nits bimodal components, we found the hybrid model\nconsistently improved performance over its compo-\nnent modalities. We conclude that the combination\nof many modalities provides the best representation\nof word meaning, and that each nontextual modal-\nity is discovering disjoint information about word\nmeaning that cannot be forced into a global latent\nstructure.\n"},{"#tail":"\n","@confidence":"0.982512384615385","#text":"\nWe would like to thank the UT Natural Lan-\nguage Learning Reading Group and the anonymous\nEMNLP reviewers for their most helpful comments\nand suggestions. We would also like to thank the\nmembers of the SemRel group at IMS for their con-\nsiderable help in the construction of BilderNetle.\nThe authors acknowledge the Texas Advanced\nComputing Center (TACC) for providing the grid\ncomputing resources necessary for these results.\nThe research presented in this paper was funded by\nthe DFG Collaborative Research Centre SFB 732\n(Stephen Roller) and the DFG Heisenberg Fellow-\nship SCHU-2580/1-1 (Sabine Schulte im Walde).\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.9985865","#text":"\nDepartment of Computer Science\nThe University of Texas at Austin\n"},{"#tail":"\n","@confidence":"0.682182","#text":"\nInstitut fu?r Maschinelle Sprachverarbeitung\nUniversita?t Stuttgart\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.989015","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998127","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.99946","@genericHeader":"introduction","#text":"\n2 Related Work\n"},{"#tail":"\n","@confidence":"0.998604","@genericHeader":"method","#text":"\n3 Data\n"},{"#tail":"\n","@confidence":"0.993689","@genericHeader":"method","#text":"\n4 Model Definition\n"},{"#tail":"\n","@confidence":"0.988945","@genericHeader":"method","#text":"\n5 Experimental Setup\n"},{"#tail":"\n","@confidence":"0.999663","@genericHeader":"method","#text":"\n6 Results\n"},{"#tail":"\n","@confidence":"0.924237","@genericHeader":"method","#text":"\n7 Qualitative Analysis of Image Clusters\n"},{"#tail":"\n","@confidence":"0.993506","@genericHeader":"conclusions","#text":"\n8 Conclusions\n"},{"#tail":"\n","@confidence":"0.966955","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.966863","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.966483","#text":"\nTable 1: Average rank correlations between\n"},{"#tail":"\n","@confidence":"0.997072","#text":"\nTable 2: Average predicted rank similarity between cue\n"}],"page":[{"#tail":"\n","@confidence":"0.967469","#text":"\n1146\n"},{"#tail":"\n","@confidence":"0.797924","#text":"\nemnlp13\n"},{"#tail":"\n","@confidence":"0.977766","#text":"\n1147\n"},{"#tail":"\n","@confidence":"0.987647","#text":"\n1148\n"},{"#tail":"\n","@confidence":"0.982094","#text":"\n1149\n"},{"#tail":"\n","@confidence":"0.907276","#text":"\n1150\n"},{"#tail":"\n","@confidence":"0.84639","#text":"\n1151\n"},{"#tail":"\n","@confidence":"0.971959","#text":"\n1152\n"},{"#tail":"\n","@confidence":"0.841867","#text":"\n1153\n"},{"#tail":"\n","@confidence":"0.534211","#text":"\n1154\n"},{"#tail":"\n","@confidence":"0.825583","#text":"\n1155\n"},{"#tail":"\n","@confidence":"0.569438","#text":"\n1156\n"},{"#tail":"\n","@confidence":"0.981044","#text":"\n1157\n"}],"figureCaption":{"#tail":"\n","@confidence":"0.999954","#text":"\nFigure 1: Example topics with prototypical images for the Text + GIST Cluster modality. The first topic shows water-\n"},"table":[{"#tail":"\n","@confidence":"0.992851380952381","#text":"\nModality K ?\nText Only\nText Only (LDA) 200 .204\nBimodal mLDA\nText + Feature Norms 150 .310 ***\nText + Assoc. Norms 200 .328 **\nText + SURF 50 .251\nText + GIST 100 .204\nText + SURF Clusters 200 .159\nText + GIST Clusters 150 .233\n3D mLDA\nText + FN + AN 250 .259\nText + FN + SURF 100 .286 *\nText + FN + GC 200 .261 *\nHybrid mLDA\nFN, AN 150+200 .390 ***\nFN, SURF 150+50 .350 ***\nFN, GC 150+150 .340 ***\nFN, AN, GC 150+200+150 .395 ***\nFN, AN, SURF 150+200+50 .404 ***\nFN, AN, SURF, GC 150+200+50+150 .406 ***\n"},{"#tail":"\n","@confidence":"0.994153875","#text":"\nModality K Assoc.\nText Only\nText Only (LDA) 200 .679\nBimodal mLDA\nText + Feature Norms 150 .676\nText + SURF 50 .789 ***\nText + GIST 100 .739 ***\nText + SURF Clusters 200 .618 ***\nText + GIST Clusters 150 .690\n3D mLDA\nText + FN + SURF 100 .722 ***\nText + FN + GC 200 .601 ***\nHybrid mLDA\nFN, SURF 150+50 .800 ***\nFN, GC 150+150 .742 ***\nFN, GC, SURF 150+150+50 .804 ***\n"}],"email":[{"#tail":"\n","@confidence":"0.993694","#text":"\nroller@cs.utexas.edu\n"},{"#tail":"\n","@confidence":"0.973955","#text":"\nschulte@ims.uni-stuttgart.de\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.824211","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.9613245","#text":"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146?1157, Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics"},"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.9999315","#text":"Department of Computer Science The University of Texas at Austin"},{"#tail":"\n","@confidence":"0.978065","#text":"Institut fu?r Maschinelle Sprachverarbeitung Universita?t Stuttgart"}],"author":[{"#tail":"\n","@confidence":"0.999948","#text":"Stephen Roller"},{"#tail":"\n","@confidence":"0.998525","#text":"Sabine Schulte im Walde"}],"abstract":{"#tail":"\n","@confidence":"0.998219291666667","#text":"Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al, 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal models to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure."},"title":{"#tail":"\n","@confidence":"0.9942535","#text":"A Multimodal LDA Model Integrating Textual, Cognitive and Visual Modalities"},"email":[{"#tail":"\n","@confidence":"0.998175","#text":"roller@cs.utexas.edu"},{"#tail":"\n","@confidence":"0.988517","#text":"schulte@ims.uni-stuttgart.de"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"volume":{"#tail":"\n","#text":"116"},"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463."},"journal":{"#tail":"\n","#text":"Psychological Review,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Andrews, Vigliocco, Vinson, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" for Computational Linguistics A Multimodal LDA Model Integrating Textual, Cognitive and Visual Modalities Stephen Roller Department of Computer Science The University of Texas at Austin roller@cs.utexas.edu Sabine Schulte im Walde Institut fu?r Maschinelle Sprachverarbeitung Universita?t Stuttgart schulte@ims.uni-stuttgart.de Abstract Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al, 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal models to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that","@endWordPosition":"100","@position":"789","annotationId":"T1","@startWordPosition":"97","@citStr":"Andrews et al, 2009"},{"#tail":"\n","#text":"structions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel com","@endWordPosition":"416","@position":"2856","annotationId":"T2","@startWordPosition":"413","@citStr":"Andrews et al, 2009"},{"#tail":"\n","#text":"n representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new metho","@endWordPosition":"949","@position":"6258","annotationId":"T3","@startWordPosition":"946","@citStr":"Andrews et al (2009)"},{"#tail":"\n","#text":"like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al, 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al, 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimod","@endWordPosition":"2474","@position":"15585","annotationId":"T4","@startWordPosition":"2471","@citStr":"Andrews et al (2009)"},{"#tail":"\n","#text":" document in the corpus is generated using the fol1149 lowing generative process: 1. A document-specific topic distribution, ?d ? Dir(?) is drawn. 2. For the ith word in the document, (a) A topic assignment zi ? ?d is drawn, (b) and a word wi ? ?zi is drawn and observed. The task of Latent Dirichlet Allocation is then to automatically infer the latent document distribution ?d for each document d ? D, and the topic distribution ?k for each of the k = {1, . . . ,K} topics, given the data. The probability that the ith word of document d is p(wi, ?d) = ? k p(wi|?k)p(zi = k|?d). 4.2 Multimodal LDA Andrews et al (2009) extend LDA to allow for the inference of document and topic distributions in a multimodal corpus. In their model, a document consists of a set of (word, feature) pairs,4 rather than just words, and documents are still modeled as mixtures of shared topics. Topics consist of multinomial distributions over words, ?k, but are extended to also include multinomial distributions over features, ?k. The generative process is amended to include these feature distributions: 1. A document-specific topic distribution, ?d ? Dir(?) is drawn. 2. For the ith (word, feature) pair in the document, (a) A topic a","@endWordPosition":"2773","@position":"17313","annotationId":"T5","@startWordPosition":"2770","@citStr":"Andrews et al (2009)"},{"#tail":"\n","#text":"ing sense of feature. and features, and topics become the key link between the text and feature modalities. 4.3 3D Multimodal LDA We can easily extend the bimodal LDA model to incorporate three or more modalities by simply performing inference over n-tuples instead of pairs, and still mandating that each modality is conditionally independent given the topic. We consider the ith tuple (wi, fi, f ?i , . . .) in document d to have a conditional probability of: p(wi, fi, f ? i , . . . , ?d) = ? k p(wi|?i)p(fi|?k)p(f ? i |? ? i) ? ? ? p(zi = k|?d) That is, we simply take the original mLDA model of Andrews et al (2009) and generalize it in the same way they generalize LDA. At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi. 4.4 Hybrid Multimodal LDA 3D Multimodal LDA assumes that all modalities share the same latent topic structure, ?d. It is possible, however, that all modalities do not share some latent structure, but the modalities can still combine in o","@endWordPosition":"3101","@position":"19121","annotationId":"T6","@startWordPosition":"3098","@citStr":"Andrews et al (2009)"},{"#tail":"\n","#text":"thm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source. 5 Experimental Setup 5.1 Generating Multimodal Corpora In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our nontextual modalities. We use the same method as Andrews et al (2009) for generating our multimodal corpora: for each word token in the text corpus, a feature is selected stochastically from the word?s feature distribution, creating a word-feature pair. Words without grounded features are all given the same placeholder feature, also resulting in a wordfeature pair.5 That is, for the feature norm modality, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc. The resulting stochastically generated corpus is used in its corresponding experiments. The 3D text-feature-association norm corpus is generated slightly di","@endWordPosition":"3635","@position":"22323","annotationId":"T7","@startWordPosition":"3631","@citStr":"Andrews et al (2009)"},{"#tail":"\n","#text":"ent. Note that, since KL divergence is a measure of dissimilarity, we use negative symmetric KL divergence so that our ? correlation coefficient is positive. For example, we compute both ?sKL(Ahornblatt, Ahorn) and ?sKL(Ahornblatt, Blatt), and so on for all 488 compound-constituent pairs, and then correlate these values with the human judgments. Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following Andrews et al (2009), we measure association norm prediction as an average of percentile ranks. For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., ?cat? is more similar to ?dog? than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once. 5.3 Model Selection and Hyperparameter Optimization In all settings, we fix all Dirichlet prio","@endWordPosition":"4321","@position":"26690","annotationId":"T8","@startWordPosition":"4318","@citStr":"Andrews et al (2009)"},{"#tail":"\n","#text":"f topics K, we run five trials of each modality for 2000 iterations for K = {50, 100, 150, 200, 250} (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features (Andrews et al, 2009; Silberer and Lapata, 2012). We also see that the SURF visual words are able to provide notable, albeit not significant, improvements over the text-only modality. This confirms that the low-level BoVW features do carry semantic information, and are useful to consider individually. The GIST vectors, on the other hand, perform almost exactly the same as the text-only model. These features, which are usually more useful for comparing overall image likeness than object likeness, do not individually contain semantic information useful for compositionality prediction. The performance of the visual ","@endWordPosition":"4586","@position":"28246","annotationId":"T9","@startWordPosition":"4583","@citStr":"Andrews et al, 2009"},{"#tail":"\n","#text":"ct which tells the time, or o?clock, as in We meet at 2 o?clock (?Wir treffen uns um 2 Uhr.?) The three prototypical pictures are not pictures of clocks, but round, detailed objects similar to clocks. We see GIST has a preference toward clustering images based on the predominant shape of the image. Here we see the clusters of GIST images are not providing a definite semantic relationship, but an overwhelming visual one. 8 Conclusions In this paper, we evaluated the role of low-level image features, SURF and GIST, for their compatibility with the multimodal Latent Dirichlet Allocation model of Andrews et al (2009). We found both fea1154 Most Probable Words Translations Prototypical Images Wasser water Schiff ship See lake Meer sea Meter meter Flu? river @card@ (number) Uhr clock Freitag Friday Sonntag Sunday Samstag Saturday Montag Monday Figure 1: Example topics with prototypical images for the Text + GIST Cluster modality. The first topic shows waterrelated words, as well scenes which often appear with water. The second shows clock-like objects, but not clocks. ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over trad","@endWordPosition":"5996","@position":"36808","annotationId":"T10","@startWordPosition":"5993","@citStr":"Andrews et al (2009)"}]},"title":{"#tail":"\n","#text":"Integrating experiential and distributional data to learn semantic representations."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mark Andrews"},{"#tail":"\n","#text":"Gabriella Vigliocco"},{"#tail":"\n","#text":"David Vinson"}]}},{"volume":{"#tail":"\n","#text":"1"},"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. In Transactions of the Association for Computational Linguistics, volume 1, pages 49?62."},"#text":"\n","pages":{"#tail":"\n","#text":"49--62"},"marker":{"#tail":"\n","#text":"Artzi, Zettlemoyer, 2013"},"title":{"#tail":"\n","#text":"Weakly supervised learning of semantic parsers for mapping instructions to actions."},"booktitle":{"#tail":"\n","#text":"In Transactions of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yoav Artzi"},{"#tail":"\n","#text":"Luke Zettlemoyer"}]}},{"volume":{"#tail":"\n","#text":"16"},"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Lisa Aziz-Zadeh, Stephen M. Wilson, Giacomo Rizzolatti, and Marco Iacoboni. 2006. Congruent embodied representations for visually presented actions and linguistic phrases describing actions. Current Biology, 16(18):1818?1823."},"journal":{"#tail":"\n","#text":"Current Biology,"},"#text":"\n","issue":{"#tail":"\n","#text":"18"},"marker":{"#tail":"\n","#text":"Aziz-Zadeh, Wilson, Rizzolatti, Iacoboni, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nificant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the","@endWordPosition":"787","@position":"5177","annotationId":"T11","@startWordPosition":"784","@citStr":"Aziz-Zadeh et al, 2006"}},"title":{"#tail":"\n","#text":"Congruent embodied representations for visually presented actions and linguistic phrases describing actions."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Lisa Aziz-Zadeh"},{"#tail":"\n","#text":"Stephen M Wilson"},{"#tail":"\n","#text":"Giacomo Rizzolatti"},{"#tail":"\n","#text":"Marco Iacoboni"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209?226."},"#text":"\n","pages":{"#tail":"\n","#text":"43--3"},"marker":{"#tail":"\n","#text":"Baroni, Bernardini, Ferraresi, Zanchetta, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ed to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al, 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens. 3.2 Cognitive Modalities Association Norms (AN) is a collection of association norms collected by Schulte im Walde et al (2012). In association norm experiments, subjects are presented with a cue word and asked to list the fi","@endWordPosition":"1422","@position":"9153","annotationId":"T12","@startWordPosition":"1418","@citStr":"Baroni et al, 2009"}},"title":{"#tail":"\n","#text":"The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Marco Baroni"},{"#tail":"\n","#text":"Silvia Bernardini"},{"#tail":"\n","#text":"Adriano Ferraresi"},{"#tail":"\n","#text":"Eros Zanchetta"}]}},{"volume":{"#tail":"\n","#text":"110"},"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. 2008. Surf: Speeded up robust features. Computer Vision and Image Understanding, 110(3):346? 359, June."},"journal":{"#tail":"\n","#text":"Computer Vision and Image Understanding,"},"#text":"\n","pages":{"#tail":"\n","#text":"359"},"issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Bay, 2008"},"title":{"#tail":"\n","#text":"Andreas Ess, Tinne Tuytelaars, and Luc Van Gool."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Herbert Bay"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet alocation. The Journal of Machine Learning Research, 3:993?1022."},"journal":{"#tail":"\n","#text":"The Journal of Machine Learning Research,"},"#text":"\n","pages":{"#tail":"\n","#text":"3--993"},"marker":{"#tail":"\n","#text":"Blei, Ng, Jordan, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" et al, 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al, 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al, 2003), or LDA, is an unsupervised Bayesian probabilistic model of text documents. It assumes that all documents are probabilistically generated from a shared set ofK common topics, where each topic is a multinomial distribution over the vocabulary (notated as ?), and documents are modeled as mixtures of these shared topics (notated as ?). LDA assumes every document in the corpus is generated using the fol1149 lowing generative process: 1. A document-specific topic distribution, ?d ? Dir(?) is drawn. 2. For the ith word in the document, (a) A topic assignment zi ? ?d is drawn, (b) and a word wi ? ?z","@endWordPosition":"2596","@position":"16340","annotationId":"T13","@startWordPosition":"2593","@citStr":"Blei et al, 2003"}},"title":{"#tail":"\n","#text":"Latent dirichlet alocation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David M Blei"},{"#tail":"\n","#text":"Andrew Y Ng"},{"#tail":"\n","#text":"Michael I Jordan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011."},"#text":"\n","marker":{"#tail":"\n","#text":"Bruni, Tran, Baroni, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tion directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multi","@endWordPosition":"426","@position":"2915","annotationId":"T14","@startWordPosition":"423","@citStr":"Bruni et al, 2011"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Elia Bruni"},{"#tail":"\n","#text":"Giang Binh Tran"},{"#tail":"\n","#text":"Marco Baroni"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Distributional semantics from text and images. Proceedings of the EMNLP 2011 Geometrical Models for Natural Language Semantics, pages 22?32."},"#text":"\n","pages":{"#tail":"\n","#text":"22--32"},"marker":{"#tail":"\n","#text":"2011"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"n arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al, 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ?gist? of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully ","@endWordPosition":"2385","@position":"15024","annotationId":"T15","@startWordPosition":"2385","@citStr":"(2011)"}},"title":{"#tail":"\n","#text":"Distributional semantics from text and images."},"booktitle":{"#tail":"\n","#text":"Proceedings of the EMNLP"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012a. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 136?145."},"#text":"\n","pages":{"#tail":"\n","#text":"136--145"},"marker":{"#tail":"\n","#text":"Bruni, Boleda, Baroni, Tran, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was ","@endWordPosition":"438","@position":"2984","annotationId":"T16","@startWordPosition":"435","@citStr":"Bruni et al, 2012"},{"#tail":"\n","#text":"ison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other wo","@endWordPosition":"1154","@position":"7576","annotationId":"T17","@startWordPosition":"1151","@citStr":"Bruni et al (2012"},{"#tail":"\n","#text":" helpful, but disjoint semantic information as association norms. 1153 We see that the image modalities are much more useful than they are in compositionality prediction. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of Bruni et al (2012b)?s suggestion that something like a distributional hypothesis of images is plausible. Once again, the clusters of images using SURF causes a dramatic drop in performance. Combined with the evidence from the compositionality assessment, this shows that the SURF clusters are actively confusing the models and not providing semantic information. GIST clusters, on the other hand, are providing a marginal improvement over the text-only model, but the result is not significant. We take a qualitative look into the GIST clusters in the next section. Once again, we see that the 3D models are ineffecti","@endWordPosition":"5368","@position":"32946","annotationId":"T18","@startWordPosition":"5365","@citStr":"Bruni et al (2012"}]},"title":{"#tail":"\n","#text":"Distributional semantics in technicolor."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Elia Bruni"},{"#tail":"\n","#text":"Gemma Boleda"},{"#tail":"\n","#text":"Marco Baroni"},{"#tail":"\n","#text":"NamKhanh Tran"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu Sebe. 2012b. Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning. In Proceedings of the 20th ACM International Conference on Multimedia, pages 1219?1228."},"#text":"\n","pages":{"#tail":"\n","#text":"1219--1228"},"marker":{"#tail":"\n","#text":"Bruni, Uijlings, Baroni, Sebe, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was ","@endWordPosition":"438","@position":"2984","annotationId":"T19","@startWordPosition":"435","@citStr":"Bruni et al, 2012"},{"#tail":"\n","#text":"ison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other wo","@endWordPosition":"1154","@position":"7576","annotationId":"T20","@startWordPosition":"1151","@citStr":"Bruni et al (2012"},{"#tail":"\n","#text":" helpful, but disjoint semantic information as association norms. 1153 We see that the image modalities are much more useful than they are in compositionality prediction. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of Bruni et al (2012b)?s suggestion that something like a distributional hypothesis of images is plausible. Once again, the clusters of images using SURF causes a dramatic drop in performance. Combined with the evidence from the compositionality assessment, this shows that the SURF clusters are actively confusing the models and not providing semantic information. GIST clusters, on the other hand, are providing a marginal improvement over the text-only model, but the result is not significant. We take a qualitative look into the GIST clusters in the next section. Once again, we see that the 3D models are ineffecti","@endWordPosition":"5368","@position":"32946","annotationId":"T21","@startWordPosition":"5365","@citStr":"Bruni et al (2012"}]},"title":{"#tail":"\n","#text":"Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 20th ACM International Conference on Multimedia,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Elia Bruni"},{"#tail":"\n","#text":"Jasper Uijlings"},{"#tail":"\n","#text":"Marco Baroni"},{"#tail":"\n","#text":"Nicu Sebe"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"David L. Chen and Raymond J. Mooney. 2011. Learning to interpret natural language navigation instructions from observations. Proceedings of the 25th AAAI Conference on Artificial Intelligence, pages 859?865, August."},"#text":"\n","pages":{"#tail":"\n","#text":"859--865"},"marker":{"#tail":"\n","#text":"Chen, Mooney, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ion. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata,","@endWordPosition":"336","@position":"2337","annotationId":"T22","@startWordPosition":"333","@citStr":"Chen and Mooney, 2011"},{"#tail":"\n","#text":" of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the pa","@endWordPosition":"827","@position":"5445","annotationId":"T23","@startWordPosition":"824","@citStr":"Chen and Mooney, 2011"}]},"title":{"#tail":"\n","#text":"Learning to interpret natural language navigation instructions from observations."},"booktitle":{"#tail":"\n","#text":"Proceedings of the 25th AAAI Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David L Chen"},{"#tail":"\n","#text":"Raymond J Mooney"}]}},{"volume":{"#tail":"\n","#text":"37"},"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal of Artificial Intelligence Research, 37(1):397?436."},"journal":{"#tail":"\n","#text":"Journal of Artificial Intelligence Research,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Chen, Kim, Mooney, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2","@endWordPosition":"823","@position":"5422","annotationId":"T24","@startWordPosition":"820","@citStr":"Chen et al, 2010"}},"title":{"#tail":"\n","#text":"Training a multilingual sportscaster: Using perceptual context to learn language."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David L Chen"},{"#tail":"\n","#text":"Joohyun Kim"},{"#tail":"\n","#text":"Raymond J Mooney"}]}},{"date":{"#tail":"\n","#text":"1990"},"issue":{"#tail":"\n","#text":"6"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual","@endWordPosition":"939","@position":"6197","annotationId":"T25","@startWordPosition":"936","@citStr":"Deerwester et al, 1990"}},"title":{"#tail":"\n","#text":"Indexing by latent semantic analysis."},"volume":{"#tail":"\n","#text":"41"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391? 407."},"journal":{"#tail":"\n","#text":"Journal of the American Society for Information Science,"},"#text":"\n","pages":{"#tail":"\n","#text":"407"},"marker":{"#tail":"\n","#text":"Deerwester, Dumais, Furnas, Landauer, Harshman, 1990"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Scott Deerwester"},{"#tail":"\n","#text":"Susan T Dumais"},{"#tail":"\n","#text":"George W Furnas"},{"#tail":"\n","#text":"Thomas K Landauer"},{"#tail":"\n","#text":"Richard Harshman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248?255. IEEE."},"#text":"\n","pages":{"#tail":"\n","#text":"248--255"},"marker":{"#tail":"\n","#text":"Deng, Dong, Socher, Li, Li, Fei-Fei, 2009"},"publisher":{"#tail":"\n","#text":"IEEE."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"final data set contains 11,714 cue-response pairs for 569 nouns and 2,589 response types. Note that the difference between association norms and feature norms is subtle, but important. In AN collection, subjects simply name related words as fast as possible, while in FN collection, subjects must carefully describe the cue. 3.3 Visual Modalities BilderNetle (?little ImageNet? in Swabian German) is our new data set of German noun-to-ImageNet synset mappings. ImageNet is a large-scale and widely used image database, built on top of WordNet, which maps words into groups of images, called synsets (Deng et al, 2009). Multiple synsets exist for each meaning of a word. For example, ImageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral. This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet. Starting with a set of noun compounds and their nominal constituents von der Heide and Borgwaldt (2009), five native German speakers and one native English speaker (including the authors of this paper) work together to map German nouns to ImageNet synsets. With the assistanc","@endWordPosition":"1780","@position":"11322","annotationId":"T26","@startWordPosition":"1777","@citStr":"Deng et al, 2009"}},"title":{"#tail":"\n","#text":"Imagenet: A large-scale hierarchical image database."},"booktitle":{"#tail":"\n","#text":"In Computer Vision and Pattern Recognition,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jia Deng"},{"#tail":"\n","#text":"Wei Dong"},{"#tail":"\n","#text":"Richard Socher"},{"#tail":"\n","#text":"Li-Jia Li"},{"#tail":"\n","#text":"Kai Li"},{"#tail":"\n","#text":"Li Fei-Fei"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Thomas Deselaers and Vittorio Ferrari. 2011. Visual and semantic similarity in imagenet. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1777?1784."},"#text":"\n","pages":{"#tail":"\n","#text":"1777--1784"},"marker":{"#tail":"\n","#text":"Deselaers, Ferrari, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"r of clusters is chosen arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al, 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ?gist? of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully ","@endWordPosition":"2385","@position":"15024","annotationId":"T27","@startWordPosition":"2382","@citStr":"Deselaers and Ferrari (2011)"}},"title":{"#tail":"\n","#text":"Visual and semantic similarity in imagenet."},"booktitle":{"#tail":"\n","#text":"In IEEE Conference on Computer Vision and Pattern Recognition,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Thomas Deselaers"},{"#tail":"\n","#text":"Vittorio Ferrari"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Matthijs Douze, Herve? Je?gou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. 2009. Evaluation of gist descriptors for web-scale image search. In Proceedings of the ACM International Conference on Image and Video Retrieval, pages 19:1?19:8."},"#text":"\n","pages":{"#tail":"\n","#text":"19--1"},"marker":{"#tail":"\n","#text":"Douze, Jegou, Sandhawalia, Amsaleg, Schmid, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ecognize using collections of codewords. To test that similar concepts should share similar visual codewords, we cluster the BoVW representations for all our images into 500 clusters with kmeans clustering, and represent each word as membership over the image clusters, forming the SURF Clusters modality. The number of clusters is chosen arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al, 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ?gist? of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered","@endWordPosition":"2330","@position":"14699","annotationId":"T28","@startWordPosition":"2327","@citStr":"Douze et al, 2009"}},"title":{"#tail":"\n","#text":"Evaluation of gist descriptors for web-scale image search."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACM International Conference on Image and Video Retrieval,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matthijs Douze"},{"#tail":"\n","#text":"Herve Jegou"},{"#tail":"\n","#text":"Harsimrat Sandhawalia"},{"#tail":"\n","#text":"Laurent Amsaleg"},{"#tail":"\n","#text":"Cordelia Schmid"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Jacob Eisenstein, Brendan O?Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model for geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277?1287."},"#text":"\n","pages":{"#tail":"\n","#text":"1277--1287"},"marker":{"#tail":"\n","#text":"Eisenstein, OConnor, Smith, Xing, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"st as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common cr","@endWordPosition":"385","@position":"2639","annotationId":"T29","@startWordPosition":"382","@citStr":"Eisenstein et al, 2010"}},"title":{"#tail":"\n","#text":"A latent variable model for geographic lexical variation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jacob Eisenstein"},{"#tail":"\n","#text":"Brendan OConnor"},{"#tail":"\n","#text":"Noah A Smith"},{"#tail":"\n","#text":"Eric P Xing"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. 2009. Describing objects by their attributes."},"#text":"\n","marker":{"#tail":"\n","#text":"Farhadi, Endres, Hoiem, Forsyth, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"porating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cog","@endWordPosition":"755","@position":"4976","annotationId":"T30","@startWordPosition":"752","@citStr":"Farhadi et al, 2009"},{"#tail":"\n","#text":"l based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recogni","@endWordPosition":"1240","@position":"8113","annotationId":"T31","@startWordPosition":"1237","@citStr":"Farhadi et al, 2009"}]},"title":{"#tail":"\n","#text":"Describing objects by their attributes."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ali Farhadi"},{"#tail":"\n","#text":"Ian Endres"},{"#tail":"\n","#text":"Derek Hoiem"},{"#tail":"\n","#text":"David Forsyth"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In IEEE Conference on Computer Vision and Pattern Recognition, pages 1778?1785."},"#text":"\n","pages":{"#tail":"\n","#text":"1778--1785"},"marker":{"#tail":"\n"},"booktitle":{"#tail":"\n","#text":"In IEEE Conference on Computer Vision and Pattern Recognition,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Yansong Feng and Mirella Lapata. 2010a. How many words is a picture worth? Automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1239?1249."},"#text":"\n","pages":{"#tail":"\n","#text":"1239--1249"},"marker":{"#tail":"\n","#text":"Feng, Lapata, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"on alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the gen","@endWordPosition":"363","@position":"2498","annotationId":"T32","@startWordPosition":"359","@citStr":"Feng and Lapata, 2010"},{"#tail":"\n","#text":"roach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of","@endWordPosition":"1111","@position":"7307","annotationId":"T33","@startWordPosition":"1108","@citStr":"Feng and Lapata (2010"}]},"title":{"#tail":"\n","#text":"How many words is a picture worth? Automatic caption generation for news images."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yansong Feng"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Yansong Feng and Mirella Lapata. 2010b. Visual information in semantic representation. In Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 91?99. Thomas L. Griffiths, Mark Steyvers, and Joshua B."},"#text":"\n","pages":{"#tail":"\n","#text":"91--99"},"marker":{"#tail":"\n","#text":"Feng, Lapata, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"on alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the gen","@endWordPosition":"363","@position":"2498","annotationId":"T34","@startWordPosition":"359","@citStr":"Feng and Lapata, 2010"},{"#tail":"\n","#text":"roach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of","@endWordPosition":"1111","@position":"7307","annotationId":"T35","@startWordPosition":"1108","@citStr":"Feng and Lapata (2010"}]},"title":{"#tail":"\n","#text":"Visual information in semantic representation."},"booktitle":{"#tail":"\n","#text":"In Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yansong Feng"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"volume":{"#tail":"\n","#text":"114"},"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211."},"journal":{"#tail":"\n","#text":"Psychological Review,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Tenenbaum, 2007"},"title":{"#tail":"\n","#text":"Topics in semantic representation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Tenenbaum"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Matthew Hoffman, David M. Blei, and Francis Bach. 2010. Online learning for latent dirichlet alocation. Advances in Neural Information Processing Systems, 23:856?864."},"#text":"\n","pages":{"#tail":"\n","#text":"23--856"},"marker":{"#tail":"\n","#text":"Hoffman, Blei, Bach, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"oncatenation of the two distributions, ?FN&Sj,w = { ?FNj,w 1 ? j ? K FN , ?Sj?KFN ,w K FN < j ? KFN +KS , where KFN indicates the number of topics for the Feature Norm modality, and likewise for KS . 4.5 Inference Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. To solve these scaling issues, we implement Online Variational Bayesian Inference (Hoffman et al, 2010; Hoffman et al, 2012) for our models. In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is avail","@endWordPosition":"3496","@position":"21480","annotationId":"T36","@startWordPosition":"3493","@citStr":"Hoffman et al, 2010"},{"#tail":"\n","#text":"s more similar to ?dog? than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once. 5.3 Model Selection and Hyperparameter Optimization In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents. We do not optimize these hyperparameters or vary them over time. The high Dirichlet priors are chosen to prevent sparsity in topic distributions, while the other parameters are selected as the best from Hoffman et al (2010). In order to optimize the number of topics K, we run five trials of each modality for 2000 iterations for K = {50, 100, 150, 200, 250} (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works usi","@endWordPosition":"4474","@position":"27592","annotationId":"T37","@startWordPosition":"4471","@citStr":"Hoffman et al (2010)"}]},"title":{"#tail":"\n","#text":"Online learning for latent dirichlet alocation."},"booktitle":{"#tail":"\n","#text":"Advances in Neural Information Processing Systems,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matthew Hoffman"},{"#tail":"\n","#text":"David M Blei"},{"#tail":"\n","#text":"Francis Bach"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Matthew Hoffman, David M. Blei, Chong Wang, and John Paisley. 2012. Stochastic variational inference. ArXiv e-prints, June."},"#text":"\n","marker":{"#tail":"\n","#text":"Hoffman, Blei, Wang, Paisley, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"wo distributions, ?FN&Sj,w = { ?FNj,w 1 ? j ? K FN , ?Sj?KFN ,w K FN < j ? KFN +KS , where KFN indicates the number of topics for the Feature Norm modality, and likewise for KS . 4.5 Inference Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. To solve these scaling issues, we implement Online Variational Bayesian Inference (Hoffman et al, 2010; Hoffman et al, 2012) for our models. In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source. 5","@endWordPosition":"3500","@position":"21502","annotationId":"T38","@startWordPosition":"3497","@citStr":"Hoffman et al, 2012"}},"title":{"#tail":"\n","#text":"Stochastic variational inference. ArXiv e-prints,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matthew Hoffman"},{"#tail":"\n","#text":"David M Blei"},{"#tail":"\n","#text":"Chong Wang"},{"#tail":"\n","#text":"John Paisley"}]}},{"volume":{"#tail":"\n","#text":"4"},"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Brendan T. Johns and Michael N. Jones. 2012. Perceptual inference through global lexical similarity. Topics in Cognitive Science, 4(1):103?120."},"journal":{"#tail":"\n","#text":"Topics in Cognitive Science,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Johns, Jones, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The mo","@endWordPosition":"434","@position":"2965","annotationId":"T39","@startWordPosition":"431","@citStr":"Johns and Jones, 2012"},{"#tail":"\n","#text":" pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work t","@endWordPosition":"1009","@position":"6655","annotationId":"T40","@startWordPosition":"1006","@citStr":"Johns and Jones (2012)"}]},"title":{"#tail":"\n","#text":"Perceptual inference through global lexical similarity."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Brendan T Johns"},{"#tail":"\n","#text":"Michael N Jones"}]}},{"volume":{"#tail":"\n","#text":"2"},"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The story picturing engine?a system for automatic text illustration. ACM Transactions on Multimedia Computing, Communications, and Applications, 2(1):68?89."},"journal":{"#tail":"\n","#text":"ACM Transactions on Multimedia Computing, Communications, and Applications,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Joshi, Wang, Li, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":". The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word me","@endWordPosition":"373","@position":"2560","annotationId":"T41","@startWordPosition":"370","@citStr":"Joshi et al, 2006"}},"title":{"#tail":"\n","#text":"The story picturing engine?a system for automatic text illustration."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dhiraj Joshi"},{"#tail":"\n","#text":"James Z Wang"},{"#tail":"\n","#text":"Jia Li"}]}},{"volume":{"#tail":"\n","#text":"7"},"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Rohit J. Kate and Raymond J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Proceedings of the 22nd Conference on Artificial Intelligence, volume 7, pages 895?900."},"#text":"\n","pages":{"#tail":"\n","#text":"895--900"},"marker":{"#tail":"\n","#text":"Kate, Mooney, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"dy of work has been devoted to multimodal or ?grounded? models of language where semantic representations of words are extended to include perceptual information. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributiona","@endWordPosition":"314","@position":"2179","annotationId":"T42","@startWordPosition":"311","@citStr":"Kate and Mooney, 2007"}},"title":{"#tail":"\n","#text":"Learning language semantics from ambiguous supervision."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 22nd Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Rohit J Kate"},{"#tail":"\n","#text":"Raymond J Mooney"}]}},{"volume":{"#tail":"\n","#text":"60"},"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91?110."},"journal":{"#tail":"\n","#text":"International Journal of Computer Vision,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Lowe, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"del by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psy","@endWordPosition":"751","@position":"4955","annotationId":"T43","@startWordPosition":"750","@citStr":"Lowe, 2004"},{"#tail":"\n","#text":"s from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recent","@endWordPosition":"1122","@position":"7367","annotationId":"T44","@startWordPosition":"1121","@citStr":"Lowe, 2004"}]},"title":{"#tail":"\n","#text":"Distinctive image features from scale-invariant keypoints."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David G Lowe"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Stefan Mathe, Afsaneh Fazly, Sven Dickinson, and Suzanne Stevenson. 2008. Learning the abstract motion semantics of verbs from captioned videos. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 1?8."},"#text":"\n","pages":{"#tail":"\n","#text":"1--8"},"marker":{"#tail":"\n","#text":"Mathe, Fazly, Dickinson, Stevenson, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We als","@endWordPosition":"1270","@position":"8292","annotationId":"T45","@startWordPosition":"1267","@citStr":"Mathe et al, 2008"}},"title":{"#tail":"\n","#text":"Learning the abstract motion semantics of verbs from captioned videos."},"booktitle":{"#tail":"\n","#text":"In IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Stefan Mathe"},{"#tail":"\n","#text":"Afsaneh Fazly"},{"#tail":"\n","#text":"Sven Dickinson"},{"#tail":"\n","#text":"Suzanne Stevenson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. 2012. Learning to parse natural language commands to a robot control system. In Proceedings of the 13th International Symposium on Experimental Robotics."},"#text":"\n","marker":{"#tail":"\n","#text":"Matuszek, Herbst, Zettlemoyer, Fox, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"re explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al,","@endWordPosition":"347","@position":"2399","annotationId":"T46","@startWordPosition":"344","@citStr":"Matuszek et al., 2012"},{"#tail":"\n","#text":"ckr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-lingu","@endWordPosition":"831","@position":"5467","annotationId":"T47","@startWordPosition":"828","@citStr":"Matuszek et al, 2012"}]},"title":{"#tail":"\n","#text":"Learning to parse natural language commands to a robot control system."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 13th International Symposium on Experimental Robotics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Cynthia Matuszek"},{"#tail":"\n","#text":"Evan Herbst"},{"#tail":"\n","#text":"Luke Zettlemoyer"},{"#tail":"\n","#text":"Dieter Fox"}]}},{"volume":{"#tail":"\n","#text":"37"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Ken McRae, George S. Cree, Mark S. Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547?559."},"journal":{"#tail":"\n","#text":"Behavior Research Methods,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"McRae, Cree, Seidenberg, McNorgan, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictio","@endWordPosition":"913","@position":"6001","annotationId":"T48","@startWordPosition":"910","@citStr":"McRae et al (2005)"}},"title":{"#tail":"\n","#text":"Semantic feature production norms for a large set of living and nonliving things."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ken McRae"},{"#tail":"\n","#text":"George S Cree"},{"#tail":"\n","#text":"Mark S Seidenberg"},{"#tail":"\n","#text":"Chris McNorgan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Tanvi S. Motwani and Raymond J. Mooney. 2012. Improving video activity recognition using object recognition and text mining. In ECAI, pages 600?605. Douglas L. Nelson, Cathy L. McEvoy, and Thomas A."},"#text":"\n","pages":{"#tail":"\n","#text":"600--605"},"marker":{"#tail":"\n","#text":"Motwani, Mooney, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al, 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unpri","@endWordPosition":"1328","@position":"8649","annotationId":"T49","@startWordPosition":"1325","@citStr":"Motwani and Mooney (2012)"}},"title":{"#tail":"\n","#text":"Improving video activity recognition using object recognition and text mining."},"booktitle":{"#tail":"\n","#text":"In ECAI,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Tanvi S Motwani"},{"#tail":"\n","#text":"Raymond J Mooney"}]}},{"volume":{"#tail":"\n","#text":"36"},"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Schreiber. 2004. The University of South Florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, & Computers, 36(3):402?407."},"journal":{"#tail":"\n","#text":"Behavior Research Methods, Instruments, & Computers,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Schreiber, 2004"},"title":{"#tail":"\n","#text":"The University of South Florida free association, rhyme, and word fragment norms."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Schreiber"}}},{"volume":{"#tail":"\n","#text":"42"},"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Aude Oliva and Antonio Torralba. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42(3):145?175."},"journal":{"#tail":"\n","#text":"International Journal of Computer Vision,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Oliva, Torralba, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elici","@endWordPosition":"749","@position":"4943","annotationId":"T50","@startWordPosition":"746","@citStr":"Oliva and Torralba, 2001"},{"#tail":"\n","#text":" feature space for a classifier, and objects can only be recognize using collections of codewords. To test that similar concepts should share similar visual codewords, we cluster the BoVW representations for all our images into 500 clusters with kmeans clustering, and represent each word as membership over the image clusters, forming the SURF Clusters modality. The number of clusters is chosen arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al, 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ?gist? of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modalit","@endWordPosition":"2321","@position":"14648","annotationId":"T51","@startWordPosition":"2317","@citStr":"Oliva and Torralba, 2001"}]},"title":{"#tail":"\n","#text":"Modeling the shape of the scene: A holistic representation of the spatial envelope."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Aude Oliva"},{"#tail":"\n","#text":"Antonio Torralba"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Advances in Neural Information Processing Systems, pages 1143?1151."},"#text":"\n","pages":{"#tail":"\n","#text":"1143--1151"},"marker":{"#tail":"\n","#text":"Ordonez, Kulkarni, Berg, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line ","@endWordPosition":"367","@position":"2521","annotationId":"T52","@startWordPosition":"364","@citStr":"Ordonez et al, 2011"}},"title":{"#tail":"\n","#text":"Im2text: Describing images using 1 million captioned photographs."},"booktitle":{"#tail":"\n","#text":"In Advances in Neural Information Processing Systems,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Vicente Ordonez"},{"#tail":"\n","#text":"Girish Kulkarni"},{"#tail":"\n","#text":"Tamara L Berg"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Devi Parikh and Kristen Grauman. 2011. Relative attributes. In International Conference on Computer Vision, pages 503?510. IEEE."},"#text":"\n","pages":{"#tail":"\n","#text":"503--510"},"marker":{"#tail":"\n","#text":"Parikh, Grauman, 2011"},"publisher":{"#tail":"\n","#text":"IEEE."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the ","@endWordPosition":"759","@position":"5003","annotationId":"T53","@startWordPosition":"756","@citStr":"Parikh and Grauman, 2011"}},"title":{"#tail":"\n","#text":"Relative attributes."},"booktitle":{"#tail":"\n","#text":"In International Conference on Computer Vision,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Devi Parikh"},{"#tail":"\n","#text":"Kristen Grauman"}]}},{"volume":{"#tail":"\n","#text":"21"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Friedemann Pulvermu?ller, Olaf Hauk, Vadim V. Nikulin, and Risto J Ilmoniemi. 2005. Functional links between motor and language systems. European Journal of Neuroscience, 21(3):793?797."},"journal":{"#tail":"\n","#text":"European Journal of Neuroscience,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Pulvermuller, Hauk, Nikulin, Ilmoniemi, 2005"},"title":{"#tail":"\n","#text":"Functional links between motor and language systems."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Friedemann Pulvermuller"},{"#tail":"\n","#text":"Olaf Hauk"},{"#tail":"\n","#text":"Vadim V Nikulin"},{"#tail":"\n","#text":"Risto J Ilmoniemi"}]}},{"volume":{"#tail":"\n","#text":"1"},"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. In Transactions of the Association for Computational Linguistics, volume 1, pages 25?36."},"#text":"\n","pages":{"#tail":"\n","#text":"25--36"},"marker":{"#tail":"\n","#text":"Regneri, Rohrbach, Wetzel, Thater, Schiele, Pinkal, 2013"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new ov","@endWordPosition":"1274","@position":"8314","annotationId":"T54","@startWordPosition":"1271","@citStr":"Regneri et al, 2013"}},"title":{"#tail":"\n","#text":"Grounding action descriptions in videos."},"booktitle":{"#tail":"\n","#text":"In Transactions of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michaela Regneri"},{"#tail":"\n","#text":"Marcus Rohrbach"},{"#tail":"\n","#text":"Dominikus Wetzel"},{"#tail":"\n","#text":"Stefan Thater"},{"#tail":"\n","#text":"Bernt Schiele"},{"#tail":"\n","#text":"Manfred Pinkal"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Marcus Rohrbach, Michael Stark, Gyo?rgy Szarvas, Iryna Gurevych, and Bernt Schiele. 2010. What helps where?and why? Semantic relatedness for knowledge transfer. In IEEE Conference on Computer Vision and Pattern Recognition, pages 910?917."},"#text":"\n","pages":{"#tail":"\n","#text":"910--917"},"marker":{"#tail":"\n","#text":"Rohrbach, Stark, Szarvas, Gurevych, Schiele, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text moda","@endWordPosition":"1299","@position":"8460","annotationId":"T55","@startWordPosition":"1296","@citStr":"Rohrbach et al (2010)"}},"title":{"#tail":"\n","#text":"What helps where?and why? Semantic relatedness for knowledge transfer."},"booktitle":{"#tail":"\n","#text":"In IEEE Conference on Computer Vision and Pattern Recognition,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Marcus Rohrbach"},{"#tail":"\n","#text":"Michael Stark"},{"#tail":"\n","#text":"Gyorgy Szarvas"},{"#tail":"\n","#text":"Iryna Gurevych"},{"#tail":"\n","#text":"Bernt Schiele"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Stephen Roller, Michael Speriosu, Sarat Rallapalli, Benjamin Wing, and Jason Baldridge. 2012. Supervised text-based geolocation using language models on an adaptive grid. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1500?1510."},"#text":"\n","pages":{"#tail":"\n","#text":"1500--1510"},"marker":{"#tail":"\n","#text":"Roller, Speriosu, Rallapalli, Wing, Baldridge, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"pply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ","@endWordPosition":"393","@position":"2686","annotationId":"T56","@startWordPosition":"390","@citStr":"Roller et al, 2012"}},"title":{"#tail":"\n","#text":"Supervised text-based geolocation using language models on an adaptive grid."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Stephen Roller"},{"#tail":"\n","#text":"Michael Speriosu"},{"#tail":"\n","#text":"Sarat Rallapalli"},{"#tail":"\n","#text":"Benjamin Wing"},{"#tail":"\n","#text":"Jason Baldridge"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Sabine Schulte im Walde, Susanne Borgwaldt, and Ronny Jauch. 2012. Association norms of german noun compounds. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 632?639, Istanbul, Turkey."},"#text":"\n","pages":{"#tail":"\n","#text":"632--639"},"marker":{"#tail":"\n","#text":"Walde, Borgwaldt, Jauch, 2012"},"location":{"#tail":"\n","#text":"Istanbul, Turkey."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ality For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al, 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens. 3.2 Cognitive Modalities Association Norms (AN) is a collection of association norms collected by Schulte im Walde et al (2012). In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind. With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words. After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types. Feature Norms (FN) is our new collection of feature norms for a group of 569 German nouns. We present subjects on Amazon Mechanical Turk with a cue noun and ask them to give","@endWordPosition":"1502","@position":"9655","annotationId":"T57","@startWordPosition":"1499","@citStr":"Walde et al (2012)"}},"title":{"#tail":"\n","#text":"Association norms of german noun compounds."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 8th International Conference on Language Resources and Evaluation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sabine Schulte im Walde"},{"#tail":"\n","#text":"Susanne Borgwaldt"},{"#tail":"\n","#text":"Ronny Jauch"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"D. Sculley. 2010. Web-scale k-means clustering. In Proceedings of the 19th International Conference on World Wide Web, pages 1177?1178."},"#text":"\n","pages":{"#tail":"\n","#text":"1177--1178"},"marker":{"#tail":"\n","#text":"Sculley, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"After the collection of all the images, we extracted simple, low-level computer vision features to use as modalities in our experiments. First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (Bay et al, 2008). SURF is a method for selecting points-of-interest within an image. It is faster and more forgiving than the commonly known SIFT algorithm. We compute SURF keypoints for every image in our data set using SimpleCV3 and randomly sample 1% of the keypoints. The keypoints are clustered into 5,000 visual codewords (centroids) using k-means clustering (Sculley, 2010), and images are then quantized over the 5,000 codewords. All images for a given word are summed together to provide an average representation for the word. We refer to this representation as the SURF modality. While this is a standard, basic BoVW model, each individual codeword on its own may not provide a large degree of semantic information; typically a BoVW representation acts predominantly as a feature space for a classifier, and objects can only be recognize using collections of codewords. To test that similar concepts should share similar visual codewords, we cluster the BoVW representa","@endWordPosition":"2153","@position":"13621","annotationId":"T58","@startWordPosition":"2152","@citStr":"Sculley, 2010"}},"title":{"#tail":"\n","#text":"Web-scale k-means clustering."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 19th International Conference on World Wide Web,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Sculley"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423?1433, Jeju Island, Korea, July."},"#text":"\n","pages":{"#tail":"\n","#text":"1423--1433"},"marker":{"#tail":"\n","#text":"Silberer, Lapata, 2012"},"location":{"#tail":"\n","#text":"Jeju Island, Korea,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"en and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model ","@endWordPosition":"430","@position":"2942","annotationId":"T59","@startWordPosition":"427","@citStr":"Silberer and Lapata, 2012"},{"#tail":"\n","#text":"ction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model","@endWordPosition":"1036","@position":"6836","annotationId":"T60","@startWordPosition":"1033","@citStr":"Silberer and Lapata (2012)"},{"#tail":"\n","#text":"ctors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al, 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al, 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al, 2003), or LDA, is an unsuper","@endWordPosition":"2500","@position":"15763","annotationId":"T61","@startWordPosition":"2497","@citStr":"Silberer and Lapata, 2012"},{"#tail":"\n","#text":"ve trials of each modality for 2000 iterations for K = {50, 100, 150, 200, 250} (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features (Andrews et al, 2009; Silberer and Lapata, 2012). We also see that the SURF visual words are able to provide notable, albeit not significant, improvements over the text-only modality. This confirms that the low-level BoVW features do carry semantic information, and are useful to consider individually. The GIST vectors, on the other hand, perform almost exactly the same as the text-only model. These features, which are usually more useful for comparing overall image likeness than object likeness, do not individually contain semantic information useful for compositionality prediction. The performance of the visual modalities reverses when we ","@endWordPosition":"4590","@position":"28274","annotationId":"T62","@startWordPosition":"4587","@citStr":"Silberer and Lapata, 2012"}]},"title":{"#tail":"\n","#text":"Grounded models of semantic representation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Carina Silberer"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2013. Models of semantic representation with visual attributes. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August."},"#text":"\n","marker":{"#tail":"\n","#text":"Silberer, Ferrari, Lapata, 2013"},"location":{"#tail":"\n","#text":"Sofia, Bulgaria,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"fforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by 1146 Andrews et al (","@endWordPosition":"446","@position":"3028","annotationId":"T63","@startWordPosition":"443","@citStr":"Silberer et al, 2013"},{"#tail":"\n","#text":"create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying ","@endWordPosition":"1222","@position":"7992","annotationId":"T64","@startWordPosition":"1219","@citStr":"Silberer et al (2013)"},{"#tail":"\n","#text":" we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al, 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al, 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al, 2003), or LDA, is an unsupervised Bayesian probabilistic model of text documents. It assumes that all documents are probabilistically generated from a shared set ofK common","@endWordPosition":"2526","@position":"15907","annotationId":"T65","@startWordPosition":"2523","@citStr":"Silberer et al, 2013"}]},"title":{"#tail":"\n","#text":"Models of semantic representation with visual attributes."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Carina Silberer"},{"#tail":"\n","#text":"Vittorio Ferrari"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D. Manning, and Andrew Y. Ng. 2013. Zero-shot learning through cross-modal transfer. International Conference on Learning Representations."},"#text":"\n","marker":{"#tail":"\n","#text":"Socher, Ganjoo, Sridhar, Bastani, Manning, Ng, 2013"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"l at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a la","@endWordPosition":"1304","@position":"8484","annotationId":"T66","@startWordPosition":"1301","@citStr":"Socher et al (2013)"}},"title":{"#tail":"\n","#text":"Zero-shot learning through cross-modal transfer."},"booktitle":{"#tail":"\n","#text":"International Conference on Learning Representations."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Richard Socher"},{"#tail":"\n","#text":"Milind Ganjoo"},{"#tail":"\n","#text":"Hamsa Sridhar"},{"#tail":"\n","#text":"Osbert Bastani"},{"#tail":"\n","#text":"Christopher D Manning"},{"#tail":"\n","#text":"Andrew Y Ng"}]}},{"volume":{"#tail":"\n","#text":"133"},"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Mark Steyvers. 2010. Combining feature norms and text data with topic models. Acta Psychologica, 133(3):234?243. Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J."},"journal":{"#tail":"\n","#text":"Acta Psychologica,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Steyvers, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ble actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational model","@endWordPosition":"418","@position":"2872","annotationId":"T67","@startWordPosition":"417","@citStr":"Steyvers, 2010"},{"#tail":"\n","#text":"ed to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over th","@endWordPosition":"993","@position":"6538","annotationId":"T68","@startWordPosition":"992","@citStr":"Steyvers (2010)"}]},"title":{"#tail":"\n","#text":"Combining feature norms and text data with topic models."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Mark Steyvers"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Teller, and Nicholas Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. Proceedings of the 25th AAAI Conference on Artificial Intelligence. Marco Tettamanti, Giovanni Buccino, Maria Cristina Saccuman, Vittorio Gallese, Massimo Danna, Paola Scifo, Ferruccio Fazio, Giacomo Rizzolatti, Stefano F."},"#text":"\n","marker":{"#tail":"\n","#text":"Teller, Roy, 2011"},"location":{"#tail":"\n","#text":"Marco Tettamanti, Giovanni Buccino, Maria Cristina Saccuman, Vittorio Gallese, Massimo Danna, Paola Scifo, Ferruccio Fazio, Giacomo Rizzolatti, Stefano F."},"title":{"#tail":"\n","#text":"Understanding natural language commands for robotic navigation and mobile manipulation."},"booktitle":{"#tail":"\n","#text":"Proceedings of the 25th AAAI Conference on Artificial Intelligence."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Teller"},{"#tail":"\n","#text":"Nicholas Roy"}]}},{"volume":{"#tail":"\n","#text":"17"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Cappa, and Daniela Perani. 2005. Listening to actionrelated sentences activates fronto-parietal motor circuits. Journal of Cognitive Neuroscience, 17(2):273? 281."},"journal":{"#tail":"\n","#text":"Journal of Cognitive Neuroscience,"},"#text":"\n","pages":{"#tail":"\n","#text":"281"},"issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Cappa, Perani, 2005"},"title":{"#tail":"\n","#text":"Listening to actionrelated sentences activates fronto-parietal motor circuits."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Cappa"},{"#tail":"\n","#text":"Daniela Perani"}]}},{"volume":{"#tail":"\n","#text":"39"},"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Luis Von Ahn. 2006. Games with a purpose. Computer, 39(6):92?94."},"journal":{"#tail":"\n","#text":"Computer,"},"#text":"\n","issue":{"#tail":"\n","#text":"6"},"marker":{"#tail":"\n","#text":"Von Ahn, 2006"},"title":{"#tail":"\n","#text":"Games with a purpose."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Luis Von Ahn"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Claudia von der Heide and Susanne Borgwaldt. 2009. Assoziationen zu Unter-, Basis- und Oberbegriffen. Eine explorative Studie. In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium, pages 51? 74."},"#text":"\n","pages":{"#tail":"\n","#text":"51--74"},"marker":{"#tail":"\n","#text":"von der Heide, Borgwaldt, 2009"},"title":{"#tail":"\n","#text":"Assoziationen zu Unter-, Basis- und Oberbegriffen. Eine explorative Studie."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Claudia von der Heide"},{"#tail":"\n","#text":"Susanne Borgwaldt"}]}},{"volume":{"#tail":"\n","#text":"11"},"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Benjamin Wing and Jason Baldridge. 2011. Simple supervised document geolocation with geodesic grids. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 11, pages 955?964."},"#text":"\n","pages":{"#tail":"\n","#text":"955--964"},"marker":{"#tail":"\n","#text":"Wing, Baldridge, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"roaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional","@endWordPosition":"389","@position":"2665","annotationId":"T69","@startWordPosition":"386","@citStr":"Wing and Baldridge, 2011"}},"title":{"#tail":"\n","#text":"Simple supervised document geolocation with geodesic grids."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Benjamin Wing"},{"#tail":"\n","#text":"Jason Baldridge"}]}}]}}]}}
