phrasing into a Linguistic Steganography system, by using paraphrases to hide information in a cover text. We propose automatically generated paraphrases as a new and useful source of transformations for Linguistic Steganography, and show that our method for checking paraphrases is effective at maintaining a high level of imperceptibility, which is crucial for effective steganography. 1 Introduction Steganography is concerned with hiding information in some cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer (Fridrich, 2009). The covert communication is such that the very act of communication is to be kept secret from outside observers. A related area is Watermarking, in which modifications are made to a cover medium in order to identify it, for example for the purposes of copyright. Here the changes may be known to an observer, and the task is to make the changes in such a way that the watermark cannot easily be removed. There is a large literature on image steganography and watermarking, in which images are modified to encode a hidden message or watermark. Image stegosystems exploit the redundancy in an image r
que. Since the difference between 11111111 and 11111110 in the value for red/green/blue intensity is likely to be undetectable by the human eye, the LSB can be used to hide information other than colour, without being perceptable by a human observer.1 A key question for any steganography system is the choice of cover medium. Given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider. However, the literature on Linguistic Steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media (Bergmair, 2007). The likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer. Language has the property that even small local changes to a text, e.g. replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world. Hence finding linguistic transformations which can be applied reliably and often is a challenging problem for Linguistic Steganography. In this paper we focus on steganography rather than watermarking, since we are interested in 
ramework, and for readers unfamiliar with linguistic steganography shows how linguistic transformations can be used to embed hidden bits in text. Section 2 describes some of the previous transformations used in Linguistic Steganography. Note that we are concerned with transformations which are 2The message may have been encrypted initially also, as in the figure, but this is not important in this paper; the key point is that the hidden message is a sequence of bits. linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words (Por et al, 2008). Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits. One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts. Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the
sed to embed hidden bits in text. Section 2 describes some of the previous transformations used in Linguistic Steganography. Note that we are concerned with transformations which are 2The message may have been encrypted initially also, as in the figure, but this is not important in this paper; the key point is that the hidden message is a sequence of bits. linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words (Por et al, 2008). Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits. One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts. Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text. In order to test the grammaticality and meaning preserving nature of a paraphrase, we employ a 
e Google ngram corpus. This technique is based on the simple hypothesis that, if the paraphrase in context has been used many times before on the web, then it is an appropriate use. We test our n-gram-based system against some human judgements of the grammaticality of paraphrases in context. We find that using larger contexts leads to a high precision system (100% when using 5-grams), but at the cost of a reduced recall. This precision-recall tradeoff reflects the inherent tradeoff between imperceptibility and payload in a Linguistic Steganography system. We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned 592 the same CCG lexical categories by the parser. This method increases the precision of the Google n-gram check with a slight loss in recall. A contribution of this paper is to advertise the Linguistic Steganography problem to the ACL community. The requirement that any linguistic transformation maintain the grammaticality and meaning of the cover text makes the problem a strong test for existing NLP technology. 2 Previous Work 2.1 Synonym Substitution The simplest and most straightforward subliminal mo
by the parser. This method increases the precision of the Google n-gram check with a slight loss in recall. A contribution of this paper is to advertise the Linguistic Steganography problem to the ACL community. The requirement that any linguistic transformation maintain the grammaticality and meaning of the cover text makes the problem a strong test for existing NLP technology. 2 Previous Work 2.1 Synonym Substitution The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms. The first lexical substitution method was proposed by Chapman and Davida (1997). Later works, such as Atallah et al (2001a), Bolshakov (2004), Taskiran et al (2006) and Topkara et al (2006b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method. Taskiran et al (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym. Topkara et al (2005) and Topkara et al (2006b) report an average embedding capacity of 0.67 bits per sen
on of the Google n-gram check with a slight loss in recall. A contribution of this paper is to advertise the Linguistic Steganography problem to the ACL community. The requirement that any linguistic transformation maintain the grammaticality and meaning of the cover text makes the problem a strong test for existing NLP technology. 2 Previous Work 2.1 Synonym Substitution The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms. The first lexical substitution method was proposed by Chapman and Davida (1997). Later works, such as Atallah et al (2001a), Bolshakov (2004), Taskiran et al (2006) and Topkara et al (2006b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method. Taskiran et al (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym. Topkara et al (2005) and Topkara et al (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.
 check with a slight loss in recall. A contribution of this paper is to advertise the Linguistic Steganography problem to the ACL community. The requirement that any linguistic transformation maintain the grammaticality and meaning of the cover text makes the problem a strong test for existing NLP technology. 2 Previous Work 2.1 Synonym Substitution The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms. The first lexical substitution method was proposed by Chapman and Davida (1997). Later works, such as Atallah et al (2001a), Bolshakov (2004), Taskiran et al (2006) and Topkara et al (2006b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method. Taskiran et al (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym. Topkara et al (2005) and Topkara et al (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method. 2.2 Syntactic Trans
ibution of this paper is to advertise the Linguistic Steganography problem to the ACL community. The requirement that any linguistic transformation maintain the grammaticality and meaning of the cover text makes the problem a strong test for existing NLP technology. 2 Previous Work 2.1 Synonym Substitution The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms. The first lexical substitution method was proposed by Chapman and Davida (1997). Later works, such as Atallah et al (2001a), Bolshakov (2004), Taskiran et al (2006) and Topkara et al (2006b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method. Taskiran et al (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym. Topkara et al (2005) and Topkara et al (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method. 2.2 Syntactic Transformations The second and the most widely used 
 based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting. The first syntactic transformation method is presented by Atallah et al (2001a). Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. Liu et al (2005), Meral et al (2007), Murphy (2001), Murphy and Vogel (2007) and Topkara et al (2006a) all belong to the syntactic transformation category. After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools. Atallah et al. (2001b) and Topkara et al (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method. 2.3 Semantic Transformations The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the curr
 that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting. The first syntactic transformation method is presented by Atallah et al (2001a). Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. Liu et al (2005), Meral et al (2007), Murphy (2001), Murphy and Vogel (2007) and Topkara et al (2006a) all belong to the syntactic transformation category. After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools. Atallah et al. (2001b) and Topkara et al (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method. 2.3 Semantic Transformations The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art 
 be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting. The first syntactic transformation method is presented by Atallah et al (2001a). Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. Liu et al (2005), Meral et al (2007), Murphy (2001), Murphy and Vogel (2007) and Topkara et al (2006a) all belong to the syntactic transformation category. After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools. Atallah et al. (2001b) and Topkara et al (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method. 2.3 Semantic Transformations The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art for NLP technol
 into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting. The first syntactic transformation method is presented by Atallah et al (2001a). Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. Liu et al (2005), Meral et al (2007), Murphy (2001), Murphy and Vogel (2007) and Topkara et al (2006a) all belong to the syntactic transformation category. After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools. Atallah et al. (2001b) and Topkara et al (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method. 2.3 Semantic Transformations The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art for NLP technology. It requires some sop
ally equivalent syntactic structure, using transformations such as passivization, topicalization and clefting. The first syntactic transformation method is presented by Atallah et al (2001a). Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. Liu et al (2005), Meral et al (2007), Murphy (2001), Murphy and Vogel (2007) and Topkara et al (2006a) all belong to the syntactic transformation category. After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools. Atallah et al. (2001b) and Topkara et al (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method. 2.3 Semantic Transformations The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art for NLP technology. It requires some sophisticated tools and kno
) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. Liu et al (2005), Meral et al (2007), Murphy (2001), Murphy and Vogel (2007) and Topkara et al (2006a) all belong to the syntactic transformation category. After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools. Atallah et al. (2001b) and Topkara et al (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method. 2.3 Semantic Transformations The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art for NLP technology. It requires some sophisticated tools and knowledge to model natural language semantics. Atallah et al (2002) used semantic transformations and embed information in textmeaning representation (TMR) trees of the text by either pruning, grafting or substituting the tr
sentence with the syntactic transformation method. 2.3 Semantic Transformations The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art for NLP technology. It requires some sophisticated tools and knowledge to model natural language semantics. Atallah et al (2002) used semantic transformations and embed information in textmeaning representation (TMR) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources. Vybornova and Macq (2007) aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing the meaning of a sentence. 3 Data Resources 3.1 Paraphrase Dictionary The cover text used for our experiments consists of newspaper sentences from Section 00 of the Penn Treebank (Marcus et al, 1993). Hence we require possible paraphrases for phrases that occur in Section 00. The paraphrase dictionary that we use was generated for us by Chris Callison-Burch, using the technique described in Callison-Burch (2008), which explo
sources. Vybornova and Macq (2007) aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing the meaning of a sentence. 3 Data Resources 3.1 Paraphrase Dictionary The cover text used for our experiments consists of newspaper sentences from Section 00 of the Penn Treebank (Marcus et al, 1993). Hence we require possible paraphrases for phrases that occur in Section 00. The paraphrase dictionary that we use was generated for us by Chris Callison-Burch, using the technique described in Callison-Burch (2008), which exploits a parallel corpus and methods developed for statistical machine translation. Table 1 gives summary statistics of the paraphrase dictionary and its coverage on Section 00 of the Penn Treebank. The length of the extracted n-gram phrases ranges from unigrams to five-grams. The coverage figure gives the percentage of sentences which have at least one phrase in the dictionary. The coverage is important for us because it determines the payload capacity of the embedding method described in Section 5. Table 2 lists some examples 5-gram phrases and paraphrases from the dictionary. The 
. Each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here. The examples show that, while some of the paraphrases are of a high quality, some are not. For example, differences is unlikely to be a suitable paraphrase for a number of people in any context. Moreover, there are some ?phrase, paraphrase? pairs which are only suitable in particular contexts. For example, year end is an unsuitable paraphrase for the end of this year in the sentence The chart compares the gold price at the end of last year with the end of this year. Barzilay and McKeown (2001) also note that the applicability of paraphrases is strongly influenced by context. Section 4 describes our method for determining if a paraphrase is suitable in a given context. 3.2 Google N-gram Data The Google n-gram data was collected by Google Research for statistical language modelling, and has been used for many tasks such as lexical disambiguation (Bergsma et al, 2009), and contains English n-grams and their observed frequency counts, for counts of at least 40. The striking feature of Figure 2: The web-based annotation system the n-gram corpus is the large number of n-grams and the siz
only suitable in particular contexts. For example, year end is an unsuitable paraphrase for the end of this year in the sentence The chart compares the gold price at the end of last year with the end of this year. Barzilay and McKeown (2001) also note that the applicability of paraphrases is strongly influenced by context. Section 4 describes our method for determining if a paraphrase is suitable in a given context. 3.2 Google N-gram Data The Google n-gram data was collected by Google Research for statistical language modelling, and has been used for many tasks such as lexical disambiguation (Bergsma et al, 2009), and contains English n-grams and their observed frequency counts, for counts of at least 40. The striking feature of Figure 2: The web-based annotation system the n-gram corpus is the large number of n-grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of English text on publicly accessible Web pages collected in January 2006. For example, the 5-gram phrase the part that you were has a count of 103. The compressed data is around 24 GB on disk. 3.3 Paraphrase Judgement Corpus The focus of the paper is to develop an automatic system for checking 
ccount, namely W 23 . However, If the paraphrase P ? fails the current (n, C) check the checking procedure will terminate and report that the paraphrase fails. In contrast, if the paraphrase passes all the (n, C) checks where C = 1 to maxC, the procedure determines the paraphrase as acceptable. What is happening is that an ngram window is effectively being shifted across the paraphrase boundary to include different amounts of context and paraphrase. 4.2 Syntactic Filter In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method. We use the Clark and Curran (2007) CCG parser to analyse the sentence before and after paraphrasing. Combinatory Categorial Grammar (CCG) is a lexicalised grammar formalism, in which CCG lexical categories ? typically expressing subcategorisation information ? are assigned to each word in a sentence. The grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing. If there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical. Hence the grammar check is at the word, rathe
