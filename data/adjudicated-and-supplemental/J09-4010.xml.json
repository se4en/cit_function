{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.6143078","#text":"\n? Faculty of Information Technology, Monash University, Wellington Road, Clayton, Victoria 3800,\nAustralia. Currently employed at Pacific Brands Services Group, Building 10, 658 Church St, Richmond,\nVictoria 3121, Australia. E-mail: yuvalmarom@gmail.com.\n?? Faculty of Information Technology, Monash University, Wellington Road, Clayton, Victoria 3800,\nAustralia. E-mail: Ingrid.Zukerman@infotech.monash.edu.au.\n"},{"#tail":"\n","@confidence":"0.959714","#text":"\nandDocument Prediction.\n3.1.1 Document Retrieval (Doc-Ret). This method follows a traditional Information Re-\ntrieval paradigm (Salton and McGill 1983), where a query is represented by the content\nterms it contains, and the system retrieves from the corpus a set of documents that\nbest match this query. In our case, the query is a new request e-mail to be addressed\nby the system, and we have considered three views of the documents in the corpus:\n(1) previous response e-mails, (2) previous request e-mails, or (3) previous request?\nresponse pairs. The first alternative corresponds to the more traditional view of retrieval\n"},{"#tail":"\n","@confidence":"0.99912675","#text":"\n1. Calculate the scores of the sentences in the predicted SCs.\n2. Remove redundant sentences from cohesive SCs; these are SCs which\ncontain similar sentences.\n3. Calculate the confidence of the generated response.\n"},{"#tail":"\n","@confidence":"0.837831833333333","#text":"\ndependent. Table 3 summarizes the thresholds required for the different methods, the\n10 We also experimented with the retention of sentences with high F-scores and with different weights for\nrecall and precision. Our results were insensitive to these variations.\n11 This change of technique had a practical motivation: As seen in Section 3.2.2, we generated many\npredictive models for sentence clusters. This process could be automated with SVMs, but would have to\nbe done manually if Decision Graphs had been used.\n"},{"#tail":"\n","@confidence":"0.774065","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\n4. Automatic Evaluation of Individual Methods\n"},{"#tail":"\n","@confidence":"0.40690725","#text":"\nconsiders other features that characterize the data sets and the behavior of the response\ngeneration methods.\n12 The topics of the omitted data sets were: servers, laptops specializing in EVO notebooks, desktops, and\nmiscellaneous.\n"},{"#tail":"\n","@confidence":"0.647102","#text":"\n(c) precision.\n"},{"#tail":"\n","@confidence":"0.774466","#text":"\n Max75: As Max50, but with w = 0.75.\n Weighted50: Use the weighted alternative for estimating performance\n(Equation (11)), and w = 0.5 in Equation (12).\n Weighted75: As Weighted50, but with w = 0.75.\nWe also devised the following baselines to help ground our results.\n Random: Select between the methods randomly.\n Gold50: Select between the methods based on their actual performance (as\nopposed to their estimated performance), using w = 0.5 in Equation (12).\n Gold75: As Gold50, but with w = 0.75.\n"}],"figure":[{"#tail":"\n","@confidence":"0.937635714285714","#text":"\nComputational Linguistics Volume 35, Number 4\nTable 2\nSummary of the response-generation methods.\nMethod Implementation Features\nDoc-Ret Cosine similarity bag of lemmas (TF.IDF)\nDoc-Pred Clustering: Snob bag of lemmas (binary)\nClassification: Decision Graphs lemma unigrams and\nbigrams (binary)\nSent-Ret Recall bag of lemmas (TF.IDF)\nSent-Pred Clustering: Snob bag of lemmas (binary)\nClassification: SVMs lemma unigrams and\nbigrams (binary)\nSent-Hybrid Combine sentence prediction\nwith sentence retrieval\n"},{"#tail":"\n","@confidence":"0.752457666666667","#text":"\nComputational Linguistics Volume 35, Number 4\nTable 4\nDetails of the data sets included in our experiments, and overview of results per data set.\nData Topic Sub-category # of Best method (coverage/\nset no. dialogues performance)\nRetrieval Prediction\n1 hand helds general 268 Doc-Ret (lo/lo) NONE\n2 hand helds DG models 1,160 Doc-Ret (med/lo) SENTENCE (med/hi)\n3 product replacement 1,236 Doc-Ret (lo/hi) ALL (hi/hi)\n4 laptops Armada models 561 Doc-Ret (med/lo) NONE\n5 laptops general 305 Doc-Ret (med/lo) SENTENCE (lo/hi)\n6 laptops merged (7 clusters) 632 Doc-Ret (med/lo) NONE\n7 desktops merged (7 clusters) 389 Doc-Ret (med/lo) Sent-Pred (lo/hi)\n8 misc. merged (10 clusters) 353 Doc-Ret (med/lo) Sent-Hybrid (med/lo)\nTOTAL 4,904\n"},{"#tail":"\n","@confidence":"0.4205972","#text":"\n(8)\nF-score =\n2? Precision? Recall\nPrecision+ Recall\n(9)\n"},{"#tail":"\n","@confidence":"0.935100777777778","#text":"\nF-score =\n{\nw\nPrecision\n+\n1? w\nRecall\n}?1\n(12)\n"}],"author":[{"#tail":"\n","@confidence":"0.974489","#text":"\nYuval Marom?\n"},{"#tail":"\n","@confidence":"0.804856","#text":"\nIngrid Zukerman??\n"}],"equation":[{"#tail":"\n","@confidence":"0.844926","#text":"\n(1)\n"},{"#tail":"\n","@confidence":"0.9996162","#text":"\nScore(sj) =\nm\n?\ni=1\nPr(SCi)? Pr(sj|SCi) (2)\n"},{"#tail":"\n","@confidence":"0.869337636363636","#text":"\nPr(SCi) =\n{\nPrecision(SCi) if SCi is very cohesive and predicted with high probability\n0 otherwise\n(3)\nwhere\n Precision(SCi) is defined as follows.\nPrecision(SCi) =\n# of times SCi was correctly predicted\n# of times SCi was predicted\n(4)\n"},{"#tail":"\n","@confidence":"0.9996495","#text":"\nCohesion(SC) = 1\nN\nN\n?\nk=1\n[ Pr(wk ? SC) ? ? ? Pr(wk ? SC) ? 1? ? ] (5)\n"},{"#tail":"\n","@confidence":"0.793414","#text":"\nConfidence = # of usable SCs\n# of possible SCs\n(6)\nwhere\n usable SCs are those that satisfy the cohesion and prediction probability\n"},{"#tail":"\n","@confidence":"0.964517142857143","#text":"\n Max:\n?pk = p\n(i? )\nk\nand ?rk = r\n(i? )\nk\n, for i? = argmaxi=1,...,NPr(ci|x) (10)\nwhere p\n(i)\nk\nand r\n(i)\nk\n"},{"#tail":"\n","@confidence":"0.8331774375","#text":"\n Weighted:\n?pk =\nN\n?\ni=1\nPr(ci|x)? p\n(i)\nk\nand ?rk =\nN\n?\ni=1\nPr(ci|x)? r\n(i)\nk\n(11)\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.982269","#text":"\n3.1 Document-Level Methods\n"},{"#tail":"\n","@confidence":"0.991967","#text":"\n3.2 Sentence-Level Methods\n"},{"#tail":"\n","@confidence":"0.99684","#text":"\n3.3 Summary\n"},{"#tail":"\n","@confidence":"0.991594","#text":"\n4.1 The Corpus\n"},{"#tail":"\n","@confidence":"0.734197","#text":"\n4.2 Experimental Set-Up\n"},{"#tail":"\n","@confidence":"0.805496","#text":"\n4.3 Results\n"},{"#tail":"\n","@confidence":"0.995232","#text":"\n4.4 Summary\n"},{"#tail":"\n","@confidence":"0.913786","#text":"\n5.1 Experimental Set-Up\n"},{"#tail":"\n","@confidence":"0.578316","#text":"\n5.2 Results\n"},{"#tail":"\n","@confidence":"0.996532","#text":"\n5.3 Summary\n"},{"#tail":"\n","@confidence":"0.998109","#text":"\n6.1 Training\n"},{"#tail":"\n","@confidence":"0.999564","#text":"\n6.2 Prediction\n"},{"#tail":"\n","@confidence":"0.998654","#text":"\n6.3 Method Selection\n"},{"#tail":"\n","@confidence":"0.979192","#text":"\n7.1 Experimental Set-Up\n"},{"#tail":"\n","@confidence":"0.586322","#text":"\n7.2 Results\n"},{"#tail":"\n","@confidence":"0.985555","#text":"\n7.3 Summary\n"}],"subsubsectionHeader":[{"#tail":"\n","@confidence":"0.860624","#text":"\n3.1.2 Document Prediction (Doc-Pred). The Doc-Ret method may fail in situations where\n"},{"#tail":"\n","@confidence":"0.625843","#text":"\n3.2.1 Sentence Retrieval (Sent-Ret). As we saw in Section 2 (Figure 1(b)), there are situa-\n"},{"#tail":"\n","@confidence":"0.922052","#text":"\n3.2.2 Sentence Prediction (Sent-Pred). As for the Doc-Pred method, the Sent-Pred method\n"},{"#tail":"\n","@confidence":"0.886093","#text":"\n3.2.3 Sentence Prediction?Retrieval Hybrid (Sent-Hybrid). As seen in cluster SC2 in Fig-\n"}],"footnote":[{"#tail":"\n","@confidence":"0.6515555","#text":"\n1 http ://customercare.telephonyonline.com/ar/telecom next generation customer.\nSubmission received: 7 November 2007; revised submission received: 20 March 2009; accepted for publication:\n"},{"#tail":"\n","@confidence":"0.985314","#text":"\n2 http://trec.nist.gov/pubs/trec15/t15 proceedings.html and\nhttp://www-nlpir.nist.gov/projects/duc/data.html.\n"},{"#tail":"\n","@confidence":"0.4620165","#text":"\n3 The examples shown in this article are reproduced verbatim from the corpus (except for URLs and phone\nnumbers which have been disguised by us), and some have user or operator errors.\n"},{"#tail":"\n","@confidence":"0.76800625","#text":"\ntemplate response e-mails (Doc-Pred).\n4 We used a binary representation, rather than a representation based on TF.IDF scores, because important\ndomain-related words, such as monitor and network, are actually quite frequent. Thus, their low TF.IDF\nscore may have an adverse influence on clustering performance. Nonetheless, in the future, it may be\nworth investigating a TF.IDF-based representation.\n5 Significant bigrams are obtained using the n-gram statistics package NSP (Banerjee and Pedersen 2003),\nwhich offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram\n(that it is not a collocation).\n"},{"#tail":"\n","@confidence":"0.5645714","#text":"\n1. The dialogues contain exactly two turns: a request e-mail followed by a\nresponse e-mail. These dialogues represent situations where a request can\nbe resolved with a single response.\n2. The response e-mails are reasonably concise (15 lines at most). This\nrestriction is based on the observation that longer responses are quite\n"},{"#tail":"\n","@confidence":"0.7912074","#text":"\n1. The first set contained responses generated by Doc-Pred and Sent-Hybrid.\nThese two methods obtained similar precision values in the automatic\nevaluation (Table 5), so we wanted to compare how they would fare with\nour judges.\n2. The second set contained responses generated by Sent-Pred and\n"}],"title":{"#tail":"\n","@confidence":"0.935476666666667","#text":"\nAn Empirical Study of Corpus-Based\nResponse Automation Methods for an\nE-mail-Based Help-Desk Domain\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.9309525","#text":"\nBanerjee, S. and T. Pedersen. 2003. The\ndesign, implementation, and use of\n"},{"#tail":"\n","@confidence":"0.999509134453782","#text":"\nComputational Linguistics Volume 35, Number 4\nthe Ngram Statistics Package. In\nCICLing 2003 ? Proceedings of the Fourth\nInternational Conference on Intelligent Text\nProcessing and Computational Linguistics,\npages 370?381, Mexico City. Mexico.\nBarr, A. and S. Tessler. 1995. Expert systems:\nA technology before its time. AI Expert,\navailable at www.stanford.edu/group/\nscip/avsgt/expertsystems/aiexpert.html.\nBarzilay, R., N. Elhadad, and K. R. McKeown.\n2001. Sentence ordering in multidocument\nsummarization. In HLT01 ? Proceedings of\nthe First Human Language Technology\nConference, pages 1?7, San Diego, CA.\nBarzilay, R. and K. R. McKeown. 2005.\nSentence fusion for multidocument news\nsummarization. Computational Linguistics,\n31(3):297?328.\nBerger, A., R. Caruana, D. Cohn, D. Freitag,\nand V. Mittal. 2000. Bridging the lexical\nchasm: Statistical approaches to\nanswer-finding. In SIGIR?00 ? Proceedings\nof the 23rd Annual International ACM\nInternational Conference on Research and\nDevelopment in Information Retrieval,\npages 192?199, Athens.\nBerger, A. and V. Mittal. 2000. Query-\nrelevant summarization using FAQs.\nIn ACL2000 ? Proceedings of the 38th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 294?301,\nHong Kong.\nBickel, S. and T. Scheffer. 2004. Learning\nfrom message pairs for automatic email\nanswering. In ECML04 ? Proceedings of the\nEuropean Conference on Machine Learning,\npages 87?98, Pisa.\nBurke, R. 2002. Hybrid recommender\nsystems. User Modeling and User-Adapted\nInteraction, 12(4):331?370.\nCarletta, J. 1996. Assessing agreement on\nclassification tasks: The Kappa statistic.\nComputational Linguistics, 22(2):249?254.\nCarmel, D., M. Shtalhaim, and A. Soffer.\n2000. eResponder: Electronic question\nresponder. In CoopIS?02 ? Proceedings\nof the 7th International Conference on\nCooperative Information Systems,\npages 150?161, Eilat.\nChang, C. C. and C. J. Lin, 2001. LIBSVM: A\nLibrary for Support Vector Machines.\nSoftware available at http://www.csie\n.ntu.edu.tw/?cjlin/libsvm.\nChu-Carroll, J., K. Czuba, J. M. Prager, and\nA. Ittycheriah. 2003. In question\nanswering, two heads are better than\none. In HLT-NAACL 2003 ? Proceedings\nof the 2003 Language Technology\nConference, pages 24?31, Edmonton.\nDalli, A., Y. Xia, and Y. Wilks. 2004. Adaptive\ninformation management: FASiL email\nsummarization system. In COLING?04 ?\nProceedings of the 20th International\nConference on Computational Linguistics,\npages 23?27, Geneva.\nDelic, K. A. and D. Lahaix. 1998.\nKnowledge harvesting, articulation, and\ndelivery. The Hewlett-Packard Journal,\nMay:74?81.\nFeng, D., E. Shaw, J. Kim, and E. Hovy.\n2006. An intelligent discussion-bot\nfor answering student queries in\nthreaded discussions. In IUI?06 ?\nProceedings of the 11th International\nConference on Intelligent User Interfaces,\npages 171?177, Sydney.\nFilatova, E. and V. Hatzivassiloglou. 2004.\nEvent-based extractive summarization.\nIn Proceedings of the ACL?04 Workshop\non Summarization, pages 104?111,\nBarcelona.\nGoldstein, J., V. Mittal, J. Carbonell, and\nM. Kantrowitz. 2000. Multi-document\nsummarization by sentence extraction.\nIn Proceedings of the ANLP/NAACL 2000\nWorkshop on Automatic Summarization,\npages 40?48, Seattle, WA.\nJijkoun, V. and M. de Rijke. 2005.\nRetrieving answers from frequently\nasked questions pages on the Web. In\nCIKM?05 ? Proceedings of the ACM 14th\nConference on Information and Knowledge\nManagement, pages 76?83, Bremen.\nLapalme, G. and L. Kosseim. 2003. Mercure:\nTowards an automatic e-mail follow-up\nsystem. IEEE Computational Intelligence\nBulletin, 2(1):14?18.\nLekakos, G. and G. M. Giaglis. 2007. A\nhybrid approach for improving\npredictive accuracy of collaborative\nfiltering algorithms. User Modeling\nand User-Adapted Interaction,\n17(1):5?40.\nLeuski, A., R. Patel, D. Traum, and\nB. Kennedy. 2006. Building effective\nquestion answering characters. In\nProceedings of the 7th SIGdial Workshop on\nDiscourse and Dialogue, pages 18?27,\nSydney.\nLin, C. Y. and E. H. Hovy. 2003. Automatic\nevaluation of summaries using n-gram\nco-occurrence statistics. In HLT-NAACL\n2003 ? Proceedings of the 2003 Language\nTechnology Conference, pages 71?78,\nEdmonton.\nMalik, R., L. V. Subramaniam, and\nS. Kaushik. 2007. Automatically\nselecting answer templates to respond to\n"},{"#tail":"\n","@confidence":"0.999528786516854","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\ncustomer emails. In IJCAI?07 ? Proceedings\nof the 20th International Joint Conference on\nArtificial Intelligence, pages 1659?1664,\nHyderabad.\nMarom, Y. and I. Zukerman. 2006.\nAutomating help-desk responses:\nA comparative study of\ninformation-gathering approaches.\nIn Proceedings of the COLING-ACL\nWorkshop on Task-Focused Summarization\nand Question Answering, pages 40?47,\nSydney.\nMarom, Y. and I. Zukerman. 2007a.\nEvaluation of a large-scale email response\nsystem. In Proceedings of the IJCAI?07\nWorkshop on Knowledge and Reasoning in\nPractical Dialogue Systems, pages 28?33,\nHyderabad.\nMarom, Y. and I. Zukerman. 2007b. A\npredictive approach to help-desk response\ngeneration. In IJCAI?07 ? Proceedings of the\n20th International Joint Conference on\nArtificial Intelligence, pages 1665?1670,\nHyderabad.\nMarom, Y., I. Zukerman, and N. Japkowicz.\n2007. A meta-learning approach for\nselecting between response automation\nstrategies in a help-desk domain. In\nAAAI-07 ? Proceedings of the 22nd\nConference on Artificial Intelligence,\npages 907?912, Vancouver.\nMolla?, D. and J. L. Vicedo. 2007. Question\nanswering in restricted domains: An\noverview. Computational Linguistics,\n33(1):41?61.\nOliver, J. J. 1993. Decision graphs?an\nextension of decision trees. In Proceedings\nof the 4th International Workshop on Artificial\nIntelligence and Statistics, pages 343?350,\nFort Lauderdale, FL.\nRotaru, M. and D. J. Litman. 2005.\nImproving question answering for\nreading comprehension tests by\ncombining multiple systems. In Proceedings\nof the AAAI 2005 Workshop on Question\nAnswering in Restricted Domains,\npages 46?50, Pittsburgh, PA.\nRoy, S. and L. V. Subramaniam. 2006.\nAutomatic generation of domain\nmodels for call-centers from noisy\ntranscriptions. In COLING-ACL?06 ?\nProceedings of the 21st International\nConference on Computational Linguistics and\n44th Annual Meeting of the Association for\nComputational Linguistics, pages 737?744,\nSydney.\nSalton, G. and M. J. McGill. 1983. An\nIntroduction to Modern Information\nRetrieval. McGraw Hill, New York.\nShrestha, L. and K. R. McKeown. 2004.\nDetection of question-answer pairs in\nemail conversations. In COLING?04 ?\nProceedings of the 20th International\nConference on Computational Linguistics,\npages 889?895, Geneva.\nSoricut, R. and E. Brill. 2006. Automatic\nquestion answering using the Web:\nBeyond the factoid. Information Retrieval,\n9(2):191?206.\nvan Rijsbergen, C. J. 1979. Information\nRetrieval. Buttersworth, London.\nVapnik, V. N. 1998. Statistical Learning Theory.\nWiley-Interscience, New York.\nWallace, C. S. 2005. Statistical and Inductive\nInference by Minimum Message Length.\nSpringer, Berlin.\nWallace, C. S. and D. M. Boulton. 1968. An\ninformation measure for classification. The\nComputer Journal, 11(2):185?194.\nWatson, I. 1997. Applying Case-Based\nReasoning: Techniques for Enterprise\nSystems. Morgan Kaufmann Publishers,\nSan Mateo, CA.\nWitten, I. H. and E. Frank. 2000. Data Mining:\nPractical Machine Learning Tools and\nTechniques with Java Implementations.\nMorgan Kaufmann Publishers, San\nFrancisco, CA.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.960380833333333","#text":"\nThis article presents an investigation of corpus-based methods for the automation of help-desk\ne-mail responses. Specifically, we investigate this problem along two operational dimensions:\n(1) information-gathering technique, and (2) granularity of the information. We consider two\ninformation-gathering techniques (retrieval and prediction) applied to information represented\nat two levels of granularity (document-level and sentence-level). Document-level methods corre-\nspond to the reuse of an existing response e-mail to address new requests. Sentence-level methods\ncorrespond to applying extractive multi-document summarization techniques to collate units of\ninformation from more than one e-mail. Evaluation of the performance of the different methods\nshows that in combination they are able to successfully automate the generation of responses\nfor a substantial portion of e-mail requests in our corpus. We also investigate a meta-selection\nprocess that learns to choose one method to address a new inquiry e-mail, thus providing a unified\nresponse automation solution.\n"},{"#tail":"\n","@confidence":"0.9974193","#text":"\nE-mail inquiries sent to help desks often ?revolve around a small set of common ques-\ntions and issues.?1 This means that help-desk operators spendmost of their time dealing\nwith problems that have been previously addressed. Further, a significant proportion of\nhelp-desk responses contain a low level of technical content, addressing, for example,\ninquiries sent to the wrong group, or requests containing insufficient detail about the\ncustomer?s problem. Organizations and clients would benefit if an automated process\nwas employed to deal with the easier problems, and the efforts of human operators\nwere focused on difficult, atypical problems.\nHowever, even the automation of responses to the ?easy? problems is a difficult\ntask. Although such inquiries revolve around a relatively small set of issues, specific\n"},{"#tail":"\n","@confidence":"0.980838071428572","#text":"\n? 2009 Association for Computational Linguistics\nComputational Linguistics Volume 35, Number 4\ncircumstances can make each inquiry unique, and hence care must be taken to compose\na response that does not confuse, irritate, or mislead the customer. It is therefore no\nsurprise that early attempts at response automation were knowledge-driven (Barr and\nTessler 1995; Watson 1997; Delic and Lahaix 1998). These systems were carefully de-\nsigned to produce relevant and correct responses, but required significant human input\nand maintenance (Delic and Lahaix 1998).\nIn recent times, such knowledge-intensive approaches to content delivery have\nbeen largely superseded by data-intensive, statistical approaches. An outcome of the\nrecent proliferation of statistical approaches, in particular in recommender systems\nand search engines, is that people have become accustomed to responses that are not\nprecisely tailored to their queries. This indicates that help-desk customers may have\nalso become more tolerant of inaccurate or incomplete automatically generated replies,\nprovided these replies are still relevant to their problem, and so long as the customers\ncan follow up with a request for human-generated responses if necessary. Despite this,\nto date, there has been little work on corpus-based approaches to help-desk response\nautomation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme and\nKosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). A\nmajor factor limiting this work is the dearth of corpora?help-desk e-mails tend to be\nproprietary and are subject to privacy issues. Further, this application lacks the kind of\nbenchmark data sets that are used in question-answering and text summarization.2\nIn this article, we report on our experiments with corpus-based techniques for the\nautomation of help-desk responses. Our study is based on a large corpus of request?\nresponse e-mail dialogues between customers and operators at Hewlett-Packard. Ob-\nservations from this corpus have led us to consider several methods that implement\ndifferent types of corpus-based strategies. Specifically, we have investigated two types\nof methods (retrieval and prediction) applied at two levels of granularity (document\nand sentence). In this article, we present these methods and compare their performance.\nA key issue in the generation of help-desk responses is the ability to determine when an\nautomatically generated response for a particular query can be sent to a user, and when\nthe query should be passed to an operator. In Section 3, we employ method-specific,\nempirically determined, applicability thresholds to make this decision; in Section 6,\nwe propose a meta-level process for selecting a response-generation method, which\nobviates the need for these thresholds.\nThe rest of the article is organized as follows. In the next section, we discuss\nproperties of the help-desk domain. In Section 3, we describe our response-generation\nmethods. An automatic evaluation of these methods is presented in Section 4, and a\nsmall user-based evaluation in Section 5. In Section 6, we describe themeta-level process\nwhich learns to select between the different methods, and evaluate its performance in\nSection 7. Related work is discussed in Section 8, and concluding remarks are presented\nin Section 9.\n"},{"#tail":"\n","@confidence":"0.984066","#text":"\nThe help-desk domain offers interesting challenges to response automation in that, on\none hand, responses are generalized to fit standard solutions, and on the other hand,\nresponses are tailored to the initiating request in order to meet specific customer needs.\n"},{"#tail":"\n","@confidence":"0.947212","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nFigure 1\nSample responses from our corpus: (a) mixed generic?specific, (b) specific, and (c) generic.\nFor example, the first sentence of the response in Figure 1(a) is tailored to the user?s\nrequest, whereas the rest of the response is generic, and may be used when replying\nto other queries.3 In addition to responses that contain such a mixture of specific and\ngeneric information, there are inquiries that warrant very specific or completely generic\nresponses, as seen in Figures 1(b) and 1(c), respectively.\nA distinctive feature of the help-desk domain is that help-desk e-mail responses\ncontain a high level of repetition and redundancy. This may be attributed to commonal-\nities in customer issues combined with the provision of in-house manuals to help-desk\noperators. These manuals connect particular topics with standard response templates,\nprescribe a particular presentation style, and even suggest specific responses to certain\nqueries. For example, Figure 2 shows two rather different response e-mails which share\na sentence (italicized). Thus, having access to these manuals would enable us to easily\nidentify prescribed sentences. More importantly, it would enable us to determine the\ncontext in which these sentences are used, which in turn would allow us to postulate\nadditional response sentences. An interesting avenue of investigation would involve\nadapting our approach to help-desk situations where such manuals are accessible.\n"},{"#tail":"\n","@confidence":"0.99229280952381","#text":"\nComputational Linguistics Volume 35, Number 4\nIf you are able to see the Internet then it sounds like it is working, you may want to get in\ntouch with your IT department to see if you need to make any changes to your settings to\nget it to work. Try performing a soft reset by pressing the stylus pen in the small hole on the bottom\nleft hand side of the Ipaq and then release.\nI would recommend doing a soft reset by pressing the stylus pen in the small hole on the left hand\nside of the Ipaq and then release. Then charge the unit overnight to make sure it has been long\nenough and then see what happens. If the battery is not charging then the unit will need to\nbe sent in for repair.\nFigure 2\nSample responses that share a sentence.\nDespite the high degree of repetition in help-desk responses, the specific issues\nraised by different customers imply that the responses sent to these customers contain\nvarying degrees of overlap (rather than being identical). Hence, providing a response\nfor a new request may involve reusing an existing response in its entirety, putting\ntogether parts of responses that match individual components of the request, or com-\nposing a completely new response. This suggests that different response-generation\nstrategies may be suitable, depending on the content of the initiating request and how\nwell it matches previous requests or responses. In our work, we focus on the first two\nof these situations, where either complete existing responses or parts of responses are\nreused to address a new request.\nThe example in Figure 1(b) illustrates a situation where specific words in the request\n(docking station and install) are also mentioned in the response. This situation suggests\na response-automation approach that follows the document retrieval paradigm (Salton\nand McGill 1983), where a new request is matched with existing response documents\n(e-mails). However, specific words in the request do not always match a response well,\nand sometimes do not match a response at all, as demonstrated by the examples in\nFigures 1(a) and 1(c), respectively.\nSometimes requests match each other quite well, suggesting an approach where a\nnew request is matched with an old one, and the corresponding response is reused.\nHowever, analysis of our corpus shows that this does not occur very often, because\nunlike response e-mails, request e-mails exhibit a high language variability: There are\nmany customers who write these e-mails, and they differ in their background, level\nof expertise, and pattern of language usage. Further, there are many requests that raise\nmultiple issues, hence matching a new request e-mail in its entirety is often not possible.\nIn situations where requests do not match existing responses or other requests,\nit may be possible instead to find correlations between requests and responses. For\nexample, the generic portion of the response in Figure 1(a), and the entire response\nin Figure 1(c), may be repeated for many different kinds of requests. If repeated suffi-\nciently, entire responses or parts of responses will be strongly correlated with particular\ncombinations of request words, such as send, battery and replace in Figure 1(a), and\nfirewall, ip, and network in Figure 1(c).\n"},{"#tail":"\n","@confidence":"0.999564333333333","#text":"\nThe properties of the help-desk domain outlined in the previous section have motivated\nus to study the response-automation task along two dimensions. The first dimension\npertains to the strategy applied to determine the information in a response, and the\n"},{"#tail":"\n","@confidence":"0.979673684210527","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nsecond dimension pertains to the granularity of the information. We implemented two\nalternative strategies. The first is a retrieval strategy that attempts to match a new\nrequest with previous requests and responses, and the second is a prediction strategy\nthat looks for correlations between requests and responses in order to predict a response\nfor a new request. For both types of strategies, we considered two levels of granularity\nfor a unit of information: document and sentence.\nWe implemented four methods according to the two alternatives for each dimen-\nsion: Document Retrieval, Document Prediction, Sentence Retrieval, and Sentence\nPrediction. We also implemented a fifth method for addressing situations such as the\nexample shown in Figure 1(a), which warrant a mixed response that has both a generic\ncomponent and a component tailored to the user?s request. This is a hybrid prediction?\nretrieval method implemented at the sentence level: Sentence Prediction?Retrieval Hy-\nbrid. These five methods are summarized in Table 2 (Section 3.3). The implementation\nof these methods relies on the judicious selection of thresholds for different aspects of\nthe processes in question. In principle, machine learning techniques could be used to\ndetermine optimal threshold values. However, owing to practical considerations, we\nselected these values by trial and error. Table 3 shows the range of values we tried for\nthese thresholds, and the values we selected (Section 3.3).\n"},{"#tail":"\n","@confidence":"0.9975088","#text":"\nA document-level method attempts to reuse an existing response document (e-mail)\nin its entirety. Unlike sentence-based methods that attempt to put together portions of\ndifferent responses (Section 3.2), a document-level approach avoids issues pertaining to\nthe coherence and completeness of a response, as a response composed by a help-desk\noperator is likely to be both coherent and complete. Therefore, if a particular request\ncan be addressed with a single existing response document, then a document reuse\napproach would be preferred. An important capability of a response-generation system\nis to be able to determine when such an approach is appropriate, and when there is\ninsufficient evidence to reuse a complete response document.\nAs stated herein, we studied two document-based methods: Document Retrieval\n"},{"#tail":"\n","@confidence":"0.9900384","#text":"\nas applied in question-answering tasks, where the terms in the question are matched\nto those in the answer documents. We consider the second alternative in order to\naddress situations such as the example in Figure 1(c), where a request might not match\na particular response, but it may match another request, yielding the response to that\nrequest. The third alternative addresses situations where a new request matches part of\nanother request and part of its response.\nWe use cosine similarity (between a request e-mail and each document in the\ncorpus) to determine a retrieval score, and pick the document with the highest score.\nThe similarity is calculated using a bag-of-lemmas representation with TF.IDF (term-\nfrequency-inverse-document frequency) weightings (Salton andMcGill 1983), but in the\n"},{"#tail":"\n","@confidence":"0.994497657142857","#text":"\nComputational Linguistics Volume 35, Number 4\nrequest-to-response option we use TF = 1, as it yields the best results. We posit that this\nhappens because a response to a request does not necessarily contain multiple instances\nof request terms. Hence, what is important when matching a request to a response is the\nnumber of (significant) terms in common, rather than their frequency. In contrast, when\nmatching a request to a request, or a request to a request?response pair, term frequency\nwould be more indicative of the goodness of the match, as the document also has a\nrequest component.\nWe consider retrieval to be successful only if the similarity score is higher than\nan applicability threshold, which is currently set empirically (Table 3). If retrieval is\nsuccessful, then the response associated with the retrieved document is reused to reply\nto the user?s request.\nWe carried out a preliminary experiment in order to compare the three variants\nof the Doc-Ret method. The evaluation is performed by considering each request\ne-mail in turn, removing it and its response from the corpus, carrying out the retrieval\nprocess, and then comparing the retrieved response with the actual response (if\nthere are several similar responses in the corpus, an appropriate response can still\nbe retrieved). The results of this experiment are shown in Table 1. The first column\nshows which document retrieval variant is being evaluated. The second column shows\nthe proportion of requests for which one or more documents were retrieved (using\nour applicability threshold). We see that matching on requests yields more retrieved\ndocuments than matching on responses, and that matching on request?response\npairs yields even more retrieved documents. For the cases where retrieval took place,\nwe used F-score (van Rijsbergen 1979; Salton and McGill 1983) to determine the\nsimilarity between the response from the top-ranked document and the real response\n(the formulas for F-score and its contributing factors, recall and precision, appear in\nSection 4.2). The third column in Table 1 shows the proportion of requests for which\nthis similarity is non-zero. Again, the third variant (matching on request?response\npairs) retrieves the highest proportion of responses that bear some similarity to the real\nresponses. The fourth column shows the average similarity between the top retrieved\nresponse and the real response for the cases where retrieval took place. Here too the\nthird variant yields the best similarity score (0.52).\nFrom this preliminary experiment it appears that the third document retrieval\nvariant is superior. Hence, we use this variant as the Doc-Ret method in subsequent\nexperiments.\n"},{"#tail":"\n","@confidence":"0.988811666666667","#text":"\nthe presence or absence of some terms in the requests triggers a generic template\nresponse. For instance, in the example in Figure 1(c), given the terms firewall and CP-2W,\nTable 1\nComparison between the three document retrieval variants.\nMatch type Percent Percent retrieved Average similarity\nretrievals docs with sim > 0 for retrieved docs\nrequest to response 11% 11% 0.40\nrequest to request 37% 26% 0.50\nrequest to request?response 43% 32% 0.52\n"},{"#tail":"\n","@confidence":"0.991483342857143","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nwe would like to retrieve the generated response. However, the first Doc-Ret alernative\nwould fail, as the response has no common terms with the request. The other two Doc-\nRet alernatives would fail if different requests in the corpus mention different issues\nabout these two terms, and thus no single request or request?response document in the\ncorpus would yield a good match with a new request that mentions these terms.\nIn order to handle such cases, we offer a predictive approach, which is guided by\ncorrelations between terms, rather thanmatches. In principle we could look for direct cor-\nrelations between request terms and response terms. However, since we have observed\nstrong regularities in the responses at the document level, we decided to reduce the\ndimensionality of the problem by abstracting the response documents and then looking\nfor correlations at this higher level. This approach did not seem profitable for request\ne-mails, which unlike responses, have a high language variability. Hence, we keep their\nrepresentation at a low level of abstraction (bag-of-lemmas).\nThe idea behind the Doc-Pred method is similar to Bickel and Scheffer?s (2004):\nResponse documents are grouped into clusters, one of these clusters is predicted for\na new request on the basis of the request?s features, and the response that is most\nrepresentative of the predicted cluster (closest to the centroid) is selected. In our case,\nthe clustering is performed by the program Snob, which implements mixture model-\ning combined with model selection based on the Minimum Message Length (MML)\ncriterion (Wallace and Boulton 1968; Wallace 2005). We chose this program because the\nnumber of clusters does not have to be specified in advance, and it returns a probabilistic\ninterpretation for its clusters (this interpretation is used by the Sent-Pred method,\nSection 3.2.2). The input to Snob is a set of binary vectors, one vector per response\ndocument. The values of a vector correspond to the presence or absence of each (lem-\nmatized) corpus word in the document in question (after removing stop-words and\nwords with very low frequency).4 The predictive model is a Decision Graph (Oliver\n1993), which, like Snob, is based on the MML principle. The Decision Graph is trained\non unigram and bigram lemmas in the request as input features,5 and the identifier\nof the response cluster that contains the actual response for the request as the target\nfeature. The model predicts which response cluster is most suitable for a given re-\nquest, and returns the probability that this prediction is correct. This probability is\nour indicator of whether the Doc-Pred method can address a new request. As for the\nDoc-Ret method, an applicability threshold for this parameter is currently determined\nempirically (Table 3).\n"},{"#tail":"\n","@confidence":"0.954269666666667","#text":"\nThe document-level methods presented in the previous section are designed to address\nsituations where requests are sufficiently specific to strongly match a previous request\nor response e-mail (Doc-Ret), or requests contain terms that are predictive of complete\n"},{"#tail":"\n","@confidence":"0.993077047619048","#text":"\nComputational Linguistics Volume 35, Number 4\nAs discussed in Section 2, there are situations that cannot be addressed by a\ndocument-level approach, because requests only predict or match portions of responses.\nAn alternative approach is to look for promising sentences from one or more previous\nresponses, and collate them into a new response. This task can be cast as extractive\nmulti-document summarization. Unlike a document reuse approach, sentence-level\napproaches need to consider issues of discourse coherence in order to ensure that the\nextracted combination of sentences is coherent or at least understandable. In our work,\nwe gather sets of sentences, and assume (but do not employ) existing approaches for\ntheir organization (Goldstein et al 2000; Barzilay, Elhadad, andMcKeown 2001; Barzilay\nand McKeown 2005).\nThe appeal of a sentence-level approach is that it supports the generation of a ?com-\nbination response? in situations where there is insufficient evidence for a single docu-\nment containing a full response, but there is enough evidence for parts of responses.\nAlthough such a combined response is generally less satisfactory than a full response,\nthe information included in it may address a user?s problem or point the user in the\nright direction. As argued in the Introduction, when it comes to obtaining information\nquickly on-line, this option may be preferable to having to wait for a human-generated\nresponse. In contrast, the document-level approach is an all-or-nothing approach: If\nthere is insufficient evidence for a complete response, then no automated response is\ngenerated.\n"},{"#tail":"\n","@confidence":"0.933080125","#text":"\ntions where the terms in response sentences match well the terms in request sentences.\nTo address these situations, we consider the Sent-Ret method, which employs retrieval\ntechniques as for the Doc-Ret method, but at the sentence level.\nThere are two main differences between Sent-Ret and Doc-Ret: (1) in Sent-Ret we\nlook for response sentences that match individual request sentences, rather than entire\ndocuments; and (2) in Sent-Ret we perform recall-based retrieval, rather than retrieval\nbased on cosine-similarity, where the request sentence is the reference document for\nthe recalled terms. The second difference is a result of the first, as a good candidate for\nsentence retrieval contains terms that appear in a request sentence, but is also likely to\ncontain additional terms that expand on the matching terms (Figure 1(b)). Thus, recall\nis calculated for each response sentence with respect to each request sentence as follows\n(since TF=1 yielded the best result for the request?response option in Doc-Ret, we also\nuse it for Sent-Ret).\nrecall =\nsum of TF.IDF of lemmas in request sentence & response sentence\nsum of TF.IDF of lemmas in request sentence\n"},{"#tail":"\n","@confidence":"0.991368333333333","#text":"\nWe retain the response sentences whose recall exceeds an empirically determined ap-\nplicability threshold (Table 3), and produce a response if at least one sentence was\nretained.\n"},{"#tail":"\n","@confidence":"0.957434","#text":"\nstarts by abstracting the responses. It clusters response sentences using the same\n"},{"#tail":"\n","@confidence":"0.975572181818182","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nFigure 3\nA fictitious example that demonstrates the Sent-Pred and Sent-Hybrid methods.\nclustering program (Snob) and bag-of-lemmas representation as Doc-Pred.6 Unlike the\nDoc-Predmethod, where only a single response cluster is predicted (resulting in a single\nresponse document being selected), the Sent-Pred method may predict several promis-\ning Sentence Clusters (SCs). A response is then composed by extracting sentences from\nthe predicted SCs.\nTo illustrate these ideas, consider the fictitious example in Figure 3, which shows\nthree small SCs (in practice SCs can have tens and even hundreds of sentences). The\nthick arrows correspond to high-confidence predictions, while the thin arrows corre-\nspond to sentence selections. The other components of the diagram demonstrate the\nworkings of the Sent-Hybridmethod (Section 3.2.3). In this example, three of the request\nterms, repair, faulty and monitor, result in a confident prediction of two SCs: SC1 and SC2.\nThe sentences in SC1 are identical, so we can arbitrarily select a sentence for inclusion\nin the generated response. The sentences in SC2 are similar but not identical, hence we\nare less confident in arbitrarily selecting a sentence from SC2, and may select more than\none sentence (see the subsequent discussion on removing redundant sentences).\nWe use a Support Vector Machine (SVM) with a Radial Basis Function kernel to\npredict SCs from users? requests.7 A separate SVM is trained for each SC, with unigram\nand bigram lemmas in a request as input features, and a binary target feature specifying\nwhether the SC contains a sentence from the response to this request. During the\n"},{"#tail":"\n","@confidence":"0.614239333333333","#text":"\nnumber of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006),\nbut the simple binary bag-of-lemmas representation yielded similar results.\n7 We employed the LIBSVM package (Chang and Lin 2001).\n"},{"#tail":"\n","@confidence":"0.560393","#text":"\nComputational Linguistics Volume 35, Number 4\nprediction stage, the SVMs predict zero or more SCs for each request, as shown in\n"},{"#tail":"\n","@confidence":"0.5919925","#text":"\nCalculating the score of a sentence. The score of each sentence sj is calculated using the\nfollowing formula.\n"},{"#tail":"\n","@confidence":"0.975344","#text":"\nwhere m is the number of SCs, Pr(sj|SCi) is the probability that sj appears in SCi\n(obtained from Snob), and Pr(SCi) is approximated as follows.\n"},{"#tail":"\n","@confidence":"0.984145857142857","#text":"\nThis measure, which reflects the reliability of an SVM that was trained\nto predict SCi, is obtained by performing 10-fold cross-validation on the\nperformance of this SVM for the training data.\n An SC is cohesive if the sentences in it are similar to each other. This\nmeans that it is possible to obtain a sentence that represents the cluster\nadequately (this is not the case for an uncohesive SC). The cohesion of\nan SC is calculated as follows.\n"},{"#tail":"\n","@confidence":"0.8226525","#text":"\nwhere N is the number of content lemmas under consideration;\nPr(wk ? SC) is the probability that (lemmatized) word wk appears in\n"},{"#tail":"\n","@confidence":"0.9833005","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nthe SC (this probability is obtained from the centroid9); and ? is an\nempirically determined threshold that establishes an upper and lower\nbound for this probability (Table 3). For values of ? close to zero,\nEquation (5) behaves like entropy, in the sense that it favors extreme\nprobabilities. It implements the idea that a cohesive group of sentences\nshould agree strongly on both the words that appear in these sentences\nand the words that are omitted. For example, the italicized sentences in\nFigure 2 belong to an SC with cohesion 0.93, whereas the opening response\nsentence in Figure 1(b) belongs to an SC that contains diverse sentences\n(about the Rompaq power management) and has cohesion 0.7. We employ\nan empirically determined SC-cohesion threshold (Table 3) to determine\nwhether an SC is sufficiently cohesive for redundant sentences to be safely\nremoved from it.\n A high-probability prediction is one where for the request in question, the\nSVM has predicted the SC with a probability that exceeds an empirically\ndetermined threshold (Table 3).\nTo illustrate the distinction between prediction probability and SVM reliability\n(precision), let us return to the request in Figure 3. SC1 is predicted with high prob-\nability for this request, because SC1 includes sentences from responses to many other\nrequests that contain the words repair and faulty. The word monitor also contributes\nto this high prediction probability, but not as strongly as repair and faulty. This is\nbecause SC1 includes sentences from responses whose requests mention different faulty\nproducts, for example, monitor, printer, or notebook. However, if there are more cases of\nfaulty monitors in the corpus than other faulty products, then requests about repairing\nmonitors will have a higher prediction probability than requests about repairing other\nproducts. In contrast to prediction probability, SVM reliability reflects its overall per-\nformance (on the training data), and is independent of particular requests. Thus, the\nSVM for SC1 has a higher reliability than that for SC3, because it is easier for an SVM\nto learn when SC1 is appropriate (predominantly from the presence of the words faulty\nand repair).\nIn order to ensure the relevance of the generated replies, we have placed tight\nrestrictions on prediction probability and cluster cohesion (Table 3), which cause the\nSent-Pred method to often return partial responses.\nRemoving redundant sentences. After calculating the raw score of each sentence,\nwe use a modified version of the Adaptive Greedy Algorithm by Filatova and\nHatzivassiloglou (2004) to penalize redundant sentences in cohesive clusters. This is\ndone by decrementing the score of a sentence that belongs to an SC for which there is a\nhigher or equal scoring sentence (if there are several highest-scoring sentences, we re-\ntain one sentence as a reference sentence?i.e., its score is not decremented). Specifically,\ngiven a sentence sk in cluster SCl which contains a sentence with a higher or equal score,\nthe contribution of SCl to Score(sk) (= Pr(SCl)? Pr(sk|SCl)) is subtracted from Score(sk).\nAfter applying these penalties, we retain only the sentences whose adjusted score is\ngreater than zero (for a highly cohesive cluster, typically only one sentence remains).\n9 For each feature in the input (i.e., lemmatized words), the centroid of the cluster contains a\nfrequency-based estimate of the probability that an item with this feature value appears in this cluster.\n"},{"#tail":"\n","@confidence":"0.981768285714286","#text":"\nComputational Linguistics Volume 35, Number 4\nCalculating the confidence of an automated response. The calculation of sentence\nscores described previously determines which sentences should be included in an auto-\nmatically generated response. In order to decide whether this response should be used,\nwe need an overall measure of the confidence in it. Our confidence measure aggregates\ninto a single number the values of the attributes used to assign a score to the individual\nsentences in a response, as follows.\n"},{"#tail":"\n","@confidence":"0.998230769230769","#text":"\nthresholds mentioned for sentence scoring, and also satisfy a further\nthreshold for SC precision (Table 3).\n possible SCs are those that satisfy a minimum prediction probability\nthreshold (Table 3).\nThis measure combines our confidence in the SCs selected to generate response sen-\ntences with the completeness of the resultant response. Confidence is represented by\nthe thresholds employed to select suitable SCs; and completeness is represented by the\nratio of the number of SCs that were deemed suitable, and the number of SCs that could\npossibly be used to generate a response. These are SCs whose prediction probability is\ngreater than 0 (i.e., there is some evidence in the corpus for their use in the generation\nof a response sentence for the current request). We also use a minimal applicability\nthreshold of 0 for the confidence measure (Table 3). This threshold reflects our notion\nthat a partial response, even a response with one sentence, may still be useful.\n"},{"#tail":"\n","@confidence":"0.99915345","#text":"\nure 3, it is possible for an SC to be strongly predicted without it being sufficiently\ncohesive for a confident selection of a representative sentence. However, sometimes\nthe ambiguity can be resolved through cues in the request. In this example, one of the\nsentences in SC2 matches the request terms better than the other sentences, as it contains\nthe word monitor. In order to capture such situations, we combine prediction confidence\nwith retrieval score to guide sentence selection (as for Sent-Pred, we use a recall measure\nwith TF = 1; the values for the thresholds mentioned herein appear in Table 3).\n For highly cohesive SCs predicted with high confidence, we select\nrepresentative sentences as described in Section 3.2.2.\n For SCs with medium cohesion that were predicted with high confidence,\nwe attempt to match the candidate response sentences with the request\nsentences. We can use a liberal (low) recall threshold here, because the\nhigh prediction confidence guarantees that the sentences in the cluster\nare suitable for the request, so there is no need for a conservative (high)\nrecall threshold. The role of retrieval in this situation is to select the\nsentence whose content terms best match the request, regardless of how\ngood the match is. For instance, in the example in Figure 3, the sentence in\nSC2 that best matches the request only matches on one word (monitor), but\nthis is sufficient to distinguish the winning sentence from the other\nsentences in SC2.\n"},{"#tail":"\n","@confidence":"0.988172629629629","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\n For uncohesive clusters or clusters predicted with low confidence, we can\nrely only on retrieval. Now we must use a more conservative recall\nthreshold to ensure that only sentences that are a good match for the\nrequest sentences are included in the response. SC3 in Figure 3 is an\nexample of an SC for which there is insufficient evidence to form strong\ncorrelations between it and request terms. However, one of its sentences is\na very good match for the second sentence in the request. In fact, all the\ncontent lemmas in that request sentence are matched, resulting in a perfect\nrecall score of 1.0 (the non-matching words are stop words), which means\nthat this response sentence is likely to be informative.\nOnce we have a set of candidate response sentences that satisfy the appropriate\nrecall thresholds, we remove redundant sentences as follows. Redundant sentences are\nremoved from cohesive clusters as described in Section 3.2.2; for SCs with medium\ncohesion, we retain the sentence with the highest recall;10 and for uncohesive SCs, we\nretain all the sentences. The rationale for this policy is that the sentences in an SC with\nmedium cohesion are sufficiently similar to each other, so the selection of more than one\nsentence may introduce redundancy. In contrast, sentences that belong to uncohesive\nSCs are deemed sufficiently dissimilar to each other, so we can select all the sentences\nthat satisfy the recall criterion.\nAs for Sent-Pred, the Sent-Hybrid method produces a response if it can return\nat least one response sentence. This happens when (a) the confidence in the highly\ncohesive SCs exceeds an applicability threshold; or (b) the confidence in one of the SCs\nwith medium or low cohesion exceeds an applicability threshold, and the number of\nsentences retrieved for this SC exceeds an applicability threshold. As for Sent-Pred,\nconfidence is calculated using Equation (6). Both applicability thresholds (confidence\nand number of retrieved sentences) are set to 0 (Table 3).\n"},{"#tail":"\n","@confidence":"0.976303333333333","#text":"\nThe focus of our work is on the general applicability of the different response automa-\ntion methods, rather than on comparing the performance of particular implementa-\ntion techniques. Hence, throughout the course of this project, the different methods\nhad minor implementational variations, which do not affect the overall insights of\nthis research. Specifically, we used Decision Graphs (Oliver 1993) for Doc-Pred, and\nSVMs (Vapnik 1998) for Sent-Pred.11 Additionally, we used unigrams for clustering\ndocuments and sentences, and unigrams and bigrams for predicting document clusters\nand sentence clusters (Sections 3.1.2 and 3.2.2). Because this variation was uniformly\nimplemented for both approaches, it does not affect their relative performance. These\nmethodological variations are summarized in Table 2.\nAs indicated at the beginning of this section, the implementation of these meth-\nods requires the selection of different thresholds, which are subjective and application\n"},{"#tail":"\n","@confidence":"0.964742166666667","#text":"\nrange of values we considered, and the values we selected. The applicability thresholds\nare boldfaced, and those learned by the meta learning process (Section 6) are indicated\nin the rightmost column (Sent-Ret is not considered by this process owing to its poor\nperformance; see Section 4).\nTable 3\nThresholds for the different response-generation methods.\n"},{"#tail":"\n","@confidence":"0.99379825","#text":"\nIn this section, we offer a comparative evaluation of the response automation methods\npresented in Section 3, where wemeasure the ability of the different methods to address\nthe requests in the corpus. We first describe the data used in our experiments, followed\nby the experimental set-up and results.\n"},{"#tail":"\n","@confidence":"0.9106716","#text":"\nOur initial corpus consisted of 30,000 e-mail dialogues between customers and help-\ndesk operators at Hewlett-Packard. The dialogues deal with a variety of user requests,\nwhich include requests for technical assistance, inquiries about products, and queries\nabout how to return faulty products or parts. To focus our work on simple dialogues,\nwe extracted a sub-corpus that satisfies two conditions:\n"},{"#tail":"\n","@confidence":"0.998926681818182","#text":"\ncomplex?they often address multiple issues or have a strong temporal\nstructure (i.e., a sequence of steps).\nThe resultant sub-corpus consisted of 6,659 dialogues, which deal with a wide range\nof topics. We were hoping to account for significant differences between groups of\ndialogues on the basis of their topic. In addition, there was a practical motivation to\nbreak up this large sub-corpus into smaller chunks for ease of handling. We therefore\napplied Snob to automatically cluster the sub-corpus into separate topic-based data sets.\nThe clustering, which was done using as input the lemmas in the subject line of the\nusers? e-mails, produced 51 data sets, some of which were quite small (11 data sets had\nless than 25 dialogues). Because Snob returns the significant terms in each cluster, we\nmerged the smaller data sets manually according to these terms?a process that yielded\n15 data sets in total, each data set containing between 135 and 1,236 e-mail dialogues.\nOwing to the time limitations of the project, the procedures described in this article\nwere applied to 8 of the 15 data sets, which contain a total of 4,904 dialogues (73.6% of\nthe sub-corpus, and 16.3% of the original corpus). These data sets, which were chosen\non the basis of their coverage of the corpus, are described in Table 4,12 together with a\nqualitative overview of our results, which are discussed in Section 4.3.\nIt is worth noting that theremay be factors other than topic that distinguish between\ndialogues, and cause differences in the performance of response generation methods for\ndifferent types of dialogues. However, these factors are not readily apparent upon initial\nanalysis. Topic-based clustering, which is readily apparent, is a reasonable starting point\nfor distinguishing between different data sets. The analysis presented in Section 4.3\n"},{"#tail":"\n","@confidence":"0.9992397","#text":"\nOur experimental set-up is designed to evaluate the ability of the different response-\ngeneration methods to address unseen request e-mails. In particular, we want to deter-\nmine the applicability of our methods to different situations, namely, whether different\nrequests are addressed only by some methods, or whether there is a significant overlap\nbetween the methods.\nOur evaluation is performed by measuring the quality of the generated responses.\nQuality is a subjective measure, which is best judged by the users of the system (i.e.,\nthe help-desk customers or operators). In Section 5, we discuss the difficulties asso-\nciated with such user studies, and describe a human-based evaluation we conducted\nfor a small subset of the responses generated by our system (Marom and Zukerman\n2007b). However, our more comprehensive evaluation is an automatic one that treats\nthe responses generated by the help-desk operators as model responses, and performs\ntext-based comparisons between the model responses and the automatically generated\nones.\nWe employ 10-fold cross-validation, where we split each data set in the corpus\ninto 10 test sets, each comprising 10% of the e-mail dialogues; the remaining 90% of\nthe dialogues constitute the training set. For each of the cross-validation folds, the\nresponses generated for the requests in the test split are compared against the actual\nresponses generated by help-desk operators for these requests. Although thismethod of\nassessment is less informative than human-based evaluations, it enables us to evaluate\nthe performance of our systemwith substantial amounts of data, and produce represen-\ntative results for a large corpus such as ours.\nWe use two measures from Information Retrieval to determine the quality of an\nautomatically generated response: precision and F-score (van Rijsbergen 1979; Salton\nand McGill 1983). Precision measures how much of the information in an automati-\ncally generated response is correct (i.e., appears in the model response), and F-score\nmeasures the overall similarity between the automatically generated response and\nthe model response. F-score is the harmonic mean of precision and recall, which\nmeasures how much of the information in the model response appears in the gener-\nated response. We consider precision separately because it does not penalize missing\n"},{"#tail":"\n","@confidence":"0.773112666666667","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\ninformation, enabling us to better assess our sentence-based methods. Precision, recall,\nand F-score are calculated as follows using a word-by-word comparison (stop-words\n"},{"#tail":"\n","@confidence":"0.998517785714286","#text":"\nThese measures are applied to responses generated after the thresholds in Table 3\nhave been used to determine the applicability or coverage of each response-generation\nmethod. Recall that the sentence-based methods can generate partial responses, many\nof which contain only one obvious and non-informative sentence, such as Thank you\nfor contacting HP and Thank You, Mike, HP eServices. We have manually excluded such\nresponses from the calculation of coverage, in order to prevent these responses from\nartificially improving this metric for the sentence-based methods. This was done by\nvisually inspecting the sentence clusters created by Snob for these methods, and remov-\ning clusters composed of non-informative sentences such as the above (between four\nand six clusters were removed from each data set in this manner). Once a response is\ndeemed to cover a request, then the full response (including these sentences) is used to\ncalculate its quality. This has a small impact on the results of our evaluation, as a typical\nresponse includes two such sentences (opening and closing), and the average length of\na response is 8.11 sentences.\n"},{"#tail":"\n","@confidence":"0.977695882352941","#text":"\nFigure 4 shows the coverage, average precision, and average F-score of each response-\ngeneration method per data set, where the averages are computed only for requests\nthat are covered by the method in question. For example, the average precision of the\nDoc-Ret method for data set no. 2, 0.39, is calculated over the 59% of the data set that\nwas covered by this method. Table 4 shows data set descriptions and sizes, together\nwith an overview of the best retrieval-based and prediction-based method for each\ndata set (SENTENCE refers to both Sent-Pred and Sent-Hybrid). The best method was\nselected manually on the basis of the results in Figure 4. To this effect, we considered the\nmethods whose coverage exceeds some minimum (e.g., 10%), and chose the method(s)\nwhich could adequately answer the largest number of queries in the data set (based\non coverage combined with F-score and precision). Table 5 presents the coverage and\nunique/best coverage of each method (the percentage of queries covered only by this\nmethod or for which this method produces a better reply than other methods), and the\naverage and standard deviation of the precision and F-score obtained by each method\n(calculated over the requests that are covered).\n13 We also employed sequence-based measures using the ROUGE tool set (Lin and Hovy 2003), with similar\nresults to those obtained with the word-by-word measures.\n"},{"#tail":"\n","@confidence":"0.91918103030303","#text":"\nAs seen in Figure 4 and Tables 4 and 5, there is great variability in coverage\nand performance of the different methods for the different data sets (this result was\nconfirmed with an ANOVA statistical test). Our results support the following specific\nobservations.\n All prediction methods perform well for data set no. 3 (product\nreplacement). The vast majority of the requests in this data set, which\ncomprises 18% of the corpus, ask for a return shipping label to be mailed\nto the customer, so that he or she can return a faulty product. Although\nthese requests often contain detailed product descriptions, the responses\nrarely refer to the actual products, and often contain the generic response\nshown in Figure 5. Thus, the prediction methods are well suited for this\ndata set, as the mention of a shipping label is a strong predictor of a\ngeneric response, either in its entirety (as done by Doc-Pred) or broken up\ninto individual sentences (as done by Sent-Pred or Sent-Hybrid). The\ngeneration of complete responses by Doc-Pred explains its slightly higher\nF-score but lower precision compared to Sent-Pred, as a complete response\nmay include sentences that are not appropriate for the situation at hand.\nAlso note that Doc-Ret has a much lower coverage than the prediction\nmethods for data set no. 3, because each request has precise information\nabout the actual product, so a new request can neither match an old\nrequest (about a different product) nor can it match the generic response.\n No method performs well for data sets no. 1 (hand-helds general),\n4 (laptops general), and 6 (laptops merged). Although Doc-Ret has some\napplicability to these data sets, it has low performance for all of them. This\nindicates that these data sets do not contain sufficient recurring cases for\nDoc-Ret to perform well, nor do they contain sufficient request?response\npairs that support the generation of predictive patterns.\n Sent-Ret performs poorly on all data sets. We postulate that this happens\nbecause the important terms in a request and a response are spread across\nYour request for a return airbill has been received and has been sent for processing. Your\nreplacement airbill will be sent to you via email within 24 hours.\nFigure 5\nA sample response from the product replacement data set (data set no. 3).\n"},{"#tail":"\n","@confidence":"0.971569913043478","#text":"\nComputational Linguistics Volume 35, Number 4\nseveral sentences in the respective documents. Hence, the match between\nindividual response and request sentences paints only a partial (and\ninaccurate) picture, and the aggregation of matching response sentences\ndoes not add up to an appropriate response.\n Overall, there is a tension between coverage and performance, whereby\nhigher coverage yields lower performance, and lower coverage results in\nhigher performance. This tension can be observed for the sentence-based\nprediction methods with respect to data sets no. 2 (hand-held DG models),\n5 (laptops general), 7 (desktops merged), and 8 (miscellaneous merged);\nfor Doc-Pred with respect to data sets no. 2 and 5 (Doc-Pred is not\napplicable to data sets no. 7 and 8); and for Doc-Ret for most data sets\n(nos. 1 and 4?8). This suggests that our applicability thresholds may need\nfurther adjustments in order to strike a better balance between coverage\nand performance (Table 3).\nLet us now examine our results separately for each of the four response-generation\nmethods that perform well.\nDoc-Ret.As seen in Table 5, Doc-Ret uniquely addresses 22% of the requests. However,\nthe performance of this method is quite variable (high standard deviation), which may\nbe due to an overly liberal setting of the applicability threshold (which results in both\npoor and good responses being generated, hence the high variability). Nevertheless,\nthere are some cases where this method uniquely addresses requests quite well. This\nhappens in situations such as that in Figure 1(b), where the initiating request is suffi-\nciently similar to other requests with the same response. In contrast, Doc-Ret would not\nwork well for a request such as that in Figure 1(a), which is quite detailed and specific,\nand hence unlikely to match any other request?response document.\nDoc-Pred. Only about a tenth of the requests covered by Doc-Pred are uniquely ad-\ndressed by this method, but the generated responses are of a fairly high quality, with\nan average precision and F-score of 0.82. As indicated previously, the higher F-score\nand lower precision of Doc-Pred (compared to Sent-Pred) may be explained by the\nfact that the complete responses produced by the Doc-Pred method sometimes contain\nspecific sentences that are inappropriate for the current situation. The rather large stan-\ndard deviation for F-score and precision suggests that Doc-Pred exhibits a somewhat\ninconsistent behavior. This may be explained by Figure 4, which shows that Doc-Pred\nperforms very well on data set no. 3 (product replacement), but not so well on the\nothers (Doc-Pred also has good F-score and precision scores for data set no. 2, but poor\ncoverage).\nSent-Pred. In contrast to Doc-Pred, Sent-Pred can find regularities at the sub-document\nlevel, and generate partial responses, which typically omit inappropriate sentences that\nmay be included by Doc-Pred. As a result, the responses generated by Sent-Pred have a\nconsistently high precision (average 0.94, standard deviation 0.13), but this can be at the\nexpense of recall, which explains the lower F-score (compared to Doc-Pred). Overall,\nthe Sent-Pred method outperforms the other methods in 5% of the cases, where it either\nuniquely addresses requests, or produces responses with a higher F-score than those\ngenerated by other methods. As an example of situations where Sent-Pred outperforms\nall other methods, consider Figure 1(a), where Sent-Pred outputs the response shown\n"},{"#tail":"\n","@confidence":"0.971135114285714","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nFigure 6\nExample showing an appropriate response generated by the Sent-Hybrid method (bottom) that\ndiffers from the model response.\nin this example, but without the first sentence. In other words, the generic portion\nof the response is confidently produced, and the specific portion is left out due to\ninsufficient evidence. This example shows the benefit of a partial response: Although\nthe response does not actually answer the user?s specific question (which would be\ndifficult to automate due to the complex nature of the request), it can potentially save\nthe user valuable time by referring him or her to the appropriate repair service.\nSent-Hybrid. The Sent-Hybrid method extends the Sent-Pred method by performing\nsentence retrieval. Sent-Hybrid?s higher coverage is achieved by the retrieval compo-\nnent, which disambiguates between groups of candidate sentences, thus enabling more\nsentences to be included in a generated response. However, this is at the expense of\nprecision. Although retrieval selects sentences that match closely a given request, these\nsentences can differ from the ?selections? made by a human operator in the model\nresponse. Precision (and hence F-score) penalizes such sentences, even when they are\nmore appropriate than those in the model response. For instance, consider the example\nat the top of Figure 6. The response is quite generic, and is used almost identically\nfor several other requests. The Sent-Hybrid method produces a very similar response,\nshown in the text at the bottom of Figure 6. Only the first sentence differs from the first\nsentence in the model response (the different parts have been italicized). The sentence\nselected by the Sent-Hybrid method, which matches more request words than the first\nsentence in the model response, was chosen from a sentence cluster with medium\ncohesion (Section 3.2.3), which contains sentences that describe different reasons for\nsetting up a repair (the matching word is screen). The rather high standard deviation\nin the precision (and hence F-score) for Sent-Hybrid may be due to these kinds of\nsituations. Nonetheless, this method outperforms the other methods in about 10% of\nthe cases, where it either uniquely addresses requests, or produces responses with a\nhigher F-score than those generated by other methods.\nAll the methods combined. The bottom row of Table 5 shows that all the methods\ntogether have a coverage of 72%, which means that at least one of the methods can\nproduce a non-empty and non-trivial response for 72% of the requests. The combined\nF-score and precision averages are calculated on the basis of the best-performing\nmethod for each request. At first glance, using the best method may appear too lenient,\n"},{"#tail":"\n","@confidence":"0.9585405","#text":"\nComputational Linguistics Volume 35, Number 4\nas in practice, we cannot always automatically select this method in advance. However,\nthese averages also suffer from the fact that in many cases only the Doc-Ret method is\napplicable, but its performance is poor. As mentioned previously, this tension between\ncoverage and performance may be attributed to our empirically determined applicabil-\nity thresholds (Section 3).\n"},{"#tail":"\n","@confidence":"0.991837314285715","#text":"\nWe have investigated the suitability of different response generation methods for the\nhelp-desk task. These methods, which vary significantly in their approach to response\nautomation, cover a wide range of situations that arise in a help-desk corpus.\nIdeally, we would like to explain our results in terms of features of the data sets, so\nthat users of our system can select a single best response-generationmethod on the basis\nof these features. However, with the exception of data set no. 3, no clear set of features\npresents itself to support such a selection. Further, as seen in Table 4, superficial features\nof the data sets, such as topic and size, are not sufficient to characterize the applicability\nand performance of the different methods. These results indicate that (1) there are\ndeeper features of data sets that must be considered in order to select a single suitable\ntechnique; or (2) the selection of a technique does not depend on features of the data\nset itself, but on the spread of situations in the data set. That is, the applicability and\nperformance of a response-generation method depends on the specifics of the situation,\nand different data sets contain a different spread of situations. This second conjecture is\nsupported by the results in Figure 4 and Table 5, which indicate that different methods\nhave pockets of unique applicability in each data set. Of course, this still begs the\nquestion of why different data sets have different spreads of situations. Unfortunately,\nwe do not have an answer to this question, and circumvent it by using themeta-learning\ntechnique described in Section 6, which has the added benefit of obviating the problem\nof selecting applicability thresholds.\nHowever, if one wishes to select a response-generation method without meta-\nlearning, the following considerations can be applied.\n Predictive methods are suitable for situations where there are relatively\nfew responses for many, varied requests. That is, the responses generalize\ncommon answers to a variety of problems. Specifically, Doc-Pred is\nsuitable when this observation applies to complete answers, as is the case\nfor data set no. 3, while sentence prediction methods are appropriate when\nparts of replies generalize common answers to a variety of problems.\nBecause the sentence-based prediction methods outperform Doc-Pred for\nall data sets except no. 3 (where the three prediction methods perform\nsimilarly), we recommend the sentence-based methods. Among these, we\nprefer Sent-Pred due to its high and consistent precision and F-score,\npending a further investigation of Sent-Hybrid.\n Doc-Ret applies to situations where there are specific problems which\nwarrant a specific complete answer.\n"},{"#tail":"\n","@confidence":"0.995899","#text":"\nThe size of our corpus necessitates an automatic evaluation in order to produce mean-\ningful results, especially because we are comparing several methods under a number of\n"},{"#tail":"\n","@confidence":"0.981073681818182","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nexperimental settings. Although our automatic evaluation has yielded useful insights,\nit has two main limitations.\n As we saw in Section 4.3 (Figure 6), appropriate responses are sometimes\npenalized when they do not match precisely the model response.\nHowever, it is often the case that there is not one single appropriate\nresponse to a query, and even a help-desk operator may respond to the\nsame question in different ways on different occasions.\n The relationship between the results obtained by the automatic evaluation\nof the responses generated by our system and people?s assessments of\nthese responses is unclear, in particular for partial responses.\nThese limitations reinforce the notion that automated responses should be assessed on\ntheir own merit, rather than with respect to some model response.\nIn Marom and Zukerman (2007a) we identified several systems that resemble ours\nin that they provide answers to queries. These systems addressed the evaluation issue\nas follows.\n Only qualitative observations of the responses were reported (no formal\nevaluation was performed) (Lapalme and Kosseim 2003; Roy and\nSubramaniam 2006).\n Only an automatic evaluation was performed, which relied on having\nmodel responses (Berger and Mittal 2000; Berger et al 2000).\n A user study was performed, but it was either very small compared to the\ncorpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or\nthe corpus itself was significantly smaller than ours (Feng et al 2006;\nLeuski et al 2006). The representativeness of the sample size was not\ndiscussed in any of these studies.\nThere are significant practical difficulties associated with conducting the user stud-\nies needed to produce meaningful results for our system. Firstly, the size of our corpus\nand the number of parameters and settings that we need to test mean that in order for\na user study to be representative, a fairly large sample involving several hundreds of\nrequest?response pairs would have to be used. Further, user-based evaluations of the\noutput produced by our system require the subjects to read relatively long request?\nresponse e-mails, which quickly becomes tedious.\nIn order to address these limitations in a practical way, we conducted a small user\nstudy where we asked four judges (graduate students from the Faculty of Informa-\ntion Technology at Monash University) to assess the responses generated by our sys-\ntem (Marom and Zukerman 2007a). Our judges were instructed to position themselves\nas help-desk customers who know that they are receiving an automated response,\nand that such a response is likely to arrive quicker than a response composed by an\noperator. Our user study assessed the response-generation methods from the following\nperspectives, which yield information that is beyond the F-score and precisionmeasures\nobtained in the automatic evaluation.\n Informativeness: Is there anything useful in the response that would\nmake it a good automatic response, given that otherwise the customer has\n"},{"#tail":"\n","@confidence":"0.9877815","#text":"\nComputational Linguistics Volume 35, Number 4\nto wait for a human-generated response? We used a scale from 0 to 3,\nwhere 0 corresponds to ?not at all informative? and 3 corresponds to\n?very informative.?\n Missing information: Is any crucial information item missing? Y/N.\n Misleading information: Is there any misleading information? Y/N. We\nasked the judges to consider only information that might misguide the\ncustomer, and ignore information that is so irrelevant that it would be\nignored by a customer who knows that the response is automated (for\nexample, receiving an answer for a printer, when the request was for a\nlaptop).\n Compare to model response: How does the generated response compare\nwith the model response? Worse/Same/Better. This is a summary\nquestion that rates a ?customer?s? overall impression of a response.\n"},{"#tail":"\n","@confidence":"0.9729028","#text":"\nWe had two specific goals for this evaluation. First, we wanted to compare document-\nlevel versus sentence-level methods. Second, we wanted to evaluate cases where\nonly the sentence-level methods can produce a response, and establish whether such\nresponses, which are often partial, provide a good alternative to a non-response. We\ntherefore presented two evaluation sets to each judge.\n"},{"#tail":"\n","@confidence":"0.9770120625","#text":"\nSent-Hybrid for requests for which Doc-Pred could not produce a\nresponse. The added benefit of this evaluation set is that it enables us to\nexamine the individual contribution of the sentence retrieval component.\nEach evaluation set contained 80 cases, randomly selected from the corpus in proportion\nto the size of each data set, where a case contained a request e-mail, the model response,\nand the responses generated by the twomethods being compared. Each judgewas given\n20 of these cases, and was asked to assess the generated responses on the four criteria\nlisted previously.14\nWemaximized the coverage of this study by allocating different cases to each judge,\nthus avoiding a situation where a particularly good or bad set of cases is evaluated\nby all judges. In addition, we tried to ensure that the sets of cases shown to the\njudges were of similar quality, so that the judges? assessments would be comparable.\nBecause the judges do not evaluate the same cases, we could not employ standard\ninter-annotator agreement measures (Carletta 1996). However, it is still necessary to\n14 We asked the judges to leave a question unanswered if they felt they did not have the technical\nknowledge to make a judgment, but this did not occur.\n"},{"#tail":"\n","@confidence":"0.963701","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nhave some measure of agreement, and control for bias from specific judges or specific\ncases. This was done by performing pairwise significance testing, treating the data from\ntwo judges as independent samples (we used the Wilcoxon Rank-Sum Test for equal\nmedians). We conducted this significance test separately for each method and each\nof the four criteria, and eliminated the data from a particular judge if he or she had\na significant disagreement with other judges. This happened with one of the judges,\nwho was significantly more lenient than the others on the Sent-Pred method for the\nfirst, second, and fourth criteria, and with another judge, who was significantly more\nstringent on the Sent-Hybrid method for the third criterion.\n"},{"#tail":"\n","@confidence":"0.975908714285714","#text":"\nto the first evaluation set, and the right-hand side to the second set.\nThe left-hand side of Figure 7(a) shows that when both Doc-Pred and Sent-Hybrid\nare applicable, the former is generally preferred, rarely receiving a zero informativeness\njudgment. Because the two methods are evaluated for the same set of cases, we can\nperform a paired significance test for differences between them. Using a Wilcoxon\nsigned rank test for a zeromedian difference, we obtain a p-value 0.01, indicating that\nthe differences in judgments between the two methods are statistically significant. The\n"},{"#tail":"\n","@confidence":"0.994004551724138","#text":"\nComputational Linguistics Volume 35, Number 4\nright-hand side of Figure 7(a), which compares the two sentence-based methods, shows\nthat there do not appear to be significant differences between our subjects? assessment of\nthese methods. This result is confirmed by the paired significance test, which produces\na p-value of 0.13.\nAs shown in Figure 7(b), the results for the missing-information and misleading-\ninformation criteria also favor the Doc-Pred method. The responses produced by\nDoc-Pred were judged to have significantly less missing information than those gen-\nerated by Sent-Hybrid (the paired significance test produces a p-value 0.01). The\nresponses produced by Doc-Pred were also judged to have less misleading informa-\ntion than those generated by Sent-Hybrid, but the paired differences between the two\nmethods are not statistically significant (the p-value is 0.125). The second evaluation\nset was judged to have missing information in approximately 55% of the cases for\nboth sentence-level methods (the p-value is 0.11, indicating an insignificant difference\nbetween these methods). This high proportion of missing information is in line with the\nrelatively low F-scores obtained in the automatic evaluation (Table 5), as missing infor-\nmation results in low recall and hence a lower F-score. The results for the misleading-\ninformation criterion also indicate no significant difference between the sentence-level\nmethods (the p-value is 1). The low proportion of misleading information is in line\nwith the high precision values obtained in the automatic evaluation (Table 5)?whereas\nresponses with a high precision may be incomplete, they generally contain correct\ninformation.\nThe left-hand side of Figure 7(c) shows that Doc-Pred receives more ?same?\nthan ?worse? judgments, although the opposite is true for Sent-Hybrid, and that both\nDoc-Pred and Sent-Hybrid receive a small proportion of ?better? judgments. The paired\nsignificance test produces a p-value 0.01, confirming that these differences are signif-\nicant. The right-hand side of Figure 7(c) shows smaller differences between Sent-Pred\nand Sent-Hybrid, and indeed the p-value for the paired differences is 0.27. Notice that\nSent-Pred does not receive any ?better? judgments, whereas Sent-Hybrid does.\n"},{"#tail":"\n","@confidence":"0.999831277777778","#text":"\nThe results of this study show that responses provided by document-level methods\nwere preferred to responses provided by sentence-level methods, but when document-\nlevel methods cannot be used, the sentence-level methods provide a good alternative.\nAdditionally, although our trial subjects showed a slight preference for the output\nproduced by the Sent-Hybrid method compared to Sent-Pred, this preference was not\nstatistically significant.\nAlthough these results confirm those obtained by the automatic evaluation (Sec-\ntion 4), the result regarding the Sent-Hybrid method is somewhat disappointing. This\nis because we hoped that our trial subjects would prefer Sent-Hybrid to Sent-Pred, as\nthe former is designed to better tailor a response to a request. However, we cannot\ndetermine from this result whether indeed there is no difference between the sentence-\nbasedmethods, or whether such a difference simply could not be observed from our test\nsample of at most 80 cases, which constitutes 1.8% of the corpus used in our automatic\nevaluation (as indicated previously, it would be quite difficult to conduct user studies\nwith a much larger data set).\nThis outcome reinforces the previously mentioned problems associated with con-\nducting meaningful user studies for a large corpus such as ours. These problems are\nexacerbated by our proportional data-selection policy, which is necessary to make the\n"},{"#tail":"\n","@confidence":"0.8553965","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\ntest set representative of the corpus, but increases the difficulty of drawing specific con-\nclusions, for example, determining whether a particular method is favored for specific\ndata sets.\n"},{"#tail":"\n","@confidence":"0.99808778125","#text":"\nIn Section 4, we employed empirically determined applicability thresholds to cir-\ncumscribe the coverage of the different response-generation methods. However, as\nshown by our results, these thresholds were sometimes sub-optimal. In this section,\nwe describe a meta-level process which can automatically select a response-generation\nmethod to address a new request without using such thresholds.\nA common way to combine different models consists of selecting the model that is\nmost confident regarding its decision (Burke 2002). However, in our case, the individual\nconfidence (applicability) measures employed by our response-generation methods are\nnot comparable (e.g., the retrieval score in Doc-Ret is different in nature from the pre-\ndiction probability in Doc-Pred). Hence, prior to selecting the most confident method,\nwe need to find a way to compare the different measures of confidence. Because the\nperformances of the different methods are comparable, we do this by establishing a link\nbetween confidence and performance. In other words, our meta-level process learns\nto predict the performance of the different methods from their confidence levels on\nthe basis of previous experience. These predictions enable our system to recommend\na particular method for handling a new (unseen) request (Marom, Zukerman, and\nJapkowicz 2007).\nFollowing Lekakos and Giaglis (2007), one approach for achieving this objective\nconsists of applying supervised learning, where a winning method is selected for each\ncase in the training set, all the training cases are labeled accordingly, and then the\nsystem is trained to predict a winner for unseen cases. However, in our situation, there\nis not always one single winner (two methods can perform similarly well for a given\nrequest), and there are different ways to pick winners (for example, based on F-score\nor precision). Therefore, such an approach would require the utilization of subjective\nheuristics for creating labels, which would significantly influence what is being learned.\nInstead, we adopt an unsupervised approach that finds patterns in the data?confidence\nvalues coupled with performance scores (Section 6.1)?and then attempts to fit unseen\ndata to these patterns (Section 6.2). Heuristics are still needed in order to decide which\nresponse-generation method to apply to an unseen case, but they are applied only after\nthe learning is complete (Section 6.3). In other words, the subjective process of setting\nperformance criteria (which should be conducted by the organization running the help-\ndesk) does not influence the machine learning process.\n"},{"#tail":"\n","@confidence":"0.996098","#text":"\nWe train the system by clustering the ?experiences? of the response-generation methods\nin addressing requests, where each experience is characterized by the value of the\nconfidence measure employed by a method and its subsequent performance, reflected\nby precision and recall (Equations (7) and (8), respectively). We then use the program\nSnob (Wallace and Boulton 1968; Wallace 2005) to cluster these experiences. Figure 8(a)\nis a projection of the centroids of the clusters produced by Snob into the three most\nsignificant dimensions discovered by Principal Component Analysis (PCA)?these di-\nmensions account for 95% of the variation in the data. The bottom part of Figure 8(b)\n"},{"#tail":"\n","@confidence":"0.881505875","#text":"\nComputational Linguistics Volume 35, Number 4\nFigure 8\nClusters of response-generation methods obtained from the training set: (a) dimensions\nproduced by PCA and (b) sample clusters.\nshows the (unprojected) centroid values of three of the clusters (the top part of the figure\nwill be discussed subsequently).15 These clusters were chosen because they illustrate\nclearly three situations of interest.\n Single winner ? Cluster 8 shows a case where a single strategy is clearly\npreferred. In this case the winner is Doc-Ret. Its precision and recall values\nin this cluster are 0.91 and 0.76, respectively.\n No winner ? Cluster 11 shows a case where none of the methods do well.\nThey all yield precision and recall values of 0.\n Multiple winners ? In Cluster 16, both Doc-Pred and Sent-Pred are\ncompetitive, yielding precision and recall values of (0.90, 0.89) and (0.97,\n0.78), respectively. A decision between the two methods depends on\nwhether we favor precision or recall, as discussed subsequently.\n"},{"#tail":"\n","@confidence":"0.860409777777778","#text":"\nWe test the system with an unseen set of requests, which we feed to each response-\ngeneration method. Each method then outputs a value for its confidence measure. We\ndo not know in advance how each method will perform?this information is missing,\nand we predict it on the basis of the clusters obtained from the training set. Our\nprediction of how well the different methods will perform on an unseen case is based\non (1) howwell the unseen case fits each of the clusters and (2) the average performance\nvalues in each cluster as indicated by its centroid.\n15 The Sent-Ret method was excluded from our experiments due to its poor performance in the comparative\nstudy described in Section 4.\n"},{"#tail":"\n","@confidence":"0.986613407407407","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nThe top part of Figure 8(b) shows an example of an unseen case whose confidence\nvalues are most similar to those in the centroid of Cluster 16. In this case, the selection\nof a method depends on whether we favor recall or precision, as Doc-Pred has a\nhigher recall than Sent-Pred, but Sent-Pred has a higher precision. Now, Cluster 15\n(not labeled in Figure 8(a)) contains similar confidence values to those of Cluster 16,\nbut its (precision, recall) values for Doc-Pred and Sent-Pred are (0.76, 0.66) and (0.84,\n0.67) respectively. If Cluster 15 had the strongest match with the unseen case, then\nSent-Pred would have been chosen, regardless of any preferences for precision or recall.\nHowever, it is not clear that the best policy consists of simply choosing the method\nsuggested by the best-matching cluster. This is particularly the case when an unseen\nexample has a reasonably good match with more than one cluster (e.g., Clusters 15\nand 16).\nThe prediction step is implemented using Snob, which is able to accept data with\nmissing values. For each unseen data point x (with missing performance values), Snob\ncalculates Pr(ci|x), the posterior probability of each cluster ci given this data point.\nThese probabilities indicate how well an unseen case matches each of the clusters. For\nexample, for the unseen case in Figure 8(b), Snob may assign posterior probabilities\nof 0.5 and 0.3 to Clusters 16 and 15, respectively (and lower probabilities to weaker-\nmatching clusters, such as Cluster 8).16\nWe utilize these probabilities in two alternative ways for estimating the per-\nformance of each response-generation method: Max, which considers only the best-\nmatching cluster (i.e., that with the highest posterior probability); and Weighted,\nwhich considers all clusters, weighted by their posterior probabilities. These techniques\nare used to calculate the estimated precision (?pk) and estimated recall (?rk) of each\nresponse-generation method methodk ? {Doc-Ret, Doc-Pred, Sent-Pred, Sent-Hybrid} as\nfollows.\n"},{"#tail":"\n","@confidence":"0.9526065","#text":"\nare the precision and recall components, respectively, for\nmethodk in the centroid of cluster i, and N is the number of clusters.\n"},{"#tail":"\n","@confidence":"0.993157285714286","#text":"\nIn order to select a method for a given request, we need to combine our estimates\nof precision and recall into an overall estimate of performance, and then choose the\nmethod with the best estimated performance. The standard approach for combining\nprecision and recall is to compute their harmonic mean, F-score, as we have done in our\n16 In principle, we could have used a classification method to predict clusters from the values of the\nconfidence measures for unseen cases. We posit that this would not have a significant effect on the\nresults, in particular for MML-based classification techniques, such as Decision Graphs (Oliver 1993).\n"},{"#tail":"\n","@confidence":"0.7341965","#text":"\nComputational Linguistics Volume 35, Number 4\ncomparative evaluation in Section 4. However, in order to accommodate different levels\nof preference towards precision or recall, as discussed herein, we use the following\nweighted F-score calculation (van Rijsbergen 1979).\n"},{"#tail":"\n","@confidence":"0.992499076923077","#text":"\nwhere w is a weight between 0 and 1 given to precision. When w = 0.5 we have the\nstandard usage of F-score (Equation (9)), and for w > 0.5, we have a preference for\nhigh precision. For example, for w = 0.5, the precision and recall values of Cluster 16\n(Figure 8(b)) translate to F-scores of 0.895 and 0.865 for Doc-Pred and Sent-Pred, respec-\ntively, leading to a choice of Doc-Pred. In contrast, for w = 0.75, the respective F-scores\nare 0.897 and 0.914, leading to a choice of Sent-Pred.\n7. Evaluation of Meta-Learning\nWe evaluate the meta-learning system by looking at the quality of the response pro-\nduced by the method selected by this system, where, as done in Section 4, quality\nis measured using F-score and precision. However, here we employ 5-fold cross-\nvalidation (instead of 10-fold) to ensure that we get a good spread of selected methods\nin each testing split. This is particularly important when only a few methods dominate\nfor a data set.\n"},{"#tail":"\n","@confidence":"0.9026585","#text":"\nIn our evaluation, we compare the alternative approaches for estimating performance\n(Equations (10) and (11)), and consider the effect of favoring precision when selecting a\nmethod via the weighted F-score calculation (Equation (12)). To perform these compar-\nisons we employ the following configurations.\n Max50: Use the argmax alternative for estimating performance\n(Equation (10)), and w = 0.5 in Equation 12.\n"},{"#tail":"\n","@confidence":"0.997559","#text":"\nAs we saw from Cluster 11 in Figure 8(b), the estimated performance can be low\nfor all the response-generation methods. Therefore, we also test these configurations in\n"},{"#tail":"\n","@confidence":"0.989021363636364","#text":"\na practical setting where the system has the choice of not selecting any method if the\nestimated performance of all the methods is poor. We envisage that a practical system\nwould behave in this manner, in the sense that a request for which none of the existing\nmethods can produce an appropriate response would be passed to an operator. As\nmentioned in Section 4.2, we consider precision to be an important practical criterion\nbecause it does not penalize partial but correct responses. Therefore, we ?implement?\nour practical system by selecting only responses whose estimated precision is above\n0.8. For these tests we also report on coverage, that is, the percentage of cases where this\ncondition is met. Note that the baselines do not have an estimated precision because\nthey do not use meta-learning. However, for completeness, we implement the practical\nsystem for them as well, with a threshold of 0.8 on actual precision.\n"},{"#tail":"\n","@confidence":"0.998306058823529","#text":"\nTable 6 shows the results of our tests averaged over all the cases in the corpus (with\nstandard deviations in parentheses). The left-hand side corresponds to the setting where\nthe system always selects a response-generation method, and the right-hand side corre-\nsponds to the setting where a method is selected only if its precision equals or exceeds\n0.8 (this is an estimated precision for the Max and Weighted configurations, and an\nactual precision for the Gold and Random baselines).\nLet us first consider the left-hand side of Table 6. As expected, the Random base-\nline has the worst performance. The Gold baselines outperform their corresponding\nmeta-learning counterparts (except for the precision of Weighted50), but the differ-\nences in precision are not statistically significant between the Gold and the Weighted\nconfigurations (using a t-test with a 1% significance level). Comparing the correspond-\ning Weighted and Max configurations, the former is superior, but this is statistically\nsignificant only for the difference in precision values between Weighted50 and Max50.\nComparing a standard F-score calculationwith a precision-favoring calculation (w = 0.5\nversus w = 0.75 in Equation (12)), as expected, precision is significantly higher for the\nlatter in all testing configurations (p < 0.01). This increase in precision is at the expense\nof a reduced F-score, but the increase is larger than the reduction.\n"},{"#tail":"\n","@confidence":"0.966720482758621","#text":"\nconfigurations for each data set.\nNow, in the right-hand side of Table 6, we see that the Random configuration has the\nbest precision and the second-best F-score,17 but its coverage is quite low (only 37.6%).\nIn contrast, the meta-learning configurations cover a proportion of the requests that\nis comparable to the coverage of the Gold baselines (approximately 57%), and all the\nresults are substantially improved; as expected, all the precision values are high, and\nalso more consistent than before (they have a lower standard deviation). These results\nare quite impressive for the meta-learning configurations, as their selection between\nmethods is based on estimated precision, as opposed to the baselines, whose selections\nare based on actual precision, which is not available in practice. Comparing the corre-\nsponding Weighted and Max configurations, there are no significant differences in F-\nscore, but Weighted outperforms Max on precision (the difference between Weighted75\nand Max75 produces a p-value of 0.035). Finally, comparing w = 0.5 with w = 0.75 for\nboth Weighted and Max, as for the All-cases results, the increase in precision is larger\nthan the reduction in F-score (p < 0.01).\nNow that we have evaluated the ability of the meta-level process to select between\nresponse-generation methods, let us inspect what happens for each data set. We saw\nin Section 4 that the various methods differ in their applicability to the different data\nsets (Figure 4). Hence, we would expect the meta-level process to select between the\nmethods differently for each data set. Figure 9 shows the method-selection proportions\nfor each data set for the Weighted50 and Weighted75 configurations, using the practical\nsetting where the system can choose not to select any method. What is immediately\nnotable is that no method is selected for two of the data sets (nos. 4 and 6). This follows\nfrom the poor performance of all the methods for these data sets (Figures 4(b) and 4(c)).\nAt first glance, it appears that the selection of the Sent-Pred method for data set no. 1\ncontradicts the results in Figure 4, which shows a low coverage of Sent-Pred for this\ndata set. However, this selection is justified by the fact that the meta-learning procedure\n17 This seemingly good performance represents the average precision and resultant F-score of all the\nresponses with actual precision ? 0.8, and is not the result of the application of a selection strategy.\n"},{"#tail":"\n","@confidence":"0.971562176470588","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nselects methods based on their historical performance (precision and recall), without\nfiltering on applicability threshold (which is the basis for coverage). Figure 9 also\nhighlights the impact of favoring precision on the selection of the Sent-Pred method\ninstead of Doc-Pred. This effect is most dramatic for data set no. 3, but it can also be\nobserved for data sets no. 2 and 5.\nOverall, Sent-Pred dominates for most data sets (except data set no. 3 under the\nWeighted50 configuration, and data set no. 8). Also notable is the fact that Sent-Hybrid\nis selected only for data set no. 7. We postulate that this happens because data set\nno. 7 merges several sub-topics, and has different versions of similar responses, where\neach version has a generic component combined with a request-specific component\n(this feature is not shared by the other merged data sets no. 6 and 8). Sent-Pred is not\nconfident enough to select one of these specific components, whereas Sent-Hybrid is.\nThis ability to select a specific response is demonstrated in Figure 10, which shows\na request from data set no. 7 and its automated response. This response belongs to a\nmedium-cohesion cluster which contains responses that share the generic segment up\nto regarding, but the rest of the response refers specifically to terms in the request.\n"},{"#tail":"\n","@confidence":"0.991904863636364","#text":"\nThe meta-learning results may be summarized as follows.\n The meta-learning system significantly outperforms the random selection\nbaseline, and is competitive with the gold baseline.\n The Weighted option for estimating performance (Equation (11)) is\npreferable to the Max option (Equation (10)), as it is better able to handle\nthe uncertainty that arises when a particular method has a wide range of\nprecision and recall values.\n A precision-favoring approach is recommended, as the resultant increase\nin precision (reflecting a higher proportion of correct information) is larger\nthan the reduction in F-score.\n Overall, Doc-Pred and Sent-Pred were the preferred methods, with\nSent-Pred clearly dominating for the Weighted75 configuration.\nSent-Hybrid was selected often for data set no. 7, and Doc-Ret was the\nonly method selected, albeit seldom, for data set no. 8.\n Because the system can estimate performance prior to producing a\nresponse, it is able to opt for a non-response rather than risk producing a\nbad one. The decision of what is a bad response should be made by the\norganization using the system. With the stringent criterion we have chosen\n(precision? 0.8), the system yields a good performance for approximately\n57% of the requests.\nFigure 10\nExample showing an appropriate response generated by the Sent-Hybrid method.\n"},{"#tail":"\n","@confidence":"0.373916","#text":"\nComputational Linguistics Volume 35, Number 4\n"},{"#tail":"\n","@confidence":"0.9996304375","#text":"\nThe automation of help-desk responses has been previously tackled using mainly\nknowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and\ncase-based reasoning (Watson 1997). Such technologies require significant human in-\nput, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the\ntechniques examined in this article are corpus-based and data-driven. The process of\ncomposing a planned response for a new request is informed by probabilistic and lexical\nproperties of the requests and responses in the corpus.\nThere are very few reported attempts at corpus-based automation of help-desk\nresponses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and\nScheffer 2004; Malik, Subramaniam, and Kaushik 2007).\neResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), re-\ntrieves a list of request?response pairs and presents a ranked list of responses to the user.\nIf the user is unsatisfied with this list, an operator is asked to generate a new response.\nThe operator is assisted in this task by the retrieval results: The system highlights the\nrequest-relevant sentences in the ranked responses. However, there is no attempt to\nautomatically generate a single response.\nBickel and Scheffer (2004) compared the performance of document retrieval and\ndocument prediction for generating help-desk responses. Their retrieval technique,\nwhich is similar to our request-to-request Doc-Ret method, matches user questions to\nthe questions in a database of question?answer pairs. Their prediction method, which is\nsimilar to Doc-Pred, is based on clustering the responses in the corpus into semantically\nequivalent answers, and then training a classifier to match a query with one of these\nclasses. The generated response is the answer that is closest to the centroid of the cluster.\nBickel and Scheffer?s results are consistent with ours, in the sense that the performance\nof the Doc-Ret method is significantly worse than that of Doc-Pred. However, it is worth\nnoting that their corpus is significantly smaller than ours (805 question?answer pairs),\ntheir questions seem to be much simpler and shorter than those in our corpus, and the\nreplies shorter and more homogeneous.\nMalik, Subramaniam, and Kaushik (2007) developed a system that builds question?\nanswer pairs from help-center e-mails, and then maps new questions to existing ques-\ntions in order to retrieve an answer. This part of their approach resembles our Doc-Ret\nmethod, but instead of retrieving entire response documents, they retrieve individual\nsentences. In addition, rather than including actual response sentences in a reply, their\nsystemmatches response sentences to pre-existing templates and returns the templates.\nLapalme and Kosseim (2003) investigated three approaches to the automatic gener-\nation of response e-mails: text classification, case-based reasoning, and question answer-\ning. Text classification was used to group request e-mails into broad categories, some\nof which, such as requests for financial reports, can be automatically addressed. The\nquestion-answering approach and the retrieval component of the case-based reasoning\napproach were data driven, using word-level matches. However, the personalization\ncomponent of the case-based reasoning approach was rule-based (e.g., rules were ap-\nplied to substitute names of individuals and companies in texts).\nWith respect to these systems, the contribution of our work lies in the consideration\nof different kinds of corpus-based approaches (namely, retrieval and prediction) applied\nat different levels of granularity (namely, document and sentence).\nTwo applications that, like help-desk, deal with question?answer pairs are: sum-\nmarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004),\nand answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000;\n"},{"#tail":"\n","@confidence":"0.995701395833333","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nBerger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important\ndifference between these applications and help-desk is that help-desk request e-mails\nare not simple queries. In fact, some e-mails do not contain any queries at all, and even\nif they do, it is not always straightforward to distinguish the queries from the text that\nprovides background information. Therefore, the generation of a help-desk response\nneeds to consider a request e-mail in its entirety, and ensure that there is sufficient\nevidence to match the request with a response or parts of responses.\nIn e-mail-thread summarization, Dalli, Xia, and Wilks (2004) applied a procedural\napproach where they recognized named entities and performed anaphora resolution\nprior to applying ranking metrics to select sentences for inclusion in a thread summary.\nHowever, their approach does not specifically address the question?answer aspect of an\ne-mail thread, potentially omitting important information. This problem was addressed\nby Shrestha and McKeown (2004), who performed supervised learning in order to\nmatch questions with answers in e-mail threads, as a first step in the summarization\nof such threads. A significant difference between our approach and theirs is in our\nuse of unsupervised learning, which is necessitated by the size of our data set. Also,\nShrestha and McKeown used high-level features for machine learning, as well as word-\nbased features. As indicated in Section 3.2.2, our Sent-Pred experiments with high-level\nfeatures (specifically syntactic features) did not improve our results. Finally, Shrestha\nand McKeown used paragraphs as a unit of information?an approach we tried late\nin our project with encouraging results. This suggests that there are situations where\none can generalize a response that is longer than a sentence but shorter than a whole\ndocument. Unfortunately, we could not pursue this avenue of research owing to time\nlimitations.\nIn FAQs, Berger and Mittal (2000) employed a sentence retrieval approach based on\na language model where the entire response to an FAQ is considered a sentence, and\nthe questions and answers are embedded in an FAQ document. They complemented\nthis approach with machine learning techniques that automatically learn the weights\nof different retrieval models. Berger et al (2000) compared two retrieval approaches\n(TF.IDF and query expansion) and two predictive approaches (statistical translation\nand latent variable models). Jijkoun and de Rijke (2005) compared different variants of\nretrieval techniques. Soricut and Brill (2006) compared a predictive approach (statistical\ntranslation), a retrieval approach based on a language-model, and a hybrid approach\nwhich combines statistical chunking and traditional retrieval. Two significant differ-\nences between help-desk and FAQs are the following.\n The responses in the help-desk corpus are personalized, which means\nthat on one hand, we must abstract from them sufficiently to obtain\nmeaningful regularities, and on the other hand, we must be careful not\nto abstract away specific information that addresses particular issues.\n Help-desk responses have much more repetition than FAQs, because\nthe corpus is made up of individual dialogues, rather than typical\nrequest?response pairs. This motivates the use of multi-document\nsummarization techniques, rather than question-answering approaches,\nto extract individual answers.\nThese issues also differentiate the help-desk application from other types of question-\nanswering applications, specifically those found in the field of restricted domain ques-\ntion answering (Molla? and Vicedo 2007).\n"},{"#tail":"\n","@confidence":"0.996589555555556","#text":"\nComputational Linguistics Volume 35, Number 4\nIn addition to the different response-generationmethods, we have proposed ameta-\nlevel strategy to combine them. This kind of meta-learning is referred to as stacking by\nthe DataMining community (Witten and Frank 2000). Lekakos andGiaglis (2007) imple-\nmented a supervised version of this approach for a recommender system, as opposed\nto our unsupervised version. They also proposed two major categories of meta-learning\napproaches for recommender systems, merging and ensemble, each subdivided into\nthemore specific subclasses suggested by Burke (2002) as follows. Themerging category\ncorresponds to techniques where the individual methods affect each other in differ-\nent ways (this category encompasses Burke?s feature combination, cascade, feature\naugmentation, and meta-level sub-categories). The ensemble category corresponds to\ntechniques where the predictions of the individual methods are combined to produce\na final prediction (this category encompasses Burke?s weighted, switching, and mixed\nsub-categories).\nOur system falls into the ensemble category, because it combines the results of\nthe various methods into a single outcome. More specifically, it belongs to Burke?s\nswitching sub-category, where a single method is selected on a case-by-case basis. A\nsimilar approach is taken in Rotaru and Litman?s (2005) reading comprehension system,\nbut their system does not perform any learning. Instead it uses a voting mechanism\nto select the answer given by the majority of methods. The question answering system\ndeveloped by Chu-Carroll et al (2003) belongs to the merging category of approaches,\nwhere the output of an individual method can be used as input to a different method\n(this corresponds to Burke?s cascade sub-category). Because the results of all the\nmethods are comparable, no learning is required: At each stage of the ?cascade of\nmethods,? the method that performs best is selected. In contrast to these two systems,\nour system employs methods that are not comparable, because they use different\nmetrics. Therefore, we need to learn from experience when to use each method.\n"},{"#tail":"\n","@confidence":"0.99962115","#text":"\nDespite its theoretical importance and commercial impact, the generation of e-mail-\nbased help-desk responses has received scant attention to date. In this article, we\nhave investigated complementary corpus-based, information-gathering methods for\nautomatically addressing help-desk requests. Our results show that a large corpus of\nrequest?response e-mail pairs supports the automation of a significant portion of the\nhelp-desk task with data-driven techniques that reuse responses or parts thereof. These\ntechniques are particularly suitable for repetitive, non-technical issues, allowing help-\ndesk operators to focus on more challenging, technical requests.\nOur results also show that different methods are applicable to different situations\nthat arise in the help-desk domain, and that the performance of different methods varies\nfor different data sets. This suggests that there must be an underlying relationship\nbetween methods and features of data sets that needs to be accounted for (Section 4.3).\nAdditionally, our results yield insights regarding the following issues.\n Retrieval versus prediction ? Some situations warrant a retrieval\napproach, whereas others require a predictive approach. The former\napplies when the content of a request matches previous cases in the\ncorpus. The latter applies when requests and responses do not match on\ncontent, but correlations exist between a few predictive words in a request\nand a response, which is often generic in these cases. A hybrid\nprediction?retrieval approach was also investigated. Our hybrid method\n"},{"#tail":"\n","@confidence":"0.994819097560976","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nextracts generalizations at the sentence level, and employs a retrieval\ncomponent that tailors the selection of sentences to the specific issues\nraised in a request e-mail. Although this appears to be a sensible approach,\nthe results of our evaluation are not conclusive, and further investigation\nis required.\n Levels of granularity ? Although simple document-level reuse methods\nare sufficient in many cases, more complex sentence-level reuse methods,\nwhich involve extractive multi-document summarization, provide a viable\nalternative when a complete response document cannot be reused. This\nhappens when there is insufficient evidence for the reuse of such a\ndocument, but there is enough evidence for a partial response.\nWe have also performed a small user study, which highlighted issues regarding the\nevaluation of a large corpus such as ours. Despite its modest size, our user study was\nuseful, as it provided a subjective evaluation of the methods considered, and linked the\nresults of our automatic evaluation with these subjective assessments.\nOur work also provides a unified solution to help-desk response automation via\nthe meta-learning component. Although our comparative investigation demonstrates\nthe applicability of the different methods, the meta-learning component provides a\nway to automatically select a method. We have offered an unsupervised approach that\nlearns which is the most promising method based on previous experience. It does so by\nlearning the relationship between the value of the confidence (applicability) measure of\neach method and its subsequent performance. This eliminates the need to set subjective\nthresholds on these confidence values, and instead transfers subjective decision-making\nto a more intuitive part of the system, namely, the actual performance of the methods,\nmeasured by the quality of the generated responses. In this way, help-desk managers\ncan decide how strict the system should be, for example, on the precision of responses.\nWe are encouraged by the fact that we have achieved a reasonable level of per-\nformance using only a simple, low-level bag-of-words representation. One avenue for\nfuture research is to investigate more sophisticated representations, such as incorpo-\nrating word-based similarity metrics into the bag-of-words representation, employing\nquery expansion during retrieval, and taking into account syntactic features during\nretrieval and prediction (recall that we incorporated grammatical and sentence-based\nsyntactic features into the Sent-Pred method without significantly affecting perfor-\nmance, Section 3).\nAnother avenue for future research is the investigation of intermediate levels of\ngranularity, such as paragraphs. Ideally there should be a mechanism that determines\ndynamically the most suitable level of granularity for capturing the regularities in a\ncollection of e-mails. An information-theoretic approach, such as the MML crite-\nrion (Wallace and Boulton 1968; Wallace 2005), may be a promising way to address this\nproblem.\n"},{"#tail":"\n","@confidence":"0.9969341","#text":"\nThis research was supported in part by grant\nLP0347470 from the Australian Research\nCouncil and by an endowment from\nHewlett-Packard. The authors also thank\nHewlett-Packard for the extensive\nanonymized help-desk data, Nathalie\nJapkowicz for her advice on the\nmeta-learning portions of this work, and the\nanonymous reviewers for their insightful\ncomments.\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.975013","#text":"\nMonash University\n"},{"#tail":"\n","@confidence":"0.864711","#text":"\nMonash University\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.994407","@genericHeader":"abstract","#text":"\n1. Introduction\n"},{"#tail":"\n","@confidence":"0.774483","@genericHeader":"keywords","#text":"\n3 June 2009.\n"},{"#tail":"\n","@confidence":"0.97184","@genericHeader":"introduction","#text":"\n2. The Help-Desk Domain\n"},{"#tail":"\n","@confidence":"0.985712","@genericHeader":"method","#text":"\n3. Response-Generation Methods\n"},{"#tail":"\n","@confidence":"0.47638","@genericHeader":"method","#text":"\n6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as\n"},{"#tail":"\n","@confidence":"0.4269","@genericHeader":"method","#text":"\n8 We tried several alternatives for representing Pr(SCi ). The best results were obtained with Equation (3).\n"},{"#tail":"\n","@confidence":"0.796524","@genericHeader":"method","#text":"\n5. User-Based Evaluation of Individual Methods\n"},{"#tail":"\n","@confidence":"0.703707","@genericHeader":"method","#text":"\n6. Meta-Learning\n"},{"#tail":"\n","@confidence":"0.797301","@genericHeader":"method","#text":"\n8. Related Research\n"},{"#tail":"\n","@confidence":"0.987371","@genericHeader":"conclusions","#text":"\n9. Conclusion\n"},{"#tail":"\n","@confidence":"0.978321","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.98753","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.602827666666667","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nTable 5\nCoverage, uniqueness, precision, and F-score for the response-generation methods.\n"},{"#tail":"\n","@confidence":"0.809385","#text":"\nMarom and Zukerman Empirical Study of Response Automation Methods\nTable 6\n"}],"page":[{"#tail":"\n","@confidence":"0.994544","#text":"\n598\n"},{"#tail":"\n","@confidence":"0.996533","#text":"\n599\n"},{"#tail":"\n","@confidence":"0.979873","#text":"\n600\n"},{"#tail":"\n","@confidence":"0.994802","#text":"\n601\n"},{"#tail":"\n","@confidence":"0.996055","#text":"\n602\n"},{"#tail":"\n","@confidence":"0.999657","#text":"\n603\n"},{"#tail":"\n","@confidence":"0.998628","#text":"\n604\n"},{"#tail":"\n","@confidence":"0.996897","#text":"\n605\n"},{"#tail":"\n","@confidence":"0.950741","#text":"\n8\n"},{"#tail":"\n","@confidence":"0.998865","#text":"\n606\n"},{"#tail":"\n","@confidence":"0.995034","#text":"\n607\n"},{"#tail":"\n","@confidence":"0.9943","#text":"\n608\n"},{"#tail":"\n","@confidence":"0.966511","#text":"\n609\n"},{"#tail":"\n","@confidence":"0.97601","#text":"\n610\n"},{"#tail":"\n","@confidence":"0.907772","#text":"\n611\n"},{"#tail":"\n","@confidence":"0.988414","#text":"\n612\n"},{"#tail":"\n","@confidence":"0.994393","#text":"\n613\n"},{"#tail":"\n","@confidence":"0.961987","#text":"\n614\n"},{"#tail":"\n","@confidence":"0.990638","#text":"\n615\n"},{"#tail":"\n","@confidence":"0.994272","#text":"\n616\n"},{"#tail":"\n","@confidence":"0.988787","#text":"\n617\n"},{"#tail":"\n","@confidence":"0.991928","#text":"\n618\n"},{"#tail":"\n","@confidence":"0.995246","#text":"\n619\n"},{"#tail":"\n","@confidence":"0.962412","#text":"\n620\n"},{"#tail":"\n","@confidence":"0.989524","#text":"\n621\n"},{"#tail":"\n","@confidence":"0.990065","#text":"\n622\n"},{"#tail":"\n","@confidence":"0.996363","#text":"\n623\n"},{"#tail":"\n","@confidence":"0.997092","#text":"\n624\n"},{"#tail":"\n","@confidence":"0.992491","#text":"\n625\n"},{"#tail":"\n","@confidence":"0.999209","#text":"\n626\n"},{"#tail":"\n","@confidence":"0.993749","#text":"\n627\n"},{"#tail":"\n","@confidence":"0.993439","#text":"\n628\n"},{"#tail":"\n","@confidence":"0.982387","#text":"\n629\n"},{"#tail":"\n","@confidence":"0.944269","#text":"\n630\n"},{"#tail":"\n","@confidence":"0.978136","#text":"\n631\n"},{"#tail":"\n","@confidence":"0.981349","#text":"\n632\n"},{"#tail":"\n","@confidence":"0.975234","#text":"\n633\n"},{"#tail":"\n","@confidence":"0.983498","#text":"\n634\n"},{"#tail":"\n","@confidence":"0.998641","#text":"\n635\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.819798","#text":"\nFigure 3. We then apply the following steps.\n"},{"#tail":"\n","@confidence":"0.980691","#text":"\nFigure 7 shows the results for the four criteria. The left-hand side of the figure pertains\n"},{"#tail":"\n","@confidence":"0.7809545","#text":"\nFigure 7\nResults of the human study for the evaluation of generated responses.\n"}],"table":[{"#tail":"\n","@confidence":"0.993837384615385","#text":"\nMethod Threshold Range Selected Learned\ntried value\nDoc-Ret cosine similarity score 0.1?0.7 0.2 YES\nDoc-Pred prediction probability 0.6?0.9 0.7 YES\nSent-Ret recall score 0.4?0.9 0.6 N/A\nSent-Pred confidence 0 YES\nSC inclusion (Equation (3))\nSC prediction probability 0.1?0.9 0.5\nSC cohesion 0.7?1.0 0.9\nprobability of a lemma in SC\n(?, Equation (5)) 0.01?0.05 0.01\nconfidence (Equation (6))\nSC precision 0.7?0.9 0.7\nSC minimum prediction probability 0.1?0.2 0.1\nSent-Hybrid confidence 0 YES\nnumber of sentences 0 YES\nSC inclusion\nSC prediction probability 0.1?0.9 0.5\nSC high cohesion 0.7?1.0 0.95\nSC medium cohesion 0.6?0.8 0.75\nsentence recall score\nliberal 0.2?0.4 0.4\nconservative 0.4?0.9 0.6\nconfidence (Equation (6))\nSC precision 0.7?0.9 0.7\nSC minimum prediction probability 0.1?0.2 0.1\n"},{"#tail":"\n","@confidence":"0.691916875","#text":"\nare excluded).13\nPrecision =\n# words in both model response and automated response\n# of words in automated response\n(7)\nRecall =\n# words in both model response and automated response\n# of words in model response\n"},{"#tail":"\n","@confidence":"0.612859333333333","#text":"\nComputational Linguistics Volume 35, Number 4\nFigure 4\nPerformance of the different methods for each data set: (a) coverage, (b) F-score, and\n"},{"#tail":"\n","@confidence":"0.997506375","#text":"\nMethod Coverage Unique Avg. (St dev.)\nor best Precision F-score\nDoc-Ret 43% 22% 0.37 (0.34) 0.35 (0.33)\nDoc-Pred 29% 3% 0.82 (0.21) 0.82 (0.24)\nSent-Ret 9% 0% 0.19 (0.19) 0.12 (0.11)\nSent-Pred 34% 5% 0.94 (0.13) 0.78 (0.18)\nSent-Hybrid 43% 10% 0.81 (0.29) 0.66 (0.25)\nCombined 72% 0.80 (0.25) 0.50 (0.33)\n"},{"#tail":"\n","@confidence":"0.998267769230769","#text":"\nPrecision and F-score for the meta-learning methods averaged over the corpus.\nAll cases Cases with precision ? 0.8\nPrecision F-score Precision F-score Coverage\nAvg. (St dev) Avg. (St dev) Avg. (St dev) Avg. (St dev) Avg.\n(actual precision ? 0.8)\nRandom 0.558 (0.37) 0.376 (0.33) 0.955 (0.06) 0.696 (0.25) 37.6%\nGold50 0.725 (0.26) 0.548 (0.30) 0.934 (0.06) 0.732 (0.26) 53.0%\nGold75 0.781 (0.25) 0.537 (0.29) 0.952 (0.06) 0.689 (0.26) 60.6%\n(estimated precision ? 0.8)\nMax50 0.704 (0.28) 0.507 (0.31) 0.844 (0.22) 0.648 (0.30) 56.7%\nMax75 0.768 (0.27) 0.498 (0.28) 0.911 (0.17) 0.629 (0.27) 56.7%\nWeighted50 0.727 (0.27) 0.512 (0.30) 0.874 (0.18) 0.649 (0.29) 57.1%\nWeighted75 0.776 (0.26) 0.499 (0.28) 0.919 (0.16) 0.626 (0.27) 57.1%\n"},{"#tail":"\n","@confidence":"0.460095333333333","#text":"\nComputational Linguistics Volume 35, Number 4\nFigure 9\nProportion of methods selected by meta-learning with the Weighted50 and Weighted75\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.373051","#tail":"\n","@no":"0","#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.932504","#text":"Monash University"},{"#tail":"\n","@confidence":"0.999142","#text":"Monash University"}],"author":[{"#tail":"\n","@confidence":"0.532804","#text":"Yuval Marom"},{"#tail":"\n","@confidence":"0.822381","#text":"Ingrid Zukerman"}],"abstract":{"#tail":"\n","@confidence":"0.992950583333333","#text":"This article presents an investigation of corpus-based methods for the automation of help-desk e-mail responses. Specifically, we investigate this problem along two operational dimensions: (1) information-gathering technique, and (2) granularity of the information. We consider two information-gathering techniques (retrieval and prediction) applied to information represented at two levels of granularity (document-level and sentence-level). Document-level methods correspond to the reuse of an existing response e-mail to address new requests. Sentence-level methods correspond to applying extractive multi-document summarization techniques to collate units of information from more than one e-mail. Evaluation of the performance of the different methods shows that in combination they are able to successfully automate the generation of responses for a substantial portion of e-mail requests in our corpus. We also investigate a meta-selection process that learns to choose one method to address a new inquiry e-mail, thus providing a unified response automation solution."},"title":{"#tail":"\n","@confidence":"0.881636333333333","#text":"An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Banerjee, S. and T. Pedersen. 2003. The design, implementation, and use of Computational Linguistics Volume 35, Number 4 the Ngram Statistics Package. In CICLing 2003 ? Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, pages 370?381, Mexico City. Mexico."},"#text":"\n","pages":{"#tail":"\n","#text":"370--381"},"marker":{"#tail":"\n","#text":"Banerjee, Pedersen, 2003"},"location":{"#tail":"\n","#text":"Mexico City. Mexico."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"to strongly match a previous request or response e-mail (Doc-Ret), or requests contain terms that are predictive of complete template response e-mails (Doc-Pred). 4 We used a binary representation, rather than a representation based on TF.IDF scores, because important domain-related words, such as monitor and network, are actually quite frequent. Thus, their low TF.IDF score may have an adverse influence on clustering performance. Nonetheless, in the future, it may be worth investigating a TF.IDF-based representation. 5 Significant bigrams are obtained using the n-gram statistics package NSP (Banerjee and Pedersen 2003), which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram (that it is not a collocation). 603 Computational Linguistics Volume 35, Number 4 As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-","@endWordPosition":"3397","@position":"22355","annotationId":"T1","@startWordPosition":"3394","@citStr":"Banerjee and Pedersen 2003"}},"title":{"#tail":"\n","#text":"The design, implementation, and use of Computational Linguistics Volume 35, Number 4 the Ngram Statistics Package. In CICLing"},"booktitle":{"#tail":"\n","#text":"Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Banerjee"},{"#tail":"\n","#text":"T Pedersen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Barr, A. and S. Tessler. 1995. Expert systems: A technology before its time. AI Expert, available at www.stanford.edu/group/ scip/avsgt/expertsystems/aiexpert.html."},"#text":"\n","marker":{"#tail":"\n","#text":"Barr, Tessler, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"stralia. E-mail: Ingrid.Zukerman@infotech.monash.edu.au. 1 http ://customercare.telephonyonline.com/ar/telecom next generation customer. Submission received: 7 November 2007; revised submission received: 20 March 2009; accepted for publication: 3 June 2009. ? 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven (Barr and Tessler 1995; Watson 1997; Delic and Lahaix 1998). These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998). In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk cus","@endWordPosition":"412","@position":"3050","annotationId":"T2","@startWordPosition":"409","@citStr":"Barr and Tessler 1995"},{"#tail":"\n","#text":"se, it is able to opt for a non-response rather than risk producing a bad one. The decision of what is a bad response should be made by the organization using the system. With the stringent criterion we have chosen (precision? 0.8), the system yields a good performance for approximately 57% of the requests. Figure 10 Example showing an appropriate response generated by the Sent-Hybrid method. 629 Computational Linguistics Volume 35, Number 4 8. Related Research The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, ","@endWordPosition":"14678","@position":"93991","annotationId":"T3","@startWordPosition":"14675","@citStr":"Barr and Tessler 1995"}]},"title":{"#tail":"\n","#text":"Expert systems: A technology before its time. AI Expert, available at www.stanford.edu/group/ scip/avsgt/expertsystems/aiexpert.html."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Barr"},{"#tail":"\n","#text":"S Tessler"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Barzilay, R., N. Elhadad, and K. R. McKeown. 2001. Sentence ordering in multidocument summarization. In HLT01 ? Proceedings of the First Human Language Technology Conference, pages 1?7, San Diego, CA."},"#text":"\n","pages":{"#tail":"\n","#text":"1--7"},"marker":{"#tail":"\n","#text":"Barzilay, Elhadad, McKeown, 2001"},"location":{"#tail":"\n","#text":"San Diego, CA."},"title":{"#tail":"\n","#text":"Sentence ordering in multidocument summarization."},"booktitle":{"#tail":"\n","#text":"In HLT01 ? Proceedings of the First Human Language Technology Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Barzilay"},{"#tail":"\n","#text":"N Elhadad"},{"#tail":"\n","#text":"K R McKeown"}]}},{"volume":{"#tail":"\n","#text":"31"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Barzilay, R. and K. R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297?328."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Barzilay, McKeown, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization (Goldstein et al 2000; Barzilay, Elhadad, andMcKeown 2001; Barzilay and McKeown 2005). The appeal of a sentence-level approach is that it supports the generation of a ?combination response? in situations where there is insufficient evidence for a single document containing a full response, but there is enough evidence for parts of responses. Although such a combined response is generally less satisfactory than a full response, the information included in it may address a user?s problem or point the user in the right direction. As argued in the Introduction, when it comes to obtaining information quickly on-line, this option may be preferable to having to wait for a human-gener","@endWordPosition":"3544","@position":"23323","annotationId":"T4","@startWordPosition":"3541","@citStr":"Barzilay and McKeown 2005"}},"title":{"#tail":"\n","#text":"Sentence fusion for multidocument news summarization."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Barzilay"},{"#tail":"\n","#text":"K R McKeown"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Berger, A., R. Caruana, D. Cohn, D. Freitag, and V. Mittal. 2000. Bridging the lexical chasm: Statistical approaches to answer-finding. In SIGIR?00 ? Proceedings of the 23rd Annual International ACM International Conference on Research and Development in Information Retrieval, pages 192?199, Athens."},"#text":"\n","pages":{"#tail":"\n","#text":"192--199"},"marker":{"#tail":"\n","#text":"Berger, Caruana, Cohn, Freitag, Mittal, 2000"},"location":{"#tail":"\n","#text":"Athens."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"onses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user stud","@endWordPosition":"10048","@position":"64742","annotationId":"T5","@startWordPosition":"10045","@citStr":"Berger et al 2000"},{"#tail":"\n","#text":" individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question?answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses. In e-mail-thread summariza","@endWordPosition":"15244","@position":"97831","annotationId":"T6","@startWordPosition":"15241","@citStr":"Berger et al 2000"},{"#tail":"\n","#text":"aging results. This suggests that there are situations where one can generalize a response that is longer than a sentence but shorter than a whole document. Unfortunately, we could not pursue this avenue of research owing to time limitations. In FAQs, Berger and Mittal (2000) employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. Berger et al (2000) compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval. Two significant differences between help-desk and FAQs are the following.  The responses in the help-desk corpus are personalized, which means that on one","@endWordPosition":"15599","@position":"100149","annotationId":"T7","@startWordPosition":"15596","@citStr":"Berger et al (2000)"}]},"title":{"#tail":"\n","#text":"Bridging the lexical chasm: Statistical approaches to answer-finding."},"booktitle":{"#tail":"\n","#text":"In SIGIR?00 ? Proceedings of the 23rd Annual International ACM International Conference on Research and Development in Information Retrieval,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Berger"},{"#tail":"\n","#text":"R Caruana"},{"#tail":"\n","#text":"D Cohn"},{"#tail":"\n","#text":"D Freitag"},{"#tail":"\n","#text":"V Mittal"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Berger, A. and V. Mittal. 2000. Queryrelevant summarization using FAQs. In ACL2000 ? Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 294?301, Hong Kong."},"#text":"\n","pages":{"#tail":"\n","#text":"294--301"},"marker":{"#tail":"\n","#text":"Berger, Mittal, 2000"},"location":{"#tail":"\n","#text":"Hong Kong."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ticular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in o","@endWordPosition":"10044","@position":"64722","annotationId":"T8","@startWordPosition":"10041","@citStr":"Berger and Mittal 2000"},{"#tail":"\n","#text":" case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question?answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient eviden","@endWordPosition":"15230","@position":"97742","annotationId":"T9","@startWordPosition":"15227","@citStr":"Berger and Mittal 2000"},{"#tail":"\n","#text":"n used high-level features for machine learning, as well as wordbased features. As indicated in Section 3.2.2, our Sent-Pred experiments with high-level features (specifically syntactic features) did not improve our results. Finally, Shrestha and McKeown used paragraphs as a unit of information?an approach we tried late in our project with encouraging results. This suggests that there are situations where one can generalize a response that is longer than a sentence but shorter than a whole document. Unfortunately, we could not pursue this avenue of research owing to time limitations. In FAQs, Berger and Mittal (2000) employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. Berger et al (2000) compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. Soricut and Brill (2006) com","@endWordPosition":"15546","@position":"99806","annotationId":"T10","@startWordPosition":"15543","@citStr":"Berger and Mittal (2000)"}]},"title":{"#tail":"\n","#text":"Queryrelevant summarization using FAQs."},"booktitle":{"#tail":"\n","#text":"In ACL2000 ? Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Berger"},{"#tail":"\n","#text":"V Mittal"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Bickel, S. and T. Scheffer. 2004. Learning from message pairs for automatic email answering. In ECML04 ? Proceedings of the European Conference on Machine Learning, pages 87?98, Pisa."},"#text":"\n","pages":{"#tail":"\n","#text":"87--98"},"marker":{"#tail":"\n","#text":"Bickel, Scheffer, 2004"},"location":{"#tail":"\n","#text":"Pisa."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"nes, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this, to date, there has been little work on corpus-based approaches to help-desk response automation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). A major factor limiting this work is the dearth of corpora?help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of request? response e-mail dialogues between customers and operators at Hewlett-Packard. Observations from this corpus have led us to consider ","@endWordPosition":"567","@position":"4135","annotationId":"T11","@startWordPosition":"564","@citStr":"Bickel and Scheffer 2004"},{"#tail":"\n","#text":"such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request?response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. Bickel and Scheffer (2004) compared the performance of document retrieval and doc","@endWordPosition":"14764","@position":"94569","annotationId":"T12","@startWordPosition":"14761","@citStr":"Bickel and Scheffer 2004"}]},"title":{"#tail":"\n","#text":"Learning from message pairs for automatic email answering."},"booktitle":{"#tail":"\n","#text":"In ECML04 ? Proceedings of the European Conference on Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Bickel"},{"#tail":"\n","#text":"T Scheffer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Burke, R. 2002. Hybrid recommender systems. User Modeling and User-Adapted Interaction, 12(4):331?370."},"#text":"\n","marker":{"#tail":"\n","#text":"Burke, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ining whether a particular method is favored for specific data sets. 6. Meta-Learning In Section 4, we employed empirically determined applicability thresholds to circumscribe the coverage of the different response-generation methods. However, as shown by our results, these thresholds were sometimes sub-optimal. In this section, we describe a meta-level process which can automatically select a response-generation method to address a new request without using such thresholds. A common way to combine different models consists of selecting the model that is most confident regarding its decision (Burke 2002). However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance. In other words, our meta-level process learns to predict the performance of the different metho","@endWordPosition":"11683","@position":"75321","annotationId":"T13","@startWordPosition":"11682","@citStr":"Burke 2002"},{"#tail":"\n","#text":"icedo 2007). 631 Computational Linguistics Volume 35, Number 4 In addition to the different response-generationmethods, we have proposed ametalevel strategy to combine them. This kind of meta-learning is referred to as stacking by the DataMining community (Witten and Frank 2000). Lekakos andGiaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems, merging and ensemble, each subdivided into themore specific subclasses suggested by Burke (2002) as follows. Themerging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke?s feature combination, cascade, feature augmentation, and meta-level sub-categories). The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke?s weighted, switching, and mixed sub-categories). Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome. More specifi","@endWordPosition":"15865","@position":"102061","annotationId":"T14","@startWordPosition":"15864","@citStr":"Burke (2002)"}]},"title":{"#tail":"\n","#text":"Hybrid recommender systems. User Modeling and User-Adapted Interaction,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R Burke"}}},{"volume":{"#tail":"\n","#text":"22"},"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Carletta, J. 1996. Assessing agreement on classification tasks: The Kappa statistic. Computational Linguistics, 22(2):249?254."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Carletta, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s being compared. Each judgewas given 20 of these cases, and was asked to assess the generated responses on the four criteria listed previously.14 Wemaximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges? assessments would be comparable. Because the judges do not evaluate the same cases, we could not employ standard inter-annotator agreement measures (Carletta 1996). However, it is still necessary to 14 We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur. 620 Marom and Zukerman Empirical Study of Response Automation Methods have some measure of agreement, and control for bias from specific judges or specific cases. This was done by performing pairwise significance testing, treating the data from two judges as independent samples (we used the Wilcoxon Rank-Sum Test for equal medians). We conducted this significance test separately for each method and each of t","@endWordPosition":"10735","@position":"69070","annotationId":"T15","@startWordPosition":"10734","@citStr":"Carletta 1996"}},"title":{"#tail":"\n","#text":"Assessing agreement on classification tasks: The Kappa statistic."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Carletta"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Carmel, D., M. Shtalhaim, and A. Soffer. 2000. eResponder: Electronic question responder. In CoopIS?02 ? Proceedings of the 7th International Conference on Cooperative Information Systems, pages 150?161, Eilat."},"#text":"\n","pages":{"#tail":"\n","#text":"150--161"},"marker":{"#tail":"\n","#text":"Carmel, Shtalhaim, Soffer, 2000"},"title":{"#tail":"\n","#text":"eResponder: Electronic question responder."},"booktitle":{"#tail":"\n","#text":"In CoopIS?02 ? Proceedings of the 7th International Conference on Cooperative Information Systems,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Carmel"},{"#tail":"\n","#text":"M Shtalhaim"},{"#tail":"\n","#text":"A Soffer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Chang, C. C. and C. J. Lin, 2001. LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie .ntu.edu.tw/?cjlin/libsvm."},"#text":"\n","marker":{"#tail":"\n","#text":"Chang, Lin, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" a Radial Basis Function kernel to predict SCs from users? requests.7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package (Chang and Lin 2001). 605 Computational Linguistics Volume 35, Number 4 prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps. 1. Calculate the scores of the sentences in the predicted SCs. 2. Remove redundant sentences from cohesive SCs; these are SCs which contain similar sentences. 3. Calculate the confidence of the generated response. Calculating the score of a sentence. The score of each sentence sj is calculated using the following formula. Score(sj) = m ? i=1 Pr(SCi)? Pr(sj|SCi) (2) where m is the number of SCs, Pr(sj|SCi) is the proba","@endWordPosition":"4229","@position":"27757","annotationId":"T16","@startWordPosition":"4226","@citStr":"Chang and Lin 2001"}},"title":{"#tail":"\n","#text":"LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie .ntu.edu.tw/?cjlin/libsvm."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"C C Chang"},{"#tail":"\n","#text":"C J Lin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Chu-Carroll, J., K. Czuba, J. M. Prager, and A. Ittycheriah. 2003. In question answering, two heads are better than one. In HLT-NAACL 2003 ? Proceedings of the 2003 Language Technology Conference, pages 24?31, Edmonton."},"#text":"\n","pages":{"#tail":"\n","#text":"24--31"},"marker":{"#tail":"\n","#text":"Chu-Carroll, Czuba, Prager, Ittycheriah, 2003"},"location":{"#tail":"\n","#text":"Edmonton."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" (this category encompasses Burke?s weighted, switching, and mixed sub-categories). Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome. More specifically, it belongs to Burke?s switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman?s (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by Chu-Carroll et al (2003) belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke?s cascade sub-category). Because the results of all the methods are comparable, no learning is required: At each stage of the ?cascade of methods,? the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method. 9. Conclusion Despite its theoretical importance and commer","@endWordPosition":"16009","@position":"103066","annotationId":"T17","@startWordPosition":"16006","@citStr":"Chu-Carroll et al (2003)"}},"title":{"#tail":"\n","#text":"In question answering, two heads are better than one."},"booktitle":{"#tail":"\n","#text":"In HLT-NAACL 2003 ? Proceedings of the 2003 Language Technology Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Chu-Carroll"},{"#tail":"\n","#text":"K Czuba"},{"#tail":"\n","#text":"J M Prager"},{"#tail":"\n","#text":"A Ittycheriah"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Dalli, A., Y. Xia, and Y. Wilks. 2004. Adaptive information management: FASiL email summarization system. In COLING?04 ? Proceedings of the 20th International Conference on Computational Linguistics, pages 23?27, Geneva."},"#text":"\n","pages":{"#tail":"\n","#text":"23--27"},"marker":{"#tail":"\n","#text":"Dalli, Xia, Wilks, 2004"},"location":{"#tail":"\n","#text":"Geneva."},"title":{"#tail":"\n","#text":"Adaptive information management: FASiL email summarization system."},"booktitle":{"#tail":"\n","#text":"In COLING?04 ? Proceedings of the 20th International Conference on Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Dalli"},{"#tail":"\n","#text":"Y Xia"},{"#tail":"\n","#text":"Y Wilks"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Delic, K. A. and D. Lahaix. 1998. Knowledge harvesting, articulation, and delivery. The Hewlett-Packard Journal, May:74?81."},"#text":"\n","marker":{"#tail":"\n","#text":"Delic, Lahaix, 1998"},"location":{"#tail":"\n","#text":"May:74?81."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"otech.monash.edu.au. 1 http ://customercare.telephonyonline.com/ar/telecom next generation customer. Submission received: 7 November 2007; revised submission received: 20 March 2009; accepted for publication: 3 June 2009. ? 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven (Barr and Tessler 1995; Watson 1997; Delic and Lahaix 1998). These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998). In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tole","@endWordPosition":"418","@position":"3087","annotationId":"T18","@startWordPosition":"415","@citStr":"Delic and Lahaix 1998"},{"#tail":"\n","#text":"ng the system. With the stringent criterion we have chosen (precision? 0.8), the system yields a good performance for approximately 57% of the requests. Figure 10 Example showing an appropriate response generated by the Sent-Hybrid method. 629 Computational Linguistics Volume 35, Number 4 8. Related Research The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request?response pairs and presents a ranke","@endWordPosition":"14701","@position":"94147","annotationId":"T19","@startWordPosition":"14698","@citStr":"Delic and Lahaix 1998"}]},"title":{"#tail":"\n","#text":"Knowledge harvesting, articulation, and delivery. The Hewlett-Packard Journal,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K A Delic"},{"#tail":"\n","#text":"D Lahaix"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Feng, D., E. Shaw, J. Kim, and E. Hovy. 2006. An intelligent discussion-bot for answering student queries in threaded discussions. In IUI?06 ? Proceedings of the 11th International Conference on Intelligent User Interfaces, pages 171?177, Sydney."},"#text":"\n","pages":{"#tail":"\n","#text":"171--177"},"marker":{"#tail":"\n","#text":"Feng, Shaw, Kim, Hovy, 2006"},"location":{"#tail":"\n","#text":"Sydney."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"esemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request?response pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read re","@endWordPosition":"10087","@position":"64964","annotationId":"T20","@startWordPosition":"10084","@citStr":"Feng et al 2006"}},"title":{"#tail":"\n","#text":"An intelligent discussion-bot for answering student queries in threaded discussions."},"booktitle":{"#tail":"\n","#text":"In IUI?06 ? Proceedings of the 11th International Conference on Intelligent User Interfaces,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Feng"},{"#tail":"\n","#text":"E Shaw"},{"#tail":"\n","#text":"J Kim"},{"#tail":"\n","#text":"E Hovy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Filatova, E. and V. Hatzivassiloglou. 2004. Event-based extractive summarization. In Proceedings of the ACL?04 Workshop on Summarization, pages 104?111, Barcelona."},"#text":"\n","pages":{"#tail":"\n","#text":"104--111"},"marker":{"#tail":"\n","#text":"Filatova, Hatzivassiloglou, 2004"},"location":{"#tail":"\n","#text":"Barcelona."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nd is independent of particular requests. Thus, the SVM for SC1 has a higher reliability than that for SC3, because it is easier for an SVM to learn when SC1 is appropriate (predominantly from the presence of the words faulty and repair). In order to ensure the relevance of the generated replies, we have placed tight restrictions on prediction probability and cluster cohesion (Table 3), which cause the Sent-Pred method to often return partial responses. Removing redundant sentences. After calculating the raw score of each sentence, we use a modified version of the Adaptive Greedy Algorithm by Filatova and Hatzivassiloglou (2004) to penalize redundant sentences in cohesive clusters. This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence?i.e., its score is not decremented). Specifically, given a sentence sk in cluster SCl which contains a sentence with a higher or equal score, the contribution of SCl to Score(sk) (= Pr(SCl)? Pr(sk|SCl)) is subtracted from Score(sk). After applying these penalties, we retain only the sentences whose adjusted score is gr","@endWordPosition":"4940","@position":"32108","annotationId":"T21","@startWordPosition":"4937","@citStr":"Filatova and Hatzivassiloglou (2004)"}},"title":{"#tail":"\n","#text":"Event-based extractive summarization."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL?04 Workshop on Summarization,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E Filatova"},{"#tail":"\n","#text":"V Hatzivassiloglou"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Goldstein, J., V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document summarization by sentence extraction."},"#text":"\n","marker":{"#tail":"\n","#text":"Goldstein, Mittal, Carbonell, Kantrowitz, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" approach, because requests only predict or match portions of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization (Goldstein et al 2000; Barzilay, Elhadad, andMcKeown 2001; Barzilay and McKeown 2005). The appeal of a sentence-level approach is that it supports the generation of a ?combination response? in situations where there is insufficient evidence for a single document containing a full response, but there is enough evidence for parts of responses. Although such a combined response is generally less satisfactory than a full response, the information included in it may address a user?s problem or point the user in the right direction. As argued in the Introduction, when it comes to obtaining information quickly on-line, t","@endWordPosition":"3536","@position":"23259","annotationId":"T22","@startWordPosition":"3533","@citStr":"Goldstein et al 2000"}},"title":{"#tail":"\n","#text":"Multi-document summarization by sentence extraction."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Goldstein"},{"#tail":"\n","#text":"V Mittal"},{"#tail":"\n","#text":"J Carbonell"},{"#tail":"\n","#text":"M Kantrowitz"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In Proceedings of the ANLP/NAACL 2000 Workshop on Automatic Summarization, pages 40?48, Seattle, WA."},"#text":"\n","pages":{"#tail":"\n","#text":"40--48"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Seattle, WA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ANLP/NAACL 2000 Workshop on Automatic Summarization,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Jijkoun, V. and M. de Rijke. 2005. Retrieving answers from frequently asked questions pages on the Web. In CIKM?05 ? Proceedings of the ACM 14th Conference on Information and Knowledge Management, pages 76?83, Bremen."},"#text":"\n","pages":{"#tail":"\n","#text":"76--83"},"marker":{"#tail":"\n","#text":"Jijkoun, de Rijke, 2005"},"location":{"#tail":"\n","#text":"Bremen."},"title":{"#tail":"\n","#text":"Retrieving answers from frequently asked questions pages on the Web. In"},"booktitle":{"#tail":"\n","#text":"CIKM?05 ? Proceedings of the ACM 14th Conference on Information and Knowledge Management,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"V Jijkoun"},{"#tail":"\n","#text":"M de Rijke"}]}},{"volume":{"#tail":"\n","#text":"2"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Lapalme, G. and L. Kosseim. 2003. Mercure: Towards an automatic e-mail follow-up system. IEEE Computational Intelligence Bulletin, 2(1):14?18."},"journal":{"#tail":"\n","#text":"IEEE Computational Intelligence Bulletin,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Lapalme, Kosseim, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"er systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this, to date, there has been little work on corpus-based approaches to help-desk response automation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). A major factor limiting this work is the dearth of corpora?help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of request? response e-mail dialogues between customers and operators at Hewlett-Packard. Observations from this corpu","@endWordPosition":"563","@position":"4109","annotationId":"T23","@startWordPosition":"560","@citStr":"Lapalme and Kosseim 2003"},{"#tail":"\n","#text":"btained by the automatic evaluation of the responses generated by our system and people?s assessments of these responses is unclear, in particular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaning","@endWordPosition":"10023","@position":"64585","annotationId":"T24","@startWordPosition":"10020","@citStr":"Lapalme and Kosseim 2003"},{"#tail":"\n","#text":"edge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request?response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. Bickel and Scheffer (2004) compared the performance of ","@endWordPosition":"14760","@position":"94543","annotationId":"T25","@startWordPosition":"14757","@citStr":"Lapalme and Kosseim 2003"},{"#tail":"\n","#text":"er and shorter than those in our corpus, and the replies shorter and more homogeneous. Malik, Subramaniam, and Kaushik (2007) developed a system that builds question? answer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their systemmatches response sentences to pre-existing templates and returns the templates. Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering. Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individu","@endWordPosition":"15074","@position":"96623","annotationId":"T26","@startWordPosition":"15071","@citStr":"Lapalme and Kosseim (2003)"}]},"title":{"#tail":"\n","#text":"Mercure: Towards an automatic e-mail follow-up system."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"G Lapalme"},{"#tail":"\n","#text":"L Kosseim"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Lekakos, G. and G. M. Giaglis. 2007. A hybrid approach for improving predictive accuracy of collaborative filtering algorithms. User Modeling and User-Adapted Interaction, 17(1):5?40."},"#text":"\n","marker":{"#tail":"\n","#text":"Lekakos, Giaglis, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance. In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience. These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007). Following Lekakos and Giaglis (2007), one approach for achieving this objective consists of applying supervised learning, where a winning method is selected for each case in the training set, all the training cases are labeled accordingly, and then the system is trained to predict a winner for unseen cases. However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision). Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which","@endWordPosition":"11809","@position":"76170","annotationId":"T27","@startWordPosition":"11806","@citStr":"Lekakos and Giaglis (2007)"}},"title":{"#tail":"\n","#text":"A hybrid approach for improving predictive accuracy of collaborative filtering algorithms. User Modeling and User-Adapted Interaction,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"G Lekakos"},{"#tail":"\n","#text":"G M Giaglis"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Leuski, A., R. Patel, D. Traum, and B. Kennedy. 2006. Building effective question answering characters. In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 18?27, Sydney."},"#text":"\n","pages":{"#tail":"\n","#text":"18--27"},"marker":{"#tail":"\n","#text":"Leuski, Patel, Traum, Kennedy, 2006"},"location":{"#tail":"\n","#text":"Sydney."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"hat they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request?response pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read relatively long reques","@endWordPosition":"10091","@position":"64984","annotationId":"T28","@startWordPosition":"10088","@citStr":"Leuski et al 2006"}},"title":{"#tail":"\n","#text":"Building effective question answering characters."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Leuski"},{"#tail":"\n","#text":"R Patel"},{"#tail":"\n","#text":"D Traum"},{"#tail":"\n","#text":"B Kennedy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Lin, C. Y. and E. H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In HLT-NAACL 2003 ? Proceedings of the 2003 Language Technology Conference, pages 71?78, Edmonton."},"#text":"\n","pages":{"#tail":"\n","#text":"71--78"},"marker":{"#tail":"\n","#text":"Lin, Hovy, 2003"},"location":{"#tail":"\n","#text":"Edmonton."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ge exceeds some minimum (e.g., 10%), and chose the method(s) which could adequately answer the largest number of queries in the data set (based on coverage combined with F-score and precision). Table 5 presents the coverage and unique/best coverage of each method (the percentage of queries covered only by this method or for which this method produces a better reply than other methods), and the average and standard deviation of the precision and F-score obtained by each method (calculated over the requests that are covered). 13 We also employed sequence-based measures using the ROUGE tool set (Lin and Hovy 2003), with similar results to those obtained with the word-by-word measures. 613 Computational Linguistics Volume 35, Number 4 Figure 4 Performance of the different methods for each data set: (a) coverage, (b) F-score, and (c) precision. 614 Marom and Zukerman Empirical Study of Response Automation Methods Table 5 Coverage, uniqueness, precision, and F-score for the response-generation methods. Method Coverage Unique Avg. (St dev.) or best Precision F-score Doc-Ret 43% 22% 0.37 (0.34) 0.35 (0.33) Doc-Pred 29% 3% 0.82 (0.21) 0.82 (0.24) Sent-Ret 9% 0% 0.19 (0.19) 0.12 (0.11) Sent-Pred 34% 5% 0.94 (","@endWordPosition":"7844","@position":"50733","annotationId":"T29","@startWordPosition":"7841","@citStr":"Lin and Hovy 2003"}},"title":{"#tail":"\n","#text":"Automatic evaluation of summaries using n-gram co-occurrence statistics."},"booktitle":{"#tail":"\n","#text":"In HLT-NAACL 2003 ? Proceedings of the 2003 Language Technology Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"C Y Lin"},{"#tail":"\n","#text":"E H Hovy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Malik, R., L. V. Subramaniam, and S. Kaushik. 2007. Automatically selecting answer templates to respond to Marom and Zukerman Empirical Study of Response Automation Methods customer emails. In IJCAI?07 ? Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1659?1664, Hyderabad."},"#text":"\n","pages":{"#tail":"\n","#text":"1659--1664"},"marker":{"#tail":"\n","#text":"Malik, Subramaniam, Kaushik, 2007"},"location":{"#tail":"\n","#text":"Hyderabad."},"title":{"#tail":"\n","#text":"Automatically selecting answer templates to respond to Marom and Zukerman Empirical Study of Response Automation Methods customer emails."},"booktitle":{"#tail":"\n","#text":"In IJCAI?07 ? Proceedings of the 20th International Joint Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Malik"},{"#tail":"\n","#text":"L V Subramaniam"},{"#tail":"\n","#text":"S Kaushik"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Marom, Y. and I. Zukerman. 2006. Automating help-desk responses: A comparative study of information-gathering approaches. In Proceedings of the COLING-ACL Workshop on Task-Focused Summarization and Question Answering, pages 40?47, Sydney."},"#text":"\n","pages":{"#tail":"\n","#text":"40--47"},"marker":{"#tail":"\n","#text":"Marom, Zukerman, 2006"},"location":{"#tail":"\n","#text":"Sydney."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"elect more than one sentence (see the subsequent discussion on removing redundant sentences). We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users? requests.7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package (Chang and Lin 2001). 605 Computational Linguistics Volume 35, Number 4 prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps. 1. Calculate the scores of the sentences in the predicted SCs. 2. Remove redundant sentences from cohesive SCs; these are SCs which contain similar sentences. 3. Calculate the confidence of the generated response. Calculating the score of a sentence. The score of each sentence sj is calcul","@endWordPosition":"4210","@position":"27626","annotationId":"T30","@startWordPosition":"4207","@citStr":"Marom and Zukerman 2006"}},"title":{"#tail":"\n","#text":"Automating help-desk responses: A comparative study of information-gathering approaches."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the COLING-ACL Workshop on Task-Focused Summarization and Question Answering,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Y Marom"},{"#tail":"\n","#text":"I Zukerman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Marom, Y. and I. Zukerman. 2007a. Evaluation of a large-scale email response system. In Proceedings of the IJCAI?07 Workshop on Knowledge and Reasoning in Practical Dialogue Systems, pages 28?33, Hyderabad."},"#text":"\n","pages":{"#tail":"\n","#text":"28--33"},"marker":{"#tail":"\n","#text":"Marom, Zukerman, 2007"},"location":{"#tail":"\n","#text":"Hyderabad."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods. Our evaluation is performed by measuring the quality of the generated responses. Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators). In Section 5, we discuss the difficulties associated with such user studies, and describe a human-based evaluation we conducted for a small subset of the responses generated by our system (Marom and Zukerman 2007b). However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones. We employ 10-fold cross-validation, where we split each data set in the corpus into 10 test sets, each comprising 10% of the e-mail dialogues; the remaining 90% of the dialogues constitute the training set. For each of the cross-validation folds, the responses generated for the requests in the test split are compared against the actual responses ","@endWordPosition":"7137","@position":"46203","annotationId":"T31","@startWordPosition":"7134","@citStr":"Marom and Zukerman 2007"},{"#tail":"\n","#text":"t match precisely the model response. However, it is often the case that there is not one single appropriate response to a query, and even a help-desk operator may respond to the same question in different ways on different occasions.  The relationship between the results obtained by the automatic evaluation of the responses generated by our system and people?s assessments of these responses is unclear, in particular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itsel","@endWordPosition":"9983","@position":"64310","annotationId":"T32","@startWordPosition":"9980","@citStr":"Marom and Zukerman (2007"},{"#tail":"\n","#text":" that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request?response pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read relatively long request? response e-mails, which quickly becomes tedious. In order to address these limitations in a practical way, we conducted a small user study where we asked four judges (graduate students from the Faculty of Information Technology at Monash University) to assess the responses generated by our system (Marom and Zukerman 2007a). Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator. Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precisionmeasures obtained in the automatic evaluation.  Informativeness: Is there anything useful in the response that would make it a good automatic response, given that otherwise the customer has 619 Computational Linguistics","@endWordPosition":"10239","@position":"65909","annotationId":"T33","@startWordPosition":"10236","@citStr":"Marom and Zukerman 2007"}]},"title":{"#tail":"\n","#text":"Evaluation of a large-scale email response system."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the IJCAI?07 Workshop on Knowledge and Reasoning in Practical Dialogue Systems,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Y Marom"},{"#tail":"\n","#text":"I Zukerman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Marom, Y. and I. Zukerman. 2007b. A predictive approach to help-desk response generation. In IJCAI?07 ? Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1665?1670, Hyderabad."},"#text":"\n","pages":{"#tail":"\n","#text":"1665--1670"},"marker":{"#tail":"\n","#text":"Marom, Zukerman, 2007"},"location":{"#tail":"\n","#text":"Hyderabad."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods. Our evaluation is performed by measuring the quality of the generated responses. Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators). In Section 5, we discuss the difficulties associated with such user studies, and describe a human-based evaluation we conducted for a small subset of the responses generated by our system (Marom and Zukerman 2007b). However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones. We employ 10-fold cross-validation, where we split each data set in the corpus into 10 test sets, each comprising 10% of the e-mail dialogues; the remaining 90% of the dialogues constitute the training set. For each of the cross-validation folds, the responses generated for the requests in the test split are compared against the actual responses ","@endWordPosition":"7137","@position":"46203","annotationId":"T34","@startWordPosition":"7134","@citStr":"Marom and Zukerman 2007"},{"#tail":"\n","#text":"t match precisely the model response. However, it is often the case that there is not one single appropriate response to a query, and even a help-desk operator may respond to the same question in different ways on different occasions.  The relationship between the results obtained by the automatic evaluation of the responses generated by our system and people?s assessments of these responses is unclear, in particular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itsel","@endWordPosition":"9983","@position":"64310","annotationId":"T35","@startWordPosition":"9980","@citStr":"Marom and Zukerman (2007"},{"#tail":"\n","#text":" that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request?response pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read relatively long request? response e-mails, which quickly becomes tedious. In order to address these limitations in a practical way, we conducted a small user study where we asked four judges (graduate students from the Faculty of Information Technology at Monash University) to assess the responses generated by our system (Marom and Zukerman 2007a). Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator. Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precisionmeasures obtained in the automatic evaluation.  Informativeness: Is there anything useful in the response that would make it a good automatic response, given that otherwise the customer has 619 Computational Linguistics","@endWordPosition":"10239","@position":"65909","annotationId":"T36","@startWordPosition":"10236","@citStr":"Marom and Zukerman 2007"}]},"title":{"#tail":"\n","#text":"A predictive approach to help-desk response generation."},"booktitle":{"#tail":"\n","#text":"In IJCAI?07 ? Proceedings of the 20th International Joint Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Y Marom"},{"#tail":"\n","#text":"I Zukerman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Marom, Y., I. Zukerman, and N. Japkowicz. 2007. A meta-learning approach for selecting between response automation strategies in a help-desk domain. In AAAI-07 ? Proceedings of the 22nd Conference on Artificial Intelligence, pages 907?912, Vancouver."},"#text":"\n","pages":{"#tail":"\n","#text":"907--912"},"marker":{"#tail":"\n","#text":"Marom, Zukerman, Japkowicz, 2007"},"location":{"#tail":"\n","#text":"Vancouver."},"title":{"#tail":"\n","#text":"A meta-learning approach for selecting between response automation strategies in a help-desk domain."},"booktitle":{"#tail":"\n","#text":"In AAAI-07 ? Proceedings of the 22nd Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Y Marom"},{"#tail":"\n","#text":"I Zukerman"},{"#tail":"\n","#text":"N Japkowicz"}]}},{"volume":{"#tail":"\n","#text":"33"},"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Molla?, D. and J. L. Vicedo. 2007. Question answering in restricted domains: An overview. Computational Linguistics, 33(1):41?61."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Molla, Vicedo, 2007"},"title":{"#tail":"\n","#text":"Question answering in restricted domains: An overview."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Molla"},{"#tail":"\n","#text":"J L Vicedo"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Oliver, J. J. 1993. Decision graphs?an extension of decision trees. In Proceedings of the 4th International Workshop on Artificial Intelligence and Statistics, pages 343?350, Fort Lauderdale, FL."},"#text":"\n","pages":{"#tail":"\n","#text":"343--350"},"marker":{"#tail":"\n","#text":"Oliver, 1993"},"location":{"#tail":"\n","#text":"Fort Lauderdale, FL."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ength (MML) criterion (Wallace and Boulton 1968; Wallace 2005). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle. The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request. As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirical","@endWordPosition":"3187","@position":"20950","annotationId":"T37","@startWordPosition":"3186","@citStr":"Oliver 1993"},{"#tail":"\n","#text":" exceeds an applicability threshold. As for Sent-Pred, confidence is calculated using Equation (6). Both applicability thresholds (confidence and number of retrieved sentences) are set to 0 (Table 3). 3.3 Summary The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically, we used Decision Graphs (Oliver 1993) for Doc-Pred, and SVMs (Vapnik 1998) for Sent-Pred.11 Additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters (Sections 3.1.2 and 3.2.2). Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2. As indicated at the beginning of this section, the implementation of these methods requires the selection of different thresholds, which are subjective and application dependent. Table 3 sum","@endWordPosition":"5996","@position":"38669","annotationId":"T38","@startWordPosition":"5995","@citStr":"Oliver 1993"},{"#tail":"\n","#text":" for a given request, we need to combine our estimates of precision and recall into an overall estimate of performance, and then choose the method with the best estimated performance. The standard approach for combining precision and recall is to compute their harmonic mean, F-score, as we have done in our 16 In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases. We posit that this would not have a significant effect on the results, in particular for MML-based classification techniques, such as Decision Graphs (Oliver 1993). 625 Computational Linguistics Volume 35, Number 4 comparative evaluation in Section 4. However, in order to accommodate different levels of preference towards precision or recall, as discussed herein, we use the following weighted F-score calculation (van Rijsbergen 1979). F-score = { w Precision + 1? w Recall }?1 (12) where w is a weight between 0 and 1 given to precision. When w = 0.5 we have the standard usage of F-score (Equation (9)), and for w > 0.5, we have a preference for high precision. For example, for w = 0.5, the precision and recall values of Cluster 16 (Figure 8(b)) translate ","@endWordPosition":"12895","@position":"82870","annotationId":"T39","@startWordPosition":"12894","@citStr":"Oliver 1993"}]},"title":{"#tail":"\n","#text":"Decision graphs?an extension of decision trees."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 4th International Workshop on Artificial Intelligence and Statistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J J Oliver"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Rotaru, M. and D. J. Litman. 2005."},"#text":"\n","marker":{"#tail":"\n","#text":"Rotaru, Litman, 2005"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Rotaru"},{"#tail":"\n","#text":"D J Litman"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"Improving question answering for reading comprehension tests by combining multiple systems. In Proceedings of the AAAI 2005 Workshop on Question Answering in Restricted Domains, pages 46?50, Pittsburgh, PA."},"#text":"\n","pages":{"#tail":"\n","#text":"46--50"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Pittsburgh, PA."},"title":{"#tail":"\n","#text":"Improving question answering for reading comprehension tests by combining multiple systems."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the AAAI 2005 Workshop on Question Answering in Restricted Domains,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Roy, S. and L. V. Subramaniam. 2006. Automatic generation of domain models for call-centers from noisy transcriptions. In COLING-ACL?06 ? Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 737?744, Sydney."},"#text":"\n","pages":{"#tail":"\n","#text":"737--744"},"marker":{"#tail":"\n","#text":"Roy, Subramaniam, 2006"},"location":{"#tail":"\n","#text":"Sydney."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"valuation of the responses generated by our system and people?s assessments of these responses is unclear, in particular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system.","@endWordPosition":"10027","@position":"64612","annotationId":"T40","@startWordPosition":"10024","@citStr":"Roy and Subramaniam 2006"}},"title":{"#tail":"\n","#text":"Automatic generation of domain models for call-centers from noisy transcriptions."},"booktitle":{"#tail":"\n","#text":"In COLING-ACL?06 ? Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Roy"},{"#tail":"\n","#text":"L V Subramaniam"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1983"},"rawString":{"#tail":"\n","#text":"Salton, G. and M. J. McGill. 1983. An Introduction to Modern Information Retrieval. McGraw Hill, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Salton, McGill, 1983"},"publisher":{"#tail":"\n","#text":"McGraw Hill,"},"location":{"#tail":"\n","#text":"New York."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":". This suggests that different response-generation strategies may be suitable, depending on the content of the initiating request and how well it matches previous requests or responses. In our work, we focus on the first two of these situations, where either complete existing responses or parts of responses are reused to address a new request. The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response. This situation suggests a response-automation approach that follows the document retrieval paradigm (Salton and McGill 1983), where a new request is matched with existing response documents (e-mails). However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively. Sometimes requests match each other quite well, suggesting an approach where a new request is matched with an old one, and the corresponding response is reused. However, analysis of our corpus shows that this does not occur very often, because unlike response e-mails, request e-mails exhibit a high language variability: There a","@endWordPosition":"1473","@position":"9943","annotationId":"T41","@startWordPosition":"1470","@citStr":"Salton and McGill 1983"},{"#tail":"\n","#text":"operator is likely to be both coherent and complete. Therefore, if a particular request can be addressed with a single existing response document, then a document reuse approach would be preferred. An important capability of a response-generation system is to be able to determine when such an approach is appropriate, and when there is insufficient evidence to reuse a complete response document. As stated herein, we studied two document-based methods: Document Retrieval andDocument Prediction. 3.1.1 Document Retrieval (Doc-Ret). This method follows a traditional Information Retrieval paradigm (Salton and McGill 1983), where a query is represented by the content terms it contains, and the system retrieves from the corpus a set of documents that best match this query. In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus: (1) previous response e-mails, (2) previous request e-mails, or (3) previous request? response pairs. The first alternative corresponds to the more traditional view of retrieval as applied in question-answering tasks, where the terms in the question are matched to those in the answer documents. We con","@endWordPosition":"2119","@position":"14189","annotationId":"T42","@startWordPosition":"2116","@citStr":"Salton and McGill 1983"},{"#tail":"\n","#text":"imilar responses in the corpus, an appropriate response can still be retrieved). The results of this experiment are shown in Table 1. The first column shows which document retrieval variant is being evaluated. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request?response pairs yields even more retrieved documents. For the cases where retrieval took place, we used F-score (van Rijsbergen 1979; Salton and McGill 1983) to determine the similarity between the response from the top-ranked document and the real response (the formulas for F-score and its contributing factors, recall and precision, appear in Section 4.2). The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request?response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took p","@endWordPosition":"2615","@position":"17313","annotationId":"T43","@startWordPosition":"2612","@citStr":"Salton and McGill 1983"},{"#tail":"\n","#text":"ing set. For each of the cross-validation folds, the responses generated for the requests in the test split are compared against the actual responses generated by help-desk operators for these requests. Although thismethod of assessment is less informative than human-based evaluations, it enables us to evaluate the performance of our systemwith substantial amounts of data, and produce representative results for a large corpus such as ours. We use two measures from Information Retrieval to determine the quality of an automatically generated response: precision and F-score (van Rijsbergen 1979; Salton and McGill 1983). Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response. F-score is the harmonic mean of precision and recall, which measures how much of the information in the model response appears in the generated response. We consider precision separately because it does not penalize missing 612 Marom and Zukerman Empirical Study of Response Automation Methods information, enabling us to better assess our sentence-","@endWordPosition":"7296","@position":"47276","annotationId":"T44","@startWordPosition":"7293","@citStr":"Salton and McGill 1983"}]},"title":{"#tail":"\n","#text":"An Introduction to Modern Information Retrieval."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"G Salton"},{"#tail":"\n","#text":"M J McGill"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Shrestha, L. and K. R. McKeown. 2004. Detection of question-answer pairs in email conversations. In COLING?04 ? Proceedings of the 20th International Conference on Computational Linguistics, pages 889?895, Geneva."},"#text":"\n","pages":{"#tail":"\n","#text":"889--895"},"marker":{"#tail":"\n","#text":"Shrestha, McKeown, 2004"},"location":{"#tail":"\n","#text":"Geneva."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ere data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question?answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to c","@endWordPosition":"15218","@position":"97658","annotationId":"T45","@startWordPosition":"15215","@citStr":"Shrestha and McKeown 2004"}},"title":{"#tail":"\n","#text":"Detection of question-answer pairs in email conversations."},"booktitle":{"#tail":"\n","#text":"In COLING?04 ? Proceedings of the 20th International Conference on Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L Shrestha"},{"#tail":"\n","#text":"K R McKeown"}]}},{"volume":{"#tail":"\n","#text":"9"},"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Soricut, R. and E. Brill. 2006. Automatic question answering using the Web: Beyond the factoid. Information Retrieval, 9(2):191?206."},"journal":{"#tail":"\n","#text":"Information Retrieval,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Soricut, Brill, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"pect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question?answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses. In e-mail-thread summarization, Dalli, Xia, and Wilks (2004) applied a procedu","@endWordPosition":"15253","@position":"97883","annotationId":"T46","@startWordPosition":"15250","@citStr":"Soricut and Brill 2006"},{"#tail":"\n","#text":"Qs, Berger and Mittal (2000) employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. Berger et al (2000) compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval. Two significant differences between help-desk and FAQs are the following.  The responses in the help-desk corpus are personalized, which means that on one hand, we must abstract from them sufficiently to obtain meaningful regularities, and on the other hand, we must be careful not to abstract away specific information that addresses particular issues.  Help-desk responses have much more repetition than ","@endWordPosition":"15632","@position":"100402","annotationId":"T47","@startWordPosition":"15629","@citStr":"Soricut and Brill (2006)"}]},"title":{"#tail":"\n","#text":"Automatic question answering using the Web: Beyond the factoid."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Soricut"},{"#tail":"\n","#text":"E Brill"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1979"},"rawString":{"#tail":"\n","#text":"van Rijsbergen, C. J. 1979. Information Retrieval. Buttersworth, London."},"#text":"\n","marker":{"#tail":"\n","#text":"van Rijsbergen, 1979"},"location":{"#tail":"\n","#text":"Buttersworth, London."},"title":{"#tail":"\n","#text":"Information Retrieval."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"C J van Rijsbergen"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Vapnik, V. N. 1998. Statistical Learning Theory. Wiley-Interscience, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Vapnik, 1998"},"publisher":{"#tail":"\n","#text":"Wiley-Interscience,"},"location":{"#tail":"\n","#text":"New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"As for Sent-Pred, confidence is calculated using Equation (6). Both applicability thresholds (confidence and number of retrieved sentences) are set to 0 (Table 3). 3.3 Summary The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically, we used Decision Graphs (Oliver 1993) for Doc-Pred, and SVMs (Vapnik 1998) for Sent-Pred.11 Additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters (Sections 3.1.2 and 3.2.2). Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2. As indicated at the beginning of this section, the implementation of these methods requires the selection of different thresholds, which are subjective and application dependent. Table 3 summarizes the thresholds required for t","@endWordPosition":"6002","@position":"38706","annotationId":"T48","@startWordPosition":"6001","@citStr":"Vapnik 1998"}},"title":{"#tail":"\n","#text":"Statistical Learning Theory."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"V N Vapnik"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Wallace, C. S. 2005. Statistical and Inductive Inference by Minimum Message Length. Springer, Berlin."},"#text":"\n","marker":{"#tail":"\n","#text":"Wallace, 2005"},"publisher":{"#tail":"\n","#text":"Springer,"},"location":{"#tail":"\n","#text":"Berlin."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"heir representation at a low level of abstraction (bag-of-lemmas). The idea behind the Doc-Pred method is similar to Bickel and Scheffer?s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request?s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case, the clustering is performed by the program Snob, which implements mixture modeling combined with model selection based on the Minimum Message Length (MML) criterion (Wallace and Boulton 1968; Wallace 2005). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.","@endWordPosition":"3096","@position":"20400","annotationId":"T49","@startWordPosition":"3095","@citStr":"Wallace 2005"},{"#tail":"\n","#text":"is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process. 6.1 Training We train the system by clustering the ?experiences? of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively). We then use the program Snob (Wallace and Boulton 1968; Wallace 2005) to cluster these experiences. Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)?these dimensions account for 95% of the variation in the data. The bottom part of Figure 8(b) 623 Computational Linguistics Volume 35, Number 4 Figure 8 Clusters of response-generation methods obtained from the training set: (a) dimensions produced by PCA and (b) sample clusters. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequ","@endWordPosition":"12057","@position":"77799","annotationId":"T50","@startWordPosition":"12056","@citStr":"Wallace 2005"}]},"title":{"#tail":"\n","#text":"Statistical and Inductive Inference by Minimum Message Length."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"C S Wallace"}}},{"volume":{"#tail":"\n","#text":"11"},"#tail":"\n","date":{"#tail":"\n","#text":"1968"},"rawString":{"#tail":"\n","#text":"Wallace, C. S. and D. M. Boulton. 1968. An information measure for classification. The Computer Journal, 11(2):185?194."},"journal":{"#tail":"\n","#text":"The Computer Journal,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Wallace, Boulton, 1968"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"iability. Hence, we keep their representation at a low level of abstraction (bag-of-lemmas). The idea behind the Doc-Pred method is similar to Bickel and Scheffer?s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request?s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case, the clustering is performed by the program Snob, which implements mixture modeling combined with model selection based on the Minimum Message Length (MML) criterion (Wallace and Boulton 1968; Wallace 2005). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the","@endWordPosition":"3094","@position":"20385","annotationId":"T51","@startWordPosition":"3091","@citStr":"Wallace and Boulton 1968"},{"#tail":"\n","#text":"d only after the learning is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process. 6.1 Training We train the system by clustering the ?experiences? of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively). We then use the program Snob (Wallace and Boulton 1968; Wallace 2005) to cluster these experiences. Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)?these dimensions account for 95% of the variation in the data. The bottom part of Figure 8(b) 623 Computational Linguistics Volume 35, Number 4 Figure 8 Clusters of response-generation methods obtained from the training set: (a) dimensions produced by PCA and (b) sample clusters. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be di","@endWordPosition":"12055","@position":"77784","annotationId":"T52","@startWordPosition":"12052","@citStr":"Wallace and Boulton 1968"}]},"title":{"#tail":"\n","#text":"An information measure for classification."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"C S Wallace"},{"#tail":"\n","#text":"D M Boulton"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Watson, I. 1997. Applying Case-Based Reasoning: Techniques for Enterprise Systems. Morgan Kaufmann Publishers, San Mateo, CA."},"#text":"\n","marker":{"#tail":"\n","#text":"Watson, 1997"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann Publishers,"},"location":{"#tail":"\n","#text":"San Mateo, CA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":".Zukerman@infotech.monash.edu.au. 1 http ://customercare.telephonyonline.com/ar/telecom next generation customer. Submission received: 7 November 2007; revised submission received: 20 March 2009; accepted for publication: 3 June 2009. ? 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven (Barr and Tessler 1995; Watson 1997; Delic and Lahaix 1998). These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998). In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may ha","@endWordPosition":"414","@position":"3063","annotationId":"T53","@startWordPosition":"413","@citStr":"Watson 1997"},{"#tail":"\n","#text":"han risk producing a bad one. The decision of what is a bad response should be made by the organization using the system. With the stringent criterion we have chosen (precision? 0.8), the system yields a good performance for approximately 57% of the requests. Figure 10 Example showing an appropriate response generated by the Sent-Hybrid method. 629 Computational Linguistics Volume 35, Number 4 8. Related Research The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). eResponder, the syst","@endWordPosition":"14683","@position":"94030","annotationId":"T54","@startWordPosition":"14682","@citStr":"Watson 1997"}]},"title":{"#tail":"\n","#text":"Applying Case-Based Reasoning: Techniques for Enterprise Systems."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"I Watson"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Witten, I. H. and E. Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann Publishers, San Francisco, CA."},"#text":"\n","marker":{"#tail":"\n","#text":"Witten, Frank, 2000"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann Publishers,"},"location":{"#tail":"\n","#text":"San Francisco, CA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" This motivates the use of multi-document summarization techniques, rather than question-answering approaches, to extract individual answers. These issues also differentiate the help-desk application from other types of questionanswering applications, specifically those found in the field of restricted domain question answering (Molla? and Vicedo 2007). 631 Computational Linguistics Volume 35, Number 4 In addition to the different response-generationmethods, we have proposed ametalevel strategy to combine them. This kind of meta-learning is referred to as stacking by the DataMining community (Witten and Frank 2000). Lekakos andGiaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems, merging and ensemble, each subdivided into themore specific subclasses suggested by Burke (2002) as follows. Themerging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke?s feature combination, cascade, feature augmentation, and meta-level sub-categories). The ensemble category co","@endWordPosition":"15819","@position":"101728","annotationId":"T55","@startWordPosition":"15816","@citStr":"Witten and Frank 2000"}},"booktitle":{"#tail":"\n","#text":"Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"I H Witten"},{"#tail":"\n","#text":"E Frank"}]}}]}}]}}
