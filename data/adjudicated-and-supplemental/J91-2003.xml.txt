 construct new models. We also show metalevel rules for interpreting &quot;but.&quot; 2. The Paragraph as a Discourse Unit 2.1 Approaches to Paragraph Analysis Recent syntactic theory--that is, in the last 30 years--has been preoccupied with sentence-level analysis. Within discourse theory, however, some significant work has been done on the analysis of written paragraphs. We can identify four different linguis- tic approaches to paragraphs: prescriptivist, psycholinguist, extualist, and discourse- oriented. The prescriptivist approach is typified in standard English grammar textbooks, such as Warriner (1963). In these sources, a paragraph is notionally defined as some- thing like a series of sentences that develop one single topic, and rules are laid down for the construction of an ideal (or at least an acceptable) paragraph. Although these dictates are fairly clear, the underlying notion of topic is not. An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, b
ur different linguis- tic approaches to paragraphs: prescriptivist, psycholinguist, extualist, and discourse- oriented. The prescriptivist approach is typified in standard English grammar textbooks, such as Warriner (1963). In these sources, a paragraph is notionally defined as some- thing like a series of sentences that develop one single topic, and rules are laid down for the construction of an ideal (or at least an acceptable) paragraph. Although these dictates are fairly clear, the underlying notion of topic is not. An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count i formation. Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al (1980). The textualist approach to paragraph 
mple of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count i formation. Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed escriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and r
y oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count i formation. Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed escriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sen
jectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count i formation. Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed escriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called &quot;episodes,&quot; and sometimes &quot;paragraphs.&quot; Accord- ing to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragr
nomic, in that he performs detailed escriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called &quot;episodes,&quot; and sometimes &quot;paragraphs.&quot; Accord- ing to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). S
types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists how little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and o
ocedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists how little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about 
sitory (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists how little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central topic
 and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists how little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central topic. A paragraph can be tho
hat a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central topic. A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance, Sells (1985, p. 8) says that the sentence &quot;Reagan thinks bananas,&quot; which is otherwise strange, is in fact acceptable if it occurs as an answer to the question &quot;What is Kissinger's favorite fruit?&quot; The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance &quot;Reagan thinks bananas&quot; only within the paragraph inwhich this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one)
sina&quot; is the subject of '`bringing&quot; and must be the referent for &quot;it.&quot; If the clause modifies &quot;port,&quot; then &quot;port&quot; is the desired referent; if the clause is attached at the level of the main verb of the sentence, then &quot;ship&quot; is the referent. But syntactic relations do not suffice to resolve anaphora: Hobbs' (1976) algorithm for resolving the reference of pronouns, depending only on the surface syntax of sentences in the text, when applied to &quot;it&quot; in the example paragraph, fails in both cases to identify the most likely referent NP. Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation. Later, Hobbs (1979, 1982) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using &quot;salience&quot; in choosing facts from this knowledge base. We will investigate the possibility that the structure of this knowledge base can actually resemble the structure of, for example, natural language dictionaries. The process of finding referents could then be automated. Determining that the most lik
ttached at the level of the main verb of the sentence, then &quot;ship&quot; is the referent. But syntactic relations do not suffice to resolve anaphora: Hobbs' (1976) algorithm for resolving the reference of pronouns, depending only on the surface syntax of sentences in the text, when applied to &quot;it&quot; in the example paragraph, fails in both cases to identify the most likely referent NP. Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation. Later, Hobbs (1979, 1982) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using &quot;salience&quot; in choosing facts from this knowledge base. We will investigate the possibility that the structure of this knowledge base can actually resemble the structure of, for example, natural language dictionaries. The process of finding referents could then be automated. Determining that the most likely subject for &quot;bringing,&quot; in the first sentence, is the noun &quot;ship&quot; is done in the following fashion. The first definition for &quot;bring&quot; in W7 (Webster's Sev
 to some hand-coded body of predicate assertions, for making these rela- tionships. This demonstrates that information eeded to identify and resolve anaphoric ref- erences can be found, to an interesting extent, in standard ictionaries and thesauri. (Other reference works could be treated as additional sources of world knowledge.) This type of consultation uses existing natural language texts as a referential evel for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. Crothers (1979, p. 112), for example, bemoans the fact that his &quot;theory lacks a world knowledge component, a mental 'encyclopedia,' which could be invoked to gen- erate inferences... &quot; With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation. In other words, we recognize it as a separate logical evel--the referential level. Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill
on in order to produce an interpretation of a text. The formalist will be presented in a number of steps in which we will elaborate one simple example: Example 1 Entering the port, a ship brought a disease. This sentence can be translated into the logical formula (ignoring only the past tense of &quot;bring&quot; and the progressive of &quot;enter'9: Definition S: enter(xl~ x2) & ship(x1) & port(x2) & bring(x3~ x4) & disease(x4) & xl = s & x2 = m & x3 = s & x4 = d, where s, m, d, are constants. We adopt he three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense r asoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural anguage inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating &quot;they&quot; to &quot;apples&quot; in the sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a): We bought the boys apples because they were so cheap 176 Zadrozny and Jensen Semantics of Paragraphs can be an example of such a most plausible choice. The main ideas of
& disease(x4) & xl = s & x2 = m & x3 = s & x4 = d, where s, m, d, are constants. We adopt he three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense r asoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural anguage inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating &quot;they&quot; to &quot;apples&quot; in the sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a): We bought the boys apples because they were so cheap 176 Zadrozny and Jensen Semantics of Paragraphs can be an example of such a most plausible choice. The main ideas of the three-level semantics can be stated as follows: 1. Reasoning takes place in a three-level structure consisting of an object level, a referential evel, and a metalevel. 2. The object level is used to describe the current situation, and in our case is reserved for the formal representation f paragraph sentences. For the sake of simplicity, the object level will consist of a first order theory. 3. Th
nding of a given piece of text has to be extracted. It constrains interpretations of the predicates of an object theory. Its structure and the extraction methods will be discussed below. 4. Understanding has as its goal construction of an interpretation of the text, i.e. building some kind of model. Since not all logically permissible models are linguistically appropriate, one needs a place, namely the metalevel, to put constraints on types of models. Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them. We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referen- tial levels for commonsense r asoning--for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner 
of text has to be extracted. It constrains interpretations of the predicates of an object theory. Its structure and the extraction methods will be discussed below. 4. Understanding has as its goal construction of an interpretation of the text, i.e. building some kind of model. Since not all logically permissible models are linguistically appropriate, one needs a place, namely the metalevel, to put constraints on types of models. Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them. We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referen- tial levels for commonsense r asoning--for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (1987, pp. 7-8)
ot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referen- tial levels for commonsense r asoning--for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (1987, pp. 7-8): ... Semantics i constrained by our models of ourselves and our worlds. We have models of up and down that are based by the way our bodies actually function. Once the word &quot;up&quot; is given its meaning relative to our experience with gravity, it is not free to &quot;slip&quot; into its opposite. &quot;Up&quot; means up and not down . . . . We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model. Mothers have a different role than fathers in this model, and thus th
nd this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model. Mothers have a different role than fathers in this model, and thus there is a reason why &quot;Death is the father of beauty&quot; fails poetically while &quot;Death is the mother of beauty&quot; succeeds .... It is precisely this &quot;grounding&quot; of logical predicates in other conceptual structures that we would like to capture. We investigate here only the &quot;grounding&quot; in logical the- ories. However, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences (cf. Woods 1987). We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identifie
e to think about constraining linguistic or logical predicates by simulating physical experiences (cf. Woods 1987). We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another pos- sibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate ncoding of the info
mantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another pos- sibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate ncoding of the infor- mation it conveys.&quot; Therefore a formalism of the kind he advocates would probably be most suitable for an implementation f our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision asystem that uses data struc- tures produced by 
ing the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another pos- sibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate ncoding of the infor- mation it conveys.&quot; Therefore a formalism of the kind he advocates would probably be most suitable for an implementation f our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision asystem that uses data struc- tures produced by a computational grammar to obtain the logical form of sentences. 3.1 Finite Representations, Finite Theories Unless explicitly stated otherwise, we assume that formulas are
an envision asystem that uses data struc- tures produced by a computational grammar to obtain the logical form of sentences. 3.1 Finite Representations, Finite Theories Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references. This means that natural language expressions such as &quot;A is B,&quot; &quot;A is the same as B,&quot; etc. are not directly represented by logical equality; similarly, &quot;not&quot; is often not treated as logical negation; cf. Hintikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred ele- ments of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1
gical negation; cf. Hintikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred ele- ments of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu- tational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from forma
ntikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred ele- ments of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu- tational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations o
logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred ele- ments of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu- tational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations of text and ba
 lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu- tational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations of text and background knowledge. But there are several ways to avoid this obstruction. For example, consider theories consisting of universal formulas without function symbols. Let Th(T) of such a theory T be defined as T plus ground clauses/sets of literals prov- able from T in standard logic. It is easily seen that it is a closure, i.e. 
nterpretation defined on a certain domain, which satisfies all formulas of T. The collection of all (finite) models of a theory T will be denoted by Mods(T). ? The set of all subformulas of a collection of formulas F is denoted by Form(F). ~ is a ground instance of a formula G if ~ contains no variables, and ~ = ~, for some substitution 0. Thus, we do not require Th(T) to be closed under substitution i stances of tautologies. Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect o weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985). But we won't pursue this topic further here. 3.2 The Structure of Background Knowledge Background knowledge is not a simple list of meaning postulates--it has a structure and it may contain contradictions and ambiguities. These actualities have to be taken into account in any realistic model of natural language understanding. For instance, the verb &quot;enter&quot; is polysemous. But, unless context specifies otherwise, &quot;to come in&quot; is a more plausible meaning than &quot;to join a group.&quot; Assuming some logical representation of this knowledge, we can write that enter(x,
efined on a certain domain, which satisfies all formulas of T. The collection of all (finite) models of a theory T will be denoted by Mods(T). ? The set of all subformulas of a collection of formulas F is denoted by Form(F). ~ is a ground instance of a formula G if ~ contains no variables, and ~ = ~, for some substitution 0. Thus, we do not require Th(T) to be closed under substitution i stances of tautologies. Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect o weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985). But we won't pursue this topic further here. 3.2 The Structure of Background Knowledge Background knowledge is not a simple list of meaning postulates--it has a structure and it may contain contradictions and ambiguities. These actualities have to be taken into account in any realistic model of natural language understanding. For instance, the verb &quot;enter&quot; is polysemous. But, unless context specifies otherwise, &quot;to come in&quot; is a more plausible meaning than &quot;to join a group.&quot; Assuming some logical representation of this knowledge, we can write that enter(x, y) --* {come
theories of &quot;enter,&quot; ship,&quot; etc. and the partial orders are represented graphically; more plausible theories are positioned higher. A path through this graph chooses an interpretation of the sentence S. For instance, the path f int = {el, shl~ pl, bl, dl} and S say together that A large boat (ship) that carries people or goods came into the harbor and carried a disease (illness). Since it is the &quot;highest&quot; path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence. Because it is also consistent, it will be chosen as a best interpretation of S, (cf. Zadrozny 1987a, 1987b). Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering. As it turns out, f~ is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story. The collection of these most plausible consistent interpretations of a given theory T is denoted by PT< (T). Then fint belongs to PT< (Th({S})), but
stent theories of T given by (H(F), <), where F = Form(T): PT<(T) = {TUT' :T '= ?f and f is a maximal element of (I~I(F), <)} Notice that PT< (T) can contain more than one theory, meaning that T is ambiguous. This is a consequence of the fact that the cartesian product is only partially ordered by <. The main reason for using ground instances ~i(Ci) in modifying the orderings is the need to deal with multiple occurrences of the same predicate, as in John went to the bank by the bank. 184 Zadrozny and Jensen Semantics of Paragraphs The above construction is also very close in spirit to Poole's (1988) method for default reasoning, where object theories are augmented by ground instances of defaults. 3.3.3 Coherence Links. The reasoning that led to the intended interpretation fi t in our discussion of dominance was based on the partial ordering of the theories of R. We want to exploit now another property of the theories of R--their coherence. Finding an interpretation for a natural anguage text or sentence typically involves an appeal to coherence. Consider $2: Entering the port, a ship brought adisaster. Using the coherence link between (b2) and (dr1) (cf. Section 3.2)--the presence of cau
 if all background knowledge were described, as in our examples, by sets of first order theories, because of the preferences and inconsistencies of meanings, we could not treat R as a flat database of facts--such a model simply would not be realistic. Rather, R must be treated as a separate logical level for these syntactic reasons, and because of its function--being a pool of possibly conflicting semantic onstraints. The last point may be seen better if we look at some differences between our system and KRYPTON, which also distinguishes between an object heory and back- ground knowledge (cf. Brachman et al 1985). KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains informa- tion expressed in a frame-based language quivalent to a fragment of FOL. However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior. From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard. In our system, we also distinguish between the &quot;definitional&quot; and factual information, but the &quot;definitional&quot; part contains collections of m
tween the &quot;definitional&quot; and factual information, but the &quot;definitional&quot; part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, &quot;coherence&quot; and &quot;dominance,&quot; which are not variants of the standard first order entailment, but abduction. The idea of using preferences among theories is new, hence it was described in more detail. &quot;Coherence,&quot; as outlined above, can be understood as a declarative (or static) version of marker passing (Hirst 1987; Charniak 1983), with one difference: the activation spreads to theories that share a predicate, not through the IS-A hierarchy, and is limited to elementary facts about predicates appearing in the text. The metalevel rules we are going to discuss in Section 6, and that deal with the Gricean maxims and the meaning of &quot;but,&quot; can be easily expressed in the languages of set theory or higher order logic, but not everything expressible in those languages makes sense in natural language. Hence, putting limitations on the expressive power of the language of the metalevel will remain as one of many o
efinitional&quot; and factual information, but the &quot;definitional&quot; part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, &quot;coherence&quot; and &quot;dominance,&quot; which are not variants of the standard first order entailment, but abduction. The idea of using preferences among theories is new, hence it was described in more detail. &quot;Coherence,&quot; as outlined above, can be understood as a declarative (or static) version of marker passing (Hirst 1987; Charniak 1983), with one difference: the activation spreads to theories that share a predicate, not through the IS-A hierarchy, and is limited to elementary facts about predicates appearing in the text. The metalevel rules we are going to discuss in Section 6, and that deal with the Gricean maxims and the meaning of &quot;but,&quot; can be easily expressed in the languages of set theory or higher order logic, but not everything expressible in those languages makes sense in natural language. Hence, putting limitations on the expressive power of the language of the metalevel will remain as one of many open problems. 4.
dicates. Now we want to restrict he notion of a partial theory by introducing the formal notions of topic and coherence. We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions. The partial theories pick up from the referential level the most obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imp
t obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information ecessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct o
ous or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information ecessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvio
t iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information ecessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect hat in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions. Let P be a paragraph, let /3 = ($1,..., Sn) be its translation into a sequence of logical formulas. The set o
real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information ecessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect hat in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions. Let P be a paragraph, let /3 = ($1,..., Sn) be its translation into a sequence of logical formulas. The set of all predicates appearing in X will be denoted by Pred(X). Definition Let T be a partial theory of a paragraph P. A sequence of predicates appearing in i6, denoted by Tp, is called a to
ions we talk about would produce an in- consistent theory; hence, the temporal, causal, and other aspects would be dealt with by consistency. But of course at this point it is just a hypothesis. An important aspect of the definition is that coherence has been defined as a prop- erty of representation--in our case, it is a property of a formal theory. The existence of the topic, the direct or indirect allusion to it, and anaphora (which will be addressed below) take up the issue of formal criteria for a paragraph definition, which was raised 189 Computational Linguistics Volume 17, Number 2 by Bond and Hayes (1983) (cf. also Section 2.1). The question of paragraph length can probably be attended to by limiting the size of p-models, perhaps after introducing some kind of metric on logical data structures. Still, our definition of coherence may not be restrictive nough: two collections of sentences, one referring to &quot;black&quot; (about black pencils, black pullovers, and black poodles), the other one about &quot;death&quot; (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about he new, broader topic &quot;black + death.&quot; This problem may be similar to the situatio
rictive nough: two collections of sentences, one referring to &quot;black&quot; (about black pencils, black pullovers, and black poodles), the other one about &quot;death&quot; (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about he new, broader topic &quot;black + death.&quot; This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words (e.g., &quot;colorless green ideas... '9, while before the advent of Chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words; Fowler (1965, p. 546) gives 10 definitions of a sentence. It then seems worth differentiating between the creation of a new concept like &quot;black + death,&quot; with a meaning given by a paraphrase of the example collection of sentences, and the acceptance of the new concept--storing it in R. In our case the concept &quot;black + death,&quot; which does not refer to any normal experiences, would be discarded as useless, although the collection of sentences would be recognized as a strange, even if coherent, paragraph. We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive xamples.
differentiating between the creation of a new concept like &quot;black + death,&quot; with a meaning given by a paraphrase of the example collection of sentences, and the acceptance of the new concept--storing it in R. In our case the concept &quot;black + death,&quot; which does not refer to any normal experiences, would be discarded as useless, although the collection of sentences would be recognized as a strange, even if coherent, paragraph. We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive xamples. This approach is taken in computational syntactic grammars (e.g. Jensen 1986); the number of unlikely parses is severely reduced whenever pos- sible, but no attempt is made to define only the so-called grammatical strings of a language. Finally, as the paragraph is a natural domain in which word senses can be reliably assigned to words or sentences can be syntactically disambiguated, larger chunks of discourse may be needed for precise assignment of topics, which we view as another type of disambiguation. Notice also that for coherence, as defined above, it does not matter whether the topic is defined as a longest, a shortest, or--simply--a sequence of predicates satis
 defined as a longest, a shortest, or--simply--a sequence of predicates satisfying the conditions (1) and (2); the existence of a sequence is equivalent with the existence of a shortest and a longest sequence. The reason for choosing a longest sequence as the topic is our belief that the topic should rather contain more information about a paragraph than less. 4.1 Comparison with Other Approaches At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others. We are going to make such a comparison with the theories proposed by J. Hobbs (1979, 1982) that represent a more computa- tionally oriented approach to coherence, and those of T.A. van Dijk and W. Kintch (1983), who are more interested in addressing psychological nd cognitive aspects of discourse coherence. The quoted works seem to be good representatives for each of the directions; they also point to related literature. The approach we advocate is compatible with the work of these researchers, we believe. There are, however, some interesting differences: first of all, we emphasize the role of paragraphs; econd, we talk about formal principles regulating the organization and
, as Hobbs' semantics eems to be.) 190 Zadrozny and Jensen Semantics of Paragraphs We shall discuss only the first two points, since the third one has already been explained. The chief difference between our approach and the other two lies in identifying the paragraph as a domain of coherence. Hobbs, van Dijk, and Kintch distinguish between &quot;local&quot; coherence~a property of subsequent sentences--and &quot;global&quot; coherence---a property of discourse as a whole. Hobbs explains coherence in terms of an inventory of &quot;local,&quot; possibly computable, coherence relations, like &quot;elaboration,&quot; &quot;occasion,&quot; etc. (Mann and Thompson 1983 give an even more detailed list of coherence relations than Hobbs.) Van Dijk and Kintch do this too, but they also describe &quot;macrostructures&quot; representing the global content of discourse, and they emphasize psychological and cognitive strategies used by people in establishing discourse coherence. Since we have linked coherence to models of paragraphs, we can talk simply about &quot;coherence&quot;-- without adjectives--as property of these models. To us the first &quot;local&quot; domain seems to be too small, and the second &quot;global&quot; one too large, for constructing meaningful computational models. To be sure, we
meaningful computational models. To be sure, we believe relations between pairs of sentences are worth investigating, especially in dialogs. However, in written discourse, the smallest domain of coherence is a paragraph, very much as the sentence is the basic domain of grammaticality (although one can also judge the correctness of phrases). To see the advantage of assuming that coherence is a property of a fragment of a text/discourse, and not a relation between subsequent sentences, let us consider for instance the text John took a train from Paris to Istanbul. He likes spinach. According to Hobbs (1979, p. 67), these two sentences are incoherent. However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.) suddenly (for Hobbs) becomes coherent. It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added. On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of
t. Example 2 PI: In 1347 a ship entered the port of Messina bringing with it the disease that came to be known as the Black Death. P2: It struck rapidly. P3: Within twenty-four hours of infection came an agonizing death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents. We assume that any formal interpretation f time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation f events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence b tween syntax and semantics expressed in the gram- matical constraint (&quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&quot;---ibid.). Howe
a ship entered the port of Messina bringing with it the disease that came to be known as the Black Death. P2: It struck rapidly. P3: Within twenty-four hours of infection came an agonizing death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents. We assume that any formal interpretation f time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation f events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence b tween syntax and semantics expressed in the gram- matical constraint (&quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&quot;---ibid.). However, as noted 
 death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents. We assume that any formal interpretation f time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation f events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence b tween syntax and semantics expressed in the gram- matical constraint (&quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&quot;---ibid.). However, as noted before, we will use a simplified version of such a logical no- tation; we will have only time, event, result, and property as primitives. After these remarks we can begin constructing the m
on. This relation would hold, for instance, between the object heory of our example paragraph and a formula expressing the equality of two constants, i and i', denoting (respectively) the &quot;infection&quot; in the sentence Within twenty- four hours of infection . . . . and the &quot;infection&quot; of the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because
f our example paragraph and a formula expressing the equality of two constants, i and i', denoting (respectively) the &quot;infection&quot; in the sentence Within twenty- four hours of infection . . . . and the &quot;infection&quot; of the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because it is enough to find one partial theory proving a g
example paragraph and a formula expressing the equality of two constants, i and i', denoting (respectively) the &quot;infection&quot; in the sentence Within twenty- four hours of infection . . . . and the &quot;infection&quot; of the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because it is enough to find one partial theory proving a given fo
the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because it is enough to find one partial theory proving a given formula.) As an example, in the case of the three-sentence paragraph, we have a partial theory T1 based on (slb) saying that &quot; 'it' hits rapidly,&quot; and T2 saying that &quot;an illness ('it') harms rapidly&quot; (s2_ex
ed model of a theory T is an element of Mods(T) that satisfies metalevel con- straints contained in M. The set of all preferred models of T is denoted by PM(T). A formula 4 of L(=), the language with equality, is weakly R + M-abductible from an object heory T, denoted by T ~-a+M G iff there exists a partial theory T E PT(T) and a preferred model M E PM(T) such that M ~ G i.e. 4 is true in at least one preferred model of the partial theory T. Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing &quot;there exists&quot; by &quot;all&quot; in the above definitions (cf. Zadrozny 1987b). We will have, however, no need for &quot;strong&quot; notions in this paper. Also, in a practical system, &quot;satisfies&quot; should be probably replaced by &quot;violates fewest.&quot; Obviously, it is better to have references of pronouns resolved than not. After all, we assume that texts make sense, and that authors know these references. That applies to references of noun phrases too. On the other hand, there must be some restrictions on possible references; we would rather assume that &quot;spinach&quot; ~ &quot;train&quot; (i.e. V x,y)(spinach(x) & train(y) --, x # y)), or &quot;ship&quot; # &quot;disease.&quot; Two elementary conditions limiting the
f Ockham's razor or abduction; it says &quot;minimize the number of things that have the property P(,, ,),&quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic lues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric (with the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, s
r abduction; it says &quot;minimize the number of things that have the property P(,, ,),&quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic lues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric (with the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is alw
umber of things that have the property P(,, ,),&quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic lues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric (with the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of re
termining the relative importance of those factors, the above metarules, and syntactic lues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric (with the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But,
th the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are b
 means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a parag
 is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential
phora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential level while the metalevel restricts ways that the model
n of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential level while the metalevel restricts ways that the model can be constructed, or, in other 
liday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential level while the metalevel restricts ways that the model can be constructed, or, in other words, provides criteria for choosing a most
ed to be true. The Maxim of Manner seems to us to be more relevant for critiquing the style of a written passage or for natural anguage generation; in the case of text generation, it can be construed as a requirement that the produced text be coherent and cohesive. We do not claim that Gla is the best or unique way of expressing the rule &quot;assume that the writer did not say too much.&quot; Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example: two sentences, regarded as a fragment of paragraph, are a variation on a theme by Hobbs (1979). Example 3 The captain is worried because the third officer can open his safe. He knows the combination. The above metarule postulating &quot;nonredundancy&quot; implies that &quot;he&quot; = &quot;the third officer, .... his&quot; = &quot;the captain's&quot; are the referents of the pronouns. This is because the formula safe(x) --, (owns(y~ x) & cmbntn(z~ x) --, knows(y~ z) & can_open(y~ x)) E Tsafe, belongs to R, since it is common knowledge about safes that they have owners, and also combinations that are known to the owners. Therefore &quot;his&quot; = &quot;the third officer's&quot; would produce a redundant formula, corresponding to the sentence
st plausible interpretation f our example sentences. Note: The reader must have noticed that we did not bother to distinguish the sentences P1, P2, Q1 and Q2 from their logical forms. Representing &quot;because&quot; and &quot;know&quot; adequately should be considered a separate topic; representing the rest (in the first order convention of this paper) is trivial. 6.1.1 Was the Use of a Gricean Maxim Necessary? Can one deal effectively with the problem of reference without axiomatized Gricean maxims, for instance by using only 201 Computational Linguistics Volume 17, Number 2 &quot;petty conversational implicature&quot; (Hobbs 1979), or the metarules of Section 5.2? It seems to us that the answer is no. As a case in point, consider the process of finding the antecedent of the anaphor &quot;he&quot; in the sentences John can open Bill's safe. He knows the combination. Hobbs (1979, 1982) proves &quot;he&quot; = &quot;John&quot; by assuming the relation of &quot;elaboration&quot; between the sentences. (Elaboration is a relation between two segments of a text. It intuitively means &quot;expressing the same thought from a different perspective,&quot; but has been defined formally as the existence of a proposition implied by both segments-- here the proposition is &quot;John can 
ules that prevent infelicitous uses of &quot;but.&quot; Connectives are function words--like conjunctions and some adverbs--that re responsible simultaneously for maintaining cohesiveness within the text and for sig- naling the nature of the relationships that hold between and among various text units. &quot;And,&quot; &quot;or,&quot; and &quot;but&quot; are the three main coordinating connectives in English. How- ever, &quot;but&quot; does not behave quite like the other two--semantically, &quot;but&quot; signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition (called &quot;adversative&quot; or &quot;contrary-to-expectation&quot; by Halliday and Hasan 1976; cf. also Quirk et al 1972, p. 672). The ship arrived but the passengers could not get off. The yacht is cheap but elegant. Comparison. In this function, the first conjunct is not so directly contradicted by the second. A contradiction exists, but we may have to 202 Zadrozny and Jensen Semantics of Paragraphs go through additional levels of implication to find it. Consider the sentence: . That basketball player is short, but he's very quick. Affirmation. This use of &quot;but&quot; always follows a negative clause, and actually augments the meaning of the preceding clause by adding supporting informati
s of &quot;but.&quot; Connectives are function words--like conjunctions and some adverbs--that re responsible simultaneously for maintaining cohesiveness within the text and for sig- naling the nature of the relationships that hold between and among various text units. &quot;And,&quot; &quot;or,&quot; and &quot;but&quot; are the three main coordinating connectives in English. How- ever, &quot;but&quot; does not behave quite like the other two--semantically, &quot;but&quot; signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition (called &quot;adversative&quot; or &quot;contrary-to-expectation&quot; by Halliday and Hasan 1976; cf. also Quirk et al 1972, p. 672). The ship arrived but the passengers could not get off. The yacht is cheap but elegant. Comparison. In this function, the first conjunct is not so directly contradicted by the second. A contradiction exists, but we may have to 202 Zadrozny and Jensen Semantics of Paragraphs go through additional levels of implication to find it. Consider the sentence: . That basketball player is short, but he's very quick. Affirmation. This use of &quot;but&quot; always follows a negative clause, and actually augments the meaning of the preceding clause by adding supporting information: The disease not only ki
hat a theory of some ? is a &quot;law&quot; is by deleting the empty interpretation f ? from the partial order. If we accept he definition of a concept as given by necessary and sufficient conditions, the theories would all appear as laws. If we subscribe to a more realistic view where definitions are given by a collection of central/prototypical and peripheral conditions, only the peripheral ones can be contradicted by &quot;but.&quot; In either formalization we get BUT_C1 as a consequence: Since &quot;laws&quot; cannot be deleted, BUT can't be applied, and hence its use in those kinds of sentences would be incorrect. W. Labov (1973) discussed sentences of the form ,This is a chair but you can sit on it. The sentence is incorrect, since the function &quot;one can sit on it&quot; belongs to the core of the concept &quot;chair&quot;; so--contrary to the role of &quot;but&quot;--the sentence does not contain any surprising new elements. Using the Metarule (BUT) and the cooperative principle of Grice, we get BUT_C2: ? but ? is incorrect, if ? -* k~ is a &quot;law.&quot; The Metarule (BUT) gives the semantics of &quot;but;&quot; the rules BUT_C1 and BUT_C2 follow from it (after formalization i a sufficiently strong rnetalanguage such as type theory or set theory). We can link
