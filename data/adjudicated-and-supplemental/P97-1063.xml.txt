ters can be eas- ily conditioned on information extrinsic to the model, providing an easy way to inte- grate pre-existing knowledge such as part- of-speech, dictionaries, word order, etc.. Our model can link word tokens in paral- lel texts as well as other translation mod- els in the literature. Unlike other transla- tion models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melb
y conditioned on information extrinsic to the model, providing an easy way to inte- grate pre-existing knowledge such as part- of-speech, dictionaries, word order, etc.. Our model can link word tokens in paral- lel texts as well as other translation mod- els in the literature. Unlike other transla- tion models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross
formation extrinsic to the model, providing an easy way to inte- grate pre-existing knowledge such as part- of-speech, dictionaries, word order, etc.. Our model can link word tokens in paral- lel texts as well as other translation mod- els in the literature. Unlike other transla- tion models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual informatio
ccuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models
uct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 a
 oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 also shows how the one- to-one assumption enables us to use a new gr
easingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 also shows how the one- to-one assumption enables us to use a new greedy competitive linki
ms that consider a much larger set of word cor- respondence possibilities. The model uses two hid- den parameters to estimate the confidence of its own predictions. The confidence stimates enable direct control of the balance between the model's preci- sion and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. 2 Co-occur rence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in 
ch larger set of word cor- respondence possibilities. The model uses two hid- den parameters to estimate the confidence of its own predictions. The confidence stimates enable direct control of the balance between the model's preci- sion and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. 2 Co-occur rence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is 
 possibilities. The model uses two hid- den parameters to estimate the confidence of its own predictions. The confidence stimates enable direct control of the balance between the model's preci- sion and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. 2 Co-occur rence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tati
on of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parame
 texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their 
ages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n
xtrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n(u,v) and inversely proportional to their marginal frequen- cies n(u) and n(v) z, following (Dunning, 1993) 2. When the L(u, v) are re-estimated, the model's hid- den parameters come into play. After initialization, the model induction algorithm iterates: 1. Find a set of &quot;links&quot; among word tokens in the bitext, using the likelihood ratios and the com- petitive linking algorithm. 2. Use the links to re-estimate A+, A-, and the likelihood ratios. 3. Repeat from Step 1 until the model converges to the desired degree. The competitive linking algorithm and its one-to-one assumption are detailed in Section 3.1. Section 3.1 explains how to re-estimate the model parameters. 3.1 Compet i t i ve L inking A 
s of u's and v's represent corresponding regions of a bitext. If uk and vk co-occur much more often than expected by chance, then any reasonable model will deem them likely to be mutual translations. If uk and Vk are indeed mutual translations, then their tendency to ZThe co-occurrence frequency of a word type pair is simply the number of times the pair co-occurs in the corpus. However, n(u) = ~-~v n(u.v), which is not the same as the frequency of u, because ach token of u can co-occur with several differentv's. 2We could just as easily use other symmetric &quot;asso- ciation&quot; measures, uch as ?2 (Gale & Church, 1991) or the Dice coefficient (Smadja, 1992). ? ? ? Uk . 1 tJk ~ = Uk+l ? ? ? t ? , ? Vk . 1 Vk Vk+l ? . . Figure 1: Uk and vk often co-occur, as do uk and uk+z. The direct association between uk and vk, and the direct association between uk and Uk+l give rise to an indirect association between v~ and uk+l. co-occur is called a direct associat ion. Now, sup- pose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association betwe
ns of a bitext. If uk and vk co-occur much more often than expected by chance, then any reasonable model will deem them likely to be mutual translations. If uk and Vk are indeed mutual translations, then their tendency to ZThe co-occurrence frequency of a word type pair is simply the number of times the pair co-occurs in the corpus. However, n(u) = ~-~v n(u.v), which is not the same as the frequency of u, because ach token of u can co-occur with several differentv's. 2We could just as easily use other symmetric &quot;asso- ciation&quot; measures, uch as ?2 (Gale & Church, 1991) or the Dice coefficient (Smadja, 1992). ? ? ? Uk . 1 tJk ~ = Uk+l ? ? ? t ? , ? Vk . 1 Vk Vk+l ? . . Figure 1: Uk and vk often co-occur, as do uk and uk+z. The direct association between uk and vk, and the direct association between uk and Uk+l give rise to an indirect association between v~ and uk+l. co-occur is called a direct associat ion. Now, sup- pose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association between vk and Uk+z arises only by virtue of
ciation between uk and Uk+l give rise to an indirect association between v~ and uk+l. co-occur is called a direct associat ion. Now, sup- pose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equiv- alence that are ignorant of indirect associations have &quot;a tendency ... to be confused by collocates&quot; (Dagan et al, 1993). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associ- ations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word to- ken v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuris
in their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equiv- alence that are ignorant of indirect associations have &quot;a tendency ... to be confused by collocates&quot; (Dagan et al, 1993). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associ- ations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word to- ken v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuristic: 1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden 
 particular word to- ken v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuristic: 1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all prob- abilities below a certain threshold to negligible values (Brown et al, 1990; Dagan et al, 1993; Chen, 1996). To retain word type pairs that are at least twice as likely to be mutual transla- tions than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly. 2. Sort all remaining likelihood estimates L(u, v) from highest to lowest. 3. Find u and v such that the likelihood ratio L(u,v) is highest. Token pairs of these types 491 n(u,v) N k(u.v) K T k+ k- B(k{n,p) = frequency of co-occurrence between word types u and v = ~&quot;\].(u.,,) n(u.v) = total number of co-occur
r half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuristic: 1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all prob- abilities below a certain threshold to negligible values (Brown et al, 1990; Dagan et al, 1993; Chen, 1996). To retain word type pairs that are at least twice as likely to be mutual transla- tions than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly. 2. Sort all remaining likelihood estimates L(u, v) from highest to lowest. 3. Find u and v such that the likelihood ratio L(u,v) is highest. Token pairs of these types 491 n(u,v) N k(u.v) K T k+ k- B(k{n,p) = frequency of co-occurrence between word types u and v = ~&quot;\].(u.,,) n(u.v) = total number of co-occurrences in the bitext = frequency
ctively, out ofn(u,v) co-occurrences. The ratio of these prob- abilities is the likelihood ratio in favor of u and v being mutual translations, for all u and v: B(ku,vln<,,,,, ),+) L(u,v) = B(ku,vln~,v, A_ ) . (61 4 C lass -Based Word- to -Word Mode ls In the basic word-to-word model, the hidden param- eters A + and A- depend only on the distributions of link frequencies generated by the competitive link- ing algorithm. More accurate models can be induced by taking into account various features of the linked tokens. For example, frequent words are translated less consistently than rare words (Melamed, 1997). To account for this difference, we can estimate sep- arate values of X + and A- for different ranges of n(u,v). Similarly, the hidden parameters can be con- ditioned on the linked parts of speech. Word order can be taken into account by conditioning the hid- den parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with en- tries in a pre-existing translation lexicon separately 493 from those that do not. This method of incorporat- ing dictionary information seems simpler than the method proposed by Brown et
ifference, we can estimate sep- arate values of X + and A- for different ranges of n(u,v). Similarly, the hidden parameters can be con- ditioned on the linked parts of speech. Word order can be taken into account by conditioning the hid- den parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with en- tries in a pre-existing translation lexicon separately 493 from those that do not. This method of incorporat- ing dictionary information seems simpler than the method proposed by Brown et ai. for their models (Brown et al, 1993b). When the hidden parameters are conditioned on different link classes, the estima- tion method does not change; it is just repeated for each link class. 5 Eva luat ion A word-to-word model of translational equivalence can be evaluated either over types or over tokens. It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available. For each kind of evaluation, we have found one case where we can come close. We induced a two-class word-to-word model of translational
 method does not change; it is just repeated for each link class. 5 Eva luat ion A word-to-word model of translational equivalence can be evaluated either over types or over tokens. It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available. For each kind of evaluation, we have found one case where we can come close. We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards, aligned using the method in (Gale & Church, 1991). One class repre- sented content-word links and the other represented function-word links 4. Link types with negative log-likelihood were discarded after each iteration. Both classes' parameters converged after six it- erations. The value of class-based models was demonstrated by the differences between the hid- den parameters for the two classes. (A +,A-) con- verged at (.78,00016) for content-class links and at (.43,.000094) for function-class links. 5.1 L ink Types The most direct way to evaluate the link types in a word-level model of translational equivalence is to treat each link type a
ent-class links and at (.43,.000094) for function-class links. 5.1 L ink Types The most direct way to evaluate the link types in a word-level model of translational equivalence is to treat each link type as a candidate translation lexi- con entry, and to measure precision and recall. This evaluation criterion carries much practical import, because many of the applications mentioned in Sec- tion 1 depend on accurate broad-coverage transla- tion lexicons. Machine readable bilingual dictionar- ies, even when they are available, have only limited coverage and rarely include domain-specific terms (Resnik & Melamed, 1997). We define the recall of a word-to-word translation model as the fraction of the bitext vocabulary repre- sented in the model. Translation model precision is a more thorny issue, because people disagree about the degree to which context should play a role in judgements of translational equivalence. We hand- evaluated the precision of the link types in our model in the context of the bitext from which the model 4Since function words can be identified by table look- up, no POS-tagger was involved. was induced, using a simple bilingual concordancer. A link type (u, v) was considered correct if u
e have tried, it is not clear how to extract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this
xtract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30%
ord is assigned the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many appl
alance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. 5.2 Link Tokens type of error errors made by errors made IBM Model 2 by our model wrong link missing link partial link class conflict tokenization paraphrase 32 12 7 3 39 7 36 10 5 2 36 TOTAL 93 96 Table 1: Erroneous link tokens generated by two translation mod
y bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. 5.2 Link Tokens type of error errors made by errors made IBM Model 2 by our model wrong link missing link partial link class conflict tokenization paraphrase 32 12 7 3 39 7 36 10 5 2 36 TOTAL 93 96 Table 1: Erroneous link tokens generated by two translation models. The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al's Model 2 on 74 million words of the Canadian Hansards. These au- thors kindly provided us with the links generated by that model in 51 aligned sentences from a held- out test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the En- glish words. Therefore, we ignored English words that were linked to nothing. The errors are classified 
Step 1 of the competitive linking algo- rithm. Each application of the word-to-word model can choose its own balance between link token pre- cision and recall. An application that calls on the word-to-word model to link words in a bitext could treat unlinked words differently from linked words, and avoid basing subsequent decisions on uncertain inputs. It is not clear how the precision/recall trade- off can be controlled in the IBM models. One advantage that Brown et al's Model i has over our word-to-word model is that their objec- tive function has no local maxima. By using the EM algorithm (Dempster et al, 1977), they can guarantee convergence towards the globally opti- mum parameter set. In contrast, the dynamic na- ture of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion. We have adopted the simple heuristic that the model &quot;has converged&quot; when this probability stops increas- ing. 6 Conc lus ion Many multilingual NLP applications need to trans- late words between different languages, but cannot afford the computational expense of modeling the full range of translation phenomena. For these ap- plications, we have designed afast algorithm for esti- mating word-t
nction words. This relatively simple two-class model linked word tokens in parallel texts as accurately as other trans- lation models in the literature, despite being trained on only one fifth as much data. Unlike other transla- tion models, the word-to-word model can automat- ically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and trans- lational entropy (Melamed, 1997). Another inter- esting extension is to broaden the definition of a &quot;word&quot; to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena. Acknowledgements The French/English software manuals were provided by Gary Adams of Sun MicroSystems Laboratories. The weather bitext was prepared at the University of Montreal, under the direction Of Richard Kit- tredge. Thanks to Alexis Nasr for hand-evaluating the wea
 models in the literature, despite being trained on only one fifth as much data. Unlike other transla- tion models, the word-to-word model can automat- ically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and trans- lational entropy (Melamed, 1997). Another inter- esting extension is to broaden the definition of a &quot;word&quot; to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena. Acknowledgements The French/English software manuals were provided by Gary Adams of Sun MicroSystems Laboratories. The weather bitext was prepared at the University of Montreal, under the direction Of Richard Kit- tredge. Thanks to Alexis Nasr for hand-evaluating the weather translation lexicon. Thanks also to Mike Collins, George Foster, Mitch Marcus, Lyle Ungar, and three anonymous reviewers
