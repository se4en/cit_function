{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"R. Moore, J. Dowding, H. Bratt, J. Gawron, \u2022 Y. Gorfu, and A. Cheyer. 1997. CommandTalk: A spoken-language interface for battlefield simulations. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 1-7."},"#text":"\n","pages":{"#tail":"\n","#text":"1--7"},"marker":{"#tail":"\n","#text":"Moore, Dowding, Bratt, Gawron, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"e basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the f","@endWordPosition":"214","@position":"1449","annotationId":"T1","@startWordPosition":"211","@citStr":"Moore et al., 1997"},{"#tail":"\n","#text":"nitor other environmental functions. In particular, our simulation allows voice access to the current and past values of the fixed sensor readings. The initial PSA speech interface demo consists of a simple simulation of the Shuttle. State parameters include the PSA's current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. 113 The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et a., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domainAndependent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recog-_ nition grammar using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances ","@endWordPosition":"1567","@position":"9794","annotationId":"T2","@startWordPosition":"1564","@citStr":"Moore et al., 1997"}]},"title":{"#tail":"\n","#text":"CommandTalk: A spoken-language interface for battlefield simulations."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth Conference on Applied Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Moore"},{"#tail":"\n","#text":"J Dowding"},{"#tail":"\n","#text":"H Bratt"},{"#tail":"\n","#text":"J Gawron"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"R. Moore, J. Dowding, H. Bratt, J. Gawron, \u2022 Y. Gorfu, and A. Cheyer. 1997. CommandTalk: A spoken-language interface for battlefield simulations. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 1-7."},"#text":"\n","pages":{"#tail":"\n","#text":"1--7"},"marker":{"#tail":"\n","#text":"Moore, Dowding, Bratt, Gawron, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"e basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the f","@endWordPosition":"214","@position":"1449","annotationId":"T3","@startWordPosition":"211","@citStr":"Moore et al., 1997"},{"#tail":"\n","#text":"nitor other environmental functions. In particular, our simulation allows voice access to the current and past values of the fixed sensor readings. The initial PSA speech interface demo consists of a simple simulation of the Shuttle. State parameters include the PSA's current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. 113 The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et a., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domainAndependent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recog-_ nition grammar using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances ","@endWordPosition":"1567","@position":"9794","annotationId":"T4","@startWordPosition":"1564","@citStr":"Moore et al., 1997"}]},"title":{"#tail":"\n","#text":"CommandTalk: A spoken-language interface for battlefield simulations."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth Conference on Applied Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Moore"},{"#tail":"\n","#text":"J Dowding"},{"#tail":"\n","#text":"H Bratt"},{"#tail":"\n","#text":"J Gawron"}]}}]}}}}
