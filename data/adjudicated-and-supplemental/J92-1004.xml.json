{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.87201","#text":"\n8 All of the categories that can initiate the RHS of rules containing its category on the LHS.\n9 All of the categories that can follow its own category anywhere on the RIdS in the common rule set\nsharing its parent's category on the LHS.\n"},{"#tail":"\n","@confidence":"0.991883428571429","#text":"\n? Do you know the most direct route to Broadway Avenue from here?\n? Can I get Chinese cuisine at Legal's?\n? I would like to walk to the subway stop from any hospital.\n? Locate a T-stop in Inman Square.\n? What kind of restaurant is located around Mount Auburn in Kendall Square of East\nCambridge?\n4. Interfaces with the Recognizer and the Back-End\n"}],"figure":[{"#tail":"\n","@confidence":"0.838738538461538","#text":"\n&quot;the boy&quot;\n&quot;a beautiful town&quot;\n&quot;a cute little baby&quot;\n&quot;the wonderful pudding&quot;\n\\[NP\\] ~ \\[article\\] [noun\\]\n\\[NP\\] ~ \\[article\\] [adjective\\] \\[noun\\]\n\\[NPI ~ \\[article\\] adjective\\] \\[adjective\\] \\[nounl\n\\[NP\\] ~ \\[articlel \\[adjective\\] \\[noun\\]\n5 In general, a particular rule will occur epeatedly in the training data, and each instantiation f the rule\nwill add to the counts on its arcs.\n6 A more complete example is given in the appendix.\n64\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\n"},{"#tail":"\n","@confidence":"0.9933346","#text":"\nComputational Linguistics Volume 18, Number 1\nSENTENCE\nI\nQUESTION\nQ-SUBJECT\nHOW QUANTIFIER NOUN-PL\nHow many pies\nDO-QUESTION\nDO SUBJECT PREDICATE\nI\nNOUN-GROUP\nL\nNOUN-PHRASE\nJ\nPROPER-NOUN\nd/d M~..e\nVERB-PHRASE-IO\nVBIO OBJECT\nI I\nbuy Q-SUI~ECT\n"},{"#tail":"\n","@confidence":"0.993818047619048","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nSENTENCE\nQ-SUBJECT\nWHAT STREET\nI I\nWhat street\nBE-QUESTION\nL I N ~ U N C T\n/s\nARTICLE A-PLACE\nI\nA-HOTEL\nI\nHOTEL-NAME\nI\nthe Hyatt\nI\nON-STREET\nON A-STREET\nI I\non Q-SUBJECT\n"},{"#tail":"\n","@confidence":"0.896211666666667","#text":"\nThe Grammar:\n(parentheses indicate optional elements)\nnumber = hundreds-p lace (tens-place) ones-place\nnumber = tens-place\nnumber = (tens-place) ones-place\nhundreds-p lace = digits (hundred)\nhundreds-p lace = a hundred (and)\ntens-place = tens\ntens-place = teens (this overgeneral izes a bit)\n"},{"#tail":"\n","@confidence":"0.958232739130435","#text":"\nComputational Linguistics Volume 18, Number 1\n2:430 &quot;four thirty&quot;\n3 :208 &quot;two oh eight&quot;\n4: 24 &quot;twenty four&quot;\n5 :114 &quot;a hundred fourteen&quot;\nThe training rules: (excluding terminals)\n1: number = hundreds-place tens-place ones-place\nhundreds-place = digits hundred and\ntens-place = tens\nones-place = digits\n2: number = hundreds-place tens-place\nhundreds-place = digits\ntens-place = tens\nnumber = hundreds-place tens-place ones-place\nhundreds-place = digits\ntens-place = oh\nones-place = digits\n4. number = tens-place ones-place\ntens-place = tens\nones-place = digits\n5. number = hundreds-place tens-place\nhundreds-place = a hundred\ntens-place = teens\n"},{"#tail":"\n","@confidence":"0.7884574","#text":"\nChildiParent, Left Sibling\nhundreds-placelnumber, start\ntens-placeinumber, start\nPath Probability\n4/5\n"},{"#tail":"\n","@confidence":"0.911159916666667","#text":"\n15 To make the story simpler, I'm ignoring probabilities on the terminal word nodes.\n83\nComputational Linguistics Volume 18, Number 1\nnumber\nFigure A.1\nI hundreds\n3/4~ 2/3~\n\\['our I\ntens\nI teens\n1/10~ 1~\nfifteen\n"}],"author":{"#tail":"\n","@confidence":"0.897518","#text":"\nStephanie Seneff*t\n"},"equation":[{"#tail":"\n","@confidence":"0.680231666666667","#text":"\n/~&quot;~_ _. _,~&quot;NNyes - ~atche s~yes \\] Initiate\n~erinmmr , , , , ->- -~next input/, ,>-'~ right siblings\nI ? ~ Initiate parent\nwith children's\nsolution (reduce)\nI\nc . yes\n~ already done? )\nFigure 2\n"},{"#tail":"\n","@confidence":"0.735049","#text":"\nN\n_1 ~log2P(wi \\] wi-1,...Wl). N Perplexity = 2 i=1\n"},{"#tail":"\n","@confidence":"0.557673222222222","#text":"\ntens-place = oh (as in &quot;four oh five&quot;)\nones-place = digits\ntens = \\[twenty thirty forty ...\\] (a terminal node with eight\nindividual words)\ndigits = \\[zero one two three four .... \\]\nteens = \\[ten eleven twelve... \\]\noh = \\[oh\\]\nhundred = \\[hundred\\]\nand = \\[and\\]\n"},{"#tail":"\n","@confidence":"0.491741","#text":"\ndigits 0 1 0 2 0 3\nhundred 0 0 1 1 0 2\nand 0 0 0 1 0 1\na 0 1 0 0 0 1\n"},{"#tail":"\n","@confidence":"0.4836695","#text":"\ni/5\n4/5~I/4\n"},{"#tail":"\n","@confidence":"0.837708666666667","#text":"\nN\n- l y~ logaP(wilwi-1, . . . Wl )\nPerplexity = 2 i=1\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.855904","#text":"\n2.1 Overview\n"},{"#tail":"\n","@confidence":"0.995779","#text":"\n2.2 Training the Probabilities\n"},{"#tail":"\n","@confidence":"0.999095","#text":"\n2.3 Control Strategy\n"},{"#tail":"\n","@confidence":"0.980395","#text":"\n2.4 Design Issues\n"},{"#tail":"\n","@confidence":"0.992843","#text":"\n2.5 Constraints and Gaps\n"},{"#tail":"\n","@confidence":"0.999535","#text":"\n3.1 Portability\n"},{"#tail":"\n","@confidence":"0.998047","#text":"\n3.2 Perplexity and Coverage in RM Task\n"},{"#tail":"\n","@confidence":"0.86435","#text":"\n3.3 Experiments within the VOYAGER domain\n"},{"#tail":"\n","@confidence":"0.976356","#text":"\n3.4 Generation Mode\n"}],"subsubsectionHeader":{"#tail":"\n","@confidence":"0.966569","#text":"\n2.5.2 Semantic Filtering. In the more recent versions of the grammar, we have im-\n"},"footnote":[{"#tail":"\n","@confidence":"0.703621125","#text":"\nSpoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139\n~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the\nOffice of Naval Research.\n1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While\n&quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this\naspect of the problem.\n2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; interchangeably.\n(~) 1992 Association for Computational Linguistics\n"},{"#tail":"\n","@confidence":"0.471608","#text":"\n11 The auxiliary verb sets the mode of the main verb to be root or past participle as appropriate.\n"},{"#tail":"\n","@confidence":"0.887627","#text":"\n13 The appendix includes an example for computing test set perplexity.\n14 In the case of TINA, all words up to the current word within each sentence are relevant.\n"}],"title":{"#tail":"\n","@confidence":"0.807847","#text":"\nTINA: A Natural Language System for\nSpoken Language Applications\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.997648457831325","#text":"\nBoisen, S.; Chow, Y.-L.; Haas, A.; lngria, R.;\nRoukos, S.; and Stallard, D. (1989). &quot;The\nBBN spoken language system.&quot; In\nProceedings, DARPA Speech and Natural\nLanguage Workshop. 106-111.\nBresnan, J., ed. (1982). The Mental\nRepresentation f Grammatical Relations.\nCambridge, MA: The MIT Press.\nChomsky, Noam (1977). &quot;On\nwh-movement.&quot; In Formal Syntax, edited\nby P. Culicover, T. Wasow, and\nA. Akmajian. New York: Academic Press.\nDe Mattia, M., and Giachin, E. P. (1989).\n&quot;Experimental results on large vocabulary\ncontinuous speech understanding.&quot;\nICASSP-89 Proceedings. 691-694.\nFillmore, C. J. (1968). &quot;The case for case.&quot; In\nUniversals in Linguistic Theory, edited by\nE. Bach and R. Harms. New York: Holt,\nRinehart, and Winston. 1-90.\nGoodine, D.; Seneff, S.; Hirschman, L.; and\nPhillips, M. (1991). &quot;Full integration of\nspeech and language understanding in\nthe MIT spoken language system.&quot; In\nProceedings, 2nd European Conference on\nSpeech Communication a d Technology.\nGenova, Italy. 24-26.\nGrishman, R.; Hirschman, L.; and Nhan,\nN. T. (1986). &quot;Discovery procedures for\nsublanguage s lectional patterns: Initial\nexperiments.&quot; Computational Linguistics\n12(3): 205-215.\nHart, P.; Nilsson, N. J.; and Raphael B.\n(1968). &quot;A formal basis for the heuristic\ndetermination f minimum cost paths.&quot;\nIEEE Transactions ofSystems, Science and\nCybernetics SSC-4(2): 100-107.\nHirschman, L.; Grishman, R.; and Sager, N.\n(1975). &quot;Grammatically-based automatic\nword class formation.&quot; Information\nProcessing and Management 11: 39-57.\nJelinek, E (1976). &quot;Continuous speech\nrecognition by statistical methods.&quot; IEEE\nProceedings 64(4): 532-556.\nKatz, S. M. (1987). &quot;Estimation of\nprobabilities from sparse data for the\nlanguage model component of a speech\nrecognizer.&quot; ASSP-35: 400-401.\nLamel, L.; Kassel, R. H.; and Seneff, S.\n(1986). &quot;Speech database development:\nDesign and analysis of the\nacoustic-phonetic corpus.&quot; In Proceedings,\nDARPA Speech Recognition Workshop. Palo\nAlto, CA. 100-109.\nLee, K. E (1989). Automatic Speech\nRecognition: The Development of the SPHINX\nSystem, Appendix I. Boston: Kluwer\nAcademic Publishers.\nLee, K. F.; Hon, H. W.; and Reddy, R. (1989).\n&quot;An overview of the SPHINX speech\nrecognition system.&quot; IEEE Transactions on\nAcoustics, Speech, and Signal Processing\n38(1): 35-46.\nNiedermair, G. Th. (1989). &quot;The use of a\nsemantic network in speech dialogue.&quot; 1st\nEuropean Conference on Speech\nCommunication and Technology, Paris,\nFrance. 26-29.\nNiemann, H. (1990). &quot;The interaction of\nword recognition and linguistic\nprocessing in speech understanding.&quot;\nInvited Lecture, NATO-ASI Workshop on\nSpeech Recognition and Understanding,\nCetraro, Italy.\nPallett, D. (1989). &quot;Benchmark tests for\nDARPA resource management database\nperformance evaluations.&quot; In Proceedings,\nICASSP-89. 536-539.\nSeneff, S.; Glass, J.; Goddeau, D.; Goodine,\nD.; Hirschman, L.; Leung, H.; Phillips, M.;\nPolifroni, J.; and Zue, V. (1991).\n&quot;Development and preliminary\nevaluation of the MIT ATIS system.&quot;\n"},{"#tail":"\n","@confidence":"0.999576960784314","#text":"\nComputational Linguistics Volume 18, Number 1\nFourth DARPA Speech and Natural\nLanguage Workshop, Asilomar, CA.\n88-93.\nViterbi, A. (1967). &quot;Error bounds for\nconvolutional codes and an\nasymptotically optimal decoding\nalgorithm.&quot; IEEE Transactions on\nInformation Theory IT-13. 260-269.\nWoods, W. A. (1970). &quot;Transition etwork\ngrammars for natural language analysis.&quot;\nCommun. of the ACM 13: 591-606.\nWoods, W. A. (1986). &quot;Semantics and\nquantification i natural language\nquestion answering.&quot; In Readings in\nNatural Language Processing, edited by\nB. J. Grosz, K. S. Jones; and B. L. Webber.\nLos Altos, CA: Morgan Kaufmann.\n205-248.\nYoung, S. R. (1989). &quot;The minds system:\nUsing context and dialog to enhance\nspeech recognition.&quot; Proceedings, DARPA\nSpeech and Natural Language Workshop.\n131-136.\nZue, V.; Daly, N.; Glass, J.; Goodine, D.;\nLeung, H.; Phillips, M.; Polifroni, J.;\nSeneff, S.; and Sodof, M. (1989a). &quot;The\ncollection and preliminary analysis of a\nspontaneous speech database.&quot; DARPA\nSpeech and Natural Language Workshop.\nHarwichport, MA. 15-18.\nZue, V.; Glass, J.; Phillips, M.; and Seneff, S.\n(1989b). &quot;The MIT SUMMIT speech\nrecognition system, a progress report.&quot;\nProceedings, DARPA Speech and Natural\nLanguage Workshop. Philadelphia. 21-23.\nZue, V.; Glass, J.; Goodine, D.; Leung, H.;\nPhillips;, M.; Polifroni, J.; and Seneff, S.\n(1990). &quot;The VOYAGER speech\nunderstanding system: Preliminary\ndevelopment and evaluation.&quot; IEEE\nInternational Conference on Acoustics, Speech\nand Signal Processing. Albuquerque, NM.\n73-76.\nZue, V.; Glass, J.; Goodine, D.; Leung, H.;\nPhillips, M.; Polifroni, J.; and Seneff, S.\n(1991). &quot;Integration of speech recognition\nand natural language processing in the\nMIT VOYAGER system.&quot; IEEE International\nConference on Acoustics, Speech and Signal\nProcessing. Toronto, Ontario. 14-17.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.985655285714286","#text":"\nA new natural language system, TINA, has been developed for applications involving spoken\nlanguage tasks. TINA integrates key ideas from context free grammars, Augmented Transition\nNetworks (ATN's), and the unification concept. TINA provides a seamless interface between\nsyntactic and semantic analysis, and also produces ahighly constraining probabilistic language\nmodel to improve recognition performance. An initial set of context-free r write rules provided by\nhand is first converted to a network structure. Probability assignments on all arcs in the network\nare obtained automatically from a set of example sentences. The parser uses a stack decoding\nsearch strategy, with a top-down control flow, and includes a feature-passing mechanism todeal\nwith long-distance movement, agreement, and semantic constraints. TINA provides an automatic\nsentence generation capability that has been effective for identifying overgeneralization problems\nas well as in producing a word-pair language model for a recognizer. The parser is currently\nintegrated with MIT's SUMMIT recognizer for use in two application domains, with the parser\nscreening recognizer outputs either at the sentential level or to filter partial theories during the\nactive search process.\n"},{"#tail":"\n","@confidence":"0.990409","#text":"\nOver the past few years, there has been a gradual paradigm shift in speech recognition\nresearch both in the U.S. and in Europe. In addition to continued research on the tran-\nscription problem, i.e., the conversion of the speech signal to text, many researchers\nhave begun to address as well the problem of speech understanding. 1 This shift is\nat least partly brought on by the realization that many of the applications involving\nhuman/machine interface using speech require an &quot;understanding&quot; of the intended\nmessage. In fact, to be truly effective, many potential applications demand that the\nsystem carry on a dialog with the user, using its knowledge base and information\ngleaned from previous sentences to achieve proper response generation. Current ad-\nvances in research and development of spoken language systems 2 can be found, for\nexample, in the proceedings of the DARPA speech and natural anguage workshops,\nas well as in publications from participants of the ESPRIT SUNDIAL project. Repre-\nsentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989),\nNiedermair (1989), Niemann (1990), and Young (1989).\n"},{"#tail":"\n","@confidence":"0.986943702127659","#text":"\nComputational Linguistics Volume 18, Number 1\nA spoken language system relies on its natural anguage component to provide\nthe meaning representation f a given sentence. Ideally, this component should also\nbe useful for providing powerful constraints o the recognizer component in terms of\npermissible syntactic and semantic structures, given the limited domain. If it is to be\nuseful for constraint, however, it must concern itself not only with coverage but also,\nand perhaps more importantly, with overgeneralization. I many existing systems,\nthe ability to parse as many sentences as possible is often achieved at the expense\nof accepting inappropriate word strings as legitimate sentences. This had not been\nviewed as a major concern in the past, since systems were typically presented only\nwith well-formed text strings, as opposed to errorful recognizer outputs.\nThe constraints can be much more effective if they are embedded in a probabilistic\nframework. The use of probabilities in a language model can lead to a substantially\nreduced perplexity 3 for the recognizer. If the natural language component's computa-\ntional and memory requirements are not excessive, and if it is organized in such a way\nthat it can easily predict a set of next-word candidates, then it can be incorporated\ninto the active search process of the recognizer, dynamically predicting possible words\nto follow a hypothesized word sequence, and pruning away hypotheses that cannot\nbe completed in any way. The natural anguage component should be able to offer\nsignificant additional constraint to the recognizer, beyond what would be available\nfrom a local word-pair or bigram 4language model, because it is able to make use of\nlong-distance constraints in requiring well-formed whole sentences.\nThis paper describes a natural language system, TINA, which attempts to address\nsome of these issues. The mechanisms were designed to support a graceful, seam-\nless interface between syntax and semantics, leading to an efficient mechanism for\nconstraining semantics. Grammar ules are written such that they describe syntactic\nstructures at the high levels of a parse tree and semantic structures at the low lev-\nels. All of the meaning-carrying content of the sentence is completely encoded in the\nnames of the categories of the parse tree, thus obviating the need for separate seman-\ntic rules. By encoding meaning in the structural entities of the parse tree, it becomes\nfeasible to realize probabilistic semantic restrictions in an efficient manner. This also\nmakes it straightforward to extract a semantic frame representation directly from an\nunannotated parse tree.\nThe context-free rules are automatically converted to a shared network structure,\nand probability assignments are derived automatically from a set of parsed sentences.\nThe probability assignment mechanism was deliberately designed to support an ability\nto predict a set of next-word candidates with associated word probabilities. Constraint\nmechanisms exist and are carried out through feature passing among nodes. A unique\naspect of the grammar is that unification constraints are expressed one-dimensionally,\nbeing associated directly with categories rather than with rules. Syntactic and semantic\nfields are passed from node to node by default, thus making available by default he\nsecond argument to unification operations. This leads to a very efficient implemen-\ntation of the constraint mechanism. Unifications introduce additional syntactic and\nsemantic onstraints such as person and number agreement and subject/verb seman-\ntic restrictions.\nThis paper is organized as follows. Section 2 contains a detailed escription of the\ngrammar and the control strategy, including syntactic and semantic onstraint mech-\n"},{"#tail":"\n","@confidence":"0.758393333333333","#text":"\nword hypotheses that may follow each word.\n4 Each word is associated with a list of the probabilites for all the words that could possibly follow it\nanywhere ina sentence.\n"},{"#tail":"\n","@confidence":"0.9883335","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nanisms. Section 3 describes a number of domain-dependent versions of the system\nthat have been implemented, and addresses, within the context of particular domains,\nseveral evaluation measures, including perplexity, coverage, and portability. Section\n4 discusses briefly two application domains involving database access in which the\nparser provides the link between a speech recognizer and the database queries. The\nlast section provides a summary and a discussion of our future plans. There is also an\nappendix, which walks through an example grammar for three-digit numbers, show-\ning how to train the probabilities, parse a sentence, and compute perplexity on a test\nsentence.\n"},{"#tail":"\n","@confidence":"0.99688475","#text":"\nThis section describes everal aspects of the system in more detail, including how the\ngrammar is generated and trained, how the control strategy operates, how constraints\n(both syntactic and semantic) are enforced, and practical issues having to do with\nefficiency and ease of debugging.\n"},{"#tail":"\n","@confidence":"0.999119727272727","#text":"\nTINA is based on a context-free grammar augmented with a set of features used to\nenforce syntactic and semantic onstraints. The grammar is converted to a network\nstructure by merging common elements on the right-hand side (RHS) of all rules\nsharing the same left-hand side (LHS) category. Each LHS category becomes associated\nwith a parent node whose children are the collection of unique categories appearing\nin the RHSs of all the rules in the common set. Each parent node establishes a two-\ndimensional array of permissible links among its children, based on the rules. Each\nchild can link forward to all of the children that appear adjacent to that child in any of\nthe shared rule set. Probabilities are determined for pairs of siblings through frequency\ncounts on rules generated by parsing a set of training sentences. The parsing process\nachieves efficiency through structure-sharing among rules, resembling in this respect\na top-down chart processor.\nThe grammar nodes are contained in a static structure describing a hierarchy of\npermissible sibling pairs given each parent, and a node-dependent set of constraint\nfilters. Each grammar node contains a name specifying its category, a two-dimensional\nprobability array of permissible links among the next lower level in the hierarchy and\na list of filter specifications tobe applied either in the top-down or the bottom-up cycle.\nWhen a sentence is parsed, a dynamic structure is created, a set of parse nodes that are\nlinked together in a hierarchical structure to form explicit paths through the grammar.\nDuring the active parse process, the parse nodes are entered into a queue prioritized\nby their path scores. Each node (except erminals) in a given parse tree enters the\nqueue exactly twice: once during the top-down cycle, during which it enters into the\nqueue all of its possible first children, and once again during the bottom-up cycle,\nduring which it enters all of its possible right siblings, given its parent. The control\nstrategy repeatedly pops the queue, advancing the active hypothesis by exactly one\nstep, and applying the appropriate node-level unifications.\nEach feature specification for each grammar node contains a feature name, a value\nor set of values for that feature, a logic function, and a specification as to whether the\nunification should take place during the top-down or during the bottom-up cycle.\nAll features are associated with nodes (categories) rather than with rules, and each\nnode performs exactly the same unifications without regard to whatever rule it might\nbe a part of. In fact, during the active parse process, a rule is not an explicit entity\nwhile it is being formed. Each instantiation of a rule takes place only at the time that\n"},{"#tail":"\n","@confidence":"0.986320857142857","#text":"\nComputational Linguistics Volume 18, Number 1\nthe next sibling is the distinguished \\[end\\] node, a special node that signifies a return\nto the level of the parent. The rule can be acquired by tracing back through the left\nsiblings, until the distinguished \\[start\\] node is encountered, although this is not done\nin practice until the entire parse is completed.\nThe parse nodes contain a set of features whose values will be modified through\nthe unification process. All modifications to features are made nondestructively b\ncopying a parse node each time a hypothesis i updated. Thus each independent\nhypothesis i associated with a particular parse node that contains all of the rele-\nvant feature information for that hypothesis. As a consequence, all hypotheses can be\npursued in parallel, and no explicit backtracking is ever done. Control is repeatedly\npassed to the currently most probable hypothesis, until a complete sentence is found\nand all of the input stream is accounted for. Additional parses can be found by simply\ncontinuing the process.\n"},{"#tail":"\n","@confidence":"0.99462772","#text":"\nThe grammar isbuilt from a set of training sentences, using a bootstrapping procedure.\nInitially, each sentence is translated by hand into a list of the rules invoked to parse\nit. After the grammar has built up a substantial knowledge of the language, many\nnew sentences can be parsed automatically, or with minimal intervention toadd a few\nnew rules incrementally. The arc probabilities can be incrementally updated after the\nsuccessful parse of each new sentence.\nThe process of converting the rules to a network form is straightforward. All rules\nwith the same LHS are combined to form a structure describing possible intercon-\nnections among children of a parent node associated with the left-hand category. A\nprobability matrix connecting each possible child with each other child is constructed\nby counting the number of times a particular sequence of two siblings occurred in the\nRHSs of the common rule set, and normalizing by counting all pairs from the partic-\nular left-sibling to any right sibling. 5Two distinguished nodes, a \\[start\\] node and an\n\\[end\\] node, are included among the children of every grammar node. A subset of the\ngrammar nodes are terminal nodes whose children are a list of vocabulary words.\nThis process can be illustrated with the use of a simple example. 6 Suppose there\nexists a grammar for noun phrases that can be expressed through the single compact\nrule form:\nRule 1\n\\[NP\\] ~ \\[article\\] (\\[adjective\\]) ( [adjective\\]) \\[noun\\]\nwhere the parentheses signify optional nodes. This grammar would be converted to\na network as shown in Figure 1, which would be stored as a single grammar node\nwith the name \\[NP\\]. The resulting rammar could be used to parse the set of phrases\nshown on the left, each of which would generate the corresponding rule shown on\nthe right.\n"},{"#tail":"\n","@confidence":"0.983446461538462","#text":"\nFigure 1 .25\nIllustration of probabilistic network obtained from four rules with the same LHS (NP), as\ngiven in the text. A parent node, named \\[NP\\], would contain these five nodes as its children,\nwith a probability matrix specifying the network connections?\nTo train the probabilities, a record is kept of the relative counts of each subseqent\nsibling, with respect to each permissible child of the parent node, in our case, \\[NP\\], as\nthey occurred in an entire set of parsed training sentences? In the example, \\[adjective\\]\nis followed three times by \\[noun\\] and once by \\[adjective\\], so the network shows a\nprobability of 1/4 for the self loop and 3/4 for the advance to \\[noun\\]? Notice that the\nsystem has now generalized to include any number of adjectives in a row. Each rule\nin general would occur multiple times in a given training set, but in addition there is\na significant amount of sharing of individual sibling pairs among different rules, the\nso-called cross-pollination effect?\nThis method of determining probabilities effectively amounts to a bigram language\nmodel 7embedded in a hierarchical structure, where a separate set of bigram statistics\nis collected on category pairs for each unique LHS category name. The method is to\nbe distinguished from the more common method of applying probabilities to entire\nrule productions, rather than to sibling pairs among a shared rule set. An advantage\nto organizing probabilities at the sibling-pair level is that it conveniently provides an\nexplicit probability estimate for a single next word, given a particular word sequence.\nThis probability can be used to represent the language model score for the next word,\nwhich, when used in conjunction with the acoustic score, provides the overall score\nfor the word.\nWe make a further simplifying assumption that each sentence has only a single\nparse associated with it. This is probably justified only in conjunction with a grammar\nthat contains emantic ategories? We have found that, within the restricted omains\nof specific applications, the first parse is essentially always a correct parse, and of-\nten, in fact, the only parse? With only a single parse from each sentence, and with\nthe grammar trained at the sibling-pair level, training probabilities becomes a triv-\nial exercise of counting and normalizing sibling-pair frequencies within the pooled\ncontext-free rule sets. Training is localized such that, conditional on the parent, there\nis an advance from one sibling to some next sibling with probability 1.0. Normaliza-\ntion requires only this locally applied constraint, making it extremely fast to train on\na set of parsed sentences. Furthermore, the method could incorporate syntactic and\nsemantic onstraints, by simply renormalizing the probabilities at run time, after paths\nthat fail due to constraints have been eliminated?\n7 A bigram language model is commonly used in speech recognition systems, where bigram statistics\n(frequency ounts on adjacent word pairs) are collected from words or word categories in sample\nsentences.\n"},{"#tail":"\n","@confidence":"0.69625","#text":"\nFunctional block diagram of control strategy. (Note: &quot;Initiate&quot; means &quot;enter into the queue\nranked by probability.&quot;)\n"},{"#tail":"\n","@confidence":"0.99228045","#text":"\nA functional block diagram of the control strategy is given in Figure 2. At any given\ntime, a set of active parse nodes are arranged on a priority queue. Each parse node\ncontains a pointer to a corresponding grammar node,, and has access to all the infor-\nmation needed to pursue its partial theory. The top node is popped from the queue,\nand it then creates a number of new nodes (either first children s or right siblings 9de-\npending on its state), and inserts them into the queue according to their probabilities.\nIf the node is an \\[end\\] node, it returns control to the parent node, giving that node\na completed subparse. As each new node is considered, unifications of syntactic and\nsemantic onstraints are performed, and may lead to failure. The process can terminate\non the first successful completion of a sentence, or tlhe Nth successful completion if\nmore than one hypothesis desired.\nA parse in TINA begins with a single parse node linked to the grammar node\n\\[sentence\\], which is entered on the queue with probability 1.0. This node creates new\nparse nodes that might have categories such as \\[statement\\], \\[question\\], and \\[request\\],\nand places them on the queue, prioritized. If \\[statement\\] is the most likely child, it\ngets popped from the queue, and returns nodes indicating \\[subject\\], \\[it\\], etc., to the\nqueue. When \\[subject\\] reaches the top of the queue, it activates units such as \\[noun\nphrase\\], \\[gerund\\], and \\[noun clause\\]. Each node, after instantiating first-children, be-\ncomes inactive, pending the return of a successful subparse from a sequence of chil-\ndren. Eventually, the cascade of first-children reaches a terminal node such as \\[article\\],\n"},{"#tail":"\n","@confidence":"0.98035362745098","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nwhich proposes a set of words to be compared with the input stream. If a match with\nan appropriate word is found, then the terminal node fills its subparse slot with an\nentry such as (\\[article\\] &quot;the&quot;), and activates all of its possible right-siblings.\nWhenever a terminal node has successfully matched an input word, the path\nprobability is reset o 1.0.1? Thus the probabilities that are used to prioritize the queue\nrepresent not the total path probability but rather the probability given the partial\nword sequence. Each path climbs up from a terminal node and back down to a next\nterminal node, with each new node adjusting the path probability by multiplying by\na new conditional probability. The resulting conditional path probability for a next\nword represents the probability of that word in its linguistic role given all preceding\nwords in their linguistic roles. With this strategy, a partial sentence does not become\nincreasingly improbable as more and more words are added.\nBecause of the sharing of common elements on the right-hand side of rules, TINA\ncan automatically generate new rules that were not explicitly provided. For instance,\nhaving seen the rule X =~ A B C and the rule X =~ B C D, the system would automat-\nically generate two new rules, X ~ B C and X =~ A B C D. Although this property\ncan potentially lead to certain problems with overgeneralization, there are a number\nof reasons why it should be viewed as a feature. First of all, it permits the system to\ngeneralize more quickly to unseen structures. For example, having seen the rule \\[ques-\ntion\\] ~ \\[aux\\] [subject\\] \\]predicate\\] (asin &quot;May I go?&quot;) and the rule \\]question\\] ~ \\]have\\]\n\\]subject\\] \\[link\\] \\[pred-adjective\\] (as in &quot;Has he been good?&quot;), the system would also\nunderstand the forms \\[question\\] ~ \\[have\\] [subject\\] \\[predicate\\] (as in &quot;Has he left?&quot;)\nand \\[question\\] ~ \\[aux\\] [subject\\] \\[link\\] \\[pred-adjective\\] (as in &quot;Should I be careful?&quot;). 11\nSecondly, it greatly simplifies the implementation, because rules do not have to be ex-\nplicitly monitored uring the parse. Given a particular parent and a particular child,\nthe system can generate the allowable right siblings without having to note who the\nleft siblings (beyond the immediate one) were. Finally, and perhaps most importantly,\nprobabilities are established on arcs connecting sibling pairs regardless of which rule\nis under construction. In this sense the arc probabilities behave like the familiar word-\nlevel bigrams of simple recognition language models (Jelinek 1976), except hat they\napply to siblings at multiple levels of the hierarchy. This makes the probabilities mean-\ningful as a product of conditional probabilities as the parse advances to deeper levels\nof the parse tree and also as it returns to higher levels of the parse tree. This approach\nimplies an independence assumption that claims that what can follow depends only\non the left sibling and the parent.\nOne negative aspect of the cross-pollination is that the system can potentially\ngeneralize to include forms that are agrammatical. For instance, the forms &quot;Pick the\nbox up&quot; and &quot;Pick up the box,&quot; if defined by the same LHS name, would allow the\nsystem to include rules producing forms such as &quot;Pick up the box up&quot; and &quot;Pick up the\nbox up the box!&quot; This problem can be overcome ither by giving the two structures\ndifferent LHS names or by grouping &quot;up the box&quot; and &quot;the box up&quot; into distinct\nparent nodes, adding another layer to the hierarchy on the RHS. Another solution is\nto use a trace mechanism to link the two positions for the object, thus preventing it\nfrom occurring in both places. A final alternative is to include a PARTICLE bit among\n10 Some modification of this scheme is necessary when the input stream is not deterministic. For the A*\nalgorithm (Hart et al 1968) as applied to speech recognition, the actual path score is typically\naugmented with an estimated score for the unseen portion. Unless some kind of normalization is done,\nthe short theories have an unfair advantage, s imply because fewer probability scores have been\nmultiplied. With a deterministic word sequence it seems reasonable to assume probability 1.0 for what\nhas been found.\n"},{"#tail":"\n","@confidence":"0.954268333333333","#text":"\nComputational Linguistics Volume 18, Number 1\nthe features which, once set, cannot be reset. In fact, tlhere were only a few situations\nwhere such problems arose, and reasonable solutions could always be found.\n"},{"#tail":"\n","@confidence":"0.9981245","#text":"\nTINA'S design includes a number of features that lead to rapid development of the\ngrammar and/or porting of the grammar to a new domain, as well as efficient im-\nplementation capabilities, in terms of both speed and memory. Among its features\nare semi-automatic raining from a set of example sentences, a sentence generation\ncapability, and a design framework that easily accomodates parallel implementations.\nIt is a two-step rocedure to acquire a grammar :from a specific set of sentences.\nThe rule set is first built up gradually, by parsing the sentences one-by-one, adding\nrules and/or constraints as needed. Once a full set of sentences has been parsed in\nthis fashion, the parse trees from the sentences are automatically converted to the\nsequence of rules used to parse each sentence. The training of both the rule set and\nthe probability assignments i then established irectly in a second pass from the\nprovided set of parsed sentences; i.e., the parsed sentences are the grammar.\nGeneration mode uses the same routines as those used by the parser, but chooses\na small subset of the permissible paths based on the outcome of a random-number\ngenerator, ather than exploring all paths and relying on an input word stream to\nresolve the correct one. Since all of the arcs have assigned probabilities, the parse tree\nis traversed by generating a random number at each node and deciding which arcs\nto select based on the outcome. The arc probabilities can be used to weigh the alter-\nnatives. Occasionally, the generator chooses a path that leads to a dead end, because\nof unanticipated constraints. Hence we in general need to keep more than one partial\ntheory alive at any given time, to avoid having to backtrack upon a failure condition.\nWe could in fact always choose to sprout wo branches at any decision point, although\nthis generally leads to a much larger queue than is really necessary. We found instead\nthat it was advantageous to monitor the size of the queue, and arbitrarily increase the\nnumber of branches kept alive from one to two whenever the queue becomes danger-\nously short, shrinking it back to one upon recovery. We have used generation mode\nto detect overgeneralizations i the grammar, to build a word-pair language model\nfor use as a simple constraint mechanism in our recognizer, and to generate random\nsentences for testing our interface with the back-end.\nA final practical feature of TINA is that, as in unification grammars, all unifications\nare nondestructive, and as a consequence, xplicit backtracking is never necessary.\nEvery hypothesis on the queue is independent of every other one, in the sense that\nactivities performed by pursuing one lead do not disturb the other active nodes. This\nfeature makes TINA an excellent candidate for parallel implementation. The control\nstrategy would simply deliver the most probable node to an available processor.\nTINA has been implemented in Commonlisp and runs on both a Sun workstation\nand a Symbolics LISP machine. A deterministic word sequence can be parsed in a\nsmall fraction of real-time on either machine. Of course, once the input is a speech\nwaveform rather than a word sequence, the uncertainty inherent in the proposed\nwords will greatly increase the search space. Until we have a better handle on control\nstrategies in the best-first search algorithm, it is impossible to predict he computational\nload for a spoken-input mode.\n"},{"#tail":"\n","@confidence":"0.999786","#text":"\nThis section describes how TINA handles everal issues that are often considered to be\npart of the task of a parser. These include agreement constraints, emantic restrictions,\n"},{"#tail":"\n","@confidence":"0.99032834","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nsubject-tagging for verbs, and long distance movement (often referred to as gaps, or the\ntrace, as in &quot;(which article)/do you think I should read (ti)?&quot;) (Chomsky 1977). The gap\nmechanism resembles the Hold register idea of ATNs (Woods 1970) and the treatment\nof bounded omination metavariables in lexical functional grammars (LFGs) (Bresnan\n1982, p. 235 ft.), but it is different from these in that the process of filling the Hold\nregister equivalent involves two steps separately initiated by two independent odes.\nOur approach to the design of a constraint mechanism is to establish a framework\ngeneral enough to handle syntactic, semantic, and, ultimately, phonological constraints\nusing identical functional procedures applied at the node level. The intent was to de-\nsign a grammar for which the rules would be kept completely free of any constraints.\nTo achieve this goal, we decided to break the constraint equations usually associated\nwith rules down into their component parts, and then to attach constraints to nodes\n(i.e., categories) as equations in a single variable. The missing variable that must be\nunified with the new information would be made available by default. In effect, the\nconstraint mechanism is thus reduced from a two-dimensional to a one-dimensional\ndomain. Thus, for example, the developer would not be permitted to write an f-\nstructure (Bresnan 1982) equation of the form \\[subj\\]inf = \\[np\\] associated with the rule\n\\[vp\\] --, \\[verb\\] [np\\] \\[inf\\], to cover, &quot;I told John to go.&quot; Instead, the \\[np\\] node (regard-\nless of its parent) would generate a CURRENT-FOCUS (defined later) from its subparse,\nwhich would be passed along passively to the verb &quot;go.&quot; The verb would then sim-\nply consult he CURRENT-FOCUS (regardless of its source) to establish the identity of its\nsubject.\nThe procedure works as follows. In the absence of any explicit instructions from its\ngrammar node, a parse node simply passes along all features to the immediate relative\n(first child in the top-down cycle, and right sibling during the bottom-up cycle12).\nAny constraints pecified by the grammar node result in a modification of certain\nfeature values. The modifications are specified through a four-tuple of (feature-name\nnew-value logic-function cycle). The possible features include person and number,\ncase, determiner (DEFINITE, INDEFINITE, PROPER, etc.), mode (ROOT, FINITE, etc.), and a\nsemantic ategory bit map. The new value, entered as a bit pattern, could be a single\nvalue, such as SINGULAR, or could be multiple valued as in the number for the noun\n&quot;fish.&quot; Furthermore, during the bottom-up cycle, the new value can be the special\nvariable top-down-setting, i.e., the value for that feature that currently occupies the slot\nin the parse node in question. This has the effect of disconnecting the node from its\nchildren, with respect o the feature in question. The logic function is one of AND, OR,\nor SET, and the cycle is either top-down or bottom-up.\nA parse node has jurisdiction over its own slots only during the bottom-up cycle.\nDuring the top-down cycle, its feature value modifications are manifested only in its\ndescendants. The node retains the values for the features that its parent delivered, and\nmay use these for unifications prior to passing information on to its right siblings. This\nadditional complexity was felt necessary to handle number agreement in questions of\nthe form &quot;Do John and Mary eat out a lot?&quot; Here, the auxiliary verb &quot;do&quot; sets the\nnumber to plural, but the two individual nouns are singular. The SUBJECT node blocks\ntransfer of number information to its children (by setting the value to all ls), but\nunifies the value for number returned during the bottom-up cycle with the value\npreviously delivered to it by its left sibling, the auxiliary verb. There is a node, \\[and-\nnoun-phrase\\], that deals specifically with compound nouns. This node blocks transfer\n12 If the right sibling happens to be the distinguished \\[end\\] node, then the features get passed up to the\nparent.\n"},{"#tail":"\n","@confidence":"0.996393666666666","#text":"\nComputational Linguistics Volume 18, Number 1\nof number information to its children and sets number to plural during the bottom-up\ncycle.\nIt has been found expedient to define a meta-level operator named &quot;detach&quot; that\ninvokes a block operation during both the top-down and bottom-up cycles. This oper-\nation has the effect of isolating the node in question from its descendents with respect\nto the particular blocked feature. This mechanism was commonly used to detach a\nsubordinate clause from a main clause with respect o the semantic bits, for example.\nThe setting that had been delivered to the node during the top-down cycle is retained\nand sent forward during the bottom-up cycle, but not communicated to the node's\nchildren. Another special blocking property can be associated with certain features, but\nthe block only applies at the point where an \\[end\\] node returns a solution to a parent.\nThis is true, for instance, of the mode for the verb.\nAlong with the syntactic and semantic features, there are also two slots that are\nconcerned with the trace mechanism, and these are used as well for semantic filtering\non key information from the past. There are some special operations concerned with\nfilling these slots and unifying semantics with these slots that will be described in\nmore detail in later sections.\nLexical entries contain three-tuple specifications of values for features; the fourth\nelement is irrelevant since there are no separate top-down and bottom-up cycles. Thus\na terminal verb node contains vocabulary entries that include settings for verb mode,\nand for person/number if the verb is finite. The plural form for nouns can be handled\nthrough a \\[pl\\] morph for the sake of efficiency. This morph sets the value of number to\nplural, regardless of its prior setting. It is the job of a parent node to unify that setting\nwith the value delivered by the left siblings of the noun.\nSome examples may help explain how the constraint mechanism works. Consider,\nfor example, the ill-formed phrase &quot;each boats.&quot; Suppose the grammar has the three\nrules, (\\[np\\] -* \\[det\\] [noun\\]), (\\[noun\\] --* \\[root-noun\\]), and (\\[noun\\] --* \\[root-noun\\] \\[pl\\]).\nThe lexical item &quot;each&quot; sets the number to singular and passes this value to the \\[noun\\]\nnode. The \\[noun\\] node blocks transfer of number to its children. &quot;Boat&quot; sets the num-\nber to singular, but the \\[pl\\] morph overrides this value, returning a plural value to the\nparent. This plural value gets unified with the singular value that had been retained\nfrom &quot;each&quot; during the top-down cycle. The unification fails and the parse dies. By\nsplitting off the plural morph, singular and plural nouns can share the bulk of their\nphonetics, thus reducing greatly the redundancy in the recognizer's matching prob-\nlem. In theory, morphs could be split off for verbs as well, but due to the large number\nof irregularities this was not done.\nSubject-verb agreement gets enforced by default, because the number information\nthat was realized during the parsing of the subject node gets passed along to the\npredicate and down to the terminal verb node. The lexical item unifies the number\ninformation, and the parse fails if the result is zero. Any nonauxiliary verb node blocks\nthe transfer of any predecessor person/number information to its right siblings during\nthe bottom-up cycle, reflecting the fact that verbs agree in person/number with their\nsubject but not their object.\nCertain nodes set the mode of the verb either during the top-down or the bottom-\nup cycle. Thus, for example, &quot;have&quot; as an auxiliary verb sets mode to PAST-PARTICIPLE\nduring the bottom-up cycle (i.e., for its right-siblings). The category \\[gerund\\] sets the\nmode to PRESENT-PARTICIPLE during the top-down cycle (for its children). Whenever a\n\\[predicate\\] node is invoked, the verb's mode has always been set by a predecessor.\n2.5.1 Gaps. The mechanism to deal with gaps resembles in certain respects the Hold\nregister idea of ATNs, but with an important difference, reflecting the design philoso-\n"},{"#tail":"\n","@confidence":"0.99146578","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nphy that no node can have access to information outside of its immediate domain. The\nmechanism involves two slots that are available in the feature vector of each parse\nnode. These are called the CURRENT-FOCUS and the FLOAT-OBJECT, respectively. The\nCURRENT-FOCUS slot contains, at any given time, a pointer to the most recently men-\ntioned content phrase in the sentence. If the FLOAT-OBJECT slot is occupied, it means\nthat there is a gap somewhere in the future that will ultimately be filled by the partial\nparse contained in the FLOAT-OBJECT. The process of getting into the FLOAT-OBJECT slot\n(which is analogous to the Hold register) requires two steps, executed independently\nby two different nodes. The first node, the generator, fills the CURRENT-FOCUS slot with\nthe subparse returned to it by its children. The second node, the activator, moves the\nCURRENT-FOCUS into the FLOAT-OBJECT position, for its children, during the top-down\ncycle. It also requires that the FLOAT-OBJECT be absorbed somewhere among its descen-\ndants by a designated absorber node, a condition that is checked uring the bottom-up\ncycle. The CURRENT-FOCUS only gets passed along to siblings and their descendants,\nand hence is unavailable to activators at higher levels of the parse tree. That is to say,\nthe CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an \\[end\\] node is\nencountered. To a first approximation, a CURRENT-FOCUS reaches only nodes that are\nc-commanded (Chomsky 1977) by its generator. Finally, certain blocker nodes block the\ntransfer of the FLOAT-OBJECT to their children.\nA simple example will help explain how this works. For the sentence &quot;(How\nmany pies)/ did Mike buy (ti)?&quot; as illustrated by the parse tree in Figure 3, the \\[q-\nsubject\\] &quot;how many pies&quot; is a generator, so it fills the CURRENT-FOCUS with its subparse.\nThe \\[do-question\\] is an activator; it moves the CURRENT-FOCUS into the FLOAT-OBJECT\nposition. Finally, the object of '~ouy,&quot; an absorber, takes the \\[q-subject\\] as its subparse.\nThe \\[do-question\\] refuses to accept any solutions from its children if the FLOAT-OBJECT\nhas not been absorbed. Thus, the sentence &quot;How many pies did Mike buy the pies?&quot;\nwould be rejected. Furthermore, the same \\[do-question\\] grammar node deals with the\nyes/no question &quot;Did Mike buy the pies?,&quot; except in this case there is no CURRENT-\nFOCUS and hence no gap.\nMore complicated sentences involving nested or chained traces are handled\nstraightforwardly b this scheme. For instance, the phrase, &quot;Which hospital was Jane\ntaken to?&quot; can be parsed correctly by TINA, identifying &quot;which hospital&quot; as the object\nof the preposition &quot;to&quot; and &quot;Jane&quot; as the object of &quot;taken.&quot; The phrase &quot;which hos-\npital&quot; gets generated by the \\[q-subject\\] and activated by the following \\[be-question\\],\nthus filling the FLOAT-OBJECT slot. When the predicate of the clause is reached, the\nword &quot;Jane&quot; is in the CURRENT-FOCUS slot, and the phrase &quot;which hospital&quot; is still in\nthe FLOAT-OBJECT slot. The \\[participial-phrase\\] for &quot;taken \\[object\\]&quot; activates &quot;Jane,&quot; but\nonly for its children. This word is ultimately absorbed by the \\[object\\] node within\nthe verb phrase. Meanwhile, the \\[participial-phrase\\] passes along the original FLOAT-\nOBJECT (&quot;which hospital&quot;) to its right sibling, the adverbial prepositional phrase, &quot;to\n\\[object\\].&quot; The phrase &quot;which hospital&quot; is finally absorbed by the preposition's object.\nThe example used to illustrate the power of ATNs (Woods 1986), &quot;John was be-\nlieved to have been shot,&quot; also parses correctly, because the \\[object\\] node following\nthe verb &quot;believed&quot; acts as both an absorber and a (re)generator. Cases of crossed\ntraces are automatically blocked because the second CURRENT-FOCUS gets moved into\nthe FLOAT-OBJECT position at the time of the second activator, overriding the preexist-\ning FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available\nat the position of the first trace, and the parse dies:\n*(Which books)/did you ask John (where)j Bill bought (ti) (tj)?\n"},{"#tail":"\n","@confidence":"0.979856785714286","#text":"\nExample of a parse tree illustrating a gap.\nThe CURRENT-FOCUS slot is not restricted to nodes that represent nouns. Some of\nthe generators are adverbial or adjectival parts of speech (pos). An absorber checks for\nagreement in POS before it can accept he FLOAT-OBJECT as its subparse. As an example,\nthe question, &quot;(How oi ly)/do you like your salad dressing (ti)?&quot; contains a \\[q-subject\\]\n&quot;how oily&quot; that is an adjective. The absorber \\[pred-adjective\\] accepts the available\nfloat-object as its subparse, but only after confirming that POS is ADJECTIVE.\nThe CURRENT-FOCUS has a number of other uses besides its role in movement.\nIt always contains the subject whenever a verb is proposed, including verbs that are\npredicative objects of another verb, as in &quot;I want to go to China.&quot; It has also been found\nto be very effective for passing semantic information to be constrained by a future\nnode, and it can play an integral role in pronoun reference. For instance, a reflexive\npronoun nearly always refers back to the CURRENT-FOCUS, whereas a nonreflexive form\nnever does, unless it is in the nominative case.\n"},{"#tail":"\n","@confidence":"0.99183975","#text":"\nplemented a number of semantic onstraints using procedures very similar to those\nused for syntactic onstraints. We found it effective to filter on the CURRENT-FOCUS's\nsemantic ategory, as well as to constrain absorbers in the gap mechanism to require a\nmatch on semantics before they could accept a FLOAT-OBJECT. Semantic ategories were\n"},{"#tail":"\n","@confidence":"0.99279504","#text":"\nParse tree for the sentence, &quot;What street is the Hyatt on?&quot;\nimplemented in a hierarchy such that, for example, RESTAURANT automatically inher-\nits the more general properties BUILDING and PLACE. We also introduced semantically\nloaded categories at the low levels of the parse tree. It seems that, as in syntax, there\nis a trade-off between the number of unique node-types and the number of constraint\nfiltering operations. At low levels of the parse tree it seems more efficient o label the\ncategories, whereas information that must pass through igher levels of the hierarchy\nis better done through constraint filters.\nAs an example, consider the sentence, &quot;(what street)/is the Hyatt on (ti)?&quot; shown\nin Figure 4. The \\[q-subject\\] places &quot;What street&quot; into the CURRENT-FOCUS slot, but this\nunit is activated to FLOAT-OBJECT status by the subsequent \\[be-question\\]. The \\[subject\\]\nnode refills the now empty CURRENT-FOCUS with &quot;the Hyatt.&quot; The node \\[a-street\\], an\nabsorber, can accept he FLOAT-OBJECT as a solution, but only if there is tight agree-\nment in semantics; i.e., it requires the identifier Street. Thus a sentence such as &quot;What\nrestaurant is the Hyatt on?&quot; would fail on semantic grounds. Furthermore, the node\n\\[on-street\\] imposes trict semantic restrictions on the CURRENT-FOCUS. Thus the sen-\ntence &quot;(What street)/ is Cambridge on (ti)?&quot; would fail because Ion-street\\] does not\npermit Region as the semantic ategory for the CURRENT-FOCUS, &quot;Cambridge.&quot;\nOne place where semantic filtering can play a powerful role is in subject/verb ela-\ntionships. This is easily accomplished within TINA'S framework because the CURRENT-\nFOCUS slot always contains the subject of a verb at the time of the verb's instantiation.\nThis is obvious in the case of a simple statement or complete clause, since the \\[subject\\]\nnode generates a current-focus, which is available as the subject of the terminal verb\nnode in the subsequent \\]predicate\\]. The same \\[subject\\] current-focus i also available\nas the subject of a verb in a predicative object of another verb, as in &quot;I want to go to\n"},{"#tail":"\n","@confidence":"0.9840006","#text":"\nComputational Linguistics Volume 18, Number 1\nChina.&quot; For the case where a verb takes an object and an infinitive phrase as argu-\nments, the \\]object\\] node replaces the current-focus with its subparse, such that when\nthe verb of the infinitive phrase is proposed, the correct subject is available. This han-\ndles cases like &quot;I asked Jane to help.&quot; With this mechanism, the two sentences &quot;I want\nto go&quot; and &quot;I want John to go&quot; can share the same parse node for the verb want.\nCertain sentences exhibit a structure that superficially resembles the verb-object-\ninfinitive-phrase pattern but should not be represented this way, such as &quot;I avoid\ncigarettes to stay healthy.&quot; Here, clearly, 'T' is the subject of &quot;stay.&quot; This can be realized\nin TINA by having a top-level rule, (\\[statement\\] ~ \\]subject\\] \\]predicate\\] \\[adjunct-why\\]).\nThe \\]object\\] node for &quot;cigarettes&quot; replaces the CURRENT-FOCUS, but the replacement\ndoes not get propagated back up to the \\[predicate\\] node (since a current-focus i\npassed only to siblings and children, but not to parents). Thus, the CURRENT-FOCUS &quot;I&quot;\nis passed on from the predicate to the adjunct, and eventually to the verb &quot;stay.&quot;\nFinally, in the case of passive voice, the CURRENT-FOCUS slot is empty at the time\nthe verb is proposed, because the CURRENT-FOCUS which was the surface-form subject\nhas been moved to the float-object position. In this case, the verb has no information\nconcerning its subject, and so it identifies it as an unbound pronoun.\nSemantic filters can also be used to prevent multiple versions of the same case\nframe (Fillmore 1968) showing up as complements. For instance, the set of comple-\nments \\[from-place\\], \\[to-place\\], and \\[at-time\\] are freely ordered following a movement\nverb such as &quot;leave.&quot; Thus a flight can &quot;leave for Chicago from Boston at nine,&quot;\nor, equivalently, &quot;leave at nine for Chicago from Boston.&quot; If these complements are\neach allowed to follow the other, then in TINA an infinite sequence of \\[from-place\\]s,\n\\[to-place\\]s and \\[at-time\\]s is possible. This is of course unacceptable, but it is straight-\nforward to have each node, as it occurs, or in a semantic bit specifying its case frame,\nand, in turn, fail if that bit has already been set. We have found that this strategy, in\nconjunction with the capability of erasing all semantic bits whenever a new clause is\nentered (through the meta level &quot;detach&quot; operation mentioned previously) serves the\ndesired goal of eliminating the unwanted redundancies.\nThus far, we have added all semantic filters by hand, and they are implemented in\na hard-fail mode, i.e., if the semantic restrictions fail, the node dies. This strategy seems\nto be adequate for the limited domains that we have worked with thus far, but they\nwill probably be inadequate for more complex domains. In principle, one could parse a\nlarge set of sentences with semantics turned off, collecting the semantic onditions that\noccurred at each node of interest. Then the system could propose to a human expert a\nset of filters for each node, based on its observations, and the human could make the\nfinal decision on whether to accept he proposals. This approach resembles the work\nby Grishman et al (1986) and Hirschman et al (1975) on selectional restrictions. The\nsemantic onditions that pass could even ultimately be associated with probabilities,\nobtained by frequency counts on their occurrences. There is obviously a great deal\nmore work to be done in this important area.\n3. Evaluat ion Measures\nThis section addresses some performance measures for a grammar, including coverage,\nportability, perplexity, and trainability. Perplexity, roughly defined as the geometric\nmean of the number of alternative word hypotheses that may follow each word in the\nsentence, is of particular concern in spoken language tasks. Portability and trainability\nconcern the ease with which an existing rammar can be ported to a new task, as well\nas the amount of training data necessary before the grammar is able to generalize well\nto unseen data.\n"},{"#tail":"\n","@confidence":"0.988056083333333","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nTo date, four distinct domain-specific versions of TINA have been implemented.\nThe first version (TIMIT) was developed for the 450 phonetically rich sentences of the\nTIMIT database (Lamel et al 1986). The second version (RM) concerns the Resource\nManagement task (Pallett 1989) that has been popular within the DARPA community\nin recent years. The third version (VOYAGER) serves as an interface both with a recog-\nnizer and with a functioning database back-end (Zue et al 1990). The VOYAGER system\ncan answer a number of different ypes of questions concerning navigation within a\ncity, as well as provide certain information about hotels, restaurants, libraries, etc.,\nwithin the region. A fourth domain-specific version is under development for the ATIS\n(Air Travel Information System) task, which has recently been designated as the new\ncommon task for the DARPA community.\n"},{"#tail":"\n","@confidence":"0.999023933333333","#text":"\nWe tested ease of portability for TINA by beginning with a grammar built from the\n450 TIMIT sentences and then deriving a grammar for the RM task. These two tasks\nrepresent very different sentence types. For instance, the overwhelming majority of\nthe TIMIT sentences are statements, whereas the RM task is made up exclusively of\nquestions and requests. The process of conversion to a new grammar involves parsing\nthe new sentences one by one, and adding context-free rules whenever a parse fails.\nThe person entering the rules must be very familiar with the grammar structure, but\nfor the most part it is straightforward to identify and incrementally add missing rules.\nThe parser identifies where in the sentence it fails, and also maintains a record of the\nsuccessful partial parses. These pieces of information usually are adequate to pinpoint\nthe problem. Once the grammar has been expanded to accomodate he new set of\nsentences, a subset grammar can be created automatically that only contains rules\nneeded in the new domain, eliminating any rules that were particular to the original\ndomain. It required less than one person-month o convert he grammar from TIMIT\nto the RM task.\n"},{"#tail":"\n","@confidence":"0.99933","#text":"\nA set of 791 sentences within the RM task have been designated as training sentences,\nand a separate set of 200 sentences as the test set. We built a subset grammar from\nthe 791 parsed training sentences, and then used this grammar to test coverage and\nperplexity on the unseen test sentences. The grammar could parse 100% of the training\nsentences and 84% of the test sentences.\nA formula for the test set perplexity (Lee 1989) is: 13\n"},{"#tail":"\n","@confidence":"0.989670666666667","#text":"\nwhere the wi are the sequence of all words in all sentences, N is the total number\nof words, including an &quot;end&quot; word after each sentence, and P(wi I Wi--I~'''Wl) is\nthe probability of the ith word given all preceding wordsJ 4 If all words are assumed\nequally likely, then P(wi \\] wi-1,.., wl) can be determined by counting all the words\nthat could follow each word in the sentence, along all workable partial theories. If the\ngrammar contains probability estimates, then these can be used in place of the equally\n"},{"#tail":"\n","@confidence":"0.993598789473684","#text":"\nlikely assumption. If the grammar's estimates reflect reality, the estimated probabilities\nwill result in a reduction in the total perplexity.\nAn average perplexity for the 167 test sentences that were parsable was computed\nfor the two conditions, without (Case 1) and with (Case 2) the estimated probabilities.\nThe result was a perplexity of 368 for Case 1, but only 41.5 for Case 2, as summarized\nin Table 1. This is with a total vocabulary size of 985 words, and with a grammar\nthat included some semantically restricted classes uch as \\[ship-name\\] and \\[readiness-\ncategory\\]. The incorporation of arc probabilities reduced the perplexity by a factor of\nnine, a clear indicator that a proper mechanism for utilizing probabilities in a grammar\ncan help significantly. An even lower perplexity could be realized within this domain\nby increasing the number of semantic nodes. In fact, this is a trend that we have\nincreasingly adopted as we move to new domains.\nWe didn't look at the test sentences while designing the grammar, nor have we yet\nlooked at those sentences that failed to parse. However, we decided to examine the\nparse trees for those sentences that produced at least one parse to determine the depth\nof the first reasonable parse. The results were essentially the same for the training and\nthe test sentences, as shown in Table 2. Both gave a reasonable parse as either the first\nor second proposed parse 96% of the time. Two of the test sentences never gave a\ncorrect parse.\n"},{"#tail":"\n","@confidence":"0.996597777777778","#text":"\nWe have recently developed a subdomain for TINA that has been incorporated into\na complete spoken language system called VOYAGER. The system provides directions\non how to get from one place to another within an urban region, and also gives\ninformation such as phone number or address for places such as restaurants, hotels,\nlibraries, etc. We have made extensive use of semantic filters within this domain, in\norder to reduce the perplexity of the recognition task as much as possible.\nTo obtain training and test data for this task, we had a number of naive sub-\njects use the system as if they were trying to obtain actual information. Their speech\nwas recorded in a simulation mode in which the speech recognition component was\n"},{"#tail":"\n","@confidence":"0.999026777777778","#text":"\nexcluded. Instead, an experimenter in a separate room typed in the utterances as\nspoken by the subject. Subsequent processing by the natural anguage and response\ngeneration components was done automatically by the computer (Zue et al 1989).\nWe were able to'collect a total, of nearly 5000 utterances in this fashion. The speech\nmaterial was then used to train the recognizer component, and the text material was\nused to train the natural language and back-end components.\nWe designated a subset of 3312 sentences as the training set, and augmented the\noriginal rules so as to cover a number of sentences that appeared to stay within the\ndomain of the back-end. We did not try to expand the rules to cover sentences that\nthe back-end could not deal with, because we wanted to keep the natural anguage\ncomponent tightly restricted to sentences with a likely overall success. In this way\nwe were able to increase the coverage of an independent test set of 560 utterances\nfrom 69% to 76%, with a corresponding increase in perplexity, as shown in Table 3.\nPerplexity was quite low even without probabilities; this is due mainly to an extensive\nsemantic filtering scheme. Probabilities decreased the perplexity by a factor of three,\nhowever, which is still quite significant. An encouraging result was that both perplexity\nand coverage were of comparable values for the training and test sets, as shown in\nthe table.\n"},{"#tail":"\n","@confidence":"0.9997482","#text":"\nAs mentioned previously, generation mode has been a very useful device for detecting\novergeneralization problems in a grammar. After the addition of a number of seman-\ntically loaded nodes and semantic filters, the VOYAGER version of the grammar is now\nrestricted mainly to sentences that are semantically as well as syntactically legitimate.\nTo illustrate this point we show in Table 4 five examples of consecutively generated\nsentences. Since these were not selectively drawn from a larger set, they accurately\nreflect he current performance l vel.\nWe also used generation mode to construct a word-pair grammar automatically for\nthe recognizer component of our VOYAGER system. To do this, over 100,000 sentences\nwere generated, and word-pair links were established for all words sharing the same\nterminal category (such as \\[restaurant-name\\], for all category-pairs appearing in the\ngenerated sentences. We could test completion by continuing until no new pairs were\nfound. The resulting word pair grammar has a perplexity of over 70, in contrast to a\nperplexity of less than nine for the grammar used to construct i . This difference reflects\nthe additional constraint of both the probabilities and the long-distance dependencies.\n"},{"#tail":"\n","@confidence":"0.998611638888889","#text":"\nAt present, we have available at MIT two systems, VOYAGER and ATIS, involving specific\napplication domains in which a person can carry on a dialog with the computer, either\nthrough spoken speech or through text input. In both of these systems, TINA provides\nthe interface between the recognizer and the application back-end. In this section, I\nwill describe our current interfaces between TINA and the recognizer and our future\nplans in this area. In addition, I will describe briefly how we currently translate the\nparse tree into a semantic frame that serves as the input to database access and text\nresponse generation. This aspect of the system is beyond the scope of this paper, and\ntherefore it will not be covered in detail.\nThe recognizer for these systems is the SUMMIT system (Zue et al 1989), which\nuses a segmental-based framework and includes an auditory model in the front-end\nprocessing. The lexicon is entered as phonetic pronunciations that are then augmented\nto account for a number of phonological rules. The search algorithm is the standard\nViterbi search (Viterbi 1967), except that the match involves a network-to-network\nalignment problem rather than sequence-to-sequence.\nWhen we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection,\nin that the recognizer produced a single best output, which was then passed to TINA for\nparsing. A simple word-pair grammar constrained the search space. If the parse failed,\nthen the sentence was rejected. We have since improved the interface by incorporating\na capability in the recognizer to propose additional solutions in turn once the first\none fails to parse (Zue et al 1991) To produce these &quot;N-best&quot; alternatives, we make\nuse of a standard A* search algorithm (Hart 1968, Jelinek 1976). Both the A* and the\nViterbi search are left-to-right search algorithms. However, the A* search is contrasted\nwith the Viterbi search in that the set of active hypotheses take up unequal segments\nof time. That is, when a hypothesis is scoring well it is allowed to procede forward,\nwhereas poorer scoring hypotheses are kept on hold.\nWe have thus far developed two versions of the control strategy, a &quot;loosely cou-\npled&quot; system and a &quot;tightly coupled&quot; system. Both versions begin with a Viterbi search\nall the way to the end of the sentence, resulting in not only the first candidate solution\nbut also partial scores for a large set of other hypotheses. If this first solution fails to\nparse, then the best-scoring partial theory is allowed to procede forward incrementally.\nIn an A* search, the main issue is how to get an estimate of the score for the unseen\nportion of the sentence. In our case, we can use the Viterbi path to the end as the\nestimate of the future score. This path is guaranteed to be the best way to get to the\nend; however, it may not parse. Hence it is a tight upper bound on the true score for\nthe rest of the sentence. The recognizer can continue to propose hypotheses until one\n"},{"#tail":"\n","@confidence":"0.998368692307692","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nsuccessfully parses, or until a quitting criterion is reached, such as an upper bound\non N.\nWhereas in the loosely coupled system the parser acts as a filter only on completed\ncandidate solutions (Zue et al 1991), the tightly coupled system allows the parser to\ndiscard partial theories that have no way of continuing. Following the Viterbi search,\neach partial theory is first extended by the parser to specify possible next words, which\nare then scored by the recognizer. We have not yet made use of TINA'S probabilities in\nadjusting the recognizer scores on the fly, but we have been able to incorporate linguis-\ntic scores to resort N-best outputs, giving a significant improvement in performance\n(Goodine et al 1991). Ultimately we want to incorporate TINA'S probabilities directly\ninto the A* search, but it is as yet unclear how to provide an appropriate upper bound\nfor the probability estimate of the unseen portion of the linguistic model.\nOnce a parser has produced an analysis of a particular sentence, the next step\nis to convert it to a meaning representation form that can be used to perform what-\never operations the user intended by speaking the sentence. We currently achieve this\ntranslation step in a second-pass treewalk through the completed parse tree. Although\nthe generation of semantic frames could be done on the fly as the parse is being pro-\nposed, it seems inappropriate o go through all of that extra work for large numbers\nof incorrect partial theories, due to the uncertainty as to the identity of the terminal\nword strings inherent in spoken input.\nWe have taken the point of view that all syntactic and semantic information can\nbe represented uniformly in strictly hierarchical structures in the parse tree. Thus the\nparse tree contains nodes such as \\[subject\\] and \\[dir-object\\] that represent structural\nroles, as well as nodes such as \\[on-street\\] and \\[a-school\\] representing specific semantic\ncategories. There are no separate semantic rules off to the side; rather, the semantic\ninformation is encoded irectly as names attached to nodes in the tree.\nExactly how to get from the parse tree to an appropriate meaning representation\nis a current research topic in our group. However, the method we are currently using\nin the ATIS domain (Seneff et al 1991) represents our most promising approach to this\nproblem. We have decided to limit semantic frame types to a small set of choices uch\nas CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional\noperation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The\nprocess of obtaining a completed semantic frame amounts to passing frames along\nfrom node to node through the completed parse tree. Each node receives a frame in\nboth a top-down and a bottom-up cycle, and modifies the frame according to spec-\nifications based on its broad-class identity (as one of noun, noun-phrase, predicate,\nquantifier, etc.). For example, a \\[subject\\] is a noun-phrase node with the label &quot;topic.&quot;\nDuring the top-down cycle, it creates a blank frame and inserts it into a &quot;topic&quot; slot\nin the frame that was handed to it. It passes the blank frame to its children, who will\nthen fill it appropriately, labeling it as a QSET or as a REFERENCE. It then passes along to\nthe right sibling the same frame that was handed to it from above, with the completed\ntopic slot filled with the information delivered by the children.\nThe raw frame that is realized through the treewalk is post-processed to simplify\nsome of the structure, as well as to augment or interpret expressions such as relative\ntime. For example, the predicate modifier in &quot;flights leaving at ten a.m.&quot; is simplified\nfrom a predicate leave to a modifier slot labeled departure-time. An expression such\nas &quot;next Tuesday&quot; is interpreted relative to today's date to fill in an actual month,\ndate, and year. Following this post-analysis step, the frame is merged with references\ncontained in a history record, to fold in information from the previous discourse.\nThe completed semantic frame is used in ATIS both to generate an SQL (Structured\nQuery Language) command to access the database and to generate a text output o be\n"},{"#tail":"\n","@confidence":"0.995155326530612","#text":"\nComputational Linguistics Volume 18, Number 1\nspoken in the interactive dialog. The SQL pattern is controlled through lists of frame\npatterns to match and query fragments o generate given the match. Text generation is\ndone by assigning appropriate temporal ordering for modifiers on nouns and for the\nmain noun. The modifiers are contained in slots associated with the QSET frame. Certain\nframes such as clock-time have special print functions that produce the appropriate\npiece of text associated with the contents.\n5. D iscuss ion\nThis paper describes a new natural language system that addresses i sues of concern\nin building a fully integrated spoken language system. The formalism provides an\nintegrated approach to representations for syntax and for semantics, and produces a\nhighly constraining language model to a speech recognizer. The grammar includes\narc probabilities reflecting the frequency of occurrence of patterns within the domain.\nThese probabilities are used to control the order in which hypotheses are considered,\nand are trained automatically from a set of parsed sentences, making it straightforward\nto tailor the grammar to a particular need. Ultimately, one could imagine the existence\nof a very large grammar that could parse almost anything, which would be subsetted\nfor a particular task by simply providing it with a set of example sentences within\nthat domain.\nThe grammar makes use of a number of other principles that we felt were im-\nportant. First of all, it explicitly incorporates into the parse tree semantic ategories\nintermixed with syntactic ones, rather than having a set of semantic rules provided\nseparately. The semantic nodes are dealt with in the same way as the syntactic nodes;\nthe consequence is that the node names alone carry essentially all of the information\nnecessary to extract a meaning representation from the sentence. The grammar is not\na semantic grammar in the usual sense, because it does include high level nodes of a\nsyntactic nature, such as noun-clause, subject, predicate, etc.\nA second important feature is that unifications are performed in a one-dimensional\nframework. That is to say, features delivered to a node by a close relative (sibling/parent/\nchild) are unified with particular feature values associated with that node. The x vari-\nable in an x-y relationship is not explicitly mentioned, but rather is assigned to be\n&quot;whatever was delivered by the relative.&quot; Thus, for example, a node such as \\[subject\\]\nunifies in exactly the same way, regardless of the rule under construction.\nAnother important feature of TINA is that the same grammar can be run in gener-\nation mode, making up random sentences by tossing the dice. This has been found to\nbe extremely useful for revealing overgeneralization problems in the grammar, as well\nas for automatically acquiring a word-pair grammar for a recognizer and producing\nsentences to test the back-end capability.\nWe discussed a number of different application domains, and gave some perfor-\nmance statistics in terms of perplexity/coverage/overgeneralization within some of\nthese domains. The most interesting result was obtained within the VOYAGER domain\n(see Sections 3.3 and 3.4). The perplexity (average number of words that can follow\na given word) decreased from 70 to 28 to 8 when the grammar changed from word-\npair (derived from the same grammar) to parser without probabilities to parser with\nprobabilities.\nWe_currently have two application domains that can carry on a spoken dialog with\na user. One, the VOYAGER domain (Zue et al 1990), answers questions about places\nof interest in an urban area, in our case, the vicinity of MIT and Harvard University.\nThe second one, ATIS (Seneff et al 1991), is a system for accessing data in the Official\n"},{"#tail":"\n","@confidence":"0.993844714285714","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nAirline Guide and booking flights. Work continues on improving all aspects of these\ndomains.\nOur current research is directed at a number of different remaining issues. As of\nthis writing, we have a fully integrated version of the VOYAGER system, using an A*\nsearch algorithm (Goodine t al. 1991). The parser produces a set of next-word candi-\ndates dynamically for each partial theory. We have not yet incorporated probabilities\nfrom TINA into the search, but they are used effectively to resort he final output sen-\ntence candidates. In order to incorporate the probabilities into the search we need a\ntight upper bound on the future linguistic score for the unseen portion of each hypoth-\nesis. This is a current research topic in our group. We also plan to experiment with\nfurther eductions in perplexity based on a discourse state. This should be particularly\neffective within the ATIS domain where the system often asks directed questions about\nas yet unresolved particulars to the flight.\n"},{"#tail":"\n","@confidence":"0.95767","#text":"\nComputation\nThis appendix walks through a pedagogical example to parse spoken digit sequences\nup to three long, as in &quot;three hundred and sixteen.&quot; Included is a set of initial context-\nfree rules, a set of training sentences, an illustration of how to compute the path prob-\nabilities from the training sentences, and an illustration of both parsing and perplexity\ncomputation for a test sentence.\nSince there are only five training sentences, a number of the arcs of the original\ngrammar are lost after training. This is a problem to be aware of in building grammars\nfrom example sentences. In the absence of a sufficient amount of training data, some\narcs will inevitably be zeroed out. Unless it is desired to intentionally filter these out\nas being outside of the new domain, one can insert some arbitrarily small probability\nfor these arcs, using, for example, an N-gram back-off model (Katz 1987).\n"},{"#tail":"\n","@confidence":"0.9476485","#text":"\nThe training sentences: (with spoken form)\nI: 144 &quot;one hundred and forty four&quot;\n"},{"#tail":"\n","@confidence":"0.818408","#text":"\nThe training pairs for &quot;hundreds-place&quot; (gathering together all rules in (1, 2, 3, 5)\nabove that have &quot;hundreds-place&quot; on the LHS:\n"},{"#tail":"\n","@confidence":"0.905435714285714","#text":"\ndigits, digits hundred, hundred and, and end\ndigits, digits end\ndigits, digits end\na, a hundred, hundred end\nThe count array for &quot;hundreds-place&quot;:\ndigits hundred and end a total\nstart 3 0 0 0 1 4\n"},{"#tail":"\n","@confidence":"0.996062666666667","#text":"\nThe probability of a transition from start o digits, within the parent node &quot;hundreds-\nplace,&quot; is just 3/4, the ratio of the number of times &quot;hundreds-place&quot; started with\n&quot;digits&quot; over the number of times it started with anything.\n"},{"#tail":"\n","@confidence":"0.53864","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nParsing the phrase &quot;four fifteen&quot; with the trained parser:\nThe initial stack: ~5\n"},{"#tail":"\n","@confidence":"0.92363825","#text":"\nAfter &quot;hundreds-place&quot; gets popped and expanded:\ndigitsJhundreds-place, start 4/5*3/4\ntens-placelnumber, start 1/5\nalhundreds-place, start 4/5.1/4 (this is a tie score with\nthe above)\nAfter &quot;digitslhundreds-place&quot; is popped and a match with &quot;four&quot; is found:\nendihundreds-place, digits\nhundredJhundreds-place, digits\ntens-placelnumber, start\nalhundreds-place, start\n2/3 (given &quot;four&quot; with certainty)\n1/3 (this is the word &quot;hundred&quot;)\n"},{"#tail":"\n","@confidence":"0.998857619047619","#text":"\nAfter &quot;endlhundreds-place, digits&quot; is popped, &quot;hundreds-place&quot; has a solution in\nhand, &quot;four.&quot; It now activates its only right sibling, &quot;tens-place.&quot; This is a different\ninstance of &quot;tens-place&quot; from the one at the third place in the stack. Its left sibling is\n&quot;hundreds-place&quot; rather than &quot;start.&quot;\ntens-placeJnumber, hundreds-place 2/3\nhundredIhundreds-place, digits i/3\ntens-placeinumber, start I/5\naihundreds-place, start 4/5,1/4\nAfter &quot;tens-place&quot; is expanded, we have:\ntensftens-place, start 2/3~3/5\nhundredihundreds-place, digits i/3\ntens-placelnumber, start i/5\naJhundreds-place, start 4/5~1/4\nteensftens-place, start 2/3.1/5\nohltens-place, start 2/3~1/5\n&quot;Tens&quot; and &quot;hundred&quot; will both get popped off and rejected, because there is no match\nwith the word &quot;fifteen.&quot; &quot;Tens-151ace&quot; will also get popped, and eventually rejected,\nbecause nothing within &quot;tens-place&quot; matches the digit &quot;four.&quot; A similar fate meets the\n&quot;a&quot; hypothesis. Finally, &quot;teens&quot; will be popped off and matched, and &quot;endltens-place,\nteens&quot; will be inserted at the top with probability 1.0. This answer will be returned\nto the parent, &quot;tens-place,&quot; and two new hypotheses will be inserted at the top of the\n"},{"#tail":"\n","@confidence":"0.939301090909091","#text":"\nPaths through the parse tree for the phrase &quot;four fifteen&quot; with associated probabilities derived\nfrom the training data.\nstack as follows:\nones-placelnumber, tens-place\nendlnumber, tens-place\n315 215\nAfter the first one is rejected, the second one finds a completed &quot;number&quot; rule and an\nempty input stream. The correct solution is now in hand. Notice that because &quot;teens&quot;\nwas a relatively rare occurrence, a number of incorrect hypotheses had to be pursued\nbefore the correct one was considered.\nComputation of perplexity, for the phrase, &quot;four fifteen:&quot;\n"},{"#tail":"\n","@confidence":"0.789007","#text":"\nThese are the three transitions with associated probabilities, following the appropriate\npaths in Figure A.I:\n"},{"#tail":"\n","@confidence":"0.9969069","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nparticular phrase. This is higher than the norm for numbers given the grammar, again\nbecause of the rare occurrence of the &quot;teens&quot; node, as well as the fact that there is\nno ones-place. This example is a bit too simple - in general there would be multiple\nways to get to a particular next word, and there are also constraints which kill certain\npaths and make it necessary to readjust probabilities on the fly. In practice, one must\nfind all possible ways to extend a word sequence, computing total path probability for\neach one, and then renormalize to assure that with probability 1.0 there is an advance\nto some next word. It is the normalized probability contribution of all paths that can\nreach the next word that is used to update the log P calculation.\n"},{"#tail":"\n","@confidence":"0.9827242","#text":"\nThis research as benefited significantly\nfrom interactions with Lynette Hirschman\nand Victor Zue. In addition, Jim Glass,\nDavid Goodine, and Christine Pao have all\nmade significant contributions to the\nprogramming of the TINA system, for which\nI am deeply grateful. I would also like to\nthank several anonymous reviewers for\ntheir careful critiques, the outcome of which\nwas a substantially improved ocument.\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.975812","#text":"\nMassachusetts Institute of Technology\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.737745","@genericHeader":"abstract","#text":"\n1. Introduction and Overv iew\n"},{"#tail":"\n","@confidence":"0.638679","@genericHeader":"method","#text":"\n3 A technical term used in speech recognition to denote the geometric mean of the number of alternative\n"},{"#tail":"\n","@confidence":"0.987545","@genericHeader":"method","#text":"\n2. Detailed Description\n"},{"#tail":"\n","@confidence":"0.556371","@genericHeader":"method","#text":"\n6. Appendix: Sample Grammar Illustrating Probability Calculation and Perplexity\n"},{"#tail":"\n","@confidence":"0.978458","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.99215","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.6299105","#text":"\nTable 4\nSample sentences generated consecutively b the VOYAGER version of TINA.\n"},{"#tail":"\n","@confidence":"0.91383575","#text":"\nfrom 1: start\nfrom 2: start\nfrom 3: start\nfrom 5: start\n"}],"page":[{"#tail":"\n","@confidence":"0.997018","#text":"\n62\n"},{"#tail":"\n","@confidence":"0.998627","#text":"\n63\n"},{"#tail":"\n","@confidence":"0.974452","#text":"\n.25\n"},{"#tail":"\n","@confidence":"0.992891","#text":"\n65\n"},{"#tail":"\n","@confidence":"0.991556","#text":"\n66\n"},{"#tail":"\n","@confidence":"0.99847","#text":"\n67\n"},{"#tail":"\n","@confidence":"0.997344","#text":"\n68\n"},{"#tail":"\n","@confidence":"0.996592","#text":"\n69\n"},{"#tail":"\n","@confidence":"0.991607","#text":"\n70\n"},{"#tail":"\n","@confidence":"0.662956","#text":"\n71\n"},{"#tail":"\n","@confidence":"0.769621","#text":"\n72\n"},{"#tail":"\n","@confidence":"0.994034","#text":"\n73\n"},{"#tail":"\n","@confidence":"0.993628","#text":"\n74\n"},{"#tail":"\n","@confidence":"0.990848","#text":"\n75\n"},{"#tail":"\n","@confidence":"0.944034","#text":"\n76\n"},{"#tail":"\n","@confidence":"0.992607","#text":"\n77\n"},{"#tail":"\n","@confidence":"0.987975","#text":"\n78\n"},{"#tail":"\n","@confidence":"0.990017","#text":"\n79\n"},{"#tail":"\n","@confidence":"0.973225","#text":"\n80\n"},{"#tail":"\n","@confidence":"0.821645","#text":"\n81\n"},{"#tail":"\n","@confidence":"0.782352","#text":"\n82\n"},{"#tail":"\n","@confidence":"0.404454","#text":"\ni/5\n"},{"#tail":"\n","@confidence":"0.983308","#text":"\n84\n"},{"#tail":"\n","@confidence":"0.948468","#text":"\n85\n"},{"#tail":"\n","@confidence":"0.995813","#text":"\n86\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.672593","#text":"\nFigure 3\n"},{"#tail":"\n","@confidence":"0.950091","#text":"\nFigure 4\n"}],"table":[{"#tail":"\n","@confidence":"0.6153645","#text":"\nComputational Linguistics Volume 18, Number 1\n\\] Pop Queue yes Y'~Ib-- ACCEPT\n"},{"#tail":"\n","@confidence":"0.963573166666667","#text":"\nComputational Linguistics Volume 18, Number 1\nTable 1\nSummary of perplexity and coverage within the Resource Management domain, for the 200\ndesignated test sentences.\nVocabulary Coverage Perplexity Perplexity\nSize No Probabilities With Probabilities\n985 84% 368 41.5\nTable 2\nRanking of first reasonable parse in the Resource Management task.\nTop 1 Top 2 Top 3 Top 7\nTraining 88% 96% 98% 100%\nTest 90% 96% 99% 99%\n"},{"#tail":"\n","@confidence":"0.87355925","#text":"\nStephanie Seneff TINA: A Natural Language System for Spoken Language Applications\nTable 3\nPerplexity and coverage data for test and training samples within the VOYAGER domain.\nData set: Test Test Training\nSystem: initial expanded expanded\nNo Prob: 20.6 27.1 25.8\nProb: 7.1 8.3 8.1\nCoverage: 69% 76% 78%\n"},{"#tail":"\n","@confidence":"0.408916","#text":"\nComputational Linguistics Volume 18, Number 1\n"},{"#tail":"\n","@confidence":"0.437615125","#text":"\nTransition Probability\n1: start ~ four 4 /5 ,3 /4 ,1 /10\n2: four --* fifteen 1 ,2 /3 ,1 ,1 /5 ,1 /10\n3: fifteen --* end 1 ,1 ,2 /5\nThus, for this example test sentence:\nlo ~ /43 1 ~ 21 1 ~;2~ ~ Y6 ) q- log2(~ ~ i-6) q- l?g2(2)\nPerplexity = 2 3\nThis comes out to about 14 words on average following a given word, for this\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.943685","#tail":"\n","@no":"0","#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.991803","#text":"Massachusetts Institute of Technology"},"author":{"#tail":"\n","@confidence":"0.9917","#text":"Stephanie Senefft"},"abstract":{"#tail":"\n","@confidence":"0.995633142857143","#text":"A new natural language system, TINA, has been developed for applications involving spoken language tasks. TINA integrates key ideas from context free grammars, Augmented Transition Networks (ATN's), and the unification concept. TINA provides a seamless interface between syntactic and semantic analysis, and also produces ahighly constraining probabilistic language model to improve recognition performance. An initial set of context-free r write rules provided by hand is first converted to a network structure. Probability assignments on all arcs in the network are obtained automatically from a set of example sentences. The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism todeal with long-distance movement, agreement, and semantic constraints. TINA provides an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer. The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process."},"title":{"#tail":"\n","@confidence":"0.999507","#text":"TINA: A Natural Language System for Spoken Language Applications"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Boisen, S.; Chow, Y.-L.; Haas, A.; lngria, R.; Roukos, S.; and Stallard, D. (1989). &quot;The BBN spoken language system.&quot; In Proceedings, DARPA Speech and Natural Language Workshop. 106-111."},"#text":"\n","pages":{"#tail":"\n","#text":"106--111"},"marker":{"#tail":"\n","#text":"Boisen, Chow, Haas, lngria, Roukos, Stallard, 1989"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"chine interface using speech require an &quot;understanding&quot; of the intended message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current ad- vances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural anguage workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Repre- sentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). Spoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139 ~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; ","@endWordPosition":"365","@position":"2463","annotationId":"T1","@startWordPosition":"362","@citStr":"Boisen et al (1989)"}},"title":{"#tail":"\n","#text":"The BBN spoken language system.&quot;"},"booktitle":{"#tail":"\n","#text":"In Proceedings, DARPA Speech and Natural Language Workshop."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Boisen"},{"#tail":"\n","#text":"Y-L Chow"},{"#tail":"\n","#text":"A Haas"},{"#tail":"\n","#text":"R lngria"},{"#tail":"\n","#text":"S Roukos"},{"#tail":"\n","#text":"D Stallard"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1982"},"rawString":{"#tail":"\n","#text":"Bresnan, J., ed. (1982). The Mental Representation f Grammatical Relations. Cambridge, MA: The MIT Press."},"#text":"\n","marker":{"#tail":"\n","#text":"Bresnan, ed, 1982"},"publisher":{"#tail":"\n","#text":"The MIT Press."},"location":{"#tail":"\n","#text":"Cambridge, MA:"},"title":{"#tail":"\n","#text":"The Mental Representation f Grammatical Relations."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Bresnan"},{"#tail":"\n","#text":"ed"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1977"},"rawString":{"#tail":"\n","#text":"Chomsky, Noam (1977). &quot;On wh-movement.&quot; In Formal Syntax, edited by P. Culicover, T. Wasow, and A. Akmajian. New York: Academic Press."},"#text":"\n","marker":{"#tail":"\n","#text":"Chomsky, 1977"},"publisher":{"#tail":"\n","#text":"Academic Press."},"location":{"#tail":"\n","#text":"New York:"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"il we have a better handle on control strategies in the best-first search algorithm, it is impossible to predict he computational load for a spoken-input mode. 2.5 Constraints and Gaps This section describes how TINA handles everal issues that are often considered to be part of the task of a parser. These include agreement constraints, emantic restrictions, 68 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in &quot;(which article)/do you think I should read (ti)?&quot;) (Chomsky 1977). The gap mechanism resembles the Hold register idea of ATNs (Woods 1970) and the treatment of bounded omination metavariables in lexical functional grammars (LFGs) (Bresnan 1982, p. 235 ft.), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent odes. Our approach to the design of a constraint mechanism is to establish a framework general enough to handle syntactic, semantic, and, ultimately, phonological constraints using identical functional procedures applied at the node level. The intent was to","@endWordPosition":"4512","@position":"28337","annotationId":"T2","@startWordPosition":"4511","@citStr":"Chomsky 1977"},{"#tail":"\n","#text":" into the FLOAT-OBJECT position, for its children, during the top-down cycle. It also requires that the FLOAT-OBJECT be absorbed somewhere among its descen- dants by a designated absorber node, a condition that is checked uring the bottom-up cycle. The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree. That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an \\[end\\] node is encountered. To a first approximation, a CURRENT-FOCUS reaches only nodes that are c-commanded (Chomsky 1977) by its generator. Finally, certain blocker nodes block the transfer of the FLOAT-OBJECT to their children. A simple example will help explain how this works. For the sentence &quot;(How many pies)/ did Mike buy (ti)?&quot; as illustrated by the parse tree in Figure 3, the \\[q- subject\\] &quot;how many pies&quot; is a generator, so it fills the CURRENT-FOCUS with its subparse. The \\[do-question\\] is an activator; it moves the CURRENT-FOCUS into the FLOAT-OBJECT position. Finally, the object of '~ouy,&quot; an absorber, takes the \\[q-subject\\] as its subparse. The \\[do-question\\] refuses to accept any solutions from it","@endWordPosition":"6041","@position":"37830","annotationId":"T3","@startWordPosition":"6040","@citStr":"Chomsky 1977"}]},"title":{"#tail":"\n","#text":"On wh-movement.&quot; In Formal Syntax, edited by"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Noam Chomsky"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"De Mattia, M., and Giachin, E. P. (1989). &quot;Experimental results on large vocabulary continuous speech understanding.&quot; ICASSP-89 Proceedings. 691-694."},"#text":"\n","pages":{"#tail":"\n","#text":"691--694"},"marker":{"#tail":"\n","#text":"De Mattia, Giachin, 1989"},"title":{"#tail":"\n","#text":"Experimental results on large vocabulary continuous speech understanding.&quot;"},"booktitle":{"#tail":"\n","#text":"ICASSP-89 Proceedings."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M De Mattia"},{"#tail":"\n","#text":"E P Giachin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1968"},"note":{"#tail":"\n","#text":"edited by"},"rawString":{"#tail":"\n","#text":"Fillmore, C. J. (1968). &quot;The case for case.&quot; In Universals in Linguistic Theory, edited by E. Bach and R. Harms. New York: Holt, Rinehart, and Winston. 1-90."},"#text":"\n","pages":{"#tail":"\n","#text":"1--90"},"marker":{"#tail":"\n","#text":"Fillmore, 1968"},"location":{"#tail":"\n","#text":"New York: Holt, Rinehart, and Winston."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" a current-focus i passed only to siblings and children, but not to parents). Thus, the CURRENT-FOCUS &quot;I&quot; is passed on from the predicate to the adjunct, and eventually to the verb &quot;stay.&quot; Finally, in the case of passive voice, the CURRENT-FOCUS slot is empty at the time the verb is proposed, because the CURRENT-FOCUS which was the surface-form subject has been moved to the float-object position. In this case, the verb has no information concerning its subject, and so it identifies it as an unbound pronoun. Semantic filters can also be used to prevent multiple versions of the same case frame (Fillmore 1968) showing up as complements. For instance, the set of comple- ments \\[from-place\\], \\[to-place\\], and \\[at-time\\] are freely ordered following a movement verb such as &quot;leave.&quot; Thus a flight can &quot;leave for Chicago from Boston at nine,&quot; or, equivalently, &quot;leave at nine for Chicago from Boston.&quot; If these complements are each allowed to follow the other, then in TINA an infinite sequence of \\[from-place\\]s, \\[to-place\\]s and \\[at-time\\]s is possible. This is of course unacceptable, but it is straight- forward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in t","@endWordPosition":"7367","@position":"46134","annotationId":"T4","@startWordPosition":"7366","@citStr":"Fillmore 1968"}},"title":{"#tail":"\n","#text":"The case for case.&quot; In Universals in Linguistic Theory,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"C J Fillmore"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Goodine, D.; Seneff, S.; Hirschman, L.; and Phillips, M. (1991). &quot;Full integration of speech and language understanding in the MIT spoken language system.&quot; In Proceedings, 2nd European Conference on Speech Communication a d Technology. Genova, Italy. 24-26."},"#text":"\n","pages":{"#tail":"\n","#text":"24--26"},"marker":{"#tail":"\n","#text":"Goodine, Seneff, Hirschman, Phillips, 1991"},"location":{"#tail":"\n","#text":"Genova,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing. Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA'S probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguis- tic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al 1991). Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model. Once a parser has produced an analysis of a particular sentence, the next step is to convert it to a meaning representation form that can be used to perform what- ever operations the user intended by speaking the sentence. We currently achieve this translation step in a second-pass treewalk through the completed parse tree. Although the generation of semantic frames co","@endWordPosition":"9967","@position":"61859","annotationId":"T5","@startWordPosition":"9964","@citStr":"Goodine et al 1991"}},"title":{"#tail":"\n","#text":"Full integration of speech and language understanding in the MIT spoken language system.&quot;"},"booktitle":{"#tail":"\n","#text":"In Proceedings, 2nd European Conference on Speech Communication a d Technology."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Goodine"},{"#tail":"\n","#text":"S Seneff"},{"#tail":"\n","#text":"L Hirschman"},{"#tail":"\n","#text":"M Phillips"}]}},{"date":{"#tail":"\n","#text":"1986"},"issue":{"#tail":"\n","#text":"3"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"il mode, i.e., if the semantic restrictions fail, the node dies. This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic onditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept he proposals. This approach resembles the work by Grishman et al (1986) and Hirschman et al (1975) on selectional restrictions. The semantic onditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area. 3. Evaluat ion Measures This section addresses some performance measures for a grammar, including coverage, portability, perplexity, and trainability. Perplexity, roughly defined as the geometric mean of the number of alternative word hypotheses that may follow each word in the sentence, is of particular concern in spoken la","@endWordPosition":"7634","@position":"47752","annotationId":"T6","@startWordPosition":"7631","@citStr":"Grishman et al (1986)"}},"title":{"#tail":"\n","#text":"Discovery procedures for sublanguage s lectional patterns: Initial experiments.&quot;"},"volume":{"#tail":"\n","#text":"12"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Grishman, R.; Hirschman, L.; and Nhan, N. T. (1986). &quot;Discovery procedures for sublanguage s lectional patterns: Initial experiments.&quot; Computational Linguistics 12(3): 205-215."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","pages":{"#tail":"\n","#text":"205--215"},"marker":{"#tail":"\n","#text":"Grishman, Hirschman, Nhan, 1986"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Grishman"},{"#tail":"\n","#text":"L Hirschman"},{"#tail":"\n","#text":"N T Nhan"}]}},{"date":{"#tail":"\n","#text":"1968"},"issue":{"#tail":"\n","#text":"2"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" rules producing forms such as &quot;Pick up the box up&quot; and &quot;Pick up the box up the box!&quot; This problem can be overcome ither by giving the two structures different LHS names or by grouping &quot;up the box&quot; and &quot;the box up&quot; into distinct parent nodes, adding another layer to the hierarchy on the RHS. Another solution is to use a trace mechanism to link the two positions for the object, thus preventing it from occurring in both places. A final alternative is to include a PARTICLE bit among 10 Some modification of this scheme is necessary when the input stream is not deterministic. For the A* algorithm (Hart et al 1968) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion. Unless some kind of normalization is done, the short theories have an unfair advantage, s imply because fewer probability scores have been multiplied. With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found. 11 The auxiliary verb sets the mode of the main verb to be root or past participle as appropriate. 67 Computational Linguistics Volume 18, Number 1 the features which, once set, cannot be reset. In fact, tlhere were","@endWordPosition":"3764","@position":"23750","annotationId":"T7","@startWordPosition":"3761","@citStr":"Hart et al 1968"}},"title":{"#tail":"\n","#text":"A formal basis for the heuristic determination f minimum cost paths.&quot;"},"volume":{"#tail":"\n","#text":"4"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Hart, P.; Nilsson, N. J.; and Raphael B. (1968). &quot;A formal basis for the heuristic determination f minimum cost paths.&quot; IEEE Transactions ofSystems, Science and Cybernetics SSC-4(2): 100-107."},"journal":{"#tail":"\n","#text":"IEEE Transactions ofSystems, Science and Cybernetics"},"#text":"\n","pages":{"#tail":"\n","#text":"100--107"},"marker":{"#tail":"\n","#text":"Hart, Nilsson, Raphael, 1968"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P Hart"},{"#tail":"\n","#text":"N J Nilsson"},{"#tail":"\n","#text":"B Raphael"}]}},{"volume":{"#tail":"\n","#text":"11"},"#tail":"\n","date":{"#tail":"\n","#text":"1975"},"rawString":{"#tail":"\n","#text":"Hirschman, L.; Grishman, R.; and Sager, N. (1975). &quot;Grammatically-based automatic word class formation.&quot; Information Processing and Management 11: 39-57."},"journal":{"#tail":"\n","#text":"Information Processing and Management"},"#text":"\n","pages":{"#tail":"\n","#text":"39--57"},"marker":{"#tail":"\n","#text":"Hirschman, Grishman, Sager, 1975"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ntic restrictions fail, the node dies. This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic onditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept he proposals. This approach resembles the work by Grishman et al (1986) and Hirschman et al (1975) on selectional restrictions. The semantic onditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area. 3. Evaluat ion Measures This section addresses some performance measures for a grammar, including coverage, portability, perplexity, and trainability. Perplexity, roughly defined as the geometric mean of the number of alternative word hypotheses that may follow each word in the sentence, is of particular concern in spoken language tasks. Portability a","@endWordPosition":"7639","@position":"47779","annotationId":"T8","@startWordPosition":"7636","@citStr":"Hirschman et al (1975)"}},"title":{"#tail":"\n","#text":"Grammatically-based automatic word class formation.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L Hirschman"},{"#tail":"\n","#text":"R Grishman"},{"#tail":"\n","#text":"N Sager"}]}},{"date":{"#tail":"\n","#text":"1976"},"issue":{"#tail":"\n","#text":"4"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ould I be careful?&quot;). 11 Secondly, it greatly simplifies the implementation, because rules do not have to be ex- plicitly monitored uring the parse. Given a particular parent and a particular child, the system can generate the allowable right siblings without having to note who the left siblings (beyond the immediate one) were. Finally, and perhaps most importantly, probabilities are established on arcs connecting sibling pairs regardless of which rule is under construction. In this sense the arc probabilities behave like the familiar word- level bigrams of simple recognition language models (Jelinek 1976), except hat they apply to siblings at multiple levels of the hierarchy. This makes the probabilities mean- ingful as a product of conditional probabilities as the parse advances to deeper levels of the parse tree and also as it returns to higher levels of the parse tree. This approach implies an independence assumption that claims that what can follow depends only on the left sibling and the parent. One negative aspect of the cross-pollination is that the system can potentially generalize to include forms that are agrammatical. For instance, the forms &quot;Pick the box up&quot; and &quot;Pick up the box,&quot; ","@endWordPosition":"3539","@position":"22468","annotationId":"T9","@startWordPosition":"3538","@citStr":"Jelinek 1976"},{"#tail":"\n","#text":"lem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse (Zue et al 1991) To produce these &quot;N-best&quot; alternatives, we make use of a standard A* search algorithm (Hart 1968, Jelinek 1976). Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold. We have thus far developed two versions of the control strategy, a &quot;loosely cou- pled&quot; system and a &quot;tightly coupled&quot; system. Both versions begin with a Viterbi search all the way to the end of the sentence, resulting in not only the first candidate solution but al","@endWordPosition":"9618","@position":"59848","annotationId":"T10","@startWordPosition":"9617","@citStr":"Jelinek 1976"}]},"title":{"#tail":"\n","#text":"Continuous speech recognition by statistical methods.&quot;"},"volume":{"#tail":"\n","#text":"64"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Jelinek, E (1976). &quot;Continuous speech recognition by statistical methods.&quot; IEEE Proceedings 64(4): 532-556."},"journal":{"#tail":"\n","#text":"IEEE Proceedings"},"#text":"\n","pages":{"#tail":"\n","#text":"532--556"},"marker":{"#tail":"\n","#text":"Jelinek, 1976"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E Jelinek"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Katz, S. M. (1987). &quot;Estimation of probabilities from sparse data for the language model component of a speech recognizer.&quot; ASSP-35: 400-401."},"#text":"\n","pages":{"#tail":"\n","#text":"400--401"},"marker":{"#tail":"\n","#text":"Katz, 1987"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e training sentences, and an illustration of both parsing and perplexity computation for a test sentence. Since there are only five training sentences, a number of the arcs of the original grammar are lost after training. This is a problem to be aware of in building grammars from example sentences. In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out. Unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an N-gram back-off model (Katz 1987). The Grammar: (parentheses indicate optional elements) number = hundreds-p lace (tens-place) ones-place number = tens-place number = (tens-place) ones-place hundreds-p lace = digits (hundred) hundreds-p lace = a hundred (and) tens-place = tens tens-place = teens (this overgeneral izes a bit) tens-place = oh (as in &quot;four oh five&quot;) ones-place = digits tens = \\[twenty thirty forty ...\\] (a terminal node with eight individual words) digits = \\[zero one two three four .... \\] teens = \\[ten eleven twelve... \\] oh = \\[oh\\] hundred = \\[hundred\\] and = \\[and\\] The training sentences: (with spoken form","@endWordPosition":"11482","@position":"71189","annotationId":"T11","@startWordPosition":"11481","@citStr":"Katz 1987"}},"title":{"#tail":"\n","#text":"Estimation of probabilities from sparse data for the language model component of a speech recognizer.&quot; ASSP-35:"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"S M Katz"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1986"},"rawString":{"#tail":"\n","#text":"Lamel, L.; Kassel, R. H.; and Seneff, S. (1986). &quot;Speech database development: Design and analysis of the acoustic-phonetic corpus.&quot; In Proceedings, DARPA Speech Recognition Workshop. Palo Alto, CA. 100-109."},"#text":"\n","pages":{"#tail":"\n","#text":"100--109"},"marker":{"#tail":"\n","#text":"Lamel, Kassel, Seneff, 1986"},"location":{"#tail":"\n","#text":"Palo Alto, CA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"native word hypotheses that may follow each word in the sentence, is of particular concern in spoken language tasks. Portability and trainability concern the ease with which an existing rammar can be ported to a new task, as well as the amount of training data necessary before the grammar is able to generalize well to unseen data. 74 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recog- nizer and with a functioning database back-end (Zue et al 1990). The VOYAGER system can answer a number of different ypes of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which ","@endWordPosition":"7806","@position":"48866","annotationId":"T12","@startWordPosition":"7803","@citStr":"Lamel et al 1986"}},"title":{"#tail":"\n","#text":"Speech database development: Design and analysis of the acoustic-phonetic corpus.&quot;"},"booktitle":{"#tail":"\n","#text":"In Proceedings, DARPA Speech Recognition Workshop."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L Lamel"},{"#tail":"\n","#text":"R H Kassel"},{"#tail":"\n","#text":"S Seneff"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Lee, K. E (1989). Automatic Speech Recognition: The Development of the SPHINX System, Appendix I. Boston: Kluwer Academic Publishers."},"#text":"\n","marker":{"#tail":"\n","#text":"Lee, 1989"},"publisher":{"#tail":"\n","#text":"Kluwer Academic Publishers."},"location":{"#tail":"\n","#text":"Boston:"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"any rules that were particular to the original domain. It required less than one person-month o convert he grammar from TIMIT to the RM task. 3.2 Perplexity and Coverage in RM Task A set of 791 sentences within the RM task have been designated as training sentences, and a separate set of 200 sentences as the test set. We built a subset grammar from the 791 parsed training sentences, and then used this grammar to test coverage and perplexity on the unseen test sentences. The grammar could parse 100% of the training sentences and 84% of the test sentences. A formula for the test set perplexity (Lee 1989) is: 13 N _1 ~log2P(wi \\] wi-1,...Wl). N Perplexity = 2 i=1 where the wi are the sequence of all words in all sentences, N is the total number of words, including an &quot;end&quot; word after each sentence, and P(wi I Wi--I~'''Wl) is the probability of the ith word given all preceding wordsJ 4 If all words are assumed equally likely, then P(wi \\] wi-1,.., wl) can be determined by counting all the words that could follow each word in the sentence, along all workable partial theories. If the grammar contains probability estimates, then these can be used in place of the equally 13 The appendix includes an","@endWordPosition":"8191","@position":"51211","annotationId":"T13","@startWordPosition":"8190","@citStr":"Lee 1989"}},"title":{"#tail":"\n","#text":"Automatic Speech Recognition: The Development of the SPHINX System, Appendix I."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"K E Lee"}}},{"volume":{"#tail":"\n","#text":"38"},"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Lee, K. F.; Hon, H. W.; and Reddy, R. (1989). &quot;An overview of the SPHINX speech recognition system.&quot; IEEE Transactions on Acoustics, Speech, and Signal Processing 38(1): 35-46."},"journal":{"#tail":"\n","#text":"IEEE Transactions on Acoustics, Speech, and Signal Processing"},"#text":"\n","pages":{"#tail":"\n","#text":"35--46"},"issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Lee, Hon, Reddy, 1989"},"title":{"#tail":"\n","#text":"An overview of the SPHINX speech recognition system.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K F Lee"},{"#tail":"\n","#text":"H W Hon"},{"#tail":"\n","#text":"R Reddy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Niedermair, G. Th. (1989). &quot;The use of a semantic network in speech dialogue.&quot; 1st European Conference on Speech Communication and Technology, Paris, France. 26-29."},"#text":"\n","marker":{"#tail":"\n","#text":"Niedermair, 1989"},"location":{"#tail":"\n","#text":"Paris,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ing&quot; of the intended message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current ad- vances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural anguage workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Repre- sentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). Spoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139 ~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; interchangeably. (~) 1992 Association for Computa","@endWordPosition":"372","@position":"2512","annotationId":"T14","@startWordPosition":"371","@citStr":"Niedermair (1989)"}},"title":{"#tail":"\n","#text":"The use of a semantic network in speech dialogue.&quot;"},"booktitle":{"#tail":"\n","#text":"1st European Conference on Speech Communication and Technology,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"G Th Niedermair"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"Niemann, H. (1990). &quot;The interaction of word recognition and linguistic processing in speech understanding.&quot; Invited Lecture, NATO-ASI Workshop on Speech Recognition and Understanding, Cetraro, Italy."},"#text":"\n","marker":{"#tail":"\n","#text":"Niemann, 1990"},"location":{"#tail":"\n","#text":"Cetraro, Italy."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"d message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current ad- vances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural anguage workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Repre- sentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). Spoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139 ~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; interchangeably. (~) 1992 Association for Computational Linguisti","@endWordPosition":"374","@position":"2528","annotationId":"T15","@startWordPosition":"373","@citStr":"Niemann (1990)"}},"title":{"#tail":"\n","#text":"The interaction of word recognition and linguistic processing in speech understanding.&quot;"},"booktitle":{"#tail":"\n","#text":"Invited Lecture, NATO-ASI Workshop on Speech Recognition and Understanding,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"H Niemann"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Pallett, D. (1989). &quot;Benchmark tests for DARPA resource management database performance evaluations.&quot; In Proceedings, ICASSP-89. 536-539."},"#text":"\n","pages":{"#tail":"\n","#text":"536--539"},"marker":{"#tail":"\n","#text":"Pallett, 1989"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" concern in spoken language tasks. Portability and trainability concern the ease with which an existing rammar can be ported to a new task, as well as the amount of training data necessary before the grammar is able to generalize well to unseen data. 74 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recog- nizer and with a functioning database back-end (Zue et al 1990). The VOYAGER system can answer a number of different ypes of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community. 3","@endWordPosition":"7817","@position":"48944","annotationId":"T16","@startWordPosition":"7816","@citStr":"Pallett 1989"}},"title":{"#tail":"\n","#text":"Benchmark tests for DARPA resource management database performance evaluations.&quot;"},"booktitle":{"#tail":"\n","#text":"In Proceedings, ICASSP-89."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Pallett"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Seneff, S.; Glass, J.; Goddeau, D.; Goodine, D.; Hirschman, L.; Leung, H.; Phillips, M.; Polifroni, J.; and Zue, V. (1991). &quot;Development and preliminary evaluation of the MIT ATIS system.&quot; Computational Linguistics Volume 18, Number 1 Fourth DARPA Speech and Natural Language Workshop, Asilomar, CA. 88-93."},"#text":"\n","pages":{"#tail":"\n","#text":"88--93"},"marker":{"#tail":"\n","#text":"Seneff, Glass, Goddeau, Goodine, Hirschman, Leung, Phillips, Polifroni, Zue, 1991"},"location":{"#tail":"\n","#text":"Asilomar, CA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"iformly in strictly hierarchical structures in the parse tree. Thus the parse tree contains nodes such as \\[subject\\] and \\[dir-object\\] that represent structural roles, as well as nodes such as \\[on-street\\] and \\[a-school\\] representing specific semantic categories. There are no separate semantic rules off to the side; rather, the semantic information is encoded irectly as names attached to nodes in the tree. Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However, the method we are currently using in the ATIS domain (Seneff et al 1991) represents our most promising approach to this problem. We have decided to limit semantic frame types to a small set of choices uch as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree. Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to spec- ifications based on its broad-class identity (as on","@endWordPosition":"10226","@position":"63436","annotationId":"T17","@startWordPosition":"10223","@citStr":"Seneff et al 1991"},{"#tail":"\n","#text":"The most interesting result was obtained within the VOYAGER domain (see Sections 3.3 and 3.4). The perplexity (average number of words that can follow a given word) decreased from 70 to 28 to 8 when the grammar changed from word- pair (derived from the same grammar) to parser without probabilities to parser with probabilities. We_currently have two application domains that can carry on a spoken dialog with a user. One, the VOYAGER domain (Zue et al 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS (Seneff et al 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues. As of this writing, we have a fully integrated version of the VOYAGER system, using an A* search algorithm (Goodine t al. 1991). The parser produces a set of next-word candi- dates dynamically for each partial theory. We have not yet incorporated probabilities from TINA into the search, but they ","@endWordPosition":"11139","@position":"69077","annotationId":"T18","@startWordPosition":"11136","@citStr":"Seneff et al 1991"}]},"title":{"#tail":"\n","#text":"Development and preliminary evaluation of the MIT ATIS system.&quot;"},"booktitle":{"#tail":"\n","#text":"Computational Linguistics Volume 18, Number 1 Fourth DARPA Speech and Natural Language Workshop,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Seneff"},{"#tail":"\n","#text":"J Glass"},{"#tail":"\n","#text":"D Goddeau"},{"#tail":"\n","#text":"D Goodine"},{"#tail":"\n","#text":"L Hirschman"},{"#tail":"\n","#text":"H Leung"},{"#tail":"\n","#text":"M Phillips"},{"#tail":"\n","#text":"J Polifroni"},{"#tail":"\n","#text":"V Zue"}]}},{"volume":{"#tail":"\n","#text":"13"},"#tail":"\n","date":{"#tail":"\n","#text":"1967"},"rawString":{"#tail":"\n","#text":"Viterbi, A. (1967). &quot;Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.&quot; IEEE Transactions on Information Theory IT-13. 260-269."},"journal":{"#tail":"\n","#text":"IEEE Transactions on Information Theory"},"#text":"\n","pages":{"#tail":"\n","#text":"260--269"},"marker":{"#tail":"\n","#text":"Viterbi, 1967"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system (Zue et al 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing. The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse (Zue et al 1991) To produce these &quot;N-best&quot; alte","@endWordPosition":"9511","@position":"59167","annotationId":"T19","@startWordPosition":"9510","@citStr":"Viterbi 1967"}},"title":{"#tail":"\n","#text":"Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Viterbi"}}},{"volume":{"#tail":"\n","#text":"13"},"#tail":"\n","date":{"#tail":"\n","#text":"1970"},"rawString":{"#tail":"\n","#text":"Woods, W. A. (1970). &quot;Transition etwork grammars for natural language analysis.&quot; Commun. of the ACM 13: 591-606."},"journal":{"#tail":"\n","#text":"Commun. of the ACM"},"#text":"\n","pages":{"#tail":"\n","#text":"591--606"},"marker":{"#tail":"\n","#text":"Woods, 1970"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"lgorithm, it is impossible to predict he computational load for a spoken-input mode. 2.5 Constraints and Gaps This section describes how TINA handles everal issues that are often considered to be part of the task of a parser. These include agreement constraints, emantic restrictions, 68 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in &quot;(which article)/do you think I should read (ti)?&quot;) (Chomsky 1977). The gap mechanism resembles the Hold register idea of ATNs (Woods 1970) and the treatment of bounded omination metavariables in lexical functional grammars (LFGs) (Bresnan 1982, p. 235 ft.), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent odes. Our approach to the design of a constraint mechanism is to establish a framework general enough to handle syntactic, semantic, and, ultimately, phonological constraints using identical functional procedures applied at the node level. The intent was to de- sign a grammar for which the rules would be kept completely free of ","@endWordPosition":"4524","@position":"28410","annotationId":"T20","@startWordPosition":"4523","@citStr":"Woods 1970"}},"title":{"#tail":"\n","#text":"Transition etwork grammars for natural language analysis.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W A Woods"}}},{"date":{"#tail":"\n","#text":"1986"},"note":{"#tail":"\n","#text":"edited by"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" is reached, the word &quot;Jane&quot; is in the CURRENT-FOCUS slot, and the phrase &quot;which hospital&quot; is still in the FLOAT-OBJECT slot. The \\[participial-phrase\\] for &quot;taken \\[object\\]&quot; activates &quot;Jane,&quot; but only for its children. This word is ultimately absorbed by the \\[object\\] node within the verb phrase. Meanwhile, the \\[participial-phrase\\] passes along the original FLOAT- OBJECT (&quot;which hospital&quot;) to its right sibling, the adverbial prepositional phrase, &quot;to \\[object\\].&quot; The phrase &quot;which hospital&quot; is finally absorbed by the preposition's object. The example used to illustrate the power of ATNs (Woods 1986), &quot;John was be- lieved to have been shot,&quot; also parses correctly, because the \\[object\\] node following the verb &quot;believed&quot; acts as both an absorber and a (re)generator. Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexist- ing FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)? 71 Computational Linguistics Volume 1","@endWordPosition":"6351","@position":"39833","annotationId":"T21","@startWordPosition":"6350","@citStr":"Woods 1986"}},"title":{"#tail":"\n","#text":"Semantics and quantification i natural language question answering.&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Woods, W. A. (1986). &quot;Semantics and quantification i  natural language question answering.&quot; In Readings in Natural Language Processing, edited by B. J. Grosz, K. S. Jones; and B. L. Webber. Los Altos, CA: Morgan Kaufmann. 205-248."},"#text":"\n","pages":{"#tail":"\n","#text":"205--248"},"marker":{"#tail":"\n","#text":"Woods, 1986"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann."},"location":{"#tail":"\n","#text":"Los Altos, CA:"},"booktitle":{"#tail":"\n","#text":"In Readings in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W A Woods"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Young, S. R. (1989). &quot;The minds system: Using context and dialog to enhance speech recognition.&quot; Proceedings, DARPA Speech and Natural Language Workshop. 131-136."},"#text":"\n","pages":{"#tail":"\n","#text":"131--136"},"marker":{"#tail":"\n","#text":"Young, 1989"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current ad- vances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural anguage workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Repre- sentative systems are described in Boisen et al (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). Spoken Language Systems Group, Laboratory for Computer Science, MIT, Cambridge MA 02139 ~This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot; interchangeably. (~) 1992 Association for Computational Linguistics Computational L","@endWordPosition":"377","@position":"2546","annotationId":"T22","@startWordPosition":"376","@citStr":"Young (1989)"}},"title":{"#tail":"\n","#text":"The minds system: Using context and dialog to enhance speech recognition.&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings, DARPA Speech and Natural Language Workshop."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"S R Young"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Zue, V.; Daly, N.; Glass, J.; Goodine, D.; Leung, H.; Phillips, M.; Polifroni, J.; Seneff, S.; and Sodof, M. (1989a). &quot;The collection and preliminary analysis of a spontaneous speech database.&quot; DARPA Speech and Natural Language Workshop. Harwichport, MA. 15-18."},"#text":"\n","pages":{"#tail":"\n","#text":"15--18"},"marker":{"#tail":"\n","#text":"Zue, Daly, Glass, Goodine, Leung, Phillips, Polifroni, Seneff, Sodof, 1989"},"location":{"#tail":"\n","#text":"Harwichport, MA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ed in a simulation mode in which the speech recognition component was 76 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Table 3 Perplexity and coverage data for test and training samples within the VOYAGER domain. Data set: Test Test Training System: initial expanded expanded No Prob: 20.6 27.1 25.8 Prob: 7.1 8.3 8.1 Coverage: 69% 76% 78% excluded. Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. Subsequent processing by the natural anguage and response generation components was done automatically by the computer (Zue et al 1989). We were able to'collect a total, of nearly 5000 utterances in this fashion. The speech material was then used to train the recognizer component, and the text material was used to train the natural language and back-end components. We designated a subset of 3312 sentences as the training set, and augmented the original rules so as to cover a number of sentences that appeared to stay within the domain of the back-end. We did not try to expand the rules to cover sentences that the back-end could not deal with, because we wanted to keep the natural anguage component tightly restricted to sentenc","@endWordPosition":"8858","@position":"55174","annotationId":"T23","@startWordPosition":"8855","@citStr":"Zue et al 1989"},{"#tail":"\n","#text":" speech or through text input. In both of these systems, TINA provides the interface between the recognizer and the application back-end. In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area. In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system (Zue et al 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing. The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar c","@endWordPosition":"9468","@position":"58879","annotationId":"T24","@startWordPosition":"9465","@citStr":"Zue et al 1989"}]},"title":{"#tail":"\n","#text":"The collection and preliminary analysis of a spontaneous speech database.&quot; DARPA Speech and Natural Language Workshop."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"V Zue"},{"#tail":"\n","#text":"N Daly"},{"#tail":"\n","#text":"J Glass"},{"#tail":"\n","#text":"D Goodine"},{"#tail":"\n","#text":"H Leung"},{"#tail":"\n","#text":"M Phillips"},{"#tail":"\n","#text":"J Polifroni"},{"#tail":"\n","#text":"S Seneff"},{"#tail":"\n","#text":"M Sodof"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Zue, V.; Glass, J.; Phillips, M.; and Seneff, S. (1989b). &quot;The MIT SUMMIT speech recognition system, a progress report.&quot; Proceedings, DARPA Speech and Natural Language Workshop. Philadelphia. 21-23."},"#text":"\n","pages":{"#tail":"\n","#text":"21--23"},"marker":{"#tail":"\n","#text":"Zue, Glass, Phillips, Seneff, 1989"},"location":{"#tail":"\n","#text":"Philadelphia."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ed in a simulation mode in which the speech recognition component was 76 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Table 3 Perplexity and coverage data for test and training samples within the VOYAGER domain. Data set: Test Test Training System: initial expanded expanded No Prob: 20.6 27.1 25.8 Prob: 7.1 8.3 8.1 Coverage: 69% 76% 78% excluded. Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. Subsequent processing by the natural anguage and response generation components was done automatically by the computer (Zue et al 1989). We were able to'collect a total, of nearly 5000 utterances in this fashion. The speech material was then used to train the recognizer component, and the text material was used to train the natural language and back-end components. We designated a subset of 3312 sentences as the training set, and augmented the original rules so as to cover a number of sentences that appeared to stay within the domain of the back-end. We did not try to expand the rules to cover sentences that the back-end could not deal with, because we wanted to keep the natural anguage component tightly restricted to sentenc","@endWordPosition":"8858","@position":"55174","annotationId":"T25","@startWordPosition":"8855","@citStr":"Zue et al 1989"},{"#tail":"\n","#text":" speech or through text input. In both of these systems, TINA provides the interface between the recognizer and the application back-end. In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area. In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system (Zue et al 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing. The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar c","@endWordPosition":"9468","@position":"58879","annotationId":"T26","@startWordPosition":"9465","@citStr":"Zue et al 1989"}]},"title":{"#tail":"\n","#text":"The MIT SUMMIT speech recognition system, a progress report.&quot; Proceedings, DARPA Speech and Natural Language Workshop."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"V Zue"},{"#tail":"\n","#text":"J Glass"},{"#tail":"\n","#text":"M Phillips"},{"#tail":"\n","#text":"S Seneff"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"Zue, V.; Glass, J.; Goodine, D.; Leung, H.; Phillips;, M.; Polifroni, J.; and Seneff, S. (1990). &quot;The VOYAGER speech understanding system: Preliminary development and evaluation.&quot; IEEE International Conference on Acoustics, Speech and Signal Processing. Albuquerque, NM. 73-76."},"#text":"\n","pages":{"#tail":"\n","#text":"73--76"},"marker":{"#tail":"\n","#text":"Zue, Glass, Goodine, Leung, Phillips, 1990"},"location":{"#tail":"\n","#text":"Albuquerque, NM."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"e grammar is able to generalize well to unseen data. 74 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recog- nizer and with a functioning database back-end (Zue et al 1990). The VOYAGER system can answer a number of different ypes of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community. 3.1 Portability We tested ease of portability for TINA by beginning with a grammar built from the 450 TIMIT sentences and then deriving a grammar for the RM task. These two tasks represent very differe","@endWordPosition":"7851","@position":"49144","annotationId":"T27","@startWordPosition":"7848","@citStr":"Zue et al 1990"},{"#tail":"\n","#text":"er of different application domains, and gave some perfor- mance statistics in terms of perplexity/coverage/overgeneralization within some of these domains. The most interesting result was obtained within the VOYAGER domain (see Sections 3.3 and 3.4). The perplexity (average number of words that can follow a given word) decreased from 70 to 28 to 8 when the grammar changed from word- pair (derived from the same grammar) to parser without probabilities to parser with probabilities. We_currently have two application domains that can carry on a spoken dialog with a user. One, the VOYAGER domain (Zue et al 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS (Seneff et al 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues. As of this writing, we have a fully integrated version of the VOYAGER system, using an A* search algorithm (Goodine t al. 1991). The pars","@endWordPosition":"11111","@position":"68917","annotationId":"T28","@startWordPosition":"11108","@citStr":"Zue et al 1990"}]},"title":{"#tail":"\n","#text":"The VOYAGER speech understanding system: Preliminary development and evaluation.&quot;"},"booktitle":{"#tail":"\n","#text":"IEEE International Conference on Acoustics, Speech and Signal Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"V Zue"},{"#tail":"\n","#text":"J Glass"},{"#tail":"\n","#text":"D Goodine"},{"#tail":"\n","#text":"H Leung"},{"#tail":"\n","#text":"Phillips"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Zue, V.; Glass, J.; Goodine, D.; Leung, H.; Phillips, M.; Polifroni, J.; and Seneff, S. (1991). &quot;Integration of speech recognition and natural language processing in the MIT VOYAGER system.&quot; IEEE International Conference on Acoustics, Speech and Signal Processing. Toronto, Ontario. 14-17."},"#text":"\n","pages":{"#tail":"\n","#text":"14--17"},"marker":{"#tail":"\n","#text":"Zue, Glass, Goodine, Leung, Phillips, Polifroni, Seneff, 1991"},"location":{"#tail":"\n","#text":"Toronto, Ontario."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence. When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse (Zue et al 1991) To produce these &quot;N-best&quot; alternatives, we make use of a standard A* search algorithm (Hart 1968, Jelinek 1976). Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold. We have thus far developed two versions of the control strategy, a &quot;loosely cou- pled&quot; system and a &quot;tightly coupled&quot; system. Both versions begin with a ","@endWordPosition":"9600","@position":"59736","annotationId":"T29","@startWordPosition":"9597","@citStr":"Zue et al 1991"},{"#tail":"\n","#text":"In our case, we can use the Viterbi path to the end as the estimate of the future score. This path is guaranteed to be the best way to get to the end; however, it may not parse. Hence it is a tight upper bound on the true score for the rest of the sentence. The recognizer can continue to propose hypotheses until one 78 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications successfully parses, or until a quitting criterion is reached, such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing. Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA'S probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguis- tic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al 1991). Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but i","@endWordPosition":"9884","@position":"61350","annotationId":"T30","@startWordPosition":"9881","@citStr":"Zue et al 1991"}]},"title":{"#tail":"\n","#text":"Integration of speech recognition and natural language processing in the MIT VOYAGER system.&quot;"},"booktitle":{"#tail":"\n","#text":"IEEE International Conference on Acoustics, Speech and Signal Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"V Zue"},{"#tail":"\n","#text":"J Glass"},{"#tail":"\n","#text":"D Goodine"},{"#tail":"\n","#text":"H Leung"},{"#tail":"\n","#text":"M Phillips"},{"#tail":"\n","#text":"J Polifroni"},{"#tail":"\n","#text":"S Seneff"}]}}]}}]}}
