{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","address":{"#tail":"\n","@confidence":"0.87731","#text":"\n2 Buccleuch Place, Edinburgh. EH8 9LW\n"},"author":{"#tail":"\n","@confidence":"0.996278","#text":"\nJames R. Curran\n"},"title":{"#tail":"\n","@confidence":"0.754261","#text":"\nBlueprint for a High Performance NLP Infrastructure\n"},"@confidence":"0.005265","reference":{"#tail":"\n","@confidence":"0.972909046153846","#text":"\nDavid Abrahams. 2003. Boost.Python C++ library.\nhttp://www.boost.ory (viewed 23/3/2003).\nAndrei Alexandrescu. 2001. Modern C++ Design: Generic\nProgramming and Design Patterns Applied. C++ In-Depth\nSeries. Addison-Wesley, New York.\nMichele Banko and Eric Brill. 2001. Scaling to very very large\ncorpora for natural language disambiguation. In Proceedings\nof the 39th annual meeting of the Association for Computa-\ntional Linguistics, pages 26?33, Toulouse, France, 9?11 July.\nSamuel Bayer, Christine Doran, and Bryan George. 2001. Dia-\nlogue interaction with the DARPA Communicator Infrastruc-\nture: The development of useful software. In J. Allan, editor,\nProceedings of HLT 2001, First International Conference on\nHuman Language Technology Research, pages 114?116, San\nDiego, CA, USA. Morgan Kaufmann.\nAvrim Blum and Tom Mitchell. 1998. Combining labeled\nand unlabeled data with co-training. In Proceedings of the\n11th Annual Conference on Computational Learning Theory,\npages 92?100, Madisson, WI, USA.\nThorsten Brants. 2000. TnT - a statistical part-of-speech tagger.\nIn Proceedings of the 6th Conference on Applied Natural\nLanguage Processing, pages 224?231, Seattle, WA, USA, 29\nApril ? 4 May.\nEric Brill. 1993. A Corpus-Based Appreach to Language\nLearning. Ph.D. thesis, Department of Computer and Infor-\nmation Science, University of Pennsylvania.\nStanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior\nfor smoothing maximum entropy models. Technical report,\nCarnegie Mellon University.\nStephen Clark, James R. Curran, and Miles Osborne. 2003.\nBootstrapping POS-taggers using unlabelled data. In Pro-\nceedings of the 7th Conference on Natural Language Learn-\ning, Edmonton, Canada, 31 May ? 1 June. (to appear).\nHamish Cunningham, Yorick Wilks, and Robert J. Gaizauskas.\n1997. GATE ? a general architecture for text engineering. In\nProceedings of the 16th International Conference on Com-\nputational Linguistics, pages 1057?1060, Copenhagen, Den-\nmark, 5?9 August.\nHamish Cunningham, Diana Maynard, C. Ursu K. Bontcheva,\nV. Tablan, and M. Dimitrov. 2002. Developing language\nprocessing components with GATE. Technical report, Uni-\nversity of Sheffield, Sheffield, UK.\nHamish Cunningham. 2000. Software Architecture for Lan-\nguage Engineering. Ph.D. thesis, University of Sheffield.\nJames R. Curran and Stephen Clark. 2003. Investigating GIS\nand smoothing for maximum entropy taggers. In Proceed-\nings of the 11th Meeting of the European Chapter of the As-\nsociation for Computational Lingustics, pages 91?98, Bu-\ndapest, Hungary, 12?17 April.\nJames R. Curran and Marc Moens. 2002. Scaling context\nspace. In Proceedings of the 40th Annual Meeting of the As-\nsociation for Computational Linguistics, Philadelphia, PA,\nUSA, 7?12 July.\nKrzysztof Czarnecki and Ulrich W. Eisenecker. 2000. Gen-\nerative Programming: Methods, Tools, and Applications.\nAddison-Wesley.\nWalter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-\ntal van den Bosch. 2002. TiMBL: Tilburg Memory-Based\nLearner reference guide. Technical Report ILK 02-10, In-\nduction of Linguistic Knowledge. Tilburg University.\nIdo Dagan and Sean P. Engelson. 1995. Committee-based sam-\npling for training probabilistic classifiers. In Proceedings of\nthe International Conference on Machine Learning, pages\n150?157, Tahoe City, CA, USA, 9?12 July.\nDavid Day, John Aberdeen, Lynette Hirschman, Robyn\nKozierok, Patricia Robinson, and Marc Vilain. 1997.\nMixed-initiative development of language processing sys-\ntems. In Proceedings of the Fifth Conference on Applied\nNatural Language Processing, pages 384?355, Washington,\nDC, USA, 31 March ? 3 April.\nCristiane Fellbaum, editor. 1998. Wordnet: an electronic lexi-\ncal database. The MIT Press, Cambridge, MA USA.\nClaire Grover, Colin Matheson, Andrei Mikheev, and Marc\nMoens. 2000. LT TTT - a flexible tokenisation tool. In Pro-\nceedings of Second International Language Resources and\nEvaluation Conference, pages 1147?1154, Athens, Greece,\n31 May ? 2 June.\nKadri Hacioglu and Bryan Pellom. 2003. A distributed archi-\ntecture for robust automatic speech recognition. In Proceed-\nings of Conference on Acoustics, Speech, and Signal Pro-\ncessing (ICASSP), Hong Kong, China, 6?10 April.\nNancy Ide, Randi Reppen, and Keith Suderman. 2002. The\namerican national corpus: More than the web can provide.\nIn Proceedings of the Third Language Resources and Eval-\nuation Conference, pages 839?844, Las Palmas, Canary Is-\nlands, Spain.\nLauri Karttunen, Tama?s Gaa?l, and Andre? Kempe. 1997. Xerox\nFinite-State Tool. Technical report, Xerox Research Centre\nEurope Grenoble, Meylan, France.\nLinguistic Data Consortium. 2003. English Gigaword Corpus.\ncatalogue number LDC2003T05.\nEdward Loper and Steven Bird. 2002. NLTK: The Natural Lan-\nguage Toolkit. In Proceedings of the Workshop on Effective\nTools and Methodologies for Teaching NLP and Computa-\ntional Linguistics, pages 63?70, Philadelphia, PA, 7 July.\nRobert Malouf. 2002. A comparison of algorithms for max-\nimum entropy parameter estimation. In Proceedings of the\n6th Conference on Natural Language Learning, pages 49?\n55, Taipei, Taiwan, 31 August ? 1 September.\nMitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz.\n1993. Building a large annotated corpus of English: The\nPenn Treebank. Computational Linguistics, 19(2):313?330.\nAndrei Mikheev, Claire Grover, and Marc Moens. 1999. Xml\ntools and architecture for named entity recognition. Journal\nof Markup Languages: Theory and Practice 1, 3:89?113.\nMehryar Mohri, Fernando C. N. Pereira, and Michael Riley.\n1998. A rational design for a weighted finite-state transducer\nlibrary. Lecture Notes in Computer Science, 1436.\nGrace Ngai and Radu Florian. 2001. Transformation-based\nlearning in the fast lane. In Proceedings of the Second\nMeeting of the North American Chapter of the Association\nfor Computational Linguistics, pages 40?47, Pittsburgh, PA,\nUSA, 2?7 June.\nAdwait Ratnaparkhi. 1996. A maximum entropy part-of-\nspeech tagger. In Proceedings of the EMNLP Conference,\npages 133?142, Philadelphia, PA, USA.\nAdwait Ratnaparkhi. 1998. Maximum Entropy Models for Nat-\nural Language Ambiguity Resolution. Ph.D. thesis, Univer-\nsity of Pennsylvania.\nEmmanuel Roche and Yves Schabes. 1997. Deterministic\npart-of-speech tagging with finite-state transducers. In Em-\nmanuel Roche and Yves Schabes, editors, Finite-State Lan-\nguage Processing, chapter 7. The MIT Press.\nHans van Halteren, Jakub Zavrel, and Walter Daelemans. 2001.\nImproving accuracy in wordclass tagging through combina-\ntion of machine learning systems. Computational Linguis-\ntics, 27(2):199?229.\nIan H. Witten and Eibe Frank. 1999. Data Mining: Practical\nMachine Learning Tools and Techniques with Java Imple-\nmentations. Morgan Kaufmann Publishers.\n"},"#tail":"\n","bodyText":[{"#tail":"\n","@confidence":"0.978634916666667","#text":"\nNatural Language Processing (NLP) system de-\nvelopers face a number of new challenges. In-\nterest is increasing for real-world systems that\nuse NLP tools and techniques. The quantity of\ntext now available for training and processing\nis increasing dramatically. Also, the range of\nlanguages and tasks being researched contin-\nues to grow rapidly. Thus it is an ideal time\nto consider the development of new experimen-\ntal frameworks. We describe the requirements,\ninitial design and exploratory implementation\nof a high performance NLP infrastructure.\n"},{"#tail":"\n","@confidence":"0.985952936507936","#text":"\nPractical interest in NLP has grown dramatically in re-\ncent years. Accuracy on fundamental tasks, such as part\nof speech (POS) tagging, named entity recognition, and\nbroad-coverage parsing continues to increase. We can\nnow construct systems that address complex real-world\nproblems such as information extraction and question an-\nswering. At the same time, progress in speech recogni-\ntion and text-to-speech technology has made complete\nspoken dialogue systems feasible. Developing these\ncomplex NLP systems involves composing many different\nNLP tools. Unfortunately, this is difficult because many\nimplementations have not been designed as components\nand only recently has input/output standardisation been\nconsidered. Finally, these tools can be difficult to cus-\ntomise and tune for a particular task.\nNLP is experiencing an explosion in the quantity of\nelectronic text available. Some of this new data will be\nmanually annotated. For example, 10 million words of\nthe American National Corpus (Ide et al, 2002) will have\nmanually corrected POS tags, a tenfold increase over the\nPenn Treebank (Marcus et al, 1993), currently used for\ntraining POS taggers. This will require more efficient\nlearning algorithms and implementations.\nHowever, the greatest increase is in the amount of raw\ntext available to be processed, e.g. the English Giga-\nword Corpus (Linguistic Data Consortium, 2003). Re-\ncent work (Banko and Brill, 2001; Curran and Moens,\n2002) has suggested that some tasks will benefit from\nusing significantly more data. Also, many potential ap-\nplications of NLP will involve processing very large text\ndatabases. For instance, biomedical text-mining involves\nextracting information from the vast body of biological\nand medical literature; and search engines may eventu-\nally apply NLP techniques to the whole web. Other po-\ntential applications must process text online or in real-\ntime. For example, Google currently answers 250 million\nqueries per day, thus processing time must be minimised.\nClearly, efficient NLP components will need to be devel-\noped. At the same time, state-of-the-art performance will\nbe needed for these systems to be of practical use.\nFinally, NLP is growing in terms of the number of\ntasks, methods and languages being researched. Al-\nthough many problems share algorithms and data struc-\ntures there is a tendency to reinvent the wheel.\nSoftware engineering research on Generative Program-\nming (Czarnecki and Eisenecker, 2000) attempts to solve\nthese problems by focusing on the development of con-\nfigurable elementary components and knowledge to com-\nbine these components into complete systems. Our in-\nfrastructure for NLP will provide high performance1 com-\nponents inspired by Generative Programming principles.\nThis paper reviews existing NLP systems and discusses\nthe requirements for an NLP infrastructure. We then de-\nscribe our overall design and exploratory implementa-\ntion. We conclude with a discussion of programming in-\nterfaces for the infrastructure including a script language\nand GUI interfaces, and web services for distributed NLP\nsystem development. We seek feedback on the overall\ndesign and implementation of our proposed infrastruc-\nture and to promote discussion about software engineer-\ning best practice in NLP.\n1We use high performance to refer to both state of the art\nperformance and high runtime efficiency.\n"},{"#tail":"\n","@confidence":"0.997576557692307","#text":"\nThere are a number of generalised NLP systems in the lit-\nerature. Many provide graphical user interfaces (GUI) for\nmanual annotation (e.g. General Architecture for Text\nEngineering (GATE) (Cunningham et al, 1997) and the\nAlembic Workbench (Day et al, 1997)) as well as NLP\ntools and resources that can be manipulated from the\nGUI. For instance, GATE currently provides a POS tag-\nger, named entity recogniser and gazetteer and ontology\neditors (Cunningham et al, 2002). GATE goes beyond\nearlier systems by using a component-based infrastruc-\nture (Cunningham, 2000) which the GUI is built on top\nof. This allows components to be highly configurable and\nsimplifies the addition of new components to the system.\nA number of stand-alone tools have also been devel-\noped. For example, the suite of LT tools (Mikheev et al,\n1999; Grover et al, 2000) perform tokenization, tagging\nand chunking on XML marked-up text directly. These\ntools also store their configuration state, e.g. the trans-\nduction rules used in LT CHUNK, in XML configuration\nfiles. This gives a greater flexibility but the tradeoff is\nthat these tools can run very slowly. Other tools have\nbeen designed around particular techniques, such as fi-\nnite state machines (Karttunen et al, 1997; Mohri et al,\n1998). However, the source code for these tools is not\nfreely available, so they cannot be extended.\nEfficiency has not been a focus for NLP research in\ngeneral. However, it will be increasingly important as\ntechniques become more complex and corpus sizes grow.\nAn example of this is the estimation of maximum en-\ntropy models, from simple iterative estimation algorithms\nused by Ratnaparkhi (1998) that converge very slowly,\nto complex techniques from the optimisation literature\nthat converge much more rapidly (Malouf, 2002). Other\nattempts to address efficiency include the fast Transfor-\nmation Based Learning (TBL) Toolkit (Ngai and Florian,\n2001) which dramatically speeds up training TBL sys-\ntems, and the translation of TBL rules into finite state ma-\nchines for very fast tagging (Roche and Schabes, 1997).\nThe TNT POS tagger (Brants, 2000) has also been de-\nsigned to train and run very quickly, tagging between\n30,000 and 60,000 words per second.\nThe Weka package (Witten and Frank, 1999) provides\na common framework for several existing machine learn-\ning methods including decision trees and support vector\nmachines. This library has been very popular because it\nallows researchers to experiment with different methods\nwithout having to modify code or reformat data.\nFinally, the Natural Language Toolkit (NLTK) is a\npackage of NLP components implemented in Python\n(Loper and Bird, 2002). Python scripting is extremely\nsimple to learn, read and write, and so using the existing\ncomponents and designing new components is simple.\n"},{"#tail":"\n","@confidence":"0.999727026315789","#text":"\nAs discussed earlier, there are two main requirements\nof the system that are covered by ?high performance?:\nspeed and state of the art accuracy. Efficiency is required\nboth in training and processing. Efficient training is re-\nquired because the amount of data available for train-\ning will increase significantly. Also, advanced methods\noften require many training iterations, for example ac-\ntive learning (Dagan and Engelson, 1995) and co-training\n(Blum and Mitchell, 1998). Processing text needs to be\nextremely efficient since many new applications will re-\nquire very large quantities of text to be processed or many\nsmaller quantities of text to be processed very quickly.\nState of the art accuracy is also important, particularly\non complex systems since the error is accumulated from\neach component in the system. There is a speed/accuracy\ntradeoff that is rarely addressed in the literature. For in-\nstance, reducing the beam search width used for tagging\ncan increase the speed without significantly reducing ac-\ncuracy. Finally, the most accurate systems are often very\ncomputationally intensive so a tradeoff may need to be\nmade here. For example, the state of the art POS tag-\nger is an ensemble of individual taggers (van Halteren\net al, 2001), each of which must process the text sepa-\nrately. Sophisticated modelling may also give improved\naccuracy at the cost of training and processing time.\nThe space efficiency of the components is important\nsince complex NLP systems will require many different\nNLP components to be executing at the same time. Also,\nlanguage processors many eventually be implemented for\nrelatively low-specification devices such as PDAs. This\nmeans that special attention will need to be paid to the\ndata-structures used in the component implementation.\nThe infrastructure should allow most data to be stored\non disk (as a configuration option since we must tradeoff\nspeed for space). Accuracy, speed and compactness are\nthe main execution goals. These goals are achieved by\nimplementing the infrastructure in C/C++, and profiling\nand optimising the algorithms and data-structures used.\n"},{"#tail":"\n","@confidence":"0.983671807692307","#text":"\nThe remaining requirements relate to the overall and\ncomponent level design of the system. Following the\nGenerative Programming paradigm, the individual com-\nponents of the system must be elementary and highly\nconfigurable. This ensures minimal redundancy between\ncomponents and makes them easier to understand, im-\nplement, test and debug. It also ensures components are\nmaximally composable and extensible. This is particu-\nlarly important in NLP because of the high redundancy\nacross tasks and approaches.\nMachine learning methods should be interchangeable:\nTransformation-based learning (TBL) (Brill, 1993) and\nMemory-based learning (MBL) (Daelemans et al, 2002)\nhave been applied to many different problems, so a sin-\ngle interchangeable component should be used to repre-\nsent each method. We will base these components on the\ndesign of Weka (Witten and Frank, 1999).\nRepresentations should be reusable: for example,\nnamed entity classification can be considered as a se-\nquence tagging task or a bag-of-words text classification\ntask. The same beam-search sequence tagging compo-\nnent should be able to be used for POS tagging, chunk-\ning and named entity classification. Feature extraction\ncomponents should be reusable since many NLP compo-\nnents share features, for instance, most sequence taggers\nuse the previously assigned tags. We will use an object-\noriented hierarchy of methods, representations and fea-\ntures to allow components to be easily interchanged. This\nhierarchy will be developed by analysing the range of\nmethods, representations and features in the literature.\nHigh levels of configurability are also very impor-\ntant. Firstly, without high levels of configurability, new\nsystems are not easy to construct by composing exist-\ning components, so reinventing the wheel becomes in-\nevitable. Secondly, different languages and tasks show a\nvery wide variation in the methods, representations, and\nfeatures that are most successful. For instance, a truly\nmultilingual tagger should be able to tag a sequence from\nleft to right or right to left. Finally, this flexibility will\nallow for research into new tasks and languages to be un-\ndertaken with minimal coding.\nEase of use is a very important criteria for an infras-\ntructure and high quality documentation and examples\nare necessary to make sense of the vast array of compo-\nnents in the system. Preconfigured standard components\n(e.g. an English POS tagger) will be supplied with the\ninfrastructure. More importantly, a Python scripting lan-\nguage interface and a graphical user interface will be built\non top of the infrastructure. This will allow components\nto be configured and composed without expertise in C++.\nThe user interface will generate code to produce stand-\nalone components in C++ or Python. Since the Python\ncomponents will not need to be compiled, they can be\ndistributed immediately.\nOne common difficulty with working on text is the\nrange of file formats and encodings that text can be\nstored in. The infrastructure will provide components to\nread/write files in many of these formats including HTML\nfiles, text files of varying standard formats, email folders,\nPostscript, Portable Document Format, Rich Text Format\nand Microsoft Word files. The infrastructure will also\nread XML and SGML marked-up files, with and without\nDTDs and XML Schemas, and provide an XPath/XSLT\nquery interface to select particular subtrees for process-\ning. All of these reading/writing components will use ex-\nisting open source software. It will also eventually pro-\nvide components to manipulate groups of files: such as it-\nerate through directories, crawl web pages, get files from\nftp, extract files from zip and tar archives. The system\nwill provide full support to standard character sets (e.g.\nUnicode) and encodings (e.g. UTF-8 and UTF-16).\nFinally, the infrastructure will provide standard imple-\nmentations, feature sets and configuration options which\nmeans that if the configuration of the components is pub-\nlished, it will be possible for anyone to reproduce pub-\nlished results. This is important because there are many\nsmall design decisions that can contribute to the accuracy\nof a system that are not typically reported in the literature.\n"},{"#tail":"\n","@confidence":"0.976372576923077","#text":"\nWhen completed the infrastructure will provide highly\nconfigurable components grouped into these broad areas:\nfile processing reading from directories, archives, com-\npressed files, sockets, HTTP and newsgroups;\ntext processing reading/writing marked-up corpora,\nHTML, emails, standard document formats and text\nfile formats used to represent annotated corpora.\nlexical processing tokenization, word segmentation and\nmorphological analysis;\nfeature extraction extracting lexical and annotation fea-\ntures from the current context in sequences, bag of\nwords from segments of text\ndata-structures and algorithms efficient lexical repre-\nsentations, lexicons, tagsets and statistics; Viterbi,\nbeam-search and n-best sequence taggers, parsing\nalgorithms;\nmachine learning methods statistical models: Na??ve\nBayes, Maximum Entropy, Conditional Random\nFields; and other methods: Decision Trees and Lists,\nTBL and MBL;\nresources APIs to WordNet (Fellbaum, 1998), Google\nand other lexical resources such as gazetteers, on-\ntologies and machine readable dictionaries;\nexisting tools integrating existing open source compo-\nnents and providing interfaces to existing tools that\nare only distributed as executables.\n"},{"#tail":"\n","@confidence":"0.999640146341463","#text":"\nThe infrastructure will be implemented in C/C++. Tem-\nplates will be used heavily to provide generality without\nsignificantly impacting on efficiency. However, because\ntemplates are a static facility we will also provide dy-\nnamic versions (using inheritance), which will be slower\nbut accessible from scripting languages and user inter-\nfaces. To provide the required configurability in the static\nversion of the code we will use policy templates (Alexan-\ndrescu, 2001), and for the dynamic version we will use\nconfiguration classes.\nA key aspect of increasing the efficiency of the system\nwill be using a common text and annotation representa-\ntion throughout the infrastructure. This means that we do\nnot need to save data to disk, and load it back into mem-\nory between each step in the process, which will provide\na significant performance increase. Further, we can use\ntechniques for making string matching and other text pro-\ncessing very fast such as making only one copy of each\nlexical item or annotation in memory. We can also load\na lexicon into memory that is shared between all of the\ncomponents, reducing the memory use.\nThe implementation has been inspired by experience\nin extracting information from very large corpora (Cur-\nran and Moens, 2002) and performing experiments on\nmaximum entropy sequence tagging (Curran and Clark,\n2003; Clark et al, 2003). We have already implemented\na POS tagger, chunker, CCG supertagger and named entity\nrecogniser using the infrastructure. These tools currently\ntrain in less than 10 minutes on the standard training ma-\nterials and tag faster than TNT, the fastest existing POS\ntagger. These tools use a highly optimised GIS imple-\nmentation and provide sophisticated Gaussian smoothing\n(Chen and Rosenfeld, 1999). We expect even faster train-\ning times when we move to conjugate gradient methods.\nThe next step of the process will be to add different sta-\ntistical models and machine learning methods. We first\nplan to add a simple Na??ve Bayes model to the system.\nThis will allow us to factor out the maximum entropy\nspecific parts of the system and produce a general com-\nponent for statistical modelling. We will then implement\nother machine learning methods and tasks.\n"},{"#tail":"\n","@confidence":"0.995291936170213","#text":"\nAlthough C++ is extremely efficient, it is not suitable for\nrapidly gluing components together to form new tools.\nTo overcome this problem we have implemented an in-\nterface to the infrastructure in the Python scripting lan-\nguage. Python has a number of advantages over other\noptions, such as Java and Perl. Python is very easy to\nlearn, read and write, and allows commands to be entered\ninteractively into the interpreter, making it ideal for ex-\nperimentation. It has already been used to implement a\nframework for teaching NLP (Loper and Bird, 2002).\nUsing the Boost.Python C++ library (Abrahams,\n2003), it is possible to reflect most of the components\ndirectly into Python with a minimal amount of coding.\nThe Boost.Python library also allows the C++ code to ac-\ncess new classes written in Python that are derived from\nthe C++ classes. This means that new and extended com-\nponents can be written in Python (although they will be\nconsiderably slower). The Python interface allows the\ncomponents to be dynamically composed, configured and\nextended in any operating system environment without\nthe need for a compiler. Finally, since Python can pro-\nduce stand-alone executables directly, it will be possible\nto create distributable code that does not require the entire\ninfrastructure or Python interpreter to be installed.\nThe basic Python reflection has already been imple-\nmented and used for large scale experiments with POS\ntagging, using pyMPI (a message passing interface library\nfor Python) to coordinate experiments across a cluster of\nover 100 machines (Curran and Clark, 2003; Clark et al,\n2003). An example of using the Python tagger interface\nis shown in Figure 1.\nOn top of the Python interface we plan to implement\na GUI interface for composing and configuring compo-\nnents. This will be implemented in wxPython which is\na platform independent GUI library that uses the native\nwindowing environment under Windows, MacOS and\nmost versions of Unix. The wxPython interface will gen-\nerate C++ and Python code that composes and config-\nures the components. Using the infrastructure, Python\nand wxPython it will be possible to generate new GUI ap-\nplications that use NLP technology.\nBecause C++ compilers are now fairly standards com-\npliant, and Python and wxPython are available for most\narchitectures, the infrastructure will be highly portable.\nFurther, we eventually plan to implement interfaces to\nother languages (in particular Java using the Java Native\nInterface (JNI) and Perl using the XS interface).\n"},{"#tail":"\n","@confidence":"0.999623692307692","#text":"\nThe final interface we intend to implement is a collec-\ntion of web services for NLP. A web service provides\na remote procedure that can be called using XML based\nencodings (XMLRPC or SOAP) of function names, argu-\nments and results transmitted via internet protocols such\nas HTTP. Systems can automatically discover and com-\nmunicate with web services that provide the functionality\nthey require by querying databases of standardised de-\nscriptions of services with WSDL and UDDI. This stan-\ndardisation of remote procedures is very exciting from a\nsoftware engineering viewpoint since it allows systems\nto be totally distributed. There have already been several\nattempts to develop distributed NLP systems for dialogue\nsystems (Bayer et al, 2001) and speech recognition (Ha-\ncioglu and Pellom, 2003). Web services will allow com-\nponents developed by different researchers in different lo-\ncations to be composed to build larger systems.\nBecause web services are of great commercial interest\nthey are already being supported strongly by many pro-\ngramming languages. For instance, web services can be\naccessed with very little code in Java, Python, Perl, C,\nC++ and Prolog. This allows us to provide NLP services\nto many systems that we could not otherwise support us-\ning a single interface definition. Since the service argu-\nments and results are primarily text and XML, the web\nservice interface will be reasonably efficient for small\n"},{"#tail":"\n","@confidence":"0.971358083333333","#text":"\nquantities of text (e.g. a single document). The second\nadvantage they have is that there is no startup costs when\ntagger loads up, which means local copies of the web\nservice could be run to reduce tagging latency. Finally,\nweb services will allow developers of resources such as\ngazetteers to provide the most up to date resources each\ntime their functionality is required.\nWe are currently in the process of implementing a POS\ntagging web service using the gSOAP library, which will\ntranslate our C infrastructure binding into web service\nwrapper code and produce the necessary XML service de-\nscription files.\n"},{"#tail":"\n","@confidence":"0.995884291666667","#text":"\nThe Generative Programming approach to NLP infras-\ntructure development will allow tools such as sentence\nboundary detectors, POS taggers, chunkers and named\nentity recognisers to be rapidly composed from many el-\nemental components. For instance, implementing an ef-\nficient version of the MXPOST POS tagger (Ratnaparkhi,\n1996) will simply involve composing and configuring the\nappropriate text file reading component, with the sequen-\ntial tagging component, the collection of feature extrac-\ntion components and the maximum entropy model com-\nponent.\nThe individual components will provide state of the art\naccuracy and be highly optimised for both time and space\nefficiency. A key design feature of this infrastructure is\nthat components share a common representation for text\nand annotations so there is no time spent reading/writing\nformatted data (e.g. XML) between stages.\nTo make the composition and configuration process\neasier we have implemented a Python scripting inter-\nface, which means that anyone can construct efficient new\ntools, without the need for much programming experi-\nence or a compiler. The development of a graphical user\ninterface on top of the infrastructure will further ease the\ndevelopment cycle.\n"},{"#tail":"\n","@confidence":"0.9960934","#text":"\nWe would like to thank Stephen Clark, Tara Murphy, and\nthe anonymous reviewers for their comments on drafts\nof this paper. This research is supported by a Common-\nwealth scholarship and a Sydney University Travelling\nscholarship.\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.9953165","#text":"\nSchool of Informatics\nUniversity of Edinburgh\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.985912","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.994338","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.952933","@genericHeader":"introduction","#text":"\n2 Existing Systems\n"},{"#tail":"\n","@confidence":"0.985113","@genericHeader":"method","#text":"\n3 Performance Requirements\n"},{"#tail":"\n","@confidence":"0.995818","@genericHeader":"method","#text":"\n4 Design Requirements\n"},{"#tail":"\n","@confidence":"0.941596","@genericHeader":"method","#text":"\n5 Components Groups\n"},{"#tail":"\n","@confidence":"0.995022","@genericHeader":"method","#text":"\n6 Implementation\n"},{"#tail":"\n","@confidence":"0.994548","@genericHeader":"method","#text":"\n7 Interfaces\n"},{"#tail":"\n","@confidence":"0.864342","@genericHeader":"method","#text":"\n8 Web services\n"},{"#tail":"\n","@confidence":"0.963455","@genericHeader":"conclusions","#text":"\n9 Conclusion\n"},{"#tail":"\n","@confidence":"0.945059","@genericHeader":"acknowledgments","#text":"\nAcknowledgements\n"},{"#tail":"\n","@confidence":"0.98627","@genericHeader":"references","#text":"\nReferences\n"}],"figureCaption":{"#tail":"\n","@confidence":"0.998355","#text":"\nFigure 1: Calling the POS tagger interactively from the Python interpreter\n"},"table":{"#tail":"\n","@confidence":"0.756192375","#text":"\n% python\nPython 2.2.1 (#1, Sep 30 2002, 20:13:03)\n[GCC 2.96 20000731 (Red Hat Linux 7.3 2.96-110)] on linux2\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n>>> import nlp.tagger\n>>> op = nlp.tagger.Options(?models/pos/options?)\n>>> print op\nnklasses = 46\n...\nalpha = 1.65\n>>> tagger = nlp.tagger.Tagger(op)\n>>> tags = tagger.tag([?The?, ?cat?, ?sat?, ?on?, ?the?, ?mat?, ?.?])\n>>> print tags\n[?DT?, ?NN?, ?VBD?, ?IN?, ?DT?, ?NN?, ?.?]\n>>> tagger.tag(?infile?, ?outfile?)\n>>>\n"},"email":{"#tail":"\n","@confidence":"0.993547","#text":"\njamesc@cogsci.ed.ac.uk\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.750479","#tail":"\n","@no":"0","address":{"#tail":"\n","@confidence":"0.772078","#text":"2 Buccleuch Place, Edinburgh. EH8 9LW"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.9999205","#text":"School of Informatics University of Edinburgh"},"author":{"#tail":"\n","@confidence":"0.999998","#text":"James R Curran"},"abstract":{"#tail":"\n","@confidence":"0.998105307692308","#text":"Natural Language Processing (NLP) system developers face a number of new challenges. Interest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched continues to grow rapidly. Thus it is an ideal time to consider the development of new experimental frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure."},"title":{"#tail":"\n","@confidence":"0.998104","#text":"Blueprint for a High Performance NLP Infrastructure"},"email":{"#tail":"\n","@confidence":"0.997243","#text":"jamesc@cogsci.ed.ac.uk"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"note":{"#tail":"\n","#text":"Boost.Python C++ library. http://www.boost.ory (viewed 23/3/2003)."},"rawString":{"#tail":"\n","#text":"David Abrahams. 2003. Boost.Python C++ library. http://www.boost.ory (viewed 23/3/2003)."},"#text":"\n","marker":{"#tail":"\n","#text":"Abrahams, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":". 7 Interfaces Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools. To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language. Python has a number of advantages over other options, such as Java and Perl. Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation. It has already been used to implement a framework for teaching NLP (Loper and Bird, 2002). Using the Boost.Python C++ library (Abrahams, 2003), it is possible to reflect most of the components directly into Python with a minimal amount of coding. The Boost.Python library also allows the C++ code to access new classes written in Python that are derived from the C++ classes. This means that new and extended components can be written in Python (although they will be considerably slower). The Python interface allows the components to be dynamically composed, configured and extended in any operating system environment without the need for a compiler. Finally, since Python can produce stand-alone executables directly, it will be possible ","@endWordPosition":"2674","@position":"17280","annotationId":"T1","@startWordPosition":"2673","@citStr":"Abrahams, 2003"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David Abrahams"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Andrei Alexandrescu. 2001. Modern C++ Design: Generic Programming and Design Patterns Applied. C++ In-Depth Series. Addison-Wesley, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Alexandrescu, 2001"},"publisher":{"#tail":"\n","#text":"Addison-Wesley,"},"location":{"#tail":"\n","#text":"New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ting tools integrating existing open source components and providing interfaces to existing tools that are only distributed as executables. 6 Implementation The infrastructure will be implemented in C/C++. Templates will be used heavily to provide generality without significantly impacting on efficiency. However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces. To provide the required configurability in the static version of the code we will use policy templates (Alexandrescu, 2001), and for the dynamic version we will use configuration classes. A key aspect of increasing the efficiency of the system will be using a common text and annotation representation throughout the infrastructure. This means that we do not need to save data to disk, and load it back into memory between each step in the process, which will provide a significant performance increase. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is sha","@endWordPosition":"2284","@position":"14926","annotationId":"T2","@startWordPosition":"2282","@citStr":"Alexandrescu, 2001"}},"title":{"#tail":"\n","#text":"Modern C++ Design: Generic Programming and Design Patterns Applied. C++ In-Depth Series."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Andrei Alexandrescu"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th annual meeting of the Association for Computational Linguistics, pages 26?33, Toulouse, France, 9?11 July."},"#text":"\n","pages":{"#tail":"\n","#text":"26--33"},"marker":{"#tail":"\n","#text":"Banko, Brill, 2001"},"location":{"#tail":"\n","#text":"Toulouse,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sk. NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al, 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al, 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime. For example, Google currently answers 250 million queries per day, thus processing time must be minimised. Clearly, efficient NLP comp","@endWordPosition":"322","@position":"2158","annotationId":"T3","@startWordPosition":"319","@citStr":"Banko and Brill, 2001"}},"title":{"#tail":"\n","#text":"Scaling to very very large corpora for natural language disambiguation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 39th annual meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michele Banko"},{"#tail":"\n","#text":"Eric Brill"}]}},{"date":{"#tail":"\n","#text":"2001"},"editor":{"#tail":"\n","#text":"In J. Allan, editor,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"edure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP. Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems (Bayer et al, 2001) and speech recognition (Hacioglu and Pellom, 2003). Web services will allow components developed by different researchers in different locations to be composed to build larger systems. Because web services are of great commercial interest they are already being supported strongly by many programming languages. For instance, web services can be accessed with very little code in Java, Python, Perl, C, C++ and Prolog. This allows us to provide NLP services to many systems that we could not otherwise support using a single interface definition. Since the service arguments and results are primaril","@endWordPosition":"3098","@position":"19936","annotationId":"T4","@startWordPosition":"3095","@citStr":"Bayer et al, 2001"}},"title":{"#tail":"\n","#text":"Dialogue interaction with the DARPA Communicator Infrastructure: The development of useful software."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Samuel Bayer, Christine Doran, and Bryan George. 2001. Dialogue interaction with the DARPA Communicator Infrastructure: The development of useful software. In J. Allan, editor, Proceedings of HLT 2001, First International Conference on Human Language Technology Research, pages 114?116, San Diego, CA, USA. Morgan Kaufmann."},"#text":"\n","pages":{"#tail":"\n","#text":"114--116"},"marker":{"#tail":"\n","#text":"Bayer, Doran, George, 2001"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann."},"location":{"#tail":"\n","#text":"San Diego, CA, USA."},"booktitle":{"#tail":"\n","#text":"Proceedings of HLT 2001, First International Conference on Human Language Technology Research,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Samuel Bayer"},{"#tail":"\n","#text":"Christine Doran"},{"#tail":"\n","#text":"Bryan George"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92?100, Madisson, WI, USA."},"#text":"\n","pages":{"#tail":"\n","#text":"92--100"},"marker":{"#tail":"\n","#text":"Blum, Mitchell, 1998"},"location":{"#tail":"\n","#text":"Madisson, WI, USA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ing is extremely simple to learn, read and write, and so using the existing components and designing new components is simple. 3 Performance Requirements As discussed earlier, there are two main requirements of the system that are covered by ?high performance?: speed and state of the art accuracy. Efficiency is required both in training and processing. Efficient training is required because the amount of data available for training will increase significantly. Also, advanced methods often require many training iterations, for example active learning (Dagan and Engelson, 1995) and co-training (Blum and Mitchell, 1998). Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly. State of the art accuracy is also important, particularly on complex systems since the error is accumulated from each component in the system. There is a speed/accuracy tradeoff that is rarely addressed in the literature. For instance, reducing the beam search width used for tagging can increase the speed without significantly reducing accuracy. Finally, the most accurate systems are often very ","@endWordPosition":"1141","@position":"7386","annotationId":"T5","@startWordPosition":"1138","@citStr":"Blum and Mitchell, 1998"}},"title":{"#tail":"\n","#text":"Combining labeled and unlabeled data with co-training."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 11th Annual Conference on Computational Learning Theory,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Avrim Blum"},{"#tail":"\n","#text":"Tom Mitchell"}]}},{"date":{"#tail":"\n","#text":"2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify code or reformat data. Finally, the Natural Language Toolkit (NLTK) is a package of NLP components implemented in Python (Loper and Bird, 2002). Python scripting is extremely simple to learn, read","@endWordPosition":"958","@position":"6200","annotationId":"T6","@startWordPosition":"957","@citStr":"Brants, 2000"}},"title":{"#tail":"\n","#text":"TnT - a statistical part-of-speech tagger."},"volume":{"#tail":"\n","#text":"4"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Thorsten Brants. 2000. TnT - a statistical part-of-speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing, pages 224?231, Seattle, WA, USA, 29 April ? 4 May."},"#text":"\n","pages":{"#tail":"\n","#text":"224--231"},"marker":{"#tail":"\n","#text":"Brants, 2000"},"location":{"#tail":"\n","#text":"Seattle, WA, USA,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 6th Conference on Applied Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Thorsten Brants"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1993"},"institution":{"#tail":"\n","#text":"Department of Computer and Information Science, University of Pennsylvania."},"rawString":{"#tail":"\n","#text":"Eric Brill. 1993. A Corpus-Based Appreach to Language Learning. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania."},"#text":"\n","marker":{"#tail":"\n","#text":"Brill, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rements The remaining requirements relate to the overall and component level design of the system. Following the Generative Programming paradigm, the individual components of the system must be elementary and highly configurable. This ensures minimal redundancy between components and makes them easier to understand, implement, test and debug. It also ensures components are maximally composable and extensible. This is particularly important in NLP because of the high redundancy across tasks and approaches. Machine learning methods should be interchangeable: Transformation-based learning (TBL) (Brill, 1993) and Memory-based learning (MBL) (Daelemans et al, 2002) have been applied to many different problems, so a single interchangeable component should be used to represent each method. We will base these components on the design of Weka (Witten and Frank, 1999). Representations should be reusable: for example, named entity classification can be considered as a sequence tagging task or a bag-of-words text classification task. The same beam-search sequence tagging component should be able to be used for POS tagging, chunking and named entity classification. Feature extraction components should be r","@endWordPosition":"1488","@position":"9653","annotationId":"T7","@startWordPosition":"1487","@citStr":"Brill, 1993"}},"title":{"#tail":"\n","#text":"A Corpus-Based Appreach to Language Learning."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Eric Brill"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical report,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"Carnegie Mellon University."},"rawString":{"#tail":"\n","#text":"Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University."},"#text":"\n","marker":{"#tail":"\n","#text":"Chen, Rosenfeld, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and Clark, 2003; Clark et al, 2003). We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999). We expect even faster training times when we move to conjugate gradient methods. The next step of the process will be to add different statistical models and machine learning methods. We first plan to add a simple Na??ve Bayes model to the system. This will allow us to factor out the maximum entropy specific parts of the system and produce a general component for statistical modelling. We will then implement other machine learning methods and tasks. 7 Interfaces Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools. To overcome this ","@endWordPosition":"2494","@position":"16211","annotationId":"T8","@startWordPosition":"2491","@citStr":"Chen and Rosenfeld, 1999"}},"title":{"#tail":"\n","#text":"A Gaussian prior for smoothing maximum entropy models."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Stanley Chen"},{"#tail":"\n","#text":"Ronald Rosenfeld"}]}},{"date":{"#tail":"\n","#text":"2003"},"note":{"#tail":"\n","#text":"(to appear)."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" back into memory between each step in the process, which will provide a significant performance increase. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and Clark, 2003; Clark et al, 2003). We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999). We expect even faster training times when we move to conjugate gradient methods. The next step of the process will be to add different statistical models and machine learning methods. We first plan to add ","@endWordPosition":"2434","@position":"15818","annotationId":"T9","@startWordPosition":"2431","@citStr":"Clark et al, 2003"},{"#tail":"\n","#text":"e components to be dynamically composed, configured and extended in any operating system environment without the need for a compiler. Finally, since Python can produce stand-alone executables directly, it will be possible to create distributable code that does not require the entire infrastructure or Python interpreter to be installed. The basic Python reflection has already been implemented and used for large scale experiments with POS tagging, using pyMPI (a message passing interface library for Python) to coordinate experiments across a cluster of over 100 machines (Curran and Clark, 2003; Clark et al, 2003). An example of using the Python tagger interface is shown in Figure 1. On top of the Python interface we plan to implement a GUI interface for composing and configuring components. This will be implemented in wxPython which is a platform independent GUI library that uses the native windowing environment under Windows, MacOS and most versions of Unix. The wxPython interface will generate C++ and Python code that composes and configures the components. Using the infrastructure, Python and wxPython it will be possible to generate new GUI applications that use NLP technology. Because C++ compiler","@endWordPosition":"2833","@position":"18276","annotationId":"T10","@startWordPosition":"2830","@citStr":"Clark et al, 2003"}]},"title":{"#tail":"\n","#text":"Bootstrapping POS-taggers using unlabelled data."},"volume":{"#tail":"\n","#text":"1"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Stephen Clark, James R. Curran, and Miles Osborne. 2003. Bootstrapping POS-taggers using unlabelled data. In Proceedings of the 7th Conference on Natural Language Learning, Edmonton, Canada, 31 May ? 1 June. (to appear)."},"#text":"\n","marker":{"#tail":"\n","#text":"Clark, Curran, Osborne, 2003"},"location":{"#tail":"\n","#text":"Edmonton, Canada, 31"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 7th Conference on Natural Language Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Stephen Clark"},{"#tail":"\n","#text":"James R Curran"},{"#tail":"\n","#text":"Miles Osborne"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Hamish Cunningham, Yorick Wilks, and Robert J. Gaizauskas. 1997. GATE ? a general architecture for text engineering. In Proceedings of the 16th International Conference on Computational Linguistics, pages 1057?1060, Copenhagen, Denmark, 5?9 August."},"#text":"\n","pages":{"#tail":"\n","#text":"1057--1060"},"marker":{"#tail":"\n","#text":"Cunningham, Wilks, Gaizauskas, 1997"},"location":{"#tail":"\n","#text":"Copenhagen,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" for the infrastructure including a script language and GUI interfaces, and web services for distributed NLP system development. We seek feedback on the overall design and implementation of our proposed infrastructure and to promote discussion about software engineering best practice in NLP. 1We use high performance to refer to both state of the art performance and high runtime efficiency. 2 Existing Systems There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al, 1997) and the Alembic Workbench (Day et al, 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al, 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al,","@endWordPosition":"652","@position":"4312","annotationId":"T11","@startWordPosition":"649","@citStr":"Cunningham et al, 1997"}},"title":{"#tail":"\n","#text":"GATE ? a general architecture for text engineering."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 16th International Conference on Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hamish Cunningham"},{"#tail":"\n","#text":"Yorick Wilks"},{"#tail":"\n","#text":"Robert J Gaizauskas"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical report,"},"date":{"#tail":"\n","#text":"2002"},"institution":{"#tail":"\n","#text":"University of Sheffield,"},"rawString":{"#tail":"\n","#text":"Hamish Cunningham, Diana Maynard, C. Ursu K. Bontcheva, V. Tablan, and M. Dimitrov. 2002. Developing language processing components with GATE. Technical report, University of Sheffield, Sheffield, UK."},"#text":"\n","marker":{"#tail":"\n","#text":"Cunningham, Maynard, Bontcheva, Tablan, Dimitrov, 2002"},"location":{"#tail":"\n","#text":"Sheffield, UK."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ware engineering best practice in NLP. 1We use high performance to refer to both state of the art performance and high runtime efficiency. 2 Existing Systems There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al, 1997) and the Alembic Workbench (Day et al, 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al, 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al, 1999; Grover et al, 2000) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibi","@endWordPosition":"695","@position":"4566","annotationId":"T12","@startWordPosition":"692","@citStr":"Cunningham et al, 2002"}},"title":{"#tail":"\n","#text":"Developing language processing components with GATE."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hamish Cunningham"},{"#tail":"\n","#text":"Diana Maynard"},{"#tail":"\n","#text":"C Ursu K Bontcheva"},{"#tail":"\n","#text":"V Tablan"},{"#tail":"\n","#text":"M Dimitrov"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"2000"},"institution":{"#tail":"\n","#text":"University of Sheffield."},"rawString":{"#tail":"\n","#text":"Hamish Cunningham. 2000. Software Architecture for Language Engineering. Ph.D. thesis, University of Sheffield."},"#text":"\n","marker":{"#tail":"\n","#text":"Cunningham, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"formance and high runtime efficiency. 2 Existing Systems There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al, 1997) and the Alembic Workbench (Day et al, 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al, 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al, 1999; Grover et al, 2000) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed a","@endWordPosition":"708","@position":"4661","annotationId":"T13","@startWordPosition":"707","@citStr":"Cunningham, 2000"}},"title":{"#tail":"\n","#text":"Software Architecture for Language Engineering."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Hamish Cunningham"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the 11th Meeting of the European Chapter of the Association for Computational Lingustics, pages 91?98, Budapest, Hungary, 12?17 April."},"#text":"\n","pages":{"#tail":"\n","#text":"91--98"},"marker":{"#tail":"\n","#text":"Curran, Clark, 2003"},"location":{"#tail":"\n","#text":"Budapest,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ata to disk, and load it back into memory between each step in the process, which will provide a significant performance increase. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and Clark, 2003; Clark et al, 2003). We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999). We expect even faster training times when we move to conjugate gradient methods. The next step of the process will be to add different statistical models and machine learning methods. W","@endWordPosition":"2430","@position":"15798","annotationId":"T14","@startWordPosition":"2427","@citStr":"Curran and Clark, 2003"},{"#tail":"\n","#text":"thon interface allows the components to be dynamically composed, configured and extended in any operating system environment without the need for a compiler. Finally, since Python can produce stand-alone executables directly, it will be possible to create distributable code that does not require the entire infrastructure or Python interpreter to be installed. The basic Python reflection has already been implemented and used for large scale experiments with POS tagging, using pyMPI (a message passing interface library for Python) to coordinate experiments across a cluster of over 100 machines (Curran and Clark, 2003; Clark et al, 2003). An example of using the Python tagger interface is shown in Figure 1. On top of the Python interface we plan to implement a GUI interface for composing and configuring components. This will be implemented in wxPython which is a platform independent GUI library that uses the native windowing environment under Windows, MacOS and most versions of Unix. The wxPython interface will generate C++ and Python code that composes and configures the components. Using the infrastructure, Python and wxPython it will be possible to generate new GUI applications that use NLP technology. ","@endWordPosition":"2829","@position":"18256","annotationId":"T15","@startWordPosition":"2826","@citStr":"Curran and Clark, 2003"}]},"title":{"#tail":"\n","#text":"Investigating GIS and smoothing for maximum entropy taggers."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 11th Meeting of the European Chapter of the Association for Computational Lingustics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"James R Curran"},{"#tail":"\n","#text":"Stephen Clark"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"James R. Curran and Marc Moens. 2002. Scaling context space. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, USA, 7?12 July."},"#text":"\n","marker":{"#tail":"\n","#text":"Curran, Moens, 2002"},"location":{"#tail":"\n","#text":"Philadelphia, PA, USA, 7?12"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al, 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al, 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime. For example, Google currently answers 250 million queries per day, thus processing time must be minimised. Clearly, efficient NLP components will need to be de","@endWordPosition":"326","@position":"2183","annotationId":"T16","@startWordPosition":"323","@citStr":"Curran and Moens, 2002"},{"#tail":"\n","#text":"n representation throughout the infrastructure. This means that we do not need to save data to disk, and load it back into memory between each step in the process, which will provide a significant performance increase. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and Clark, 2003; Clark et al, 2003). We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999). We expect even faster training times when we move to conjugate gradient methods. The next step of ","@endWordPosition":"2418","@position":"15711","annotationId":"T17","@startWordPosition":"2414","@citStr":"Curran and Moens, 2002"}]},"title":{"#tail":"\n","#text":"Scaling context space."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"James R Curran"},{"#tail":"\n","#text":"Marc Moens"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Krzysztof Czarnecki and Ulrich W. Eisenecker. 2000. Generative Programming: Methods, Tools, and Applications. Addison-Wesley."},"#text":"\n","marker":{"#tail":"\n","#text":"Czarnecki, Eisenecker, 2000"},"publisher":{"#tail":"\n","#text":"Addison-Wesley."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" Other potential applications must process text online or in realtime. For example, Google currently answers 250 million queries per day, thus processing time must be minimised. Clearly, efficient NLP components will need to be developed. At the same time, state-of-the-art performance will be needed for these systems to be of practical use. Finally, NLP is growing in terms of the number of tasks, methods and languages being researched. Although many problems share algorithms and data structures there is a tendency to reinvent the wheel. Software engineering research on Generative Programming (Czarnecki and Eisenecker, 2000) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems. Our infrastructure for NLP will provide high performance1 components inspired by Generative Programming principles. This paper reviews existing NLP systems and discusses the requirements for an NLP infrastructure. We then describe our overall design and exploratory implementation. We conclude with a discussion of programming interfaces for the infrastructure including a script language and GUI interfaces, and web services for di","@endWordPosition":"481","@position":"3184","annotationId":"T18","@startWordPosition":"478","@citStr":"Czarnecki and Eisenecker, 2000"}},"title":{"#tail":"\n","#text":"Generative Programming: Methods, Tools, and Applications."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Krzysztof Czarnecki"},{"#tail":"\n","#text":"Ulrich W Eisenecker"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical Report ILK 02-10,"},"date":{"#tail":"\n","#text":"2002"},"institution":{"#tail":"\n","#text":"Induction of Linguistic Knowledge. Tilburg University."},"rawString":{"#tail":"\n","#text":"Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2002. TiMBL: Tilburg Memory-Based Learner reference guide. Technical Report ILK 02-10, Induction of Linguistic Knowledge. Tilburg University."},"#text":"\n","marker":{"#tail":"\n","#text":"Daelemans, Zavrel, van der Sloot, van den Bosch, 2002"},"title":{"#tail":"\n","#text":"TiMBL: Tilburg Memory-Based Learner reference guide."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Walter Daelemans"},{"#tail":"\n","#text":"Jakub Zavrel"},{"#tail":"\n","#text":"Ko van der Sloot"},{"#tail":"\n","#text":"Antal van den Bosch"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Ido Dagan and Sean P. Engelson. 1995. Committee-based sampling for training probabilistic classifiers. In Proceedings of the International Conference on Machine Learning, pages 150?157, Tahoe City, CA, USA, 9?12 July."},"#text":"\n","pages":{"#tail":"\n","#text":"150--157"},"marker":{"#tail":"\n","#text":"Dagan, Engelson, 1995"},"location":{"#tail":"\n","#text":"Tahoe City, CA, USA, 9?12"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ython (Loper and Bird, 2002). Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple. 3 Performance Requirements As discussed earlier, there are two main requirements of the system that are covered by ?high performance?: speed and state of the art accuracy. Efficiency is required both in training and processing. Efficient training is required because the amount of data available for training will increase significantly. Also, advanced methods often require many training iterations, for example active learning (Dagan and Engelson, 1995) and co-training (Blum and Mitchell, 1998). Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly. State of the art accuracy is also important, particularly on complex systems since the error is accumulated from each component in the system. There is a speed/accuracy tradeoff that is rarely addressed in the literature. For instance, reducing the beam search width used for tagging can increase the speed without significantly reducing accuracy. Finally,","@endWordPosition":"1135","@position":"7344","annotationId":"T19","@startWordPosition":"1132","@citStr":"Dagan and Engelson, 1995"}},"title":{"#tail":"\n","#text":"Committee-based sampling for training probabilistic classifiers."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the International Conference on Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ido Dagan"},{"#tail":"\n","#text":"Sean P Engelson"}]}},{"date":{"#tail":"\n","#text":"1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" and GUI interfaces, and web services for distributed NLP system development. We seek feedback on the overall design and implementation of our proposed infrastructure and to promote discussion about software engineering best practice in NLP. 1We use high performance to refer to both state of the art performance and high runtime efficiency. 2 Existing Systems There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al, 1997) and the Alembic Workbench (Day et al, 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al, 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al, 1999; Grover et al, 2000) perform tokenizat","@endWordPosition":"660","@position":"4356","annotationId":"T20","@startWordPosition":"657","@citStr":"Day et al, 1997"}},"title":{"#tail":"\n","#text":"Mixed-initiative development of language processing systems."},"volume":{"#tail":"\n","#text":"3"},"#tail":"\n","rawString":{"#tail":"\n","#text":"David Day, John Aberdeen, Lynette Hirschman, Robyn Kozierok, Patricia Robinson, and Marc Vilain. 1997. Mixed-initiative development of language processing systems. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 384?355, Washington, DC, USA, 31 March ? 3 April."},"#text":"\n","pages":{"#tail":"\n","#text":"384--355"},"marker":{"#tail":"\n","#text":"Day, Aberdeen, Hirschman, Kozierok, Robinson, Vilain, 1997"},"location":{"#tail":"\n","#text":"Washington, DC, USA, 31"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth Conference on Applied Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Day"},{"#tail":"\n","#text":"John Aberdeen"},{"#tail":"\n","#text":"Lynette Hirschman"},{"#tail":"\n","#text":"Robyn Kozierok"},{"#tail":"\n","#text":"Patricia Robinson"},{"#tail":"\n","#text":"Marc Vilain"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"editor":{"#tail":"\n","#text":"Cristiane Fellbaum, editor."},"rawString":{"#tail":"\n","#text":"Cristiane Fellbaum, editor. 1998. Wordnet: an electronic lexical database. The MIT Press, Cambridge, MA USA."},"#text":"\n","marker":{"#tail":"\n","#text":"1998"},"publisher":{"#tail":"\n","#text":"The MIT Press,"},"location":{"#tail":"\n","#text":"Cambridge, MA USA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al, 1997; Mohri et al, 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) prov","@endWordPosition":"889","@position":"5753","annotationId":"T21","@startWordPosition":"889","@citStr":"(1998)"}},"title":{"#tail":"\n","#text":"Wordnet: an electronic lexical database."},"@valid":"true"},{"date":{"#tail":"\n","#text":"2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e Alembic Workbench (Day et al, 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al, 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al, 1999; Grover et al, 2000) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al, 1997; Mohri et al, 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be","@endWordPosition":"758","@position":"4938","annotationId":"T22","@startWordPosition":"755","@citStr":"Grover et al, 2000"}},"title":{"#tail":"\n","#text":"LT TTT - a flexible tokenisation tool."},"volume":{"#tail":"\n","#text":"2"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Claire Grover, Colin Matheson, Andrei Mikheev, and Marc Moens. 2000. LT TTT - a flexible tokenisation tool. In Proceedings of Second International Language Resources and Evaluation Conference, pages 1147?1154, Athens, Greece, 31 May ? 2 June."},"#text":"\n","pages":{"#tail":"\n","#text":"1147--1154"},"marker":{"#tail":"\n","#text":"Grover, Matheson, Mikheev, Moens, 2000"},"location":{"#tail":"\n","#text":"Athens, Greece, 31"},"booktitle":{"#tail":"\n","#text":"In Proceedings of Second International Language Resources and Evaluation Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Claire Grover"},{"#tail":"\n","#text":"Colin Matheson"},{"#tail":"\n","#text":"Andrei Mikheev"},{"#tail":"\n","#text":"Marc Moens"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Kadri Hacioglu and Bryan Pellom. 2003. A distributed architecture for robust automatic speech recognition. In Proceedings of Conference on Acoustics, Speech, and Signal Processing (ICASSP), Hong Kong, China, 6?10 April."},"#text":"\n","marker":{"#tail":"\n","#text":"Hacioglu, Pellom, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"codings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP. Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems (Bayer et al, 2001) and speech recognition (Hacioglu and Pellom, 2003). Web services will allow components developed by different researchers in different locations to be composed to build larger systems. Because web services are of great commercial interest they are already being supported strongly by many programming languages. For instance, web services can be accessed with very little code in Java, Python, Perl, C, C++ and Prolog. This allows us to provide NLP services to many systems that we could not otherwise support using a single interface definition. Since the service arguments and results are primarily text and XML, the web service interface will be r","@endWordPosition":"3106","@position":"19987","annotationId":"T23","@startWordPosition":"3102","@citStr":"Hacioglu and Pellom, 2003"}},"title":{"#tail":"\n","#text":"A distributed architecture for robust automatic speech recognition."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Conference on Acoustics, Speech, and Signal Processing (ICASSP), Hong Kong,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kadri Hacioglu"},{"#tail":"\n","#text":"Bryan Pellom"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Nancy Ide, Randi Reppen, and Keith Suderman. 2002. The american national corpus: More than the web can provide. In Proceedings of the Third Language Resources and Evaluation Conference, pages 839?844, Las Palmas, Canary Islands, Spain."},"#text":"\n","pages":{"#tail":"\n","#text":"839--844"},"marker":{"#tail":"\n","#text":"Ide, Reppen, Suderman, 2002"},"location":{"#tail":"\n","#text":"Las Palmas, Canary Islands,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"xt-to-speech technology has made complete spoken dialogue systems feasible. Developing these complex NLP systems involves composing many different NLP tools. Unfortunately, this is difficult because many implementations have not been designed as components and only recently has input/output standardisation been considered. Finally, these tools can be difficult to customise and tune for a particular task. NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al, 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al, 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text datab","@endWordPosition":"258","@position":"1749","annotationId":"T24","@startWordPosition":"255","@citStr":"Ide et al, 2002"}},"title":{"#tail":"\n","#text":"The american national corpus: More than the web can provide."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Third Language Resources and Evaluation Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nancy Ide"},{"#tail":"\n","#text":"Randi Reppen"},{"#tail":"\n","#text":"Keith Suderman"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical report,"},"date":{"#tail":"\n","#text":"1997"},"institution":{"#tail":"\n","#text":"Xerox Research Centre Europe Grenoble,"},"rawString":{"#tail":"\n","#text":"Lauri Karttunen, Tama?s Gaa?l, and Andre? Kempe. 1997. Xerox Finite-State Tool. Technical report, Xerox Research Centre Europe Grenoble, Meylan, France."},"#text":"\n","marker":{"#tail":"\n","#text":"Karttunen, Gaal, Kempe, 1997"},"location":{"#tail":"\n","#text":"Meylan, France."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al, 1999; Grover et al, 2000) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al, 1997; Mohri et al, 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Tran","@endWordPosition":"822","@position":"5342","annotationId":"T25","@startWordPosition":"819","@citStr":"Karttunen et al, 1997"}},"title":{"#tail":"\n","#text":"Xerox Finite-State Tool."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Lauri Karttunen"},{"#tail":"\n","#text":"Tamas Gaal"},{"#tail":"\n","#text":"Andre Kempe"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Linguistic Data Consortium. 2003. English Gigaword Corpus. catalogue number LDC2003T05."},"#text":"\n","marker":{"#tail":"\n","#text":"Consortium, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ise and tune for a particular task. NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al, 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al, 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime. For example, Google currently answers 250 million queries per day, thus processing time must be mi","@endWordPosition":"315","@position":"2122","annotationId":"T26","@startWordPosition":"314","@citStr":"Consortium, 2003"}},"title":{"#tail":"\n","#text":"English Gigaword Corpus. catalogue number LDC2003T05."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Linguistic Data Consortium"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit. In Proceedings of the Workshop on Effective Tools and Methodologies for Teaching NLP and Computational Linguistics, pages 63?70, Philadelphia, PA, 7 July."},"#text":"\n","pages":{"#tail":"\n","#text":"63--70"},"marker":{"#tail":"\n","#text":"Loper, Bird, 2002"},"location":{"#tail":"\n","#text":"Philadelphia, PA,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify code or reformat data. Finally, the Natural Language Toolkit (NLTK) is a package of NLP components implemented in Python (Loper and Bird, 2002). Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple. 3 Performance Requirements As discussed earlier, there are two main requirements of the system that are covered by ?high performance?: speed and state of the art accuracy. Efficiency is required both in training and processing. Efficient training is required because the amount of data available for training will increase significantly. Also, advanced methods often require many training iterations, for example active learning (Dagan and Engelson, 1995) an","@endWordPosition":"1044","@position":"6747","annotationId":"T27","@startWordPosition":"1041","@citStr":"Loper and Bird, 2002"},{"#tail":"\n","#text":"ill then implement other machine learning methods and tasks. 7 Interfaces Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools. To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language. Python has a number of advantages over other options, such as Java and Perl. Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation. It has already been used to implement a framework for teaching NLP (Loper and Bird, 2002). Using the Boost.Python C++ library (Abrahams, 2003), it is possible to reflect most of the components directly into Python with a minimal amount of coding. The Boost.Python library also allows the C++ code to access new classes written in Python that are derived from the C++ classes. This means that new and extended components can be written in Python (although they will be considerably slower). The Python interface allows the components to be dynamically composed, configured and extended in any operating system environment without the need for a compiler. Finally, since Python can produce s","@endWordPosition":"2667","@position":"17227","annotationId":"T28","@startWordPosition":"2664","@citStr":"Loper and Bird, 2002"}]},"title":{"#tail":"\n","#text":"NLTK: The Natural Language Toolkit."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Workshop on Effective Tools and Methodologies for Teaching NLP and Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Edward Loper"},{"#tail":"\n","#text":"Steven Bird"}]}},{"date":{"#tail":"\n","#text":"2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rticular techniques, such as finite state machines (Karttunen et al, 1997; Mohri et al, 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This ","@endWordPosition":"907","@position":"5882","annotationId":"T29","@startWordPosition":"906","@citStr":"Malouf, 2002"}},"title":{"#tail":"\n","#text":"A comparison of algorithms for maximum entropy parameter estimation."},"volume":{"#tail":"\n","#text":"1"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the 6th Conference on Natural Language Learning, pages 49? 55, Taipei, Taiwan, 31 August ? 1 September."},"#text":"\n","pages":{"#tail":"\n","#text":"49--55"},"marker":{"#tail":"\n","#text":"Malouf, 2002"},"location":{"#tail":"\n","#text":"Taipei, Taiwan, 31"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 6th Conference on Natural Language Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Robert Malouf"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313?330."},"#text":"\n","marker":{"#tail":"\n","#text":"Marcus, Santorini, Marcinkiewicz, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"x NLP systems involves composing many different NLP tools. Unfortunately, this is difficult because many implementations have not been designed as components and only recently has input/output standardisation been considered. Finally, these tools can be difficult to customise and tune for a particular task. NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al, 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al, 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biolo","@endWordPosition":"275","@position":"1851","annotationId":"T30","@startWordPosition":"272","@citStr":"Marcus et al, 1993"}},"title":{"#tail":"\n","#text":"Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mitchell Marcus"},{"#tail":"\n","#text":"Beatrice Santorini"},{"#tail":"\n","#text":"Mary Marcinkiewicz"}]}},{"volume":{"#tail":"\n","#text":"1"},"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Andrei Mikheev, Claire Grover, and Marc Moens. 1999. Xml tools and architecture for named entity recognition. Journal of Markup Languages: Theory and Practice 1, 3:89?113."},"journal":{"#tail":"\n","#text":"Journal of Markup Languages: Theory and Practice"},"#text":"\n","pages":{"#tail":"\n","#text":"3--89"},"marker":{"#tail":"\n","#text":"Mikheev, Grover, Moens, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"m et al, 1997) and the Alembic Workbench (Day et al, 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al, 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al, 1999; Grover et al, 2000) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al, 1997; Mohri et al, 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general","@endWordPosition":"754","@position":"4917","annotationId":"T31","@startWordPosition":"751","@citStr":"Mikheev et al, 1999"}},"title":{"#tail":"\n","#text":"Xml tools and architecture for named entity recognition."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Andrei Mikheev"},{"#tail":"\n","#text":"Claire Grover"},{"#tail":"\n","#text":"Marc Moens"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 1998. A rational design for a weighted finite-state transducer library. Lecture Notes in Computer Science, 1436."},"journal":{"#tail":"\n","#text":"Lecture Notes in Computer Science,"},"#text":"\n","pages":{"#tail":"\n","#text":"1436"},"marker":{"#tail":"\n","#text":"Mohri, Pereira, Riley, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al, 1999; Grover et al, 2000) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al, 1997; Mohri et al, 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Lea","@endWordPosition":"826","@position":"5362","annotationId":"T32","@startWordPosition":"823","@citStr":"Mohri et al, 1998"}},"title":{"#tail":"\n","#text":"A rational design for a weighted finite-state transducer library."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mehryar Mohri"},{"#tail":"\n","#text":"Fernando C N Pereira"},{"#tail":"\n","#text":"Michael Riley"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Grace Ngai and Radu Florian. 2001. Transformation-based learning in the fast lane. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 40?47, Pittsburgh, PA, USA, 2?7 June."},"#text":"\n","pages":{"#tail":"\n","#text":"40--47"},"marker":{"#tail":"\n","#text":"Ngai, Florian, 2001"},"location":{"#tail":"\n","#text":"Pittsburgh, PA, USA,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify co","@endWordPosition":"925","@position":"6006","annotationId":"T33","@startWordPosition":"922","@citStr":"Ngai and Florian, 2001"}},"title":{"#tail":"\n","#text":"Transformation-based learning in the fast lane."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Grace Ngai"},{"#tail":"\n","#text":"Radu Florian"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Adwait Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of the EMNLP Conference, pages 133?142, Philadelphia, PA, USA."},"#text":"\n","pages":{"#tail":"\n","#text":"133--142"},"marker":{"#tail":"\n","#text":"Ratnaparkhi, 1996"},"location":{"#tail":"\n","#text":"Philadelphia, PA, USA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ces each time their functionality is required. We are currently in the process of implementing a POS tagging web service using the gSOAP library, which will translate our C infrastructure binding into web service wrapper code and produce the necessary XML service description files. 9 Conclusion The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components. For instance, implementing an efficient version of the MXPOST POS tagger (Ratnaparkhi, 1996) will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component. The individual components will provide state of the art accuracy and be highly optimised for both time and space efficiency. A key design feature of this infrastructure is that components share a common representation for text and annotations so there is no time spent reading/writing formatted data (e.g. XML) between stages. To make the composition and configuration process eas","@endWordPosition":"3443","@position":"22138","annotationId":"T34","@startWordPosition":"3442","@citStr":"Ratnaparkhi, 1996"}},"title":{"#tail":"\n","#text":"A maximum entropy part-ofspeech tagger."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the EMNLP Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Adwait Ratnaparkhi"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1998"},"institution":{"#tail":"\n","#text":"University of Pennsylvania."},"rawString":{"#tail":"\n","#text":"Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania."},"#text":"\n","marker":{"#tail":"\n","#text":"Ratnaparkhi, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al, 1997; Mohri et al, 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) prov","@endWordPosition":"889","@position":"5753","annotationId":"T35","@startWordPosition":"888","@citStr":"Ratnaparkhi (1998)"}},"title":{"#tail":"\n","#text":"Maximum Entropy Models for Natural Language Ambiguity Resolution."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Adwait Ratnaparkhi"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Emmanuel Roche and Yves Schabes. 1997. Deterministic part-of-speech tagging with finite-state transducers. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, chapter 7. The MIT Press."},"#text":"\n","marker":{"#tail":"\n","#text":"Roche, Schabes, 1997"},"publisher":{"#tail":"\n","#text":"MIT Press."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"creasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify code or reformat data. Finally, the Natural Language Toolkit (NLTK) is a package of NLP components implemented in Python (Loper and Bird, 2002). Python scripting","@endWordPosition":"952","@position":"6165","annotationId":"T36","@startWordPosition":"949","@citStr":"Roche and Schabes, 1997"}},"title":{"#tail":"\n","#text":"Deterministic part-of-speech tagging with finite-state transducers."},"booktitle":{"#tail":"\n","#text":"In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, chapter 7. The"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Emmanuel Roche"},{"#tail":"\n","#text":"Yves Schabes"}]}},{"volume":{"#tail":"\n","#text":"27"},"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Hans van Halteren, Jakub Zavrel, and Walter Daelemans. 2001. Improving accuracy in wordclass tagging through combination of machine learning systems. Computational Linguistics, 27(2):199?229."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"van Halteren, Zavrel, Daelemans, 2001"},"title":{"#tail":"\n","#text":"Improving accuracy in wordclass tagging through combination of machine learning systems."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hans van Halteren"},{"#tail":"\n","#text":"Jakub Zavrel"},{"#tail":"\n","#text":"Walter Daelemans"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Ian H. Witten and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann Publishers."},"#text":"\n","marker":{"#tail":"\n","#text":"Witten, Frank, 1999"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann Publishers."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"s used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify code or reformat data. Finally, the Natural Language Toolkit (NLTK) is a package of NLP components implemented in Python (Loper and Bird, 2002). Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple. 3 Performance Requirements As discussed earlier, there are ","@endWordPosition":"984","@position":"6348","annotationId":"T37","@startWordPosition":"981","@citStr":"Witten and Frank, 1999"},{"#tail":"\n","#text":"mal redundancy between components and makes them easier to understand, implement, test and debug. It also ensures components are maximally composable and extensible. This is particularly important in NLP because of the high redundancy across tasks and approaches. Machine learning methods should be interchangeable: Transformation-based learning (TBL) (Brill, 1993) and Memory-based learning (MBL) (Daelemans et al, 2002) have been applied to many different problems, so a single interchangeable component should be used to represent each method. We will base these components on the design of Weka (Witten and Frank, 1999). Representations should be reusable: for example, named entity classification can be considered as a sequence tagging task or a bag-of-words text classification task. The same beam-search sequence tagging component should be able to be used for POS tagging, chunking and named entity classification. Feature extraction components should be reusable since many NLP components share features, for instance, most sequence taggers use the previously assigned tags. We will use an objectoriented hierarchy of methods, representations and features to allow components to be easily interchanged. This hiera","@endWordPosition":"1531","@position":"9911","annotationId":"T38","@startWordPosition":"1528","@citStr":"Witten and Frank, 1999"}]},"booktitle":{"#tail":"\n","#text":"Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ian H Witten"},{"#tail":"\n","#text":"Eibe Frank"}]}}]}}]}}
