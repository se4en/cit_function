{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Michel Galley, Kathleen McKeown, Julia Hirschberg, and Elizabeth Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies. In Proceedings of ACL, pages 669\u2013676."},"#text":"\n","pages":{"#tail":"\n","#text":"669--676"},"marker":{"#tail":"\n","#text":"Galley, McKeown, Hirschberg, Shriberg, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about con","@endWordPosition":"911","@position":"6171","annotationId":"T1","@startWordPosition":"908","@citStr":"Galley et al., 2004"}},"title":{"#tail":"\n","#text":"Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michel Galley"},{"#tail":"\n","#text":"Kathleen McKeown"},{"#tail":"\n","#text":"Julia Hirschberg"},{"#tail":"\n","#text":"Elizabeth Shriberg"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 710\u2013718, Suntec, Singapore, August."},"#text":"\n","pages":{"#tail":"\n","#text":"710--718"},"marker":{"#tail":"\n","#text":"Garera, Yarowsky, 2009"},"location":{"#tail":"\n","#text":"Suntec, Singapore,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consis","@endWordPosition":"894","@position":"6057","annotationId":"T2","@startWordPosition":"891","@citStr":"Garera and Yarowsky, 2009"}},"title":{"#tail":"\n","#text":"Modeling latent biographic attributes in conversational genres."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nikesh Garera"},{"#tail":"\n","#text":"David Yarowsky"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing, pages 1\u20139."},"#text":"\n","pages":{"#tail":"\n","#text":"1--9"},"marker":{"#tail":"\n","#text":"Giampiccolo, Magnini, Dagan, Dolan, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"f conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 1 Introduction Textual entailment has received increasing attention in recent years (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Textual entailment has mainly focused on inference from written text in monologue. Recent years also observed an increasing amount of conversational data such as conversation scripts of meetings, call center records, court proceedings, as well as","@endWordPosition":"188","@position":"1348","annotationId":"T3","@startWordPosition":"185","@citStr":"Giampiccolo et al., 2007"},{"#tail":"\n","#text":"mpirical findings have shown that augmented representation with conversation structures is important in conversation entailment. Combining conversation structures with explicit modeling of long distance relations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants ","@endWordPosition":"738","@position":"5016","annotationId":"T4","@startWordPosition":"735","@citStr":"Giampiccolo et al., 2007"},{"#tail":"\n","#text":"gnment between two nouns or two verbs. We trained an alignment model for nouns and one for verbs separately. Table 1 summarizes a set of features used in the alignment models. Most of these features are shared by the model for noun alignment and the model for verb alignment. These features include whether the two strings are the same, two terms have the same stem, the similarity between the two terms either based on WordNet or distributional statistics (Lin, 1998). To learn the alignment model for nouns, we annotated the noun alignments for the development data used in PASCAL RTE-3 Challenge (Giampiccolo et al., 2007) and trained a logistic regression model based on the features in Table 1. Cross-validation on the same dataset shows relatively satisfying performance (96.4% precision and 94.9% recall). In this paper, we focus on the alignment between verbs Noun Verb Align. Align. Verb be identification ✓ String equality ✓ ✓ Stemmed equality ✓ ✓ Acronym equality ✓ Named entity equality ✓ WordNet similarity ✓ ✓ Distributional similarity ✓ ✓ Subject consistency ✓ Object consistency ✓ Table 1: Features for alignment models since it appears more difficult. A major difference between noun alignment and verb align","@endWordPosition":"3009","@position":"19200","annotationId":"T5","@startWordPosition":"3006","@citStr":"Giampiccolo et al., 2007"}]},"title":{"#tail":"\n","#text":"The third pascal recognizing textual entailment challenge."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Danilo Giampiccolo"},{"#tail":"\n","#text":"Bernardo Magnini"},{"#tail":"\n","#text":"Ido Dagan"},{"#tail":"\n","#text":"Bill Dolan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2007. Extracting social networks and biographical facts from conversational speech transcripts. In Proceedings of ACL, pages 1040\u20131047."},"#text":"\n","pages":{"#tail":"\n","#text":"1040--1047"},"marker":{"#tail":"\n","#text":"Jing, Kambhatla, Roukos, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nt RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hy","@endWordPosition":"901","@position":"6094","annotationId":"T6","@startWordPosition":"898","@citStr":"Jing et al., 2007"}},"title":{"#tail":"\n","#text":"Extracting social networks and biographical facts from conversational speech transcripts."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hongyan Jing"},{"#tail":"\n","#text":"Nanda Kambhatla"},{"#tail":"\n","#text":"Salim Roukos"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Gabriel Murray and Giuseppe Carenini. 2008. Summarizing spoken and written conversations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773\u2013782, Honolulu, Hawaii, October."},"#text":"\n","pages":{"#tail":"\n","#text":"773--782"},"marker":{"#tail":"\n","#text":"Murray, Carenini, 2008"},"location":{"#tail":"\n","#text":"Honolulu, Hawaii,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, t","@endWordPosition":"925","@position":"6273","annotationId":"T7","@startWordPosition":"922","@citStr":"Murray and Carenini, 2008"}},"title":{"#tail":"\n","#text":"Summarizing spoken and written conversations."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Gabriel Murray"},{"#tail":"\n","#text":"Giuseppe Carenini"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 172\u2013180, Los Angeles, California, June."},"#text":"\n","pages":{"#tail":"\n","#text":"172--180"},"marker":{"#tail":"\n","#text":"Ritter, Cherry, Dolan, 2010"},"location":{"#tail":"\n","#text":"Los Angeles, California,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"on data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that i","@endWordPosition":"942","@position":"6382","annotationId":"T8","@startWordPosition":"939","@citStr":"Ritter et al., 2010"}},"title":{"#tail":"\n","#text":"Unsupervised modeling of twitter conversations."},"booktitle":{"#tail":"\n","#text":"In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alan Ritter"},{"#tail":"\n","#text":"Colin Cherry"},{"#tail":"\n","#text":"Bill Dolan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 170\u2013179, Singapore, August."},"#text":"\n","pages":{"#tail":"\n","#text":"170--179"},"marker":{"#tail":"\n","#text":"Somasundaran, Namata, Wiebe, Getoor, 2009"},"location":{"#tail":"\n","#text":"Singapore,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collect","@endWordPosition":"887","@position":"6006","annotationId":"T9","@startWordPosition":"884","@citStr":"Somasundaran et al., 2009"}},"title":{"#tail":"\n","#text":"Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Swapna Somasundaran"},{"#tail":"\n","#text":"Galileo Namata"},{"#tail":"\n","#text":"Janyce Wiebe"},{"#tail":"\n","#text":"Lise Getoor"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Chen Zhang and Joyce Chai. 2009. What do we know about conversation participants: Experiments on conversation entailment. In Proceedings of the SIGDIAL 2009 Conference, pages 206\u2013215."},"#text":"\n","pages":{"#tail":"\n","#text":"206--215"},"marker":{"#tail":"\n","#text":"Zhang, Chai, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"grounding between participants, different linguistic phenomena of utterances, and conversation implicatures. Traditional approaches dealing with textual entailment were not designed to handle these unique conversation behaviors and thus to support automated entailment from conversation scripts. Example 1: Conversation Segment: B: My mother also was very very independent. She had her own, still had her own little house and still driving her own car, A: Yeah. B: at age eighty-three. Hypothesis: (1) B\u2019s mother is eighty-three. (2) B is eighty-three. To address this limitation, our previous work (Zhang and Chai, 2009) has initiated an investigation on the problem of conversation entailment. The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the 756 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756\u2013766, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics conversation segment while the second hypothesis cannot. While our previous work ha","@endWordPosition":"402","@position":"2776","annotationId":"T10","@startWordPosition":"399","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system","@endWordPosition":"963","@position":"6525","annotationId":"T11","@startWordPosition":"960","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" for textual entailment: an alignment stage followed by an entailment stage. Building upon our previous work, in this paper, we systematically examine different representations of the conversation segment and different modeling of long distance relations between language constituents. We compare the roles of these different representations on the performance of entailment prediction using a larger testing dataset that was not previously evaluated. This analysis allows better understanding of the problem and provides insight on 757 potential solutions. 3 Overall Framework In our previous work (Zhang and Chai, 2009), conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d1 ∧ ... ∧ dm, and a hypothesis H represented by another set of clauses H = h1 ∧ ... ∧ hn, the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows. This is based on a simple assumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses. P(D I-- H|D, H) = P(D I-- h1, ... , ","@endWordPosition":"1164","@position":"7863","annotationId":"T12","@startWordPosition":"1161","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" verb and its aligned object is used as a measure of the object consistency. In Example 2, to decide whether the conversation term see (x,, in Figure 1(a), 1(b), and 2) and the hypothesis term watch (x4 in Figure 1(c), 1(d)) should be aligned, we first identify the subject of x4 in the hypothesis, which is x2 (A). We then look for 761 x2\u2019s alignments in the conversation segment, among which x9 (You) is the closest to x11 (see). In Figure 2(a), we find the distance between x11 and x9 is 3. Using the implicit modeling of argument consistency, we follow the same approach as in our previous work (Zhang and Chai, 2009) and trained a logistic regression model to predict verb alignment based on the features in Table 1. 6.1.2 Explicit Modeling of AC The second approach captures argument consistency based on explicit modeling of the relationship between a verb and its aligned subject (or object). Given a pair of verb terms (x, y), let sy be the subject of y and sx be the aligned entity of sy in the conversation closest to x, we use the string describing the path from x to sx as the feature to capture subject consistency. For example, in Figure 2(a), the path from x11 to x9 is V \u2014* V \u2014* V +\u2014 N. This string repre","@endWordPosition":"3439","@position":"21714","annotationId":"T13","@startWordPosition":"3436","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"easure the closeness between any two verbs. Again this model is trained from our development data described in Zhang and Chai (2009). Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x5 = suggests from the hypothesis and u4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in (Zhang and Chai, 2009). 6.2 Applications in Inference Model As mentioned earlier, once an alignment is established, the inference model is to predict whether each clause in the hypothesis is entailed from the conversation segment. Two separate models were Conversation Segment sB sA x1=A x2=Sleeping with the Enemy x3=seen x4=have x5=A x6=that x7=is really great x8=have heard x9=A x10=one x11=see x12=go x13=have u1=yes_no_question u2=no_answer u3=statement u4=opinion Figure 3: The alignment result for Example 2 used to handle the inference of property clauses (hj(x)) and and the inference of relational clauses (hj(x,","@endWordPosition":"3732","@position":"23389","annotationId":"T14","@startWordPosition":"3729","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"gmented representation Figure 4: Evaluation of verb alignment were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants\u2019 beliefs and opinions (34% and 35%); (3) desire: participants\u2019 desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g. A stops B from doing something; A disagreees with B on something, 8% and 12%) Note that in our original work (Zhang and Chai, 2009), only development data were used to show some initial observations. Here we trained our models on the development data and results shown are from the testing data. 7.1 Evaluation of Alignment Models The evaluation of alignment models is based on pairwise decision. For each pair of terms (x, y), where x is from a conversation segment and y is from a hypothesis, we measure whether the model correctly predicts that the two terms should or should not be aligned. Because the alignment classification has extremely unbalanced classes, we use precisionrecall of true alignments as evaluation metrics. ","@endWordPosition":"4485","@position":"27865","annotationId":"T15","@startWordPosition":"4482","@citStr":"Zhang and Chai, 2009"}]},"title":{"#tail":"\n","#text":"What do we know about conversation participants: Experiments on conversation entailment."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the SIGDIAL 2009 Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chen Zhang"},{"#tail":"\n","#text":"Joyce Chai"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Chen Zhang and Joyce Chai. 2009. What do we know about conversation participants: Experiments on conversation entailment. In Proceedings of the SIGDIAL 2009 Conference, pages 206\u2013215."},"#text":"\n","pages":{"#tail":"\n","#text":"206--215"},"marker":{"#tail":"\n","#text":"Zhang, Chai, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"grounding between participants, different linguistic phenomena of utterances, and conversation implicatures. Traditional approaches dealing with textual entailment were not designed to handle these unique conversation behaviors and thus to support automated entailment from conversation scripts. Example 1: Conversation Segment: B: My mother also was very very independent. She had her own, still had her own little house and still driving her own car, A: Yeah. B: at age eighty-three. Hypothesis: (1) B\u2019s mother is eighty-three. (2) B is eighty-three. To address this limitation, our previous work (Zhang and Chai, 2009) has initiated an investigation on the problem of conversation entailment. The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the 756 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756\u2013766, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics conversation segment while the second hypothesis cannot. While our previous work ha","@endWordPosition":"402","@position":"2776","annotationId":"T16","@startWordPosition":"399","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system","@endWordPosition":"963","@position":"6525","annotationId":"T17","@startWordPosition":"960","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" for textual entailment: an alignment stage followed by an entailment stage. Building upon our previous work, in this paper, we systematically examine different representations of the conversation segment and different modeling of long distance relations between language constituents. We compare the roles of these different representations on the performance of entailment prediction using a larger testing dataset that was not previously evaluated. This analysis allows better understanding of the problem and provides insight on 757 potential solutions. 3 Overall Framework In our previous work (Zhang and Chai, 2009), conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d1 ∧ ... ∧ dm, and a hypothesis H represented by another set of clauses H = h1 ∧ ... ∧ hn, the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows. This is based on a simple assumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses. P(D I-- H|D, H) = P(D I-- h1, ... , ","@endWordPosition":"1164","@position":"7863","annotationId":"T18","@startWordPosition":"1161","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" verb and its aligned object is used as a measure of the object consistency. In Example 2, to decide whether the conversation term see (x,, in Figure 1(a), 1(b), and 2) and the hypothesis term watch (x4 in Figure 1(c), 1(d)) should be aligned, we first identify the subject of x4 in the hypothesis, which is x2 (A). We then look for 761 x2\u2019s alignments in the conversation segment, among which x9 (You) is the closest to x11 (see). In Figure 2(a), we find the distance between x11 and x9 is 3. Using the implicit modeling of argument consistency, we follow the same approach as in our previous work (Zhang and Chai, 2009) and trained a logistic regression model to predict verb alignment based on the features in Table 1. 6.1.2 Explicit Modeling of AC The second approach captures argument consistency based on explicit modeling of the relationship between a verb and its aligned subject (or object). Given a pair of verb terms (x, y), let sy be the subject of y and sx be the aligned entity of sy in the conversation closest to x, we use the string describing the path from x to sx as the feature to capture subject consistency. For example, in Figure 2(a), the path from x11 to x9 is V \u2014* V \u2014* V +\u2014 N. This string repre","@endWordPosition":"3439","@position":"21714","annotationId":"T19","@startWordPosition":"3436","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"easure the closeness between any two verbs. Again this model is trained from our development data described in Zhang and Chai (2009). Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x5 = suggests from the hypothesis and u4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in (Zhang and Chai, 2009). 6.2 Applications in Inference Model As mentioned earlier, once an alignment is established, the inference model is to predict whether each clause in the hypothesis is entailed from the conversation segment. Two separate models were Conversation Segment sB sA x1=A x2=Sleeping with the Enemy x3=seen x4=have x5=A x6=that x7=is really great x8=have heard x9=A x10=one x11=see x12=go x13=have u1=yes_no_question u2=no_answer u3=statement u4=opinion Figure 3: The alignment result for Example 2 used to handle the inference of property clauses (hj(x)) and and the inference of relational clauses (hj(x,","@endWordPosition":"3732","@position":"23389","annotationId":"T20","@startWordPosition":"3729","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"gmented representation Figure 4: Evaluation of verb alignment were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants\u2019 beliefs and opinions (34% and 35%); (3) desire: participants\u2019 desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g. A stops B from doing something; A disagreees with B on something, 8% and 12%) Note that in our original work (Zhang and Chai, 2009), only development data were used to show some initial observations. Here we trained our models on the development data and results shown are from the testing data. 7.1 Evaluation of Alignment Models The evaluation of alignment models is based on pairwise decision. For each pair of terms (x, y), where x is from a conversation segment and y is from a hypothesis, we measure whether the model correctly predicts that the two terms should or should not be aligned. Because the alignment classification has extremely unbalanced classes, we use precisionrecall of true alignments as evaluation metrics. ","@endWordPosition":"4485","@position":"27865","annotationId":"T21","@startWordPosition":"4482","@citStr":"Zhang and Chai, 2009"}]},"title":{"#tail":"\n","#text":"What do we know about conversation participants: Experiments on conversation entailment."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the SIGDIAL 2009 Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chen Zhang"},{"#tail":"\n","#text":"Joyce Chai"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Chen Zhang and Joyce Chai. 2009. What do we know about conversation participants: Experiments on conversation entailment. In Proceedings of the SIGDIAL 2009 Conference, pages 206\u2013215."},"#text":"\n","pages":{"#tail":"\n","#text":"206--215"},"marker":{"#tail":"\n","#text":"Zhang, Chai, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"grounding between participants, different linguistic phenomena of utterances, and conversation implicatures. Traditional approaches dealing with textual entailment were not designed to handle these unique conversation behaviors and thus to support automated entailment from conversation scripts. Example 1: Conversation Segment: B: My mother also was very very independent. She had her own, still had her own little house and still driving her own car, A: Yeah. B: at age eighty-three. Hypothesis: (1) B\u2019s mother is eighty-three. (2) B is eighty-three. To address this limitation, our previous work (Zhang and Chai, 2009) has initiated an investigation on the problem of conversation entailment. The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the 756 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756\u2013766, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics conversation segment while the second hypothesis cannot. While our previous work ha","@endWordPosition":"402","@position":"2776","annotationId":"T22","@startWordPosition":"399","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system","@endWordPosition":"963","@position":"6525","annotationId":"T23","@startWordPosition":"960","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" for textual entailment: an alignment stage followed by an entailment stage. Building upon our previous work, in this paper, we systematically examine different representations of the conversation segment and different modeling of long distance relations between language constituents. We compare the roles of these different representations on the performance of entailment prediction using a larger testing dataset that was not previously evaluated. This analysis allows better understanding of the problem and provides insight on 757 potential solutions. 3 Overall Framework In our previous work (Zhang and Chai, 2009), conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d1 ∧ ... ∧ dm, and a hypothesis H represented by another set of clauses H = h1 ∧ ... ∧ hn, the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows. This is based on a simple assumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses. P(D I-- H|D, H) = P(D I-- h1, ... , ","@endWordPosition":"1164","@position":"7863","annotationId":"T24","@startWordPosition":"1161","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" verb and its aligned object is used as a measure of the object consistency. In Example 2, to decide whether the conversation term see (x,, in Figure 1(a), 1(b), and 2) and the hypothesis term watch (x4 in Figure 1(c), 1(d)) should be aligned, we first identify the subject of x4 in the hypothesis, which is x2 (A). We then look for 761 x2\u2019s alignments in the conversation segment, among which x9 (You) is the closest to x11 (see). In Figure 2(a), we find the distance between x11 and x9 is 3. Using the implicit modeling of argument consistency, we follow the same approach as in our previous work (Zhang and Chai, 2009) and trained a logistic regression model to predict verb alignment based on the features in Table 1. 6.1.2 Explicit Modeling of AC The second approach captures argument consistency based on explicit modeling of the relationship between a verb and its aligned subject (or object). Given a pair of verb terms (x, y), let sy be the subject of y and sx be the aligned entity of sy in the conversation closest to x, we use the string describing the path from x to sx as the feature to capture subject consistency. For example, in Figure 2(a), the path from x11 to x9 is V \u2014* V \u2014* V +\u2014 N. This string repre","@endWordPosition":"3439","@position":"21714","annotationId":"T25","@startWordPosition":"3436","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"easure the closeness between any two verbs. Again this model is trained from our development data described in Zhang and Chai (2009). Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x5 = suggests from the hypothesis and u4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in (Zhang and Chai, 2009). 6.2 Applications in Inference Model As mentioned earlier, once an alignment is established, the inference model is to predict whether each clause in the hypothesis is entailed from the conversation segment. Two separate models were Conversation Segment sB sA x1=A x2=Sleeping with the Enemy x3=seen x4=have x5=A x6=that x7=is really great x8=have heard x9=A x10=one x11=see x12=go x13=have u1=yes_no_question u2=no_answer u3=statement u4=opinion Figure 3: The alignment result for Example 2 used to handle the inference of property clauses (hj(x)) and and the inference of relational clauses (hj(x,","@endWordPosition":"3732","@position":"23389","annotationId":"T26","@startWordPosition":"3729","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"gmented representation Figure 4: Evaluation of verb alignment were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants\u2019 beliefs and opinions (34% and 35%); (3) desire: participants\u2019 desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g. A stops B from doing something; A disagreees with B on something, 8% and 12%) Note that in our original work (Zhang and Chai, 2009), only development data were used to show some initial observations. Here we trained our models on the development data and results shown are from the testing data. 7.1 Evaluation of Alignment Models The evaluation of alignment models is based on pairwise decision. For each pair of terms (x, y), where x is from a conversation segment and y is from a hypothesis, we measure whether the model correctly predicts that the two terms should or should not be aligned. Because the alignment classification has extremely unbalanced classes, we use precisionrecall of true alignments as evaluation metrics. ","@endWordPosition":"4485","@position":"27865","annotationId":"T27","@startWordPosition":"4482","@citStr":"Zhang and Chai, 2009"}]},"title":{"#tail":"\n","#text":"What do we know about conversation participants: Experiments on conversation entailment."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the SIGDIAL 2009 Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chen Zhang"},{"#tail":"\n","#text":"Joyce Chai"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Chen Zhang and Joyce Chai. 2009. What do we know about conversation participants: Experiments on conversation entailment. In Proceedings of the SIGDIAL 2009 Conference, pages 206\u2013215."},"#text":"\n","pages":{"#tail":"\n","#text":"206--215"},"marker":{"#tail":"\n","#text":"Zhang, Chai, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"grounding between participants, different linguistic phenomena of utterances, and conversation implicatures. Traditional approaches dealing with textual entailment were not designed to handle these unique conversation behaviors and thus to support automated entailment from conversation scripts. Example 1: Conversation Segment: B: My mother also was very very independent. She had her own, still had her own little house and still driving her own car, A: Yeah. B: at age eighty-three. Hypothesis: (1) B\u2019s mother is eighty-three. (2) B is eighty-three. To address this limitation, our previous work (Zhang and Chai, 2009) has initiated an investigation on the problem of conversation entailment. The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the 756 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756\u2013766, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics conversation segment while the second hypothesis cannot. While our previous work ha","@endWordPosition":"402","@position":"2776","annotationId":"T28","@startWordPosition":"399","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system","@endWordPosition":"963","@position":"6525","annotationId":"T29","@startWordPosition":"960","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" for textual entailment: an alignment stage followed by an entailment stage. Building upon our previous work, in this paper, we systematically examine different representations of the conversation segment and different modeling of long distance relations between language constituents. We compare the roles of these different representations on the performance of entailment prediction using a larger testing dataset that was not previously evaluated. This analysis allows better understanding of the problem and provides insight on 757 potential solutions. 3 Overall Framework In our previous work (Zhang and Chai, 2009), conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d1 ∧ ... ∧ dm, and a hypothesis H represented by another set of clauses H = h1 ∧ ... ∧ hn, the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows. This is based on a simple assumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses. P(D I-- H|D, H) = P(D I-- h1, ... , ","@endWordPosition":"1164","@position":"7863","annotationId":"T30","@startWordPosition":"1161","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" verb and its aligned object is used as a measure of the object consistency. In Example 2, to decide whether the conversation term see (x,, in Figure 1(a), 1(b), and 2) and the hypothesis term watch (x4 in Figure 1(c), 1(d)) should be aligned, we first identify the subject of x4 in the hypothesis, which is x2 (A). We then look for 761 x2\u2019s alignments in the conversation segment, among which x9 (You) is the closest to x11 (see). In Figure 2(a), we find the distance between x11 and x9 is 3. Using the implicit modeling of argument consistency, we follow the same approach as in our previous work (Zhang and Chai, 2009) and trained a logistic regression model to predict verb alignment based on the features in Table 1. 6.1.2 Explicit Modeling of AC The second approach captures argument consistency based on explicit modeling of the relationship between a verb and its aligned subject (or object). Given a pair of verb terms (x, y), let sy be the subject of y and sx be the aligned entity of sy in the conversation closest to x, we use the string describing the path from x to sx as the feature to capture subject consistency. For example, in Figure 2(a), the path from x11 to x9 is V \u2014* V \u2014* V +\u2014 N. This string repre","@endWordPosition":"3439","@position":"21714","annotationId":"T31","@startWordPosition":"3436","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"easure the closeness between any two verbs. Again this model is trained from our development data described in Zhang and Chai (2009). Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x5 = suggests from the hypothesis and u4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in (Zhang and Chai, 2009). 6.2 Applications in Inference Model As mentioned earlier, once an alignment is established, the inference model is to predict whether each clause in the hypothesis is entailed from the conversation segment. Two separate models were Conversation Segment sB sA x1=A x2=Sleeping with the Enemy x3=seen x4=have x5=A x6=that x7=is really great x8=have heard x9=A x10=one x11=see x12=go x13=have u1=yes_no_question u2=no_answer u3=statement u4=opinion Figure 3: The alignment result for Example 2 used to handle the inference of property clauses (hj(x)) and and the inference of relational clauses (hj(x,","@endWordPosition":"3732","@position":"23389","annotationId":"T32","@startWordPosition":"3729","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"gmented representation Figure 4: Evaluation of verb alignment were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants\u2019 beliefs and opinions (34% and 35%); (3) desire: participants\u2019 desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g. A stops B from doing something; A disagreees with B on something, 8% and 12%) Note that in our original work (Zhang and Chai, 2009), only development data were used to show some initial observations. Here we trained our models on the development data and results shown are from the testing data. 7.1 Evaluation of Alignment Models The evaluation of alignment models is based on pairwise decision. For each pair of terms (x, y), where x is from a conversation segment and y is from a hypothesis, we measure whether the model correctly predicts that the two terms should or should not be aligned. Because the alignment classification has extremely unbalanced classes, we use precisionrecall of true alignments as evaluation metrics. ","@endWordPosition":"4485","@position":"27865","annotationId":"T33","@startWordPosition":"4482","@citStr":"Zhang and Chai, 2009"}]},"title":{"#tail":"\n","#text":"What do we know about conversation participants: Experiments on conversation entailment."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the SIGDIAL 2009 Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chen Zhang"},{"#tail":"\n","#text":"Joyce Chai"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Chen Zhang and Joyce Chai. 2009. What do we know about conversation participants: Experiments on conversation entailment. In Proceedings of the SIGDIAL 2009 Conference, pages 206\u2013215."},"#text":"\n","pages":{"#tail":"\n","#text":"206--215"},"marker":{"#tail":"\n","#text":"Zhang, Chai, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"grounding between participants, different linguistic phenomena of utterances, and conversation implicatures. Traditional approaches dealing with textual entailment were not designed to handle these unique conversation behaviors and thus to support automated entailment from conversation scripts. Example 1: Conversation Segment: B: My mother also was very very independent. She had her own, still had her own little house and still driving her own car, A: Yeah. B: at age eighty-three. Hypothesis: (1) B\u2019s mother is eighty-three. (2) B is eighty-three. To address this limitation, our previous work (Zhang and Chai, 2009) has initiated an investigation on the problem of conversation entailment. The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the 756 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756\u2013766, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics conversation segment while the second hypothesis cannot. While our previous work ha","@endWordPosition":"402","@position":"2776","annotationId":"T34","@startWordPosition":"399","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system","@endWordPosition":"963","@position":"6525","annotationId":"T35","@startWordPosition":"960","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" for textual entailment: an alignment stage followed by an entailment stage. Building upon our previous work, in this paper, we systematically examine different representations of the conversation segment and different modeling of long distance relations between language constituents. We compare the roles of these different representations on the performance of entailment prediction using a larger testing dataset that was not previously evaluated. This analysis allows better understanding of the problem and provides insight on 757 potential solutions. 3 Overall Framework In our previous work (Zhang and Chai, 2009), conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d1 ∧ ... ∧ dm, and a hypothesis H represented by another set of clauses H = h1 ∧ ... ∧ hn, the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows. This is based on a simple assumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses. P(D I-- H|D, H) = P(D I-- h1, ... , ","@endWordPosition":"1164","@position":"7863","annotationId":"T36","@startWordPosition":"1161","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":" verb and its aligned object is used as a measure of the object consistency. In Example 2, to decide whether the conversation term see (x,, in Figure 1(a), 1(b), and 2) and the hypothesis term watch (x4 in Figure 1(c), 1(d)) should be aligned, we first identify the subject of x4 in the hypothesis, which is x2 (A). We then look for 761 x2\u2019s alignments in the conversation segment, among which x9 (You) is the closest to x11 (see). In Figure 2(a), we find the distance between x11 and x9 is 3. Using the implicit modeling of argument consistency, we follow the same approach as in our previous work (Zhang and Chai, 2009) and trained a logistic regression model to predict verb alignment based on the features in Table 1. 6.1.2 Explicit Modeling of AC The second approach captures argument consistency based on explicit modeling of the relationship between a verb and its aligned subject (or object). Given a pair of verb terms (x, y), let sy be the subject of y and sx be the aligned entity of sy in the conversation closest to x, we use the string describing the path from x to sx as the feature to capture subject consistency. For example, in Figure 2(a), the path from x11 to x9 is V \u2014* V \u2014* V +\u2014 N. This string repre","@endWordPosition":"3439","@position":"21714","annotationId":"T37","@startWordPosition":"3436","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"easure the closeness between any two verbs. Again this model is trained from our development data described in Zhang and Chai (2009). Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x5 = suggests from the hypothesis and u4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in (Zhang and Chai, 2009). 6.2 Applications in Inference Model As mentioned earlier, once an alignment is established, the inference model is to predict whether each clause in the hypothesis is entailed from the conversation segment. Two separate models were Conversation Segment sB sA x1=A x2=Sleeping with the Enemy x3=seen x4=have x5=A x6=that x7=is really great x8=have heard x9=A x10=one x11=see x12=go x13=have u1=yes_no_question u2=no_answer u3=statement u4=opinion Figure 3: The alignment result for Example 2 used to handle the inference of property clauses (hj(x)) and and the inference of relational clauses (hj(x,","@endWordPosition":"3732","@position":"23389","annotationId":"T38","@startWordPosition":"3729","@citStr":"Zhang and Chai, 2009"},{"#tail":"\n","#text":"gmented representation Figure 4: Evaluation of verb alignment were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants\u2019 beliefs and opinions (34% and 35%); (3) desire: participants\u2019 desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g. A stops B from doing something; A disagreees with B on something, 8% and 12%) Note that in our original work (Zhang and Chai, 2009), only development data were used to show some initial observations. Here we trained our models on the development data and results shown are from the testing data. 7.1 Evaluation of Alignment Models The evaluation of alignment models is based on pairwise decision. For each pair of terms (x, y), where x is from a conversation segment and y is from a hypothesis, we measure whether the model correctly predicts that the two terms should or should not be aligned. Because the alignment classification has extremely unbalanced classes, we use precisionrecall of true alignments as evaluation metrics. ","@endWordPosition":"4485","@position":"27865","annotationId":"T39","@startWordPosition":"4482","@citStr":"Zhang and Chai, 2009"}]},"title":{"#tail":"\n","#text":"What do we know about conversation participants: Experiments on conversation entailment."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the SIGDIAL 2009 Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chen Zhang"},{"#tail":"\n","#text":"Joyce Chai"}]}}]}}}}
