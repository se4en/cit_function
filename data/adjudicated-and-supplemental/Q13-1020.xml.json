{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.8441995","#text":"\na. Determine that the left child of PRP?RB is\na one-word non-terminal (labeled with PRP);\nb. Expand PRP and generate the word ?we? for\nPRP;\n6 In our experiment, we set pexpand to 1/3 to encourage\nsmall tree fragments.\n"},{"#tail":"\n","@confidence":"0.992840428571429","#text":"\nc. Determine that the right child of PRP?RB is\na two-word non-terminal;\nd. Utilize the predetermined RB and a POS tag\nVBP to form the tag of the two-word non-\nterminal: VBP+RB;\ne. Expand VBP+RB (to VBP and RB);\nf. Do not expand VBP and RB.\n"},{"#tail":"\n","@confidence":"0.736187727272727","#text":"\na. Decide to generate one source word;\nb. Generate the source word ??? (wo-men) ?;\nc. Insert the first variable after the word;\nd. Insert the second variable between the word\nand the first variable.\nIntuitively, a good translation grammar should\ncarry both small translation rules with enough\ngenerality and large rules with enough context\ninformation. DeNero and Klein (2007) proposed\nthis statement, and Cohn and Blunsom (2009) has\nverified it in their experiments with parse trees.\n"}],"figure":[{"#tail":"\n","@confidence":"0.957670076923077","#text":"\njin-tian jian-mianwo-men zai-ci\nPRP+VBP\ntoday\nNN\nwe\nPRP\nmeet\nVBP\nagain\nRB\n?? ?? ?? ??\nPRP...RB\nNN...RB\n"},{"#tail":"\n","@confidence":"0.995415166666667","#text":"\njian-mianwo-men zai-ci\ns-node\nwe\nPRP\nmeet\nVBP\nagain\nRB\n?? ?? ??\nPRP...RB\nPRP+VBP\njian-mianwo-men zai-ci\ns-node\nwe\nPRP\nmeet\nVBP\nagain\nRB\n?? ?? ??\nPRP...RB\nVBP+RB\n(b) ?=1(a) ?=0\nRotate\n"},{"#tail":"\n","@confidence":"0.994422035714286","#text":"\njin-tian jian-mianwo-men zai-ci\nPRP+VBP\nToday\nNN\nwe\nPRP\nmeet\nVBP\nagain\nRB\n?? ?? ?? ??\nNN...VBP\nNN...RB\njin-tian jian-mianwo-men zai-ci\nVBP+RB\nToday\nNN\nwe\nPRP\nmeet\nVBP\nagain\nRB\n?? ?? ?? ??\nPRP...RB\nNN...RB\n(a) ?=0 (b) ?=1\nTwo-level-right-Rotate\n"},{"#tail":"\n","@confidence":"0.994195535714285","#text":"\n1.239E+08\n1.243E+08\n1.247E+08\n1.251E+08\n1.255E+08\n1.259E+08\n100 200 300 400 500 600 700 800 900 1000\nNumber of Sampling Iterations\nN\ne\ng\na\nti\nv\ne\n-L\no\ng\nL\nik\ne\nli\nh\no\no\nd random 1\nrandom 2\nrandom 3\n"},{"#tail":"\n","@confidence":"0.994121941176471","#text":"\n1.035E+07\n1.040E+07\n1.045E+07\n1.050E+07\n1.055E+07\n1.060E+07\n100 200 300 400 500 600 700 800 900 1000\nNumber of Sampling Iterations\nT\no\nta\nl\nN\nu\nm\nb\ne\nr\no\nf\nF\nro\nn\nti\ne\nr\nN\no\nd\ne\ns\nrandom 1\nrandom 2\nrandom 3\n"},{"#tail":"\n","@confidence":"0.993720454545455","#text":"\n0\n200k\n400k\n600k\n800k\n1000k\n2 3 4 5 6 7 8 9 10 >=11\nU-Tree\nbinary parse tree\nNumber of Nodes in the Target Tree Fragment\nN\num\nbe\nr\nof\nR\nul\nes\nNumber of Words and Variables in the Source String\n0\n300k\n600k\n900k\n1200k\n1 2 3 4 5 6 7\nN\num\nbe\nr\nof\nR\nul\nes\n"},{"#tail":"\n","@confidence":"0.9927321875","#text":"\nwas\nQP\ndollarsUS1500only\nVBD NNSNNPCDRB\nNP\nNP\n? ???????\nNP-COMP\n(a) binary parse tree\n(b) U-tree\nwas dollarsUS1500only\nVBD NNSNNPCDRB\n? ???????\nVBD+RB NNP+NNS\nCD...NNS\nVBD...NNS\n"},{"#tail":"\n","@confidence":"0.974141785714286","#text":"\nitalic nodes with shadows denote frontier nodes.\nwas QP\nonly\nVBD\nCD:x0RB\nNP\nNP\nNP-COMP:x1\nwas only\nVBD RB\n? ?VBD+RB\n? x1x0?\nR1:\nR2:\n"}],"address":{"#tail":"\n","@confidence":"0.264491","#text":"\nSubmitted 12/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.\n"},"author":{"#tail":"\n","@confidence":"0.969408","#text":"\nFeifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong\n"},"equation":[{"#tail":"\n","@confidence":"0.965106","#text":"\ni tN is the set of target non-terminals,\ni t tS N? is the start root non-terminal, and\ni P is the production rule set.\n"},{"#tail":"\n","@confidence":"0.963643285714286","#text":"\nPRP...RB\nwe\nPRP\nVBP:x0 RB:x1\nVBP+RB\n?? x1 x0\nwo-men\n"},{"#tail":"\n","@confidence":"0.970463285714286","#text":"\n1\n( ) (  |)\nn\ni i\ni\np d p r N\n? (1)\n"},{"#tail":"\n","@confidence":"0.9743926","#text":"\non it:\n0 0\n |~ ( )\n |, ~ ( , (  |) )\nN\nN N N\nr N Multi\nP DP P N\nT\nT D D < (2)\n"},{"#tail":"\n","@confidence":"0.95689965","#text":"\n0\n0\n(  |)\n(  |, , , ) i\ni\nr N ii\ni N i\nN N\nn P r N\np r r N P\nn\nDD D\n\n\n\n\n (3)\nWhere n-i ri denotes the number of ri in ir , and n\n-i\nN\n"},{"#tail":"\n","@confidence":"0.950376","#text":"\n0 (  |) (  |) (  |)P r N P frag N P str frag ? (4)\n"},{"#tail":"\n","@confidence":"0.587877","#text":"\n(  |)P str frag in Equation (4) is the probability of\n"},{"#tail":"\n","@confidence":"0.9518595","#text":"\nvar\n1\n1 1\n(  |) ( ;1)\n ||\npoisson sw\nsw\nsw\nc\nc\ns i\nP str frag P c\nc i\nu u? ? (5)\n"},{"#tail":"\n","@confidence":"0.7939455","#text":"\n0 2 1\n0 1 2\n:\n... ( ( : : ) : )\nleftr x x x\nPRP RB PRP VBP x PRP x VBP x RB\no\n\n"},{"#tail":"\n","@confidence":"0.99843225","#text":"\n0 0 1 0 1\n1 1 0 0 1\n: ... ( : : )\n: ( : : )\nright\nright\nr x x PRP RB x PRP x VBP RB\nr x x VBP RB x VBP x RB\n\n\no \no \n"},{"#tail":"\n","@confidence":"0.970156833333334","#text":"\n0 1\n0 1 0\n( 0) (  |)\n( 1) ( ,  |)\n(  |) (  |, )\nleft\nright right\nright right right\np p r r\np p r r r\np r r p r r r\n\n\n \n \n  \n< v\n< v\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.997138","#text":"\n4.1 Base Distribution\n"},{"#tail":"\n","@confidence":"0.990365","#text":"\n5.1 Initialization State\n"},{"#tail":"\n","@confidence":"0.986851","#text":"\n5.2 The Gibbs Operators\n"},{"#tail":"\n","@confidence":"0.982542","#text":"\n6.1 Experimental Setup\n"},{"#tail":"\n","@confidence":"0.997379","#text":"\n6.2 Analysis of The Gibbs Sampler\n"},{"#tail":"\n","@confidence":"0.997813","#text":"\n6.3 Analysis of the U-tree Structure\n"},{"#tail":"\n","@confidence":"0.983159","#text":"\n6.4 Final Translation Results\n"},{"#tail":"\n","@confidence":"0.997674","#text":"\n6.5 Large Data\n"}],"title":{"#tail":"\n","@confidence":"0.921928","#text":"\nUnsupervised Tree Induction for Tree-based Translation\n"},"@confidence":"0.000000","reference":[{"#tail":"\n","@confidence":"0.624468","#text":"\nLDC2003E07, LDC2004T07, LDC2005T06,\nLDC2002L27, LDC2005T10 and LDC2005T34.\n"},{"#tail":"\n","@confidence":"0.999399942028986","#text":"\nPhil Blunsom, Trevor Cohn, Miles Osborne. 2008.\nBayesian synchronous grammar induction. In\nAdvances in Neural Information Processing Systems,\nvolume 21, pages 161-168.\nPhil Blunsom, Trevor Cohn, Chris Dyer, and Miles\nOsborne. 2009. A gibbs sampler for phrasal\nsynchronous grammar induction. In Proc. of ACL\n2009, pages 782-790.\nPhil Blunsom and Trevor Cohn. 2010. Inducing\nsynchronous grammars with slice sampling. In Proc.\nof NAACL 2010, pages 238-241.\nDavid Burkett and Dan Klein. 2008. Two languages are\nbetter than one (for syntactic Parsing). In Proc. of\nEMNLP 2008, pages 877-886.\nDavid Burkett, John Blitzer, and Dan Klein. 2010. Joint\nparsing and alignment with weakly synchronized\ngrammars. In Proc. of NAACL 2010, pages 127-135.\nDavid Burkett and Dan Klein. 2012. Transforming trees\nto improve syntactic convergence. In Proc. of\nEMNLP 2012, pages 863-872.\nDavid Chiang. 2007. Hierarchical phrase-based\ntranslation. Computational Linguistics, 33 (2). pages\n201-228.\nDekai Wu. 1996. A polynomial-time algorithm for\nstatistical machine translation. In Proc. of ACL 1996,\npages 152-158.\nDekai Wu. 1997. Stochastic inversion transduction\ngrammars and bilingual parsing of parallel corpora.\nComputational Linguistics, 23:377-404.\nTrevor Cohn and Phil Blunsom. 2009. A bayesian\nmodel of syntax-directed tree to string grammar\ninduction. In Proc. of EMNLP 2009, pages 352-361.\nTrevor Cohn, Phil Blunsom, and Sharon Goldwater.\n2010. Inducing tree-substitution grammars. Journal\nof Machine Learning Research, pages 3053-3096.\nBrooke Cowan, Ivona Kucerova and Michael Collins.\n2006. A discriminative model for tree-to-tree\ntranslation. In Proc. of EMNLP 2006, pages 232-241.\nJohn DeNero and Dan Klein. 2007. Tailoring word\nalignments to syntactic machine translation. In Proc.\nof ACL 2007, pages 17-24.\nJohn DeNero and Jakob Uszkoreit. 2011. Inducing\nsentence structure from parallel corpora for\nreordering. In Proc. of EMNLP 2011, pages 193-203.\nChris Dyer. 2010. Two monolingual parses are better\nthan one (synchronous parse). In Proc. of NAACL\n2010, pages 263-266.\nJason Eisner. 2003. Learning non-isomorphic tree\nmappings for machine translation. In Proc. of ACL\n2003, pages 205-208.\nMichel Galley, Mark Hopkins, Kevin Knight and Daniel\nMarcu. 2004. What?s in a translation rule. In Proc. of\nHLT-NAACL 2004, pages 273?280.\nMichel Galley, Jonathan Graehl, Kevin Knight, Daniel\nMarcu, Steve DeNeefe, Wei Wang and Ignacio\nThayer. 2006. Scalable inference and training of\ncontext-rich syntactic translation models. In Proc. of\nACL-COLING 2006, pages 961-968.\nJonathan Weese, Juri Ganitkevitch, Chris Callison-\nBurch, Matt Post and Adam Lopez. 2011. Joshua 3.0:\nsyntax-based machine translation with the thrax\nGrammar Extractor. In Proc of WMT11, pages 478-\n484.\nLiang Huang, Kevin Knight and Aravind Joshi. 2006. A\nsyntax-directed translator with extended domain of\nlocality. In Proc. of AMTA 2006, pages 65-73.\nPhilipp Koehn, Franz Och, and Daniel Marcu. 2003.\nStatistical phrase-based translation, In Proc. of\nHLT/NAACL 2003, pages 48-54.\n"},{"#tail":"\n","@confidence":"0.999703826530612","#text":"\nPhilipp Koehn. 2004. Statistical significance tests for\nmachine translation evaluation. In Proc. of EMNLP\n2004, pages 388?395.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichDUG =HQV &KULV '\\HU DQG 2QG?HM %RMDU. 2007.\nMoses: open source toolkit for statistical machine\ntranslation. In Proc. of ACL 2007, pages 177-180.\nAbby Levenberg, Chris Dyer and Phil Blunsom. 2012.\nA bayesian model for learning SCFGs with\ndiscontiguous Rules. In Proc. of EMNLP 2012, pages\n223-232.\nZhifei Li, Chris Callison-Burch, Chris Dyer, Juri\nGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,\nWren N.G. Thornton, Jonathan Weese and Omar F.\nZaidan. 2009. Joshua: An open source toolkit for\nparsing-based machine translation. In Proc. of ACL\n2009, pages 135-139.\nShujie Liu, Chi-Ho Li, Mu Li, Ming Zhou. 2012. Re-\ntraining monolingual parser bilingually for syntactic\nSMT. In Proc. of EMNLP 2012, pages 854-862.\nYang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-\nstring alignment template for statistical machine\ntranslation. In Proc. of ACL-COLING 2006, pages\n609-616.\nYang Liu, Yajuan Lv and Qun Liu. 2009. Improving\ntree-to-tree translation with packed forests. In Proc.\nof ACL-IJCNLP 2009, pages 558-566.\nDaniel Marcu, Wei Wang, Abdessamad Echihabi and\nKevin Knight. 2006. SPMT: Statistical machine\ntranslation with syntactified target language phrases.\nIn Proc. of EMNLP 2006, pages 44-52.\nFranz Och, 2003. Minimum error rate training in\nstatistical machine translation. In Proc. of ACL 2003,\npages 160-167.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation. In Proc. of ACL\n2002, pages 311-318.\nSlav Petrov, Leon Barrett, Romain Thibaux and Dan\nKlein. 2006. Learning accurate, compact, and\ninterpretable tree annotation. In Proc. of COLING-\nACL 2006, pages 433-440.\nChris Quirk, Arul Menezes and Colin Cherry. 2005.\nDependency treelet translation: syntactically\ninformed phrasal SMT. In Proc. of ACL 2005, pages\n271-279.\nLibin Shen, Jinxi Xu and Ralph Weischedel. 2008. A\nnew string-to-dependency machine translation\nalgorithm with a target dependency language model.\nIn Proc. of ACL-08, pages 577-585.\nWei Wang, Kevin Knight, and Daniel Marcu. 2007.\nBinarizing syntax trees to improve syntax-based\nmachine translation accuracy. In Proc. of EMNLP\n2007, pages 746-754.\nWei Wang, Jonathan May, Kevin Knight, and Daniel\nMarcu. 2010. Re-structuring, re-labeling, and re-\naligning for syntax-based machine translation.\nComputational Linguistics, 36(2):247?277.\nFeifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing\nZong. 2012. Tree-based translation without using\nparse trees. In Proc. of COLING 2012, pages 3037-\n3054.\nHao Zhang, Liang Huang, Daniel Gildea and Kevin\nKnight. 2006. Synchronous binarization for machine\ntranslation. In Proc. of HLT-NAACL 2006, pages\n256-263.\nHao Zhang, Daniel Gildea, and David Chiang. 2008.\nExtracting synchronous grammars rules from word\nlevel alignments in linear time. In Proc. of COLING\n2008, pages 1081-1088.\nHao Zhang, Licheng Fang, Peng Xu, Xiaoyun Wu.\n2011a. Binarized forest to string translation. In Proc.\nof ACL 2011, pages 835-845.\nHui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew\nLim Tan. 2009. Forest-based tree sequence to string\ntranslation model. In Proc. of ACL-IJCNLP 2009,\npages 172-180.\nJiajun Zhang, Feifei Zhai and Chengqing Zong. 2011b.\nAugmenting string-to-tree translation models with\nfuzzy use of source-side syntax. In Proc. of EMNLP\n2011, pages 204-215.\nMin Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew\nLim Tan and Sheng Li. 2007. A tree-to-tree\nalignment-based model for statistical Machine\ntranslation. MT-Summit-07. pages 535-542\nMin Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, Chew\nLim Tan and Sheng Li. 2008. A tree sequence\nalignment-based tree-to-tree translation model. In\nProc. of ACL 2008, pages 559-567.\nAndreas Zollmann and Ashish Venugopal. 2006. Syntax\naugmented machine translation via chart parsing. In\nProc. of Workshop on Statistical Machine\nTranslation 2006, pages 138-141.\nAndreas Zollmann and Stephan Vogel. 2011. A word-\nclass approach to labeling PSCFG rules for machine\ntranslation. In Proc. of ACL 2011, pages 1-11.\n"}],"#tail":"\n","bodyText":[{"#tail":"\n","@confidence":"0.999611352941176","#text":"\nIn current research, most tree-based translation\nmodels are built directly from parse trees. In\nthis study, we go in another direction and build\na translation model with an unsupervised tree\nstructure derived from a novel non-parametric\nBayesian model. In the model, we utilize\nsynchronous tree substitution grammars (STSG)\nto capture the bilingual mapping between\nlanguage pairs. To train the model efficiently,\nwe develop a Gibbs sampler with three novel\nGibbs operators. The sampler is capable of\nexploring the infinite space of tree structures by\nperforming local changes on the tree nodes.\nExperimental results show that the string-to-\ntree translation system using our Bayesian tree\nstructures significantly outperforms the strong\nbaseline string-to-tree system using parse trees.\n"},{"#tail":"\n","@confidence":"0.959420266666667","#text":"\nIn recent years, tree-based translation models1 are\ndrawing more and more attention in the\ncommunity of statistical machine translation\n(SMT). Due to their remarkable ability to\nincorporate context structure information and long\ndistance reordering into the translation process,\ntree-based translation models have shown\npromising progress in improving translation\nquality (Liu et al, 2006, 2009; Quirk et al, 2005;\nGalley et al, 2004, 2006; Marcu et al, 2006; Shen\net al, 2008; Zhang et al, 2011b).\nHowever, tree-based translation models always\nsuffer from two major challenges: 1) They are\nusually built directly from parse trees, which are\ngenerated by supervised linguistic parsers.\n"},{"#tail":"\n","@confidence":"0.98526115","#text":"\nusing tree structures on one side or both sides.\nHowever, for many language pairs, it is difficult to\nacquire such corresponding linguistic parsers due\nto the lack of Tree-bank resources for training. 2)\nParse trees are actually only used to model and\nexplain the monolingual structure, rather than the\nbilingual mapping between language pairs. This\nindicates that parse trees are usually not the\noptimal choice for training tree-based translation\nmodels (Wang et al, 2010).\nBased on the above analysis, we can conclude\nthat the tree structure that is independent from\nTree-bank resources and simultaneously considers\nthe bilingual mapping inside the bilingual sentence\npairs would be a good choice for building tree-\nbased translation models.\nTherefore, complying with the above conditions,\nwe propose an unsupervised tree structure for tree-\nbased translation models in this study. In the\nstructures, tree nodes are labeled by combining the\nword classes of their boundary words rather than\nby syntactic labels, such as NP, VP. Furthermore,\nusing these node labels, we design a generative\nBayesian model to infer the final tree structure\nbased on synchronous tree substitution grammars\n(STSG) 2 . STSG is derived from the word\nalignments and thus can grasp the bilingual\nmapping effectively.\nTraining the Bayesian model is difficult due to\nthe exponential space of possible tree structures for\neach training instance. We therefore develop an\nefficient Gibbs sampler with three novel Gibbs\noperators for training. The sampler is capable of\nexploring the infinite space of tree structures by\nperforming local changes on the tree nodes.\n2 We believe it is possible to design a model to infer the\nnode label and tree structure jointly. We plan this as\nfuture work, and here, we focus only on inferring the\ntree structure in terms of the node labels derived from\nword classes.\n"},{"#tail":"\n","@confidence":"0.9980652","#text":"\nThe tree structure formed in this way is\nindependent from the Tree-bank resources and\nsimultaneously exploits the bilingual mapping\neffectively. Experiments show that the proposed\nunsupervised tree (U-tree) is more effective and\nreasonable for tree-based translation than the parse\ntree.\nThe main contributions of this study are as\nfollows:\n1) Instead of the parse tree, we propose a\nBayesian model to induce a U-tree for tree-\nbased translation. The U-tree exploits the\nbilingual mapping effectively and does not\nrely on any Tree-bank resources.\n2) We design a Gibbs sampler with three novel\nGibbs operators to train the Bayesian model\nefficiently.\nThe remainder of the paper is organized as\nfollows. Section 2 introduces the related work.\nSection 3 describes the STSG generation process,\nand Section 4 depicts the adopted Bayesian model.\nSection 5 describes the Gibbs sampling algorithm\nand Gibbs operators. In Section 6, we analyze the\nachieved U-trees and evaluate their effectiveness.\nFinally, we conclude the paper in Section 7.\n"},{"#tail":"\n","@confidence":"0.948268470588235","#text":"\nIn this study, we move in a new direction to build a\ntree-based translation model with effective\nunsupervised U-tree structures.\nFor unsupervised tree structure induction,\nDeNero and Uszkoreit (2011) adopted a parallel\nparsing model to induce unlabeled trees of source\nsentences for syntactic pre-reordering. Our\nprevious work (Zhai et al, 2012) designed an EM-\nbased method to construct unsupervised trees for\ntree-based translation models. This work differs\nfrom the above work in that we design a novel\nBayesian model to induce unsupervised U-trees,\nand prior knowledge can be encoded into the\nmodel more freely and effectively.\nBlunsom et al (2008, 2009, 2010) utilized\nBayesian methods to learn synchronous context\nfree grammars (SCFG) from a parallel corpus. The\nobtained SCFG is further used in a phrase-based\nand hierarchical phrase-based system (Chiang,\n2007). Levenberg et al (2012) employed a\nBayesian method to learn discontinuous SCFG\nrules. This study differs from their work because\nwe concentrate on constructing tree structures for\ntree-based translation models. Our U-trees are\nlearned based on STSG, which is more appropriate\nfor tree-based translation models than SCFG.\nBurkett and Klein (2008) and Burkett et al\n(2010) focused on joint parsing and alignment.\nThey utilized the bilingual Tree-bank to train a\njoint model for both parsing and word alignment.\nCohn and Blunsom (2009) adopted a Bayesian\nmethod to infer an STSG by exploring the space of\nalignments based on parse trees. Liu et al (2012)\nre-trained the linguistic parsers bilingually based\non word alignment. Burkett and Klein (2012)\nutilized a transformation-based method to learn a\nsequence of monolingual tree transformations for\ntranslation. Compared to their work, we do not rely\non any Tree-bank resources and focus on\ngenerating effective unsupervised tree structures\nfor tree-based translation models.\nZollmann and Venugopal (2006) substituted the\nnon-terminal X in hierarchical phrase-based model\nby extended syntactic categories. Zollmann and\nVogel (2011) further labeled the SCFG rules with\nPOS tags and unsupervised word classes. Our work\ndiffers from theirs in that we present a Bayesian\nmodel to learn effective STSG translation rules and\nU-tree structures for tree-based translation models,\nrather than designing a labeling strategy for\ntranslation rules.\n"},{"#tail":"\n","@confidence":"0.953293454545454","#text":"\nIn this work, we induce effective U-trees for the\nstring-to-tree translation model, which is based on\na synchronous tree substitution grammar (STSG)\nbetween source strings and target tree fragments.\nWe take STSG as the generation grammar to match\nthe translation model. Typically, such an STSG3 is\na 5-tuple as follows:\n( , , , , )s t t tG N S P ? ?\nwhere:\ni s? and t? represent the set of source and\ntarget words, respectively,\n"},{"#tail":"\n","@confidence":"0.998333","#text":"\nsides. Here we only consider the special case where the\nsource side is actually a string.\n"},{"#tail":"\n","@confidence":"0.99942356","#text":"\nApart from the start non-terminal tS , we define\nall the other non-terminals in tN by word classes.\nInspired by (Zollmann and Vogel, 2011), we\ndivide these non-terminals into three categories:\none-word, two-word and multi-word non-terminals.\nThe one-word non-terminal is a word class, such as\nC, meaning that it dominates a word whose word\nclass is C. Two-word non-terminals are used to\nstand for two word strings. They are labeled in the\nform of C1+C2, where C1 and C2 are the word\nclasses of the two words separately. Accordingly,\nmulti-word non-terminals represent the strings\ncontaining more than two words. They are labeled\nas C1?Cn, demanding that the word classes of the\nleftmost word and the rightmost word are C1 and\nCn, respectively.\nWe use POS tag to play the role of word class4.\nFor example, the head node of the rule in Figure 1\nis a multi-word non-terminal PRP?RB. It requires\nthat the POS tags of the leftmost and rightmost\nword must be PRP and RB, respectively. Xiong et\nal. (2006) showed that the boundary word is an\neffective indicator for phrase reordering. Thus, we\nbelieve that combining the word class of boundary\nwords can denote the whole phrase well.\n"},{"#tail":"\n","@confidence":"0.994866733333333","#text":"\nEach production rule in P consists of a source\nstring and a target tree fragment. In the target tree\nfragment, each internal node is labeled with a non-\nterminal in tN , and each leaf node is labeled with\neither a target word in t? or a non-terminal in tN .\nThe source string in a production rule comprises\nsource words and variables. Each variable\ncorresponds to a leaf non-terminal in the target tree\nfragment. In the STSG, the production rule is used\nto rewrite the root node into a string and a tree\nfragment. For example, in Figure 1, the rule\nrewrites the head node PRP?RB into the\ncorresponding string and fragment.\nAn STSG derivation refers to the process of\ngenerating a specific source string and target tree\n"},{"#tail":"\n","@confidence":"0.999384272727273","#text":"\nfrom manual resources to some extent. In future, we\nplan to design a method to learn effective unsupervised\nlabels for the non-terminals.\nstructure by production rules. This process begins\nwith the start non-terminal tS and an empty source\nstring. We repeatedly choose production rules to\nrewrite the leaf non-terminals and expand the\nstring until no leaf non-terminal is left. Finally, we\nacquire a source string and a target tree structure\ndefined by the derivation. The probability of a\nderivation is given as follows:\n"},{"#tail":"\n","@confidence":"0.985952425","#text":"\nwhere the derivation comprises a sequence of rules\nd=(r1,?,rn), and Ni represents the root node of rule\nri. Hence, for a specific bilingual sentence pair, we\ncan generate the best target-side tree structure\nbased on the STSG, independent from the Tree-\nbank resources. The STSG used in the above\nprocess is learned by the Bayesian model that is\ndetailed in the next section.\nActually, SCFG can also be used to build the U-\ntrees. We do not use SCFG because most of the\ntree-based models are based on STSG. In our\nBayesian model, the U-trees are optimized through\nselecting a set of STSG rules. These STSG rules\nare consistent with the translation rules used in the\ntree-based models.\nAnother reason is that STSG has a stronger\nexpressive power on tree construction than SCFG.\nIn a STSG-based U-tree or a STSG rule, although\nnot linguistically informed, the nodes labeled by\nPOS tags are also effective on distinguishing\ndifferent ones. However, with SCFG, we have to\ndiscard all the internal nodes (i.e., flattening the U-\ntrees or rules) to express the same sequence,\nleading to a poor ability of distinguishing different\nU-trees and production rules. Thus, using STSG,\nwe can build more specific U-trees for translation.\nIn addition, we find that the Bayesian SCFG\ngrammar cannot even significantly outperform the\nheuristic SCFG grammar (Blunsom et al 2009)5.\nThis would indicate that the SCFG-based\nderivation tree as by-product is also not such good\nfor tree-based translation models. Considering the\nabove reasons, we believe that the STSG-based\nlearning procedure would result in a better\ntranslation grammar for tree-based models.\n5 In (Blunsom et al, 2009), for Chinese-to-English\ntranslation, the Bayesian SCFG grammar only\noutperform the heuristic SCFG grammar by 0.1 BLEU\npoints on NIST MT 2004 and 0.6 BLEU points on NIST\nMT 2005 in the NEWS domain.\n"},{"#tail":"\n","@confidence":"0.9655958","#text":"\nIn this section, we present a Bayesian model to\nlearn STSG defined in section 3. In the model, we\nuse ?N to denote the probability distribution\n(  |)p r N in Equation (1). ?N follows a multinomial\ndistribution and we impose a Dirichlet prior (DP)\n"},{"#tail":"\n","@confidence":"0.973024090909091","#text":"\nwhere 0 (  |)P N< (base distribution) is used to assign\nprior probabilities to the STSG production rules. ?N\ncontrols the model?s tendency to either reuse\nexisting rules or create new ones using the base\ndistribution 0 (  |)P N< .\nInstead of denoting the multinomial distribution\nexplicitly with a specific ?N, we integrate over all\npossible values of ?N to achieve the probabilities of\nrules. This integration results in the following\nconditional probability for rule ri given the\npreviously observed rules r-i = r1 ,?, ri-1:\n"},{"#tail":"\n","@confidence":"0.989976888888889","#text":"\nrepresents the total count of rules rewriting non-\nterminal N in ir . Thanks to the exchangeability of\nthe model, all permutations of the rules are actually\nequiprobable. This means that we can compute the\nprobability of each rule based on the previous and\nsubsequent rules (i.e. consider each rule as the last\none). This characteristic allows us to design an\nefficient Gibbs sampling algorithm to train the\nBayesian model.\n"},{"#tail":"\n","@confidence":"0.998730285714286","#text":"\nThe base distribution 0 (  |)P r N is designed to\nassign prior probabilities to the STSG production\nrules. Because each rule r consists of a target tree\nfragment frag and a source string str in the model,\nwe follow Cohn and Blunsom (2009) and\ndecompose the prior probability 0 (  |)P r N into two\nfactors as follows:\n"},{"#tail":"\n","@confidence":"0.985350647058823","#text":"\nwhere (  |)P frag N is the probability of\nproducing the target tree fragment frag. To\ngenerate frag, Cohn and Blunsom (2009) used a\ngeometric prior to decide how many child nodes to\nassign each node. Differently, we require that each\nmulti-word non-terminal node must have two child\nnodes. This is because the binary structure has\nbeen verified to be very effective for tree-based\ntranslation (Wang et al, 2007; Zhang et al, 2011a).\nThe generation process starts at root node N. At\nfirst, root node N is expanded into two child nodes.\nThen, each newly generated node will be checked\nto expand into two new child nodes with\nprobability pexpand. This process repeats until all the\nnew non-terminal nodes are checked. Obviously,\npexpand controls the scale of tree fragments, where a\nlarge pexpand corresponds to large fragments\n"},{"#tail":"\n","@confidence":"0.989951172413793","#text":"\nnew terminal nodes (words) are drawn uniformly\nfrom the target-side vocabulary, and the non-\nterminal nodes are created by asking two questions\nas follows:\n1) What type is the node, one-word, two-\nword or multi-word non-terminal?\n2) What tag is used to label the node?\nThe answer to question 1) is chosen from a\nuniform distribution, i.e., the probability is 1/3 for\neach type of non-terminal. The entire generation\nprocess is in a top-down manner, i.e., generating a\nparent node first and then its children.\nWith respect to question 2), because the father\nnode has determined the POS tags of boundary\nwords, we only need one POS tag to generate the\nlabel of the current node. For example, in Figure 1,\nas the father node PRP?RB demands that the POS\ntag of the rightmost word is RB, the right child of\nPRP?RB must also satisfy this condition.\nTherefore, we choose a POS tag VBP and obtain\nthe label VBP+RB. The POS tag is drawn\nuniformly from the POS tag set. If the current node\nis a one-word non-terminal, question 2) is\nunnecessary. Similarly, with respect to the two-\nword non-terminal node, questions 1) and 2) are\nboth unnecessary for its two child nodes because\nthey have already been defined by their father node.\nAs an example of the generative process, the\ntree fragment in Figure 1 is created as follows:\n"},{"#tail":"\n","@confidence":"0.969348","#text":"\ngenerating the source string, which contains\nseveral source words and variables. Inspired by\n(Blunsom et al, 2009) and (Cohn and Blunsom,\n2009), we define (  |)P str frag as follows:\n"},{"#tail":"\n","@confidence":"0.962144933333333","#text":"\nwhere csw is the number of words in the source\nstring. ?s means the source vocabulary set. Further,\ncvar denotes the number of variables, which is\ndetermined by the tree fragment frag.\nAs shown in Equation(5), we first determine\nhow many source words to generate using a\nPoisson distribution Ppoisson(csw;1), which imposes a\nstable preference for short source strings. Then, we\ndraw each source word from a uniform distribution\nover ?s. Afterwards, we insert the variables into\nthe string. The variables are inserted one at a time\nusing a uniform distribution over the possible\npositions. This factor discourages more variables.\nFor the example rule in Figure 1, the generative\nprocess of the source string is:\n"},{"#tail":"\n","@confidence":"0.975653875","#text":"\nOur base distribution is also designed based on\nthis intuition. Considering the two factors in our\nbase distribution, we penalize both large target tree\nfragments with many nodes and long source strings\nwith many words and variables. The Bayesian\nmodel tends to select both small and frequent\nSTSG production rules to construct the U-trees.\nWith these types of trees, we can extract small\nrules with good generality and simultaneously\nobtain large rules with enough context information\nby composition. We will show the effectiveness of\nour U-trees in the verification experiment.\n5 Model Training by Gibbs Sampling\nIn this section, we introduce a collapsed Gibbs\nsampler, which enables us to train the Bayesian\nmodel efficiently.\n"},{"#tail":"\n","@confidence":"0.963360878048781","#text":"\nAt first, we use random binary trees to initialize the\nsampler. To get the initial U-trees, we recursively\nand randomly segment a sentence into two parts\nand simultaneously create a tree node to dominate\neach part. The created tree nodes are labeled by the\nnon-terminals described in section 3.\nUsing the initial target U-trees, source sentences\nand word alignment, we extract minimal GHKM\ntranslation rules7 in terms of frontier nodes (Galley\net al, 2004). Frontier nodes are the tree nodes that\ncan map onto contiguous substrings on the source\nside via word alignment. For example, the bold\nitalic nodes with shadows in Figure 2 are frontier\nnodes. In addition, it should be noted that the word\nalignment is fixed8, and we only explore the entire\nspace of tree structures in our sampler. Differently,\nCohn and Blunsom (2009) designed a sampler to\ninfer an STSG by fixing the tree structure and\nexploring the space of alignment. We believe that\nit is possible to investigate the space of both tree\nstructure and alignment simultaneously. This\nsubject will be one of our future work topics.\nFor each training instance (a pair of source\nsentence and target U-tree structure), the extracted\nGHKM minimal translation rules compose a\nunique STSG derivation9. Moreover, all the rules\ndeveloped from the training data constitute an\ninitial STSG for the Gibbs sampler.\n7 We attach the unaligned word to the lowest frontier\nnode that can cover it in terms of word alignment.\n8 The sampler might reinforce the frequent alignment\nerrors (AE), which would harm the translation model\n(TM). Actually, the frequent AEs also greatly impair the\nconventional TM. Besides, our sampler encourages the\ncorrect alignments and simultaneously discourages the\ninfrequent AEs. Thus, compared with the conventional\nTMs, we believe that our final TM would not be worse\ndue to AEs. Our final experiments verify this point and\nwe will conduct a much detailed analysis in future.\n9 We only use the minimal GHKM rules (Galley et al,\n2004) here to reduce the complexity of the sampler.\n"},{"#tail":"\n","@confidence":"0.992774222222222","#text":"\nbold italic nodes with shadows are frontier nodes.\nUnder this initial STSG, the sampler modifies\nthe initial U-trees (initial sample) to create a series\nof new ones (new samples) by the Gibbs operators.\nConsequently, new STSGs are created based on the\nnew U-trees simultaneously and used for the next\nsampling operation. Repeatedly and after a number\nof iterations, we can obtain the final U-trees for\nbuilding translation models.\n"},{"#tail":"\n","@confidence":"0.919311488888889","#text":"\nIn this section, we develop three novel Gibbs\noperators for the sampler. They explore the entire\nspace of the U-tree structures by performing local\nchanges on the tree nodes.\nFor a U-tree of a given sentence, we define s-\nnode as the non-root node covering at least two\nwords. Thus, the set of s-node contains all the tree\nnodes except the root node, the pre-terminal nodes\nand leaf nodes, which we call non-s-node. For\nexample, in Figure 2, PRB?RB and PRP+VBP are\ns-nodes, while NN and NN?RB are non-s-nodes.\nSince the POS tag sequence of the sentence is\nfixed, all non-s-nodes would stay unchanged in all\npossible U-trees of the sentence. Based on this fact,\nour Gibbs operators work only on s-nodes.\nFurther, we assign 3 descendant candidates (DC)\nfor each s-node: its left child, right child and its\nsibling. For example, in Figure 3, the 3 DCs for the\ns-node are node PRP, VBP and RB respectively.\nAccording to the different DCs it governs, every s-\nnode might be in one of the two different states:\n1) Left state: as Figure 3(a) shows, the s-node\ngoverns the left two DCs, PRP and VBP,\nand is labeled PRP+VBP.\n2) Right state: as Figure 3(b) shows, the s-node\ngoverns the right two DCs, VBP and RB, and\nis labeled VBP+RB.\nFor a specific U-tree, the states of s-nodes are fixed.\nThus, by changing an s-node?s state, we can easily\ntransform this U-tree to another one, i.e., from the\ncurrent sample to a new one.\nTo formulate the U-tree transformation process,\nwe associate a binary variable ??{0,1} with each\ns-node, indicating whether the s-node is in the left\n?  or right state ?   Then we can change\nthe U-tree by changing value of the ? parameters.\nOur first Gibbs operator, Rotate, just works by\nsampling value of the ?parameters, one at a time,\nand changing the U-tree accordingly. For example,\nin Figure 3(a), the s-node is currently in the left\nVWDWH?  :HVDPSOHWKH?RIWKLVQRGHDQGLI\nWKHVDPSOHGYDOXHRI?LVZHNHHSWKHVWUXFWXUH\nunchanged, i.e., in the left state. Otherwise, we\nchange its state to the right state ?  , and\ntransform the U-tree to Figure 3(b) accordingly.\n"},{"#tail":"\n","@confidence":"0.9556748","#text":"\nstate respectively. The bold italic nodes with shadows in\nthe figure are frontier nodes.\nObviously, towards an s-node for sampling, the\ntwo values of ? would define two different U-trees.\nUsing the GHKM algorithm (Galley et al 2004),\nwe can get two different STSG derivations from\nthe two U-trees based on the fixed word alignment.\nEach derivation carries a set of STSG rules (i.e.,\nminimal GHKM translation rules) of its own. In\nthe two derivations, the STSG rules defined by the\ntwo states include the one rooted at the s-node?s\nlowest ancestor frontier node, and the one rooted at\nthe s-node if it is a frontier node. For instance, in\nFigure 3(a), as the s-node is not a frontier node, the\nleft state (? ) defines only one rule:\n"},{"#tail":"\n","@confidence":"0.961601","#text":"\nDifferently, in Figure 3(b), the s-node is a\nfrontier node and thus the right state (? 1) defines\ntwo rules:\n"},{"#tail":"\n","@confidence":"0.993602","#text":"\nUsing these STSG rules, the two derivations are\nevaluated as follows (We use the value of ? to\ndenote the corresponding STSG derivation):\n"},{"#tail":"\n","@confidence":"0.989083488888889","#text":"\nWhere r refers to the conditional context, i.e., the\nset of all other rules in the training data. All the\nprobabilities in the above formulas are computed\nby Equation(3). We then normalize the two scores\nand sample a value of ? based on them. With the\nBayesian model described in section 4, the sampler\nZLOOSUHIHUWKH?WKDWSURGXFHVVPDOODQGIUHTXHQW\nSTSG rules. This tendency results in more frontier\nnodes in the U-tree (i.e., the s-node tends to be in\nthe state that is a frontier node), which will factor\nthe training instance into more small STSG rules.\nIn this way, the overall likelihood of the bilingual\ndata is improved by the sampler.\nTheoretically, the Rotate operator is capable of\narriving at any possible U-tree from the initial U-\ntree. This is because we can first convert the initial\nU-tree to a left branch tree by the Rotate operator,\nand then transform it to any other U-tree. However,\nit may take a long time to do so. Thus, to speed up\nthe structure transformation process, we employ a\nTwo-level-Rotate operator, which takes a pair of s-\nnodes in a parent-child relationship as a unit for\nsampling. Similar to the Rotate operator, we also\nassign a binary variable ??{0,1} to each unit and\nupdate the U-tree by sampling the value of ?. The\nmethod of sampling ? is similar to the one used for\n?. Figure 4 shows an example of the operator. As\nshown in Figure 4(a), the unit NN?VBP and\nPRP+VBP is in the left state (?=0), and governs\nthe left three descendants: NN, PRP, and VBP. By\nthe Two-level-Rotate operator, we can convert the\nunit to Figure 4(b), i.e., the ULJKWVWDWH?=1). Just as\nFigure 4(b) shows, the governed descendants of the\nunit are turned to PRP, VBP, and RB.\nIt may be confusing when choosing the parent-\nchild s-node pair for sampling because the parent\nnode always faces two choices: combining the left\nchild or right child for sampling. To avoid\nconfusion, we split the Two-level-Rotate operator\ninto two operators: Two-level-left-Rotate operator,\nwhich works with the parent node and its left child,\nand Two-level-right-Rotate operator, which only\nconsiders the parent node and its right child 10 .\nTherefore, the operator used in Figure 4 is a Two-\nlevel-right-Rotate operator.\n"},{"#tail":"\n","@confidence":"0.9819810625","#text":"\nThe bold italic nodes with shadows in the Figure are\nfrontier nodes.\nDuring sampling, for each training instance, the\nsampler first applies the Two-level-left-Rotate\noperator to all candidate pairs of s-nodes (parent s-\nnode and its left child s-node) in the U-tree. After\nthat, the Two-level-right-Rotate operator is applied\nto all the candidate pairs of s-nodes (parent s-node\nand its right child s-node). Then, we use the Rotate\noperator on every s-node in the U-tree. By utilizing\nthe operators separately, we can guarantee that our\nsampler satisfies detailed balance. We visit all the\ntraining instances in a random order (one iteration).\nAfter a number of iterations, we can obtain the\nfinal U-tree structures and build the tree-based\ntranslation model accordingly.\n"},{"#tail":"\n","@confidence":"0.983205272727273","#text":"\nThe experiments are conducted on Chinese-to-\nEnglish translation. The training data are the FBIS\ncorpus with approximately 7.1 million Chinese\nwords and 9.2 million English words. We obtain\nthe bidirectional word alignment with GIZA++,\nand then adopt the grow-diag-final-and strategy to\nobtain the final symmetric alignment. We train a 5-\ngram language model on the Xinhua portion of the\nEnglish Gigaword corpus and the English part of\n10 We can also take more nodes as a unit for sampling,\nbut this would make the algorithm much more complex.\n"},{"#tail":"\n","@confidence":"0.958559959183673","#text":"\nthe training data. For tuning and testing, we use the\nNIST MT 2003 evaluation data as the development\nset, and use the NIST MT04 and MT05 data as the\ntest set. We use MERT (Och, 2004) to tune\nparameters. Since MERT is prone to search errors,\nwe run MERT 5 times and select the best tuning\nparameters in the tuning set. The translation quality\nis evaluated by case-insensitive BLEU-4 with the\nshortest length penalty. The statistical significance\ntest is performed by the re-sampling approach\n(Koehn, 2004).\nTo create the baseline system, we use the open-\nsource Joshua 4.0 system (Ganitkevitch et al, 2012)\nto build a hierarchical phrase-based (HPB) system,\nand a syntax-augmented MT (SAMT) 11 system\n(Zollmann and Venugopal, 2006) respectively.\nThe translation system used for testing the\neffectiveness of our U-trees is our in-house string-\nto-tree system (abbreviated as s2t). The system is\nimplemented based on (Galley et al, 2006) and\n(Marcu et al 2006). In the system, we extract both\nthe minimal GHKM rules (Galley et al, 2004), and\nthe rules of SPMT Model 1 (Galley et al, 2006)\nwith phrases up to length L=5 on the source side.\nWe then obtain the composed rules by composing\ntwo or three adjacent minimal rules.\nTo build the above s2t system, we first use the\nparse tree, which is generated by parsing the\nEnglish side of the bilingual data with the Berkeley\nparser (Petrov et al, 2006). Then, we binarize the\nEnglish parse trees using the head binarization\napproach (Wang et al, 2007) and use the resulting\nbinary parse trees to build another s2t system.\nFor the U-trees, we run the Gibbs sampler for\n1000 iterations on the whole corpus. The sampler\nuses 1,087s per iteration, on average, using a single\ncore, 2.3 GHz Intel Xeon machine. For the\nhyperparameters, we set ? to 0.1 and pexpand = 1/3\nto give a preference to the rules with small\nfragments. We built an s2t translation system with\nthe achieved U-trees after the 1000th iteration. We\nonly use one sample to extract the translation\ngrammar because multiple samples would result in\na grammar that would be too large.\n11 From (Zollmann and Vogel, 2011), we find that the\nperformance of SAMT system is similar with the\nmethod of labeling SCFG rules with POS tags. Thus, to\nbe convenient, we only conduct experiments with the\nSAMT system.\n"},{"#tail":"\n","@confidence":"0.958273333333333","#text":"\nTo evaluate the effectiveness of the Gibbs sampler,\nwe explore the change of the training data?s\nlikelihood with increasing sampling iterations.\n"},{"#tail":"\n","@confidence":"0.973208","#text":"\nthe number of sampling iterations. In the figure, random\n1 to 3 refers to three independent runs of the sampler\nwith different initial U-trees as initialization states.\nFigure 5 depicts the negative-log likelihood of\nthe training data after several sampling iterations.\nThe results show that the overall likelihood of the\ntraining data is improved by the sampler. Moreover,\ncomparing the three independent runs, we see that\nalthough the sampler begins with different initial\nU-trees, the training data?s likelihood is always\nsimilar during sampling. This demonstrates that\nour sampler is not sensitive to the random initial\nU-trees and can always arrive at a good final state\nbeginning from different initialization states. Thus,\nwe only utilize the U-trees from random 1 for\nfurther analysis hereafter.\n"},{"#tail":"\n","@confidence":"0.993467","#text":"\nAcquiring better U-trees for translation is our final\npurpose. However, are the U-trees achieved by the\n"},{"#tail":"\n","@confidence":"0.997901230769231","#text":"\nGibbs sampler appropriate for the tree-based\ntranslation model?\nTo answer this question, we first analyze the\neffect of the sampler on the U-trees. Figure 6\nshows the total number of frontier nodes in the\ntraining data during sampling. The results show\nthat the number of frontier nodes increases with\nincreased sampling. This tendency indicates that\nour sampler prefers the tree structure with more\nfrontier nodes. Consequently, the final U-tree\nstructures can always be factored into many small\nminimal translation rules. Just as we have argued\nin section 4.1, this is beneficial for a good\ntranslation grammar.\nTo demonstrate the above analysis, Figure 7\nshows a visual comparison between our U-tree\n(from random 1) and the binary parse tree (found\nby head binarization). Because the traditional parse\ntree is not binarized, we do not consider it for this\nanalysis. Figure 7 shows that whether it is the\ntarget tree fragment or the source string of the rule,\nour U-trees always tend to obtain the smaller\nones12. This comparison verifies that our Bayesian\ntree induction model is effective in shifting the tree\nstructures away from complex minimal rules,\nwhich tend to negatively affect translation.\n"},{"#tail":"\n","@confidence":"0.971651166666667","#text":"\nstatistics comparing our U-trees and binary parse trees.\n12 Binary parse trees get more tree fragments with two\nnodes than U-trees. This is because there are many\nunary edges in the binary parse trees, while no unary\nedge exists in our U-trees.\nSpecifically, we show an example of a binary\nparse tree and our U-tree in Figure 8. The example\nU-tree is more conducive to extracting effective\ntranslation rules. For example, to translate the\nChinese phrase ?? ??, we can extract a rule (R2\nin Figure 9) directly from the U-tree because the\nphrase ?? ?? is governed by a frontier node, i.e.,\nnode ?VBD+RB?. However, because no node\ngoverns ?? ?? in the binary parse tree, we can\nonly obtain a rule (R1 in Figure 9) with many extra\nnodes and edges, such as node CD in R1. Due to\nthese extra things, R1 is too large to show good\ngenerality.\n"},{"#tail":"\n","@confidence":"0.928217","#text":"\n?? ? .? R1 is extracted from Figure 8(a), i.e., the\nbinary parse tree. R2 is from Figure 8(b), i.e., the U-tree.\n"},{"#tail":"\n","@confidence":"0.998928818181818","#text":"\nBased on the above analysis, we can conclude\nthat our proposed U-tree structures are conducive\nto extracting small, minimal translation rules. This\nindicates that the U-trees are more consistent with\nthe word alignment and are good at capturing\nbilingual mapping information. Therefore, because\nparse trees are always constrained by cross-lingual\nstructure divergence, we believe that the proposed\nU-trees would result in a better translation\ngrammar. We demonstrate this conclusion in the\nnext sub-section.\n"},{"#tail":"\n","@confidence":"0.99916004","#text":"\nThe final translation results are shown in Table 1.\nIn the table, lines 3-6 refer to the string-to-tree\nsystems built with different types of tree structures.\nTable 1 shows that all our s2t systems\noutperform the Joshua (HPB) and Joshua (SAMT)\nsystem significantly. This comparison verifies the\nsuperiority of our in-house s2t system. Moreover,\nthe results shown in Table 1 also demonstrate the\neffectiveness of head binarization, which helps to\nimprove the s2t system using parse trees in all\ntranslation tasks.\nTo test the effectiveness of our U-trees, we give\nthe s2t translation system using the U-trees (from\nrandom 1). The results show that the system using\nU-trees achieves the best translation result from all\nof the systems. It surpasses the s2t system using\nparse trees by 1.47 BLEU points on MT04 and\n1.44 BLEU points on MT05. Moreover, even using\nthe binary parse trees, the achieved s2t system is\nstill lower than our U-tree-based s2t system by\n0.97 BLEU points on the combined test set. From\nthe translation results, we can validate our former\nanalysis that the U-trees generated by our Bayesian\ntree induction model are more appropriate for\nstring-to-tree translation than parse trees.\n"},{"#tail":"\n","@confidence":"0.93831975","#text":"\ns2t systems using different types of trees. The ?*? and\n?#? denote that the results are significantly better than\nthe Joshua (SAMT) system and the s2t system using\nparse trees (p<0.01).\n"},{"#tail":"\n","@confidence":"0.938193266666666","#text":"\nWe also conduct an experiment on a larger\nbilingual training data from the LDC corpus13. The\ntraining corpus contains 2.1M sentence pairs with\napproximately 27.7M Chinese words and 31.9M\nEnglish words. Similarly, we train a 5-gram\nlanguage model using the Xinhua portion of the\nEnglish Gigaword corpus and the English part of\nthe training corpus. With the same settings as\nbefore, we run the Gibbs sampler for 1000\niterations and utilize the final U-tree structure to\nbuild a string-to-tree translation system.\nThe final BLEU score results are shown in Table\n2. In the scenario with a large data, the string-to-\ntree system using our U-trees still significantly\noutperforms the system using parse trees.\n"},{"#tail":"\n","@confidence":"0.631528","#text":"\nthe large training data. The meaning of ?*? and ?#? are\nsimilar to Table 1.\n"},{"#tail":"\n","@confidence":"0.995010833333333","#text":"\nIn this paper, we explored a new direction to build\na tree-based model based on unsupervised\nBayesian trees rather than supervised parse trees.\nTo achieve this purpose, we have made two major\nefforts in this paper:\n(1) We have proposed a novel generative\nBayesian model to induce effective U-trees for\ntree-based translation. We utilized STSG in the\nmodel to grasp bilingual mapping information. We\nfurther imposed a reasonable hierarchical prior on\nthe tree structures, encouraging small and frequent\nminimal rules for translation.\n(2) To train the Bayesian tree induction\nmodel efficiently, we developed a Gibbs sampler\nwith three novel Gibbs operators. The operators are\ndesigned specifically to explore the infinite space\nof tree structures by performing local changes on\nthe tree structure.\n"},{"#tail":"\n","@confidence":"0.9987665","#text":"\nExperiments on the string-to-tree translation\nmodel demonstrated that our U-trees are better\nthan the parse trees. The translation results verify\nthat the well-designed unsupervised trees are\nactually more appropriate for tree-based translation\nthan parse trees. Therefore, we believe that the\nunsupervised tree structure would be a promising\nresearch direction for tree-based translation.\nIn future, we plan to testify our sampler with\nvarious initial trees, such as the tree structure\nformed by (Zhang et al, 2008). We also plan to\nperform a detailed empirical comparison between\nSTST and SCFG under our settings. Moreover, we\nwill further conduct experiments to compare our\nmethods with other relevant works, such as (Cohn\nand Blunsom, 2009) and (Burkett and Klein, 2012).\n"},{"#tail":"\n","@confidence":"0.768085428571428","#text":"\nWe would like to thank Philipp Koehn and three\nanonymous reviewers for their valuable comments\nand suggestions. The research work has been\nfunded by the Hi-Tech Research and Development\nProgram (?863? Program) of China under Grant\nNo. 2011AA01A207, 2012AA011101, and\n2012AA011102.\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.428761","#text":"\nTransactions of the Association for Computational Linguistics, 1 (2013) 243?254. Action Editor: Philipp Koehn.\n"},{"#tail":"\n","@confidence":"0.805244","#text":"\nNational Laboratory of Pattern Recognition, Institute of Automation,\nChinese Academy of Sciences, Beijing, China\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.989796","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998308","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.29435","@genericHeader":"introduction","#text":"\n1 A tree-based translation model is defined as a model\n"},{"#tail":"\n","@confidence":"0.999636","@genericHeader":"related work","#text":"\n2 Related Work\n"},{"#tail":"\n","@confidence":"0.999041","@genericHeader":"method","#text":"\n3 The STSG Generation Process\n"},{"#tail":"\n","@confidence":"0.650495","@genericHeader":"method","#text":"\n3 Generally, an STSG involves tree fragments on both\n"},{"#tail":"\n","@confidence":"0.770115","@genericHeader":"method","#text":"\n4 The demand of a POS tagger impairs the independence\n"},{"#tail":"\n","@confidence":"0.998238","@genericHeader":"method","#text":"\n4 Bayesian Model\n"},{"#tail":"\n","@confidence":"0.990669","@genericHeader":"method","#text":"\n6. The\n"},{"#tail":"\n","@confidence":"0.994848","@genericHeader":"method","#text":"\n6 Experiments\n"},{"#tail":"\n","@confidence":"0.981696","@genericHeader":"evaluation","#text":"\n7 Conclusion and Future Work\n"},{"#tail":"\n","@confidence":"0.329416","@genericHeader":"conclusions","#text":"\n13 LDC category number : LDC2000T50, LDC2002E18,\n"},{"#tail":"\n","@confidence":"0.966952","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.826522","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.99818","#text":"\nTable 1. Results (in case-insensitive BLEU-4 scores) of\n"},{"#tail":"\n","@confidence":"0.995388","#text":"\nTable 2. Results (in case-insensitive BLEU-4 scores) for\n"}],"page":[{"#tail":"\n","@confidence":"0.997198","#text":"\n243\n"},{"#tail":"\n","@confidence":"0.998697","#text":"\n244\n"},{"#tail":"\n","@confidence":"0.999321","#text":"\n245\n"},{"#tail":"\n","@confidence":"0.876096","#text":"\n246\n"},{"#tail":"\n","@confidence":"0.663121","#text":"\n247\n"},{"#tail":"\n","@confidence":"0.921238","#text":"\n248\n"},{"#tail":"\n","@confidence":"0.995188","#text":"\n249\n"},{"#tail":"\n","@confidence":"0.954178","#text":"\n250\n"},{"#tail":"\n","@confidence":"0.961274","#text":"\n251\n"},{"#tail":"\n","@confidence":"0.994263","#text":"\n252\n"},{"#tail":"\n","@confidence":"0.958156","#text":"\n253\n"},{"#tail":"\n","@confidence":"0.999294","#text":"\n254\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.95208","#text":"\nFigure 1. An example of an STSG production rule.\n"},{"#tail":"\n","@confidence":"0.99939","#text":"\nFigure 2. Illustration of an initial U-tree structure. The\n"},{"#tail":"\n","@confidence":"0.99287","#text":"\nFigure 3. Illustration of the Rotate operator. In the\nfigure, (a) and (b) denote the s-node?s left state and right\n"},{"#tail":"\n","@confidence":"0.999941","#text":"\nFigure 4. Illustration of the Two-level-Rotate operator.\n"},{"#tail":"\n","@confidence":"0.997547","#text":"\nFigure 5. Histograms of the training data?s likelihood vs.\n"},{"#tail":"\n","@confidence":"0.903122","#text":"\nFigure 6. The total number of frontier nodes for the\nthree independent runs.\n"},{"#tail":"\n","@confidence":"0.999305","#text":"\nFigure 7. Histograms over minimal translation rule\n"},{"#tail":"\n","@confidence":"0.7974305","#text":"\nFigure 8. Example of different tree structures. The node\nNP-COMP is achieved by head binarization. The bold\n"},{"#tail":"\n","@confidence":"0.999909","#text":"\nFigure 9. Example rules to translate the Chinese phrase\n"}],"table":[{"#tail":"\n","@confidence":"0.998843833333333","#text":"\nSystem MT04 MT05 All\nJoshua (HPB) 31.73 28.82 30.64\nJoshua (SAMT) 32.48 29.77 31.56\ns2t (parse-tree) 33.73* 30.25* 32.75*\ns2t (binary-parse-tree) 34.09* 30.99*# 32.92*\ns2t (U-tree) 35.20*# 31.69*# 33.89*#\n"},{"#tail":"\n","@confidence":"0.998806833333333","#text":"\nSystem MT04 MT05 All\nJoshua (HPB) 34.55 33.11 34.01\nJoshua (SAMT) 34.76 33.72 34.37\ns2t (parse-tree) 36.40* 34.53* 35.70*\ns2t (binary-parse-tree) 37.38*# 35.14*# 36.54*#\ns2t (U-tree) 38.02*# 36.12*# 37.34*#\n"}],"email":{"#tail":"\n","@confidence":"0.983913","#text":"\n{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.894782","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.9905115","#text":"Transactions of the Association for Computational Linguistics, 1 (2013) 243?254. Action Editor: Philipp Koehn. Submitted 12/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics."},"address":{"#tail":"\n","@confidence":"0.993687","#text":"Chinese Academy of Sciences, Beijing, China"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.999681","#text":"National Laboratory of Pattern Recognition, Institute of Automation,"},"author":[{"#tail":"\n","@confidence":"0.951805","#text":"Feifei Zhai"},{"#tail":"\n","@confidence":"0.951805","#text":"Jiajun Zhang"},{"#tail":"\n","@confidence":"0.951805","#text":"Yu Zhou"},{"#tail":"\n","@confidence":"0.951805","#text":"Chengqing Zong"}],"abstract":{"#tail":"\n","@confidence":"0.999889277777778","#text":"In current research, most tree-based translation models are built directly from parse trees. In this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model. In the model, we utilize synchronous tree substitution grammars (STSG) to capture the bilingual mapping between language pairs. To train the model efficiently, we develop a Gibbs sampler with three novel Gibbs operators. The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes. Experimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees."},"title":{"#tail":"\n","@confidence":"0.970642","#text":"Unsupervised Tree Induction for Tree-based Translation"},"email":[{"#tail":"\n","@confidence":"0.987524","#text":"ffzhai@nlpr.ia.ac.cn"},{"#tail":"\n","@confidence":"0.987524","#text":"jjzhang@nlpr.ia.ac.cn"},{"#tail":"\n","@confidence":"0.987524","#text":"yzhou@nlpr.ia.ac.cn"},{"#tail":"\n","@confidence":"0.987524","#text":"cqzong@nlpr.ia.ac.cn"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"volume":{"#tail":"\n","#text":"21"},"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Phil Blunsom, Trevor Cohn, Miles Osborne. 2008. Bayesian synchronous grammar induction. In Advances in Neural Information Processing Systems, volume 21, pages 161-168."},"#text":"\n","pages":{"#tail":"\n","#text":"161--168"},"marker":{"#tail":"\n","#text":"Blunsom, Cohn, Osborne, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"w direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on ","@endWordPosition":"845","@position":"5600","annotationId":"T1","@startWordPosition":"842","@citStr":"Blunsom et al (2008"}},"title":{"#tail":"\n","#text":"Bayesian synchronous grammar induction."},"booktitle":{"#tail":"\n","#text":"In Advances in Neural Information Processing Systems,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Phil Blunsom"},{"#tail":"\n","#text":"Trevor Cohn"},{"#tail":"\n","#text":"Miles Osborne"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proc. of ACL 2009, pages 782-790."},"#text":"\n","pages":{"#tail":"\n","#text":"782--790"},"marker":{"#tail":"\n","#text":"Blunsom, Cohn, Dyer, Osborne, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"power on tree construction than SCFG. In a STSG-based U-tree or a STSG rule, although not linguistically informed, the nodes labeled by POS tags are also effective on distinguishing different ones. However, with SCFG, we have to discard all the internal nodes (i.e., flattening the Utrees or rules) to express the same sequence, leading to a poor ability of distinguishing different U-trees and production rules. Thus, using STSG, we can build more specific U-trees for translation. In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar (Blunsom et al 2009)5. This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models. 5 In (Blunsom et al, 2009), for Chinese-to-English translation, the Bayesian SCFG grammar only outperform the heuristic SCFG grammar by 0.1 BLEU points on NIST MT 2004 and 0.6 BLEU points on NIST MT 2005 in the NEWS domain. 245 4 Bayesian Model In this section, we present a Bayesian model to learn STSG defined in sectio","@endWordPosition":"1914","@position":"11988","annotationId":"T2","@startWordPosition":"1911","@citStr":"Blunsom et al 2009"},{"#tail":"\n","#text":"mine that the left child of PRP?RB is a one-word non-terminal (labeled with PRP); b. Expand PRP and generate the word ?we? for PRP; 6 In our experiment, we set pexpand to 1/3 to encourage small tree fragments. 246 c. Determine that the right child of PRP?RB is a two-word non-terminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the two-word nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB. ( | )P str frag in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by (Blunsom et al, 2009) and (Cohn and Blunsom, 2009), we define ( | )P str frag as follows: var 1 1 1 ( | ) ( ;1) | | poisson sw sw sw c c s i P str frag P c c i u u? ? (5) where csw is the number of words in the source string. ?s means the source vocabulary set. Further, cvar denotes the number of variables, which is determined by the tree fragment frag. As shown in Equation(5), we first determine how many source words to generate using a Poisson distribution Ppoisson(csw;1), which imposes a stable preference for short source strings. Then, we draw each source word from a uniform distribution over ?s. Afterwards, ","@endWordPosition":"2859","@position":"17087","annotationId":"T3","@startWordPosition":"2856","@citStr":"Blunsom et al, 2009"}]},"title":{"#tail":"\n","#text":"A gibbs sampler for phrasal synchronous grammar induction."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Phil Blunsom"},{"#tail":"\n","#text":"Trevor Cohn"},{"#tail":"\n","#text":"Chris Dyer"},{"#tail":"\n","#text":"Miles Osborne"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Proc."},"#text":"\n","marker":{"#tail":"\n","#text":"Blunsom, Cohn, 2010"},"title":{"#tail":"\n","#text":"Inducing synchronous grammars with slice sampling."},"booktitle":{"#tail":"\n","#text":"In Proc."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Phil Blunsom"},{"#tail":"\n","#text":"Trevor Cohn"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"of NAACL 2010, pages 238-241."},"#text":"\n","pages":{"#tail":"\n","#text":"238--241"},"marker":{"#tail":"\n","#text":"NAACL, 2010"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"of NAACL"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic Parsing). In Proc. of EMNLP 2008, pages 877-886."},"#text":"\n","pages":{"#tail":"\n","#text":"877--886"},"marker":{"#tail":"\n","#text":"Burkett, Klein, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"to the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on genera","@endWordPosition":"927","@position":"6163","annotationId":"T4","@startWordPosition":"924","@citStr":"Burkett and Klein (2008)"}},"title":{"#tail":"\n","#text":"Two languages are better than one (for syntactic Parsing)."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Burkett"},{"#tail":"\n","#text":"Dan Klein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In Proc. of NAACL 2010, pages 127-135."},"#text":"\n","pages":{"#tail":"\n","#text":"127--135"},"marker":{"#tail":"\n","#text":"Burkett, Blitzer, Klein, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervis","@endWordPosition":"932","@position":"6188","annotationId":"T5","@startWordPosition":"929","@citStr":"Burkett et al (2010)"}},"title":{"#tail":"\n","#text":"Joint parsing and alignment with weakly synchronized grammars."},"booktitle":{"#tail":"\n","#text":"In Proc. of NAACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Burkett"},{"#tail":"\n","#text":"John Blitzer"},{"#tail":"\n","#text":"Dan Klein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"David Burkett and Dan Klein. 2012. Transforming trees to improve syntactic convergence. In Proc. of EMNLP 2012, pages 863-872."},"#text":"\n","pages":{"#tail":"\n","#text":"863--872"},"marker":{"#tail":"\n","#text":"Burkett, Klein, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cause we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG tr","@endWordPosition":"993","@position":"6565","annotationId":"T6","@startWordPosition":"990","@citStr":"Burkett and Klein (2012)"}},"title":{"#tail":"\n","#text":"Transforming trees to improve syntactic convergence."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP 2012,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Burkett"},{"#tail":"\n","#text":"Dan Klein"}]}},{"date":{"#tail":"\n","#text":"2007"},"issue":{"#tail":"\n","#text":"2"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space ","@endWordPosition":"876","@position":"5817","annotationId":"T7","@startWordPosition":"875","@citStr":"Chiang, 2007"}},"title":{"#tail":"\n","#text":"Hierarchical phrase-based translation."},"volume":{"#tail":"\n","#text":"33"},"#tail":"\n","rawString":{"#tail":"\n","#text":"David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33 (2). pages 201-228."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"201--228"},"marker":{"#tail":"\n","#text":"Chiang, 2007"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David Chiang"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Dekai Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. of ACL 1996, pages 152-158."},"#text":"\n","pages":{"#tail":"\n","#text":"152--158"},"marker":{"#tail":"\n","#text":"Wu, 1996"},"title":{"#tail":"\n","#text":"A polynomial-time algorithm for statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dekai Wu"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377-404."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"23--377"},"marker":{"#tail":"\n","#text":"Wu, 1997"},"title":{"#tail":"\n","#text":"Stochastic inversion transduction grammars and bilingual parsing of parallel corpora."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dekai Wu"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Trevor Cohn and Phil Blunsom. 2009. A bayesian model of syntax-directed tree to string grammar induction. In Proc. of EMNLP 2009, pages 352-361."},"#text":"\n","pages":{"#tail":"\n","#text":"352--361"},"marker":{"#tail":"\n","#text":"Cohn, Blunsom, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended ","@endWordPosition":"958","@position":"6350","annotationId":"T8","@startWordPosition":"955","@citStr":"Cohn and Blunsom (2009)"},{"#tail":"\n","#text":"in ir . Thanks to the exchangeability of the model, all permutations of the rules are actually equiprobable. This means that we can compute the probability of each rule based on the previous and subsequent rules (i.e. consider each rule as the last one). This characteristic allows us to design an efficient Gibbs sampling algorithm to train the Bayesian model. 4.1 Base Distribution The base distribution 0 ( | )P r N is designed to assign prior probabilities to the STSG production rules. Because each rule r consists of a target tree fragment frag and a source string str in the model, we follow Cohn and Blunsom (2009) and decompose the prior probability 0 ( | )P r N into two factors as follows: 0 ( | ) ( | ) ( | )P r N P frag N P str frag ? (4) where ( | )P frag N is the probability of producing the target tree fragment frag. To generate frag, Cohn and Blunsom (2009) used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al, 2007; Zhang et al, 2011a). The generation process starts at root","@endWordPosition":"2338","@position":"14187","annotationId":"T9","@startWordPosition":"2335","@citStr":"Cohn and Blunsom (2009)"},{"#tail":"\n","#text":"f PRP?RB is a one-word non-terminal (labeled with PRP); b. Expand PRP and generate the word ?we? for PRP; 6 In our experiment, we set pexpand to 1/3 to encourage small tree fragments. 246 c. Determine that the right child of PRP?RB is a two-word non-terminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the two-word nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB. ( | )P str frag in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by (Blunsom et al, 2009) and (Cohn and Blunsom, 2009), we define ( | )P str frag as follows: var 1 1 1 ( | ) ( ;1) | | poisson sw sw sw c c s i P str frag P c c i u u? ? (5) where csw is the number of words in the source string. ?s means the source vocabulary set. Further, cvar denotes the number of variables, which is determined by the tree fragment frag. As shown in Equation(5), we first determine how many source words to generate using a Poisson distribution Ppoisson(csw;1), which imposes a stable preference for short source strings. Then, we draw each source word from a uniform distribution over ?s. Afterwards, we insert the variables into ","@endWordPosition":"2864","@position":"17116","annotationId":"T10","@startWordPosition":"2861","@citStr":"Cohn and Blunsom, 2009"},{"#tail":"\n","#text":"one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word ??? (wo-men) ?; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously obtain large rules with enough context information by composition. We will show the effectiveness of our U-tr","@endWordPosition":"3086","@position":"18378","annotationId":"T11","@startWordPosition":"3083","@citStr":"Cohn and Blunsom (2009)"},{"#tail":"\n","#text":" each part. The created tree nodes are labeled by the non-terminals described in section 3. Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules7 in terms of frontier nodes (Galley et al, 2004). Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler. Differently, Cohn and Blunsom (2009) designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics. For each training instance (a pair of source sentence and target U-tree structure), the extracted GHKM minimal translation rules compose a unique STSG derivation9. Moreover, all the rules developed from the training data constitute an initial STSG for the Gibbs sampler. 7 We attach the unaligned word to the lowest frontier node that ","@endWordPosition":"3349","@position":"20015","annotationId":"T12","@startWordPosition":"3346","@citStr":"Cohn and Blunsom (2009)"}]},"title":{"#tail":"\n","#text":"A bayesian model of syntax-directed tree to string grammar induction."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Trevor Cohn"},{"#tail":"\n","#text":"Phil Blunsom"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, pages 3053-3096."},"journal":{"#tail":"\n","#text":"Journal of Machine Learning Research,"},"#text":"\n","pages":{"#tail":"\n","#text":"3053--3096"},"marker":{"#tail":"\n","#text":"Cohn, Blunsom, Goldwater, 2010"},"title":{"#tail":"\n","#text":"Inducing tree-substitution grammars."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Trevor Cohn"},{"#tail":"\n","#text":"Phil Blunsom"},{"#tail":"\n","#text":"Sharon Goldwater"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Brooke Cowan, Ivona Kucerova and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proc. of EMNLP 2006, pages 232-241."},"#text":"\n","pages":{"#tail":"\n","#text":"232--241"},"marker":{"#tail":"\n","#text":"Cowan, 2006"},"title":{"#tail":"\n","#text":"Ivona Kucerova and Michael Collins."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Brooke Cowan"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In Proc."},"#text":"\n","marker":{"#tail":"\n","#text":"DeNero, Klein, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ariables into the string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word ??? (wo-men) ?; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously obtain large rules with enough context information by co","@endWordPosition":"3078","@position":"18325","annotationId":"T13","@startWordPosition":"3075","@citStr":"DeNero and Klein (2007)"}},"title":{"#tail":"\n","#text":"Tailoring word alignments to syntactic machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"John DeNero"},{"#tail":"\n","#text":"Dan Klein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"of ACL 2007, pages 17-24."},"#text":"\n","pages":{"#tail":"\n","#text":"17--24"},"marker":{"#tail":"\n","#text":"ACL, 2007"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"of ACL"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"John DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reordering. In Proc. of EMNLP 2011, pages 193-203."},"#text":"\n","pages":{"#tail":"\n","#text":"193--203"},"marker":{"#tail":"\n","#text":"DeNero, Uszkoreit, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Bayesian model efficiently. The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in","@endWordPosition":"774","@position":"5150","annotationId":"T14","@startWordPosition":"771","@citStr":"DeNero and Uszkoreit (2011)"}},"title":{"#tail":"\n","#text":"Inducing sentence structure from parallel corpora for reordering."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"John DeNero"},{"#tail":"\n","#text":"Jakob Uszkoreit"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Chris Dyer. 2010. Two monolingual parses are better than one (synchronous parse). In Proc. of NAACL 2010, pages 263-266."},"#text":"\n","pages":{"#tail":"\n","#text":"263--266"},"marker":{"#tail":"\n","#text":"Dyer, 2010"},"title":{"#tail":"\n","#text":"Two monolingual parses are better than one (synchronous parse)."},"booktitle":{"#tail":"\n","#text":"In Proc. of NAACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Chris Dyer"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. of ACL 2003, pages 205-208."},"#text":"\n","pages":{"#tail":"\n","#text":"205--208"},"marker":{"#tail":"\n","#text":"Eisner, 2003"},"title":{"#tail":"\n","#text":"Learning non-isomorphic tree mappings for machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Jason Eisner"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What?s in a translation rule. In Proc. of HLT-NAACL 2004, pages 273?280."},"#text":"\n","pages":{"#tail":"\n","#text":"273--280"},"marker":{"#tail":"\n","#text":"Galley, Hopkins, Knight, Marcu, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rathe","@endWordPosition":"234","@position":"1722","annotationId":"T15","@startWordPosition":"231","@citStr":"Galley et al, 2004"},{"#tail":"\n","#text":"ning by Gibbs Sampling In this section, we introduce a collapsed Gibbs sampler, which enables us to train the Bayesian model efficiently. 5.1 Initialization State At first, we use random binary trees to initialize the sampler. To get the initial U-trees, we recursively and randomly segment a sentence into two parts and simultaneously create a tree node to dominate each part. The created tree nodes are labeled by the non-terminals described in section 3. Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules7 in terms of frontier nodes (Galley et al, 2004). Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler. Differently, Cohn and Blunsom (2009) designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will","@endWordPosition":"3286","@position":"19645","annotationId":"T16","@startWordPosition":"3283","@citStr":"Galley et al, 2004"},{"#tail":"\n","#text":"word to the lowest frontier node that can cover it in terms of word alignment. 8 The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM). Actually, the frequent AEs also greatly impair the conventional TM. Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs. Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs. Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules (Galley et al, 2004) here to reduce the complexity of the sampler. 247 jin-tian jian-mianwo-men zai-ci PRP+VBP today NN we PRP meet VBP again RB ?? ?? ?? ?? PRP...RB NN...RB Figure 2. Illustration of an initial U-tree structure. The bold italic nodes with shadows are frontier nodes. Under this initial STSG, the sampler modifies the initial U-trees (initial sample) to create a series of new ones (new samples) by the Gibbs operators. Consequently, new STSGs are created based on the new U-trees simultaneously and used for the next sampling operation. Repeatedly and after a number of iterations, we can obtain the fin","@endWordPosition":"3542","@position":"21196","annotationId":"T17","@startWordPosition":"3539","@citStr":"Galley et al, 2004"},{"#tail":"\n","#text":"ts state to the right state ? , and transform the U-tree to Figure 3(b) accordingly. jian-mianwo-men zai-ci s-node we PRP meet VBP again RB ?? ?? ?? PRP...RB PRP+VBP jian-mianwo-men zai-ci s-node we PRP meet VBP again RB ?? ?? ?? PRP...RB VBP+RB (b) ?=1(a) ?=0 Rotate Figure 3. Illustration of the Rotate operator. In the figure, (a) and (b) denote the s-node?s left state and right state respectively. The bold italic nodes with shadows in the figure are frontier nodes. Obviously, towards an s-node for sampling, the two values of ? would define two different U-trees. Using the GHKM algorithm (Galley et al 2004), we can get two different STSG derivations from the two U-trees based on the fixed word alignment. Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node?s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node. For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (? ) defines only one rule: 0 2 1 0 1 2 : ... ( ( : : ) : ) leftr x x x PRP RB PRP VBP x PRP x VBP x RB o  Differently, in Fig","@endWordPosition":"4101","@position":"24502","annotationId":"T18","@startWordPosition":"4098","@citStr":"Galley et al 2004"},{"#tail":"\n","#text":"alty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the ","@endWordPosition":"5181","@position":"30497","annotationId":"T19","@startWordPosition":"5178","@citStr":"Galley et al, 2004"}]},"title":{"#tail":"\n","#text":"What?s in a translation rule."},"booktitle":{"#tail":"\n","#text":"In Proc. of HLT-NAACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michel Galley"},{"#tail":"\n","#text":"Mark Hopkins"},{"#tail":"\n","#text":"Kevin Knight"},{"#tail":"\n","#text":"Daniel Marcu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of ACL-COLING 2006, pages 961-968."},"#text":"\n","pages":{"#tail":"\n","#text":"961--968"},"marker":{"#tail":"\n","#text":"Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"g set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees","@endWordPosition":"5162","@position":"30398","annotationId":"T20","@startWordPosition":"5159","@citStr":"Galley et al, 2006"}},"title":{"#tail":"\n","#text":"Scalable inference and training of context-rich syntactic translation models."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL-COLING"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michel Galley"},{"#tail":"\n","#text":"Jonathan Graehl"},{"#tail":"\n","#text":"Kevin Knight"},{"#tail":"\n","#text":"Daniel Marcu"},{"#tail":"\n","#text":"Steve DeNeefe"},{"#tail":"\n","#text":"Wei Wang"},{"#tail":"\n","#text":"Ignacio Thayer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Jonathan Weese, Juri Ganitkevitch, Chris CallisonBurch, Matt Post and Adam Lopez. 2011. Joshua 3.0: syntax-based machine translation with the thrax Grammar Extractor. In Proc of WMT11, pages 478-484."},"#text":"\n","pages":{"#tail":"\n","#text":"478--484"},"marker":{"#tail":"\n","#text":"Weese, Ganitkevitch, CallisonBurch, Post, Lopez, 2011"},"title":{"#tail":"\n","#text":"Joshua 3.0: syntax-based machine translation with the thrax Grammar Extractor."},"booktitle":{"#tail":"\n","#text":"In Proc of WMT11,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jonathan Weese"},{"#tail":"\n","#text":"Juri Ganitkevitch"},{"#tail":"\n","#text":"Chris CallisonBurch"},{"#tail":"\n","#text":"Matt Post"},{"#tail":"\n","#text":"Adam Lopez"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Liang Huang, Kevin Knight and Aravind Joshi. 2006. A syntax-directed translator with extended domain of locality. In Proc. of AMTA 2006, pages 65-73."},"#text":"\n","pages":{"#tail":"\n","#text":"65--73"},"marker":{"#tail":"\n","#text":"Huang, Knight, Joshi, 2006"},"title":{"#tail":"\n","#text":"A syntax-directed translator with extended domain of locality."},"booktitle":{"#tail":"\n","#text":"In Proc. of AMTA"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Liang Huang"},{"#tail":"\n","#text":"Kevin Knight"},{"#tail":"\n","#text":"Aravind Joshi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Philipp Koehn, Franz Och, and Daniel Marcu. 2003."},"#text":"\n","marker":{"#tail":"\n","#text":"Koehn, Och, Marcu, 2003"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Franz Och"},{"#tail":"\n","#text":"Daniel Marcu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Statistical phrase-based translation, In Proc. of HLT/NAACL 2003, pages 48-54."},"#text":"\n","pages":{"#tail":"\n","#text":"48--54"},"marker":{"#tail":"\n","#text":"2003"},"title":{"#tail":"\n","#text":"Statistical phrase-based translation,"},"booktitle":{"#tail":"\n","#text":"In Proc. of HLT/NAACL"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004, pages 388?395."},"#text":"\n","pages":{"#tail":"\n","#text":"388--395"},"marker":{"#tail":"\n","#text":"Koehn, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" can also take more nodes as a unit for sampling, but this would make the algorithm much more complex. 249 the training data. For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set. We use MERT (Och, 2004) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to len","@endWordPosition":"5096","@position":"29972","annotationId":"T21","@startWordPosition":"5095","@citStr":"Koehn, 2004"}},"title":{"#tail":"\n","#text":"Statistical significance tests for machine translation evaluation."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Philipp Koehn"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, RichDUG =HQV &KULV '\\HU DQG 2QG?HM %RMDU. 2007."},"#text":"\n","marker":{"#tail":"\n","#text":"Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, 2007"},"booktitle":{"#tail":"\n","#text":"RichDUG =HQV &KULV '\\HU DQG 2QG?HM %RMDU."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Hieu Hoang"},{"#tail":"\n","#text":"Alexandra Birch"},{"#tail":"\n","#text":"Chris Callison-Burch"},{"#tail":"\n","#text":"Marcello Federico"},{"#tail":"\n","#text":"Nicola Bertoldi"},{"#tail":"\n","#text":"Brooke Cowan"},{"#tail":"\n","#text":"Wade Shen"},{"#tail":"\n","#text":"Christine Moran"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Moses: open source toolkit for statistical machine translation. In Proc. of ACL 2007, pages 177-180."},"#text":"\n","pages":{"#tail":"\n","#text":"177--180"},"marker":{"#tail":"\n","#text":"2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word ??? (wo-men) ?; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously obtain large rules with enough context information by co","@endWordPosition":"3078","@position":"18325","annotationId":"T22","@startWordPosition":"3078","@citStr":"(2007)"}},"title":{"#tail":"\n","#text":"Moses: open source toolkit for statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Abby Levenberg, Chris Dyer and Phil Blunsom. 2012."},"#text":"\n","marker":{"#tail":"\n","#text":"Levenberg, Dyer, Blunsom, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on p","@endWordPosition":"880","@position":"5841","annotationId":"T23","@startWordPosition":"877","@citStr":"Levenberg et al (2012)"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Abby Levenberg"},{"#tail":"\n","#text":"Chris Dyer"},{"#tail":"\n","#text":"Phil Blunsom"}]}},{"#tail":"\n","institution":{"#tail":"\n","#text":"Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane"},"rawString":{"#tail":"\n","#text":"A bayesian model for learning SCFGs with discontiguous Rules. In Proc. of EMNLP 2012, pages 223-232. Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N.G. Thornton, Jonathan Weese and Omar F."},"#text":"\n","pages":{"#tail":"\n","#text":"223--232"},"marker":{"#tail":"\n","#text":"Schwartz, Weese, Omar, "},"title":{"#tail":"\n","#text":"A bayesian model for learning SCFGs with discontiguous Rules."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP 2012,"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Wren N G Thornton Schwartz"},{"#tail":"\n","#text":"Jonathan Weese"},{"#tail":"\n","#text":"F Omar"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proc. of ACL 2009, pages 135-139."},"#text":"\n","pages":{"#tail":"\n","#text":"135--139"},"marker":{"#tail":"\n","#text":"Zaidan, 2009"},"title":{"#tail":"\n","#text":"Joshua: An open source toolkit for parsing-based machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Zaidan"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Shujie Liu, Chi-Ho Li, Mu Li, Ming Zhou. 2012. Retraining monolingual parser bilingually for syntactic SMT. In Proc. of EMNLP 2012, pages 854-862."},"#text":"\n","pages":{"#tail":"\n","#text":"854--862"},"marker":{"#tail":"\n","#text":"Liu, Li, Li, Zhou, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word clas","@endWordPosition":"980","@position":"6469","annotationId":"T24","@startWordPosition":"977","@citStr":"Liu et al (2012)"}},"title":{"#tail":"\n","#text":"Retraining monolingual parser bilingually for syntactic SMT."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP 2012,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Shujie Liu"},{"#tail":"\n","#text":"Chi-Ho Li"},{"#tail":"\n","#text":"Mu Li"},{"#tail":"\n","#text":"Ming Zhou"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proc. of ACL-COLING 2006, pages 609-616."},"#text":"\n","pages":{"#tail":"\n","#text":"609--616"},"marker":{"#tail":"\n","#text":"Liu, Liu, Lin, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"es on the tree nodes. Experimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model","@endWordPosition":"225","@position":"1677","annotationId":"T25","@startWordPosition":"222","@citStr":"Liu et al, 2006"}},"title":{"#tail":"\n","#text":"Tree-tostring alignment template for statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL-COLING"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yang Liu"},{"#tail":"\n","#text":"Qun Liu"},{"#tail":"\n","#text":"Shouxun Lin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Yang Liu, Yajuan Lv and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proc."},"#text":"\n","marker":{"#tail":"\n","#text":"Liu, 2009"},"title":{"#tail":"\n","#text":"Yajuan Lv and Qun Liu."},"booktitle":{"#tail":"\n","#text":"In Proc."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Yang Liu"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"of ACL-IJCNLP 2009, pages 558-566."},"#text":"\n","pages":{"#tail":"\n","#text":"558--566"},"marker":{"#tail":"\n","#text":"ACL-IJCNLP, 2009"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"of ACL-IJCNLP"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Daniel Marcu, Wei Wang, Abdessamad Echihabi and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases."},"#text":"\n","marker":{"#tail":"\n","#text":"Marcu, Wang, Echihabi, Knight, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ee translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapp","@endWordPosition":"239","@position":"1747","annotationId":"T26","@startWordPosition":"236","@citStr":"Marcu et al, 2006"},{"#tail":"\n","#text":"ality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees to build another s2t s","@endWordPosition":"5167","@position":"30421","annotationId":"T27","@startWordPosition":"5164","@citStr":"Marcu et al 2006"}]},"title":{"#tail":"\n","#text":"SPMT: Statistical machine translation with syntactified target language phrases."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Daniel Marcu"},{"#tail":"\n","#text":"Wei Wang"},{"#tail":"\n","#text":"Abdessamad Echihabi"},{"#tail":"\n","#text":"Kevin Knight"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"In Proc. of EMNLP 2006, pages 44-52."},"#text":"\n","pages":{"#tail":"\n","#text":"44--52"},"marker":{"#tail":"\n","#text":"2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"k to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution gram","@endWordPosition":"1035","@position":"6871","annotationId":"T28","@startWordPosition":"1035","@citStr":"(2006)"},{"#tail":"\n","#text":" for two word strings. They are labeled in the form of C1+C2, where C1 and C2 are the word classes of the two words separately. Accordingly, multi-word non-terminals represent the strings containing more than two words. They are labeled as C1?Cn, demanding that the word classes of the leftmost word and the rightmost word are C1 and Cn, respectively. We use POS tag to play the role of word class4. For example, the head node of the rule in Figure 1 is a multi-word non-terminal PRP?RB. It requires that the POS tags of the leftmost and rightmost word must be PRP and RB, respectively. Xiong et al. (2006) showed that the boundary word is an effective indicator for phrase reordering. Thus, we believe that combining the word class of boundary words can denote the whole phrase well. PRP...RB we PRP VBP:x0 RB:x1 VBP+RB ?? x1 x0 wo-men Figure 1. An example of an STSG production rule. Each production rule in P consists of a source string and a target tree fragment. In the target tree fragment, each internal node is labeled with a nonterminal in tN , and each leaf node is labeled with either a target word in t? or a non-terminal in tN . The source string in a production rule comprises source words an","@endWordPosition":"1402","@position":"9026","annotationId":"T29","@startWordPosition":"1402","@citStr":"(2006)"}]},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Franz Och, 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160-167."},"#text":"\n","pages":{"#tail":"\n","#text":"160--167"},"marker":{"#tail":"\n","#text":"Och, 2003"},"title":{"#tail":"\n","#text":"Minimum error rate training in statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Franz Och"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proc. of ACL 2002, pages 311-318."},"#text":"\n","pages":{"#tail":"\n","#text":"311--318"},"marker":{"#tail":"\n","#text":"Papineni, Roukos, Ward, Zhu, 2002"},"title":{"#tail":"\n","#text":"BLEU: A method for automatic evaluation of machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kishore Papineni"},{"#tail":"\n","#text":"Salim Roukos"},{"#tail":"\n","#text":"Todd Ward"},{"#tail":"\n","#text":"WeiJing Zhu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of COLINGACL 2006, pages 433-440."},"#text":"\n","pages":{"#tail":"\n","#text":"433--440"},"marker":{"#tail":"\n","#text":"Petrov, Barrett, Thibaux, Klein, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set ? to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the trans","@endWordPosition":"5248","@position":"30858","annotationId":"T30","@startWordPosition":"5245","@citStr":"Petrov et al, 2006"}},"title":{"#tail":"\n","#text":"Learning accurate, compact, and interpretable tree annotation."},"booktitle":{"#tail":"\n","#text":"In Proc. of COLINGACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Slav Petrov"},{"#tail":"\n","#text":"Leon Barrett"},{"#tail":"\n","#text":"Romain Thibaux"},{"#tail":"\n","#text":"Dan Klein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Chris Quirk, Arul Menezes and Colin Cherry. 2005."},"#text":"\n","marker":{"#tail":"\n","#text":"Quirk, Menezes, Cherry, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"xperimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monoling","@endWordPosition":"230","@position":"1702","annotationId":"T31","@startWordPosition":"227","@citStr":"Quirk et al, 2005"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chris Quirk"},{"#tail":"\n","#text":"Arul Menezes"},{"#tail":"\n","#text":"Colin Cherry"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Dependency treelet translation: syntactically informed phrasal SMT. In Proc. of ACL 2005, pages 271-279."},"#text":"\n","pages":{"#tail":"\n","#text":"271--279"},"marker":{"#tail":"\n","#text":"2005"},"title":{"#tail":"\n","#text":"Dependency treelet translation: syntactically informed phrasal SMT."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proc. of ACL-08, pages 577-585."},"#text":"\n","pages":{"#tail":"\n","#text":"577--585"},"marker":{"#tail":"\n","#text":"Shen, Xu, Weischedel, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"em using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between langua","@endWordPosition":"243","@position":"1765","annotationId":"T32","@startWordPosition":"240","@citStr":"Shen et al, 2008"}},"title":{"#tail":"\n","#text":"A new string-to-dependency machine translation algorithm with a target dependency language model."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL-08,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Libin Shen"},{"#tail":"\n","#text":"Jinxi Xu"},{"#tail":"\n","#text":"Ralph Weischedel"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Wei Wang, Kevin Knight, and Daniel Marcu. 2007."},"#text":"\n","marker":{"#tail":"\n","#text":"Wang, Knight, Marcu, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"frag and a source string str in the model, we follow Cohn and Blunsom (2009) and decompose the prior probability 0 ( | )P r N into two factors as follows: 0 ( | ) ( | ) ( | )P r N P frag N P str frag ? (4) where ( | )P frag N is the probability of producing the target tree fragment frag. To generate frag, Cohn and Blunsom (2009) used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al, 2007; Zhang et al, 2011a). The generation process starts at root node N. At first, root node N is expanded into two child nodes. Then, each newly generated node will be checked to expand into two new child nodes with probability pexpand. This process repeats until all the new non-terminal nodes are checked. Obviously, pexpand controls the scale of tree fragments, where a large pexpand corresponds to large fragments 6. The new terminal nodes (words) are drawn uniformly from the target-side vocabulary, and the nonterminal nodes are created by asking two questions as follows: 1) What type is the node","@endWordPosition":"2444","@position":"14727","annotationId":"T33","@startWordPosition":"2441","@citStr":"Wang et al, 2007"},{"#tail":"\n","#text":"). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set ? to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 From ","@endWordPosition":"5264","@position":"30957","annotationId":"T34","@startWordPosition":"5261","@citStr":"Wang et al, 2007"}]},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Wei Wang"},{"#tail":"\n","#text":"Kevin Knight"},{"#tail":"\n","#text":"Daniel Marcu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proc. of EMNLP 2007, pages 746-754."},"#text":"\n","pages":{"#tail":"\n","#text":"746--754"},"marker":{"#tail":"\n","#text":"2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word ??? (wo-men) ?; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously obtain large rules with enough context information by co","@endWordPosition":"3078","@position":"18325","annotationId":"T35","@startWordPosition":"3078","@citStr":"(2007)"}},"title":{"#tail":"\n","#text":"Binarizing syntax trees to improve syntax-based machine translation accuracy."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP"},"@valid":"true"},{"volume":{"#tail":"\n","#text":"36"},"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, re-labeling, and realigning for syntax-based machine translation. Computational Linguistics, 36(2):247?277."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Wang, May, Knight, Marcu, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"y built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. This indicates that parse trees are usually not the optimal choice for training tree-based translation models (Wang et al, 2010). Based on the above analysis, we can conclude that the tree structure that is independent from Tree-bank resources and simultaneously considers the bilingual mapping inside the bilingual sentence pairs would be a good choice for building treebased translation models. Therefore, complying with the above conditions, we propose an unsupervised tree structure for treebased translation models in this study. In the structures, tree nodes are labeled by combining the word classes of their boundary words rather than by syntactic labels, such as NP, VP. Furthermore, using these node labels, we design ","@endWordPosition":"357","@position":"2503","annotationId":"T36","@startWordPosition":"354","@citStr":"Wang et al, 2010"}},"title":{"#tail":"\n","#text":"Re-structuring, re-labeling, and realigning for syntax-based machine translation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Wei Wang"},{"#tail":"\n","#text":"Jonathan May"},{"#tail":"\n","#text":"Kevin Knight"},{"#tail":"\n","#text":"Daniel Marcu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong. 2012. Tree-based translation without using parse trees. In Proc. of COLING 2012, pages 3037-3054."},"#text":"\n","pages":{"#tail":"\n","#text":"3037--3054"},"marker":{"#tail":"\n","#text":"Zhai, Zhang, Zhou, Zong, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"on process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG","@endWordPosition":"796","@position":"5296","annotationId":"T37","@startWordPosition":"793","@citStr":"Zhai et al, 2012"}},"title":{"#tail":"\n","#text":"Tree-based translation without using parse trees."},"booktitle":{"#tail":"\n","#text":"In Proc. of COLING 2012,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Feifei Zhai"},{"#tail":"\n","#text":"Jiajun Zhang"},{"#tail":"\n","#text":"Yu Zhou"},{"#tail":"\n","#text":"Chengqing Zong"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Hao Zhang, Liang Huang, Daniel Gildea and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proc. of HLT-NAACL 2006, pages 256-263."},"#text":"\n","pages":{"#tail":"\n","#text":"256--263"},"marker":{"#tail":"\n","#text":"Zhang, Huang, Gildea, Knight, 2006"},"title":{"#tail":"\n","#text":"Synchronous binarization for machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of HLT-NAACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hao Zhang"},{"#tail":"\n","#text":"Liang Huang"},{"#tail":"\n","#text":"Daniel Gildea"},{"#tail":"\n","#text":"Kevin Knight"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Hao Zhang, Daniel Gildea, and David Chiang. 2008."},"#text":"\n","marker":{"#tail":"\n","#text":"Zhang, Gildea, Chiang, 2008"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hao Zhang"},{"#tail":"\n","#text":"Daniel Gildea"},{"#tail":"\n","#text":"David Chiang"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Extracting synchronous grammars rules from word level alignments in linear time. In Proc. of COLING 2008, pages 1081-1088."},"#text":"\n","pages":{"#tail":"\n","#text":"1081--1088"},"marker":{"#tail":"\n","#text":"2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing","@endWordPosition":"847","@position":"5613","annotationId":"T38","@startWordPosition":"845","@citStr":"(2008, 2009, 2010)"}},"title":{"#tail":"\n","#text":"Extracting synchronous grammars rules from word level alignments in linear time."},"booktitle":{"#tail":"\n","#text":"In Proc. of COLING"},"@valid":"true"},{"#tail":"\n","rawString":{"#tail":"\n","#text":"Hao Zhang, Licheng Fang, Peng Xu, Xiaoyun Wu. 2011a. Binarized forest to string translation. In Proc."},"#text":"\n","marker":{"#tail":"\n","#text":"Zhang, "},"title":{"#tail":"\n","#text":"Licheng Fang, Peng Xu, Xiaoyun Wu. 2011a. Binarized forest to string translation."},"booktitle":{"#tail":"\n","#text":"In Proc."},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Hao Zhang"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"of ACL 2011, pages 835-845."},"#text":"\n","pages":{"#tail":"\n","#text":"835--845"},"marker":{"#tail":"\n","#text":"ACL, 2011"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"of ACL"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew Lim Tan. 2009. Forest-based tree sequence to string translation model. In Proc. of ACL-IJCNLP 2009, pages 172-180."},"#text":"\n","pages":{"#tail":"\n","#text":"172--180"},"marker":{"#tail":"\n","#text":"Zhang, Zhang, Li, Aw, Tan, 2009"},"title":{"#tail":"\n","#text":"Forest-based tree sequence to string translation model."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL-IJCNLP"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hui Zhang"},{"#tail":"\n","#text":"Min Zhang"},{"#tail":"\n","#text":"Haizhou Li"},{"#tail":"\n","#text":"Aiti Aw"},{"#tail":"\n","#text":"Chew Lim Tan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Jiajun Zhang, Feifei Zhai and Chengqing Zong. 2011b."},"#text":"\n","marker":{"#tail":"\n","#text":"Zhang, 2011"},"title":{"#tail":"\n","#text":"Feifei Zhai and Chengqing Zong."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Jiajun Zhang"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Augmenting string-to-tree translation models with fuzzy use of source-side syntax. In Proc. of EMNLP 2011, pages 204-215."},"#text":"\n","pages":{"#tail":"\n","#text":"204--215"},"marker":{"#tail":"\n","#text":"2011"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ently. The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in","@endWordPosition":"774","@position":"5150","annotationId":"T39","@startWordPosition":"774","@citStr":"(2011)"},{"#tail":"\n","#text":"SG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution grammar (STSG) between source strings and target tree fragments. We take STSG as the generation grammar to match the translation m","@endWordPosition":"1051","@position":"6997","annotationId":"T40","@startWordPosition":"1051","@citStr":"(2011)"}]},"title":{"#tail":"\n","#text":"Augmenting string-to-tree translation models with fuzzy use of source-side syntax."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim Tan and Sheng Li. 2007. A tree-to-tree alignment-based model for statistical Machine translation. MT-Summit-07. pages 535-542 Min Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, Chew Lim Tan and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proc. of ACL 2008, pages 559-567."},"#text":"\n","pages":{"#tail":"\n","#text":"535--542"},"marker":{"#tail":"\n","#text":"Zhang, Jiang, Aw, Sun, Tan, Li, 2007"},"location":{"#tail":"\n","#text":"Min Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, Chew Lim Tan"},"title":{"#tail":"\n","#text":"A tree-to-tree alignment-based model for statistical Machine translation."},"booktitle":{"#tail":"\n","#text":"MT-Summit-07."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Min Zhang"},{"#tail":"\n","#text":"Hongfei Jiang"},{"#tail":"\n","#text":"Ai Ti Aw"},{"#tail":"\n","#text":"Jun Sun"},{"#tail":"\n","#text":"Chew Lim Tan"},{"#tail":"\n","#text":"Sheng Li"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. of Workshop on Statistical Machine Translation 2006, pages 138-141."},"#text":"\n","pages":{"#tail":"\n","#text":"138--141"},"marker":{"#tail":"\n","#text":"Zollmann, Venugopal, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution gram","@endWordPosition":"1035","@position":"6871","annotationId":"T41","@startWordPosition":"1032","@citStr":"Zollmann and Venugopal (2006)"},{"#tail":"\n","#text":"t set, and use the NIST MT04 and MT05 data as the test set. We use MERT (Och, 2004) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of th","@endWordPosition":"5131","@position":"30196","annotationId":"T42","@startWordPosition":"5128","@citStr":"Zollmann and Venugopal, 2006"}]},"title":{"#tail":"\n","#text":"Syntax augmented machine translation via chart parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of Workshop on Statistical Machine Translation"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Andreas Zollmann"},{"#tail":"\n","#text":"Ashish Venugopal"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Andreas Zollmann and Stephan Vogel. 2011. A wordclass approach to labeling PSCFG rules for machine translation. In Proc. of ACL 2011, pages 1-11."},"#text":"\n","pages":{"#tail":"\n","#text":"1--11"},"marker":{"#tail":"\n","#text":"Zollmann, Vogel, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"thod to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution grammar (STSG) between source strings and target tree fragments. We take STSG as the generation grammar to match the translation m","@endWordPosition":"1051","@position":"6997","annotationId":"T43","@startWordPosition":"1048","@citStr":"Zollmann and Vogel (2011)"},{"#tail":"\n","#text":"and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set ? to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 From (Zollmann and Vogel, 2011), we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags. Thus, to be convenient, we only conduct experiments with the SAMT system. 6.2 Analysis of The Gibbs Sampler To evaluate the effectiveness of the Gibbs sampler, we explore the change of the training data?s likelihood with increasing sampling iterations. 1.239E+08 1.243E+08 1.247E+08 1.251E+08 1.255E+08 1.259E+08 100 200 300 400 500 600 700 800 900 1000 Number of Sampling Iterations N e g a ti v e -L o g L ik e li h o o d random 1 random 2 random 3 Figure 5. Histograms of the training d","@endWordPosition":"5373","@position":"31583","annotationId":"T44","@startWordPosition":"5370","@citStr":"Zollmann and Vogel, 2011"}]},"title":{"#tail":"\n","#text":"A wordclass approach to labeling PSCFG rules for machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Andreas Zollmann"},{"#tail":"\n","#text":"Stephan Vogel"}]}}]}}]}}
