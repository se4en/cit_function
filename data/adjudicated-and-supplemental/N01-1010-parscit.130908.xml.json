{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"volume":{"#tail":"\n","#text":"24"},"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Li, H. and Abe, N. (1998). Generalizing Case Frames Using a Thesaurus and the MDL Principle, Computational Linguistics, 24(2)."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Li, Abe, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"resjan, 1973) or logical polysemy (Pustejovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &quot;bank&quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the W","@endWordPosition":"729","@position":"4834","annotationId":"T1","@startWordPosition":"726","@citStr":"Li and Abe, 1998"},{"#tail":"\n","#text":"t of internal/leaf nodes in the tree, and each node represents a set of all leaf nodes in a subtree rooted by the node. Such a set is also considered as a cluster.3 Clusters in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint. For instance, Figure 1 shows an example thesaurus tree and one possible tree-cut [AIRCRAFT, ball, kite, puzzle], which is indicated by a thick curve in the figure. There are also four other possible tree-cuts for this tree: [airplane, helicopter, ball, kite, puzzle], [airplane, helicopter, TOY], [AIRCRAFT, TOY] and [ARTIFACT]. In (Li and Abe, 1998), the tree-cut technique was applied to the problem of acquiring general3A leaf node is also a cluster whose cardinality is 1. ized case frame patterns from a corpus. Thus, each node/word in the tree received as its value the number of instances where the word occurred as a case role (subject, object etc.) of a given verb. Then the acquisition of a generalized case frame was viewed as a problem of selecting the best tree-cut model that estimates the true probability distribution, given a sample corpus data. Formally, a tree-cut model M is a pair consisting of a tree-cut F and a probability par","@endWordPosition":"1081","@position":"7005","annotationId":"T2","@startWordPosition":"1078","@citStr":"Li and Abe, 1998"},{"#tail":"\n","#text":"ster Ci, and Pki=1 P(Ci) = 1. Note that P(C) is the probability of cluster C = {n1, \u2022\u2022, nm } as a whole, that is, P(C) = Pj=1 P(nj). For example, suppose a corpus contains 10 instances of verb-object relation for the verb &quot;fly&quot;, and the frequencies of object nouns n, denoted f(n), are as follows: f(airplane) = 5, f(helicopter) = 3, f(ball) = 0, f(kite) = 2, f(puzzle) = 0. Then, the set of treecut models for the example thesaurus tree shown in Figure 1 includes ([airplane, helicopter, TOY], [.5, .3, .2]) and ([AIRCRAFT, TOY], [.8, .2]). 2.2 The MDL Principle To select the best tree-cut model, (Li and Abe, 1998) uses the Minimal Description Length (MDL). The MDL is a principle of data compression in Information Theory which states that, for a given dataset, the best model is the one which requires the minimum length (often measured in bits) to encode the model (the model description length) and the data (the data description length) (Rissanen, 1978). Thus, the MDL principle captures the trade-off between the simplicity of a model, which is measured by the number of clusters in a tree-cut, and the goodness of fit to the data, which is measured by the estimation accuracy of the probability distribution","@endWordPosition":"1348","@position":"8419","annotationId":"T3","@startWordPosition":"1345","@citStr":"Li and Abe, 1998"},{"#tail":"\n","#text":" = P(C) and P(C) = f(S1) (7) Note the equation (7) essentially computes the Maximum Likelihood Estimate (MLE) for all n.5 A table in Figure 1 shows the MDL lengths for all five tree-cut models. The best model is the one with the tree-cut [AIRCRAFT, ball, kite, puzzle]. 3 Clustering Systematic Polysemy Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy from WordNet. In this section, we give a summary of this method, and describe the cluster pairs obtained by the method. 4For justification and detailed explanation of these formulas, see (Li and Abe, 1998). 5 In our previous work, we used entropy instead of NILE. That is because the lexicon represents true population, not samples; thus there is no additional data to estimate. 3.1 Extraction Method In our previous work, systematically related word senses are derived as binary cluster pairs, by applying the extraction procedure to a combination of two WordNet (sub)trees. This process is done in the following three steps. In the first step, all leaf nodes of the two trees are assigned a value of either 1, if a node/word appears in both trees, or 0 otherwise.6 In the second step, the tree-cut techn","@endWordPosition":"1848","@position":"11235","annotationId":"T4","@startWordPosition":"1845","@citStr":"Li and Abe, 1998"}]},"title":{"#tail":"\n","#text":"Generalizing Case Frames Using a Thesaurus and the MDL Principle,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"H Li"},{"#tail":"\n","#text":"N Abe"}]}},{"volume":{"#tail":"\n","#text":"24"},"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Li, H. and Abe, N. (1998). Generalizing Case Frames Using a Thesaurus and the MDL Principle, Computational Linguistics, 24(2)."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Li, Abe, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"resjan, 1973) or logical polysemy (Pustejovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &quot;bank&quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the W","@endWordPosition":"729","@position":"4834","annotationId":"T5","@startWordPosition":"726","@citStr":"Li and Abe, 1998"},{"#tail":"\n","#text":"t of internal/leaf nodes in the tree, and each node represents a set of all leaf nodes in a subtree rooted by the node. Such a set is also considered as a cluster.3 Clusters in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint. For instance, Figure 1 shows an example thesaurus tree and one possible tree-cut [AIRCRAFT, ball, kite, puzzle], which is indicated by a thick curve in the figure. There are also four other possible tree-cuts for this tree: [airplane, helicopter, ball, kite, puzzle], [airplane, helicopter, TOY], [AIRCRAFT, TOY] and [ARTIFACT]. In (Li and Abe, 1998), the tree-cut technique was applied to the problem of acquiring general3A leaf node is also a cluster whose cardinality is 1. ized case frame patterns from a corpus. Thus, each node/word in the tree received as its value the number of instances where the word occurred as a case role (subject, object etc.) of a given verb. Then the acquisition of a generalized case frame was viewed as a problem of selecting the best tree-cut model that estimates the true probability distribution, given a sample corpus data. Formally, a tree-cut model M is a pair consisting of a tree-cut F and a probability par","@endWordPosition":"1081","@position":"7005","annotationId":"T6","@startWordPosition":"1078","@citStr":"Li and Abe, 1998"},{"#tail":"\n","#text":"ster Ci, and Pki=1 P(Ci) = 1. Note that P(C) is the probability of cluster C = {n1, \u2022\u2022, nm } as a whole, that is, P(C) = Pj=1 P(nj). For example, suppose a corpus contains 10 instances of verb-object relation for the verb &quot;fly&quot;, and the frequencies of object nouns n, denoted f(n), are as follows: f(airplane) = 5, f(helicopter) = 3, f(ball) = 0, f(kite) = 2, f(puzzle) = 0. Then, the set of treecut models for the example thesaurus tree shown in Figure 1 includes ([airplane, helicopter, TOY], [.5, .3, .2]) and ([AIRCRAFT, TOY], [.8, .2]). 2.2 The MDL Principle To select the best tree-cut model, (Li and Abe, 1998) uses the Minimal Description Length (MDL). The MDL is a principle of data compression in Information Theory which states that, for a given dataset, the best model is the one which requires the minimum length (often measured in bits) to encode the model (the model description length) and the data (the data description length) (Rissanen, 1978). Thus, the MDL principle captures the trade-off between the simplicity of a model, which is measured by the number of clusters in a tree-cut, and the goodness of fit to the data, which is measured by the estimation accuracy of the probability distribution","@endWordPosition":"1348","@position":"8419","annotationId":"T7","@startWordPosition":"1345","@citStr":"Li and Abe, 1998"},{"#tail":"\n","#text":" = P(C) and P(C) = f(S1) (7) Note the equation (7) essentially computes the Maximum Likelihood Estimate (MLE) for all n.5 A table in Figure 1 shows the MDL lengths for all five tree-cut models. The best model is the one with the tree-cut [AIRCRAFT, ball, kite, puzzle]. 3 Clustering Systematic Polysemy Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy from WordNet. In this section, we give a summary of this method, and describe the cluster pairs obtained by the method. 4For justification and detailed explanation of these formulas, see (Li and Abe, 1998). 5 In our previous work, we used entropy instead of NILE. That is because the lexicon represents true population, not samples; thus there is no additional data to estimate. 3.1 Extraction Method In our previous work, systematically related word senses are derived as binary cluster pairs, by applying the extraction procedure to a combination of two WordNet (sub)trees. This process is done in the following three steps. In the first step, all leaf nodes of the two trees are assigned a value of either 1, if a node/word appears in both trees, or 0 otherwise.6 In the second step, the tree-cut techn","@endWordPosition":"1848","@position":"11235","annotationId":"T8","@startWordPosition":"1845","@citStr":"Li and Abe, 1998"}]},"title":{"#tail":"\n","#text":"Generalizing Case Frames Using a Thesaurus and the MDL Principle,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"H Li"},{"#tail":"\n","#text":"N Abe"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Pustejovsky, J. (1995). The Generative Lexicon, The MIT Press."},"#text":"\n","marker":{"#tail":"\n","#text":"Pustejovsky, 1995"},"publisher":{"#tail":"\n","#text":"The MIT Press."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"as discourse analysis, IE and MT. More recently, (Gonzalo et al., 2000) also discusses potential usefulness of systematic polysemy for clustering word senses for IR. However, extracting systematic relations from large sense inventories is a difficult task. Most often, this procedure is done manually. For example, WordNet cousin relations were identified manually by the WordNet lexicographers. A similar effort was also made in the EuroWordnet project (Vossen et 'Systematic polysemy (in the sense we use in this paper) is also referred to as regular polysemy (Apresjan, 1973) or logical polysemy (Pustejovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &quot;bank&quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000","@endWordPosition":"640","@position":"4270","annotationId":"T9","@startWordPosition":"639","@citStr":"Pustejovsky, 1995"}},"title":{"#tail":"\n","#text":"The Generative Lexicon,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Pustejovsky"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Tomuro, N. (2000). Automatic Extraction of Systematic Polysemy Using Tree-cut. In Proceedings of the ANLP/NAACL-00 Workshop on Syntactic and Semantic Complexity in Natural Language Processing, Seattle, WA."},"#text":"\n","marker":{"#tail":"\n","#text":"Tomuro, 2000"},"location":{"#tail":"\n","#text":"Seattle, WA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &quot;bank&quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partiti","@endWordPosition":"735","@position":"4871","annotationId":"T10","@startWordPosition":"734","@citStr":"Tomuro, 2000"},{"#tail":"\n","#text":"gth of O, and 1S1 is the size of S. Finally, the data description length L(S1F; O) is the length required to encode the whole sample data. It is calculated as L(S1F; O) = \u2014 log2P(n) (6) nES where, for each n E C and each C E F, P(n) = P(C) and P(C) = f(S1) (7) Note the equation (7) essentially computes the Maximum Likelihood Estimate (MLE) for all n.5 A table in Figure 1 shows the MDL lengths for all five tree-cut models. The best model is the one with the tree-cut [AIRCRAFT, ball, kite, puzzle]. 3 Clustering Systematic Polysemy Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy from WordNet. In this section, we give a summary of this method, and describe the cluster pairs obtained by the method. 4For justification and detailed explanation of these formulas, see (Li and Abe, 1998). 5 In our previous work, we used entropy instead of NILE. That is because the lexicon represents true population, not samples; thus there is no additional data to estimate. 3.1 Extraction Method In our previous work, systematically related word senses are derived as binary cluster pairs, by applying the extraction procedure to a combination of two WordNet (sub)","@endWordPosition":"1810","@position":"10999","annotationId":"T11","@startWordPosition":"1809","@citStr":"Tomuro, 2000"}]},"title":{"#tail":"\n","#text":"Automatic Extraction of Systematic Polysemy Using Tree-cut."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ANLP/NAACL-00 Workshop on Syntactic and Semantic Complexity in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"N Tomuro"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Tomuro, N. (2000). Automatic Extraction of Systematic Polysemy Using Tree-cut. In Proceedings of the ANLP/NAACL-00 Workshop on Syntactic and Semantic Complexity in Natural Language Processing, Seattle, WA."},"#text":"\n","marker":{"#tail":"\n","#text":"Tomuro, 2000"},"location":{"#tail":"\n","#text":"Seattle, WA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &quot;bank&quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partiti","@endWordPosition":"735","@position":"4871","annotationId":"T12","@startWordPosition":"734","@citStr":"Tomuro, 2000"},{"#tail":"\n","#text":"gth of O, and 1S1 is the size of S. Finally, the data description length L(S1F; O) is the length required to encode the whole sample data. It is calculated as L(S1F; O) = \u2014 log2P(n) (6) nES where, for each n E C and each C E F, P(n) = P(C) and P(C) = f(S1) (7) Note the equation (7) essentially computes the Maximum Likelihood Estimate (MLE) for all n.5 A table in Figure 1 shows the MDL lengths for all five tree-cut models. The best model is the one with the tree-cut [AIRCRAFT, ball, kite, puzzle]. 3 Clustering Systematic Polysemy Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy from WordNet. In this section, we give a summary of this method, and describe the cluster pairs obtained by the method. 4For justification and detailed explanation of these formulas, see (Li and Abe, 1998). 5 In our previous work, we used entropy instead of NILE. That is because the lexicon represents true population, not samples; thus there is no additional data to estimate. 3.1 Extraction Method In our previous work, systematically related word senses are derived as binary cluster pairs, by applying the extraction procedure to a combination of two WordNet (sub)","@endWordPosition":"1810","@position":"10999","annotationId":"T13","@startWordPosition":"1809","@citStr":"Tomuro, 2000"}]},"title":{"#tail":"\n","#text":"Automatic Extraction of Systematic Polysemy Using Tree-cut."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ANLP/NAACL-00 Workshop on Syntactic and Semantic Complexity in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"N Tomuro"}}}]}}}}
