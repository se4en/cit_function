{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Rapp, Reinhard; Zock, Michael (2010). Utilizing Citations of Foreign Words in Corpus-Based Dictionary Generation. Proceedings of NLPIX 2010."},"#text":"\n","marker":{"#tail":"\n","#text":"Rapp, Zock, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"losely related to newspaper text. It is usually carefully edited, and the vocabulary is geared towards easy understanding for the intended readership. This implies that foreign word citations are kept to a minimum. In contrast, the WaCky Corpora have been downloaded from the web and represent a great variety of text types and styles. Hence, not all texts can be expected to have been carefully edited, and mixes between languages are probably more frequent than with newsticker text. As in this work English is the main source language, and as we have dealt with it as a target language already in Rapp & Zock (2010), we do not use the respective English versions of these corpora here. We also do not use the Wikipedia XML Corpora (Denoyer et al., 2006) as these greatly vary in size for different languages which makes comparisons across languages somewhat problematic. In contrast, the sizes of the above corpora are within the same order of magnitude (1 billion words each), which is why we do not control for corpus size here. 17 Concerning the number of foreign words within these corpora, we might expect that, given the status of English as the world\u2019s premiere language, English foreign words should be the ","@endWordPosition":"1214","@position":"7400","annotationId":"T1","@startWordPosition":"1211","@citStr":"Rapp & Zock (2010)"},{"#tail":"\n","#text":"n principle possible, we do not make any finer distinctions concerning the quality of a match. A problem that we face in our approach is what we call the homograph trap. What we mean by this term is that a foreign word occurring in a corpus of a particular language may also be a valid word in this language, yet possibly with a different meaning. For example, if the German word rot (meaning red) occurs in an English corpus, its occurrences can not easily be distinguished from occurrences of the English word rot, which is a verb describing the process of decay. Having dealt with this problem in Rapp & Zock (2010) we will not elaborate on it here, rather we will suggest a workaround. The idea is to look only at a very restricted vocabulary, namely the words defined in our gold standard. There we have 100 words in each of the five languages, i.e. 500 words altogether. The question is how many of these words occur more often than once. Note, however, that apart from English (which was the starting point for the gold standard), repetitions can occur not only across languages but also within a language. For example, the Spanish word sueño means both sleep and dream, which are distinct entries in the list. ","@endWordPosition":"2221","@position":"13364","annotationId":"T2","@startWordPosition":"2218","@citStr":"Rapp & Zock (2010)"},{"#tail":"\n","#text":" expensive, we decided to replace in our corpora all words absent from the gold standard by a common designator for unknown words. Also, in our evaluations, for the target language vocabulary we only use the words occurring in the respective column of the gold standard. So far, we always computed translations to single source words. However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm. As suggested in Rapp & Zock (2010) this can be done by looking up the ranks of each of the four given words (i.e. the words occurring in a particular word equation) within the association vector of a translation candidate, and by multiplying these ranks. So for each candidate we obtain a product of ranks. We then assume that the candidate with the smallest product will be the best translation.3 Let us illustrate this by an example: If the given words are the variants of the word nervous in English, French, German, and Spanish, i.e. nervous, nerveux, nervös, and nervioso, and if we want to find out their translation into Italia","@endWordPosition":"2589","@position":"15509","annotationId":"T3","@startWordPosition":"2586","@citStr":"Rapp & Zock (2010)"},{"#tail":"\n","#text":"from that of the other languages. These are mostly Romanic in type, with English somewhere in between. Therefore, the little overlap in vocabulary might make it hard for French, Italian, and Spanish writers to understand and use German foreign words. 3) Little improvement for several source words The right column in Table 1 shows the scores if (using the product-of-ranks algorithm) four source languages are taken into account in parallel. As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal. This contrasts with the findings described in Rapp & Zock (2010) where significant improvements could be achieved by increasing the number of source languages. So this casts some doubt on these. However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement. This is not the case here, where we try to improve on a score of around 50 for English. Remember that this is a somewhat conservative score as we count correct but alternative translations, as errors. As this is already a performance much closer to the optimum, making further performance gains is more difficult.","@endWordPosition":"3759","@position":"22233","annotationId":"T4","@startWordPosition":"3756","@citStr":"Rapp & Zock (2010)"},{"#tail":"\n","#text":"ource word; CF = corpus frequency of English source word; ET = expected translation according to gold standard; RE = computed rank of expected translation; CT = computed translation. 4 Summary and Future Work In this paper we made an attempt to solve the difficult problem of identifying word translations on the basis of a single monolingual cor21 pus, whereby the same corpus is used for several language pairs. The basic idea underlying our work is to look at foreign words, to compute their co-occurrence-based associations, and to consider these as translations of the respective words. Whereas Rapp & Zock (2010) dealt only with an English corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora. We were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair. For example, it is more promising to look at occurrences of English words in a German corpus rather than the other way around. Because of the special status of English it is also advisable to use it as a pivot wherever possible. Perhaps surprisingly, the work may have implications regarding cognitive models of second la","@endWordPosition":"4243","@position":"24983","annotationId":"T5","@startWordPosition":"4240","@citStr":"Rapp & Zock (2010)"}]},"title":{"#tail":"\n","#text":"Utilizing Citations of Foreign Words in Corpus-Based Dictionary Generation."},"booktitle":{"#tail":"\n","#text":"Proceedings of NLPIX"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Reinhard Rapp"},{"#tail":"\n","#text":"Michael Zock"}]}}}}}}
