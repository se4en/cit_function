{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":{"#tail":"\n","@confidence":"0.9825685625","#text":"\n1. add all feature templates to set S ,the set of\nselected feature templates C0 is null\n2. for i = 0 to n-1, n is the number of elements\nin S\n3. Pi =0\n4. for each feature template ftj in set S\n5. C?i = Ci + ftj\n6. train a model with features extracted\nby C? i and test on development set\n7. if the result P? > Pi\n8. Pi = P? , k= j\n9. end for\n10. Ci+1 = Ci + ftk\n11. S = S ? ftk\n12. end for\n13. the set Cm correspondent to Pm, which is\n"},"figure":[{"#tail":"\n","@confidence":"0.723937","#text":"\nIP\n?? ?? ????\nP NN NT\nNP-PN-SBJ VP\nPP-BNF VP\nVV\nNP-OBJ NP\nNN\n?? ?? ??\nf1 NN\n????? ?\nAD NN P\nARG2ADVP\nARG0 PP-TMP ARGM-TMP\nhas the Sanxia Project insurance provide\nARGM-ADV\n"},{"#tail":"\n","@confidence":"0.970138888888889","#text":"\nInput Semantic Roles\nA binary-category classifier\nA 5-category\nclassifier for\nARGXs\nA 17-category\nclassifier for\nARGMs\nOutput: Semantic Role tags\n"}],"address":[{"#tail":"\n","@confidence":"0.976627","#text":"\nBeijing, 100871, China\n"},{"#tail":"\n","@confidence":"0.976784","#text":"\nBeijing, 100871, China\n"}],"author":[{"#tail":"\n","@confidence":"0.98674","#text":"\nWeiwei Ding\n"},{"#tail":"\n","@confidence":"0.970453","#text":"\nBaobao Chang\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.740883","#text":"\n3.1 Linguistic Discrepancy of Different Se-\nmantic Role Groups\n"},{"#tail":"\n","@confidence":"0.999157","#text":"\n3.2 System Architecture\n"},{"#tail":"\n","@confidence":"0.9589115","#text":"\n3.3 Integrating the Idea of Exploiting Argu-\nment Interdependence\n"},{"#tail":"\n","@confidence":"0.795891","#text":"\n5.1 Classifier\n"},{"#tail":"\n","@confidence":"0.991282","#text":"\n5.2 Data\n"},{"#tail":"\n","@confidence":"0.338805","#text":"\nBaseline Hierarchical\n"}],"footnote":[{"#tail":"\n","@confidence":"0.682570375","#text":"\n4 in Chinese PropBank. The adjuncts are labeled\nwith ?ARGM?.\n3 Building a Hierarchical Semantic Role\nClassifier\nIn this section, we will discuss the linguistic fun-\ndaments of the construction of a hierarchical se-\n1 This sentence is extracted from chtb_082.fid of Chinese\nPropBank 1.0, and we made some simplifications on it.\n"},{"#tail":"\n","@confidence":"0.6719616","#text":"\nZhang Le. This toolkit is available at\nhttp://homepages.inf.ed.ac.uk/s045\n0736/maxent_toolkit.html. It can well\nhandle the multi-category classification problem\nand it is quite efficient.\n"}],"title":{"#tail":"\n","@confidence":"0.9692595","#text":"\nImproving Chinese Semantic Role Classification\nwith Hierarchical Feature Selection Strategy\n"},"@confidence":"0.000001","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.999371038461538","#text":"\nBaker, Collin F., Charles J. Fillmore, and John B. Lowe.\n1998. The Berkeley FrameNet project. In Proceed-\nings of the 17th international conference on Compu-\ntational linguistics, Montreal, Canada.\nBoas, Hans C. 2002. Bilingual FrameNet dictionaries\nfor machine translation. In Proceedings of LREC\n2002, Las Palmas, Spain.\nCarreras, Xavier and Llu?s M?rquez. 2004. Introduction\nto the conll-2004 shared task: Semantic role labeling.\nIn Proceedings of the Eighth Conference on Natural\nLanguage Learning, Boston, Massachusetts.\nCarreras, Xavier and Llu?s M?rquez. 2005. Introduction\nto the conll-2005 shared task: Semantic role labeling.\nIn Proceedings of the Nineth Conference on Natural\nLanguage Learning, Ann Arbor, Michigan.\nM?rquez, Llu?s, Xavier Carreras, Kenneth C. Litkowski,\nSuzanne Stevenson. 2008. Semantic Role Labeling:\nAn Introduction to the Special Issue, Computational\nlinguistics. 34(2):146-159..\nGildea, Daniel and Daniel Jurafsky. 2002. Automatic\nlabeling of semantic roles. Computational Linguistics,\n28(3): 245-288.\nJiang, Zheng Ping, Jia Li, Hwee Tou Ng. 2005. Seman-\ntic Argument Classification Exploiting Argument In-\nterdependence. In 19th International Joint Confer-\nence on Artificial Intelligence. Edinburgh, Scotland.\n"},{"#tail":"\n","@confidence":"0.999719835616438","#text":"\nKingsbury, Paul and Martha Palmer. 2002. From Tree-\nBank to PropBank. In Proceedings of the 3rd Interna-\ntional Conference on Language Resources and\nEvaluation, Las Palmas, Spain.\nKipper, Karin, Hoa Trang Dang, and Martha Palmer.\n2000. Class-based construction of a verb lexicon. In\nProceedings of the Seventeenth National Conference\non Artificial Intelligence and Twelfth Conference on\nInnovative Applications of Artificial Intelligence,\nAustin, Texas, USA.\nMoschitti. Alessandro. 2004. A Study on Convolution\nKernels for Shallow Statistic Parsing. In Proceedings\nof the 42nd Meeting of the Association for Computa-\ntional Linguistics, Barcelona, Spain.\nMoschitti, Alessandro, Ana-Maria Giuglea, Bonaven-\ntura Coppola, and Roberto Basili. 2005. Hierarchical\nsemantic role labeling. In Proceedings of the Nineth\nConference on Natural Language Learning, Ann Ar-\nbor, Michigan.\nNarayanan, Srini and Sanda Harabagiu. 2004. Question\nanswering based on semantic structures. In Proceed-\nings of the 20th International Conference on Compu-\ntational Linguistics, Geneva, Switzerland.\nPradhan, Sameer, Kadri Hacioglu, Valerie Kruglery,\nWayne Ward, James H. Martin, Daniel Jurafskyz.\n2004. Support vector learning for semantic argument\nclassification. Machine Learning Journal, 60(1-3):11-\n39.\nSun, Honglin and Daniel Jurafsky. 2004. Shallow Se-\nmantic Parsing of Chinese. In Proceedings of the\nHuman Language Technology Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics, Boston, Massachusetts.\nSurdeanu, Mihai, Sanda Harabagiu, John Williams, and\nPaul Aarseth. 2003. Using predicate-argument struc-\ntures for information extraction. In Proceedings of the\n41st Annual Meeting of the Association for Computa-\ntional Linguistics, Ann Arbor, Michigan.\nWang, Hui, Weidong Zhan, Shiwen Yu. 2003. The\nSpecification of The Semantic Knowledge-base of\nContemporary Chinese, In Journal of Chinese Lan-\nguage and Computing, 13(2):159-176.\nXue, Nianwen and Martha Palmer. 2003. Annotating the\nPropositions in the Penn Chinese Treebank. In Pro-\nceedings of the 2nd SIGHAN Workshop on Chinese\nLanguage Processing, Sapporo, Japan.\nXue, Nianwen and Martha Palmer. 2004. Calibrating\nfeatures for semantic role labeling. In Proceedings of\nthe Conference on Empirical Methods in Natural\nLanguage Processing, Barcelona, Spain.\nXue, Nianwen and Martha Palmer. 2005. Automatic\nsemantic role labeling for Chinese verbs. In 19th In-\nternational Joint Conference on Artificial Intelligence.\nEdinburgh, Scotland.\nXue, Nianwen, Fei Xia, Fu dong Chiou, and Martha\nPalmer. 2005. The Penn Chinese TreeBank: Phrase\nStructure Annotation of a Large Corpus. Natural\nLanguage Engineering, 11(2):207?238.\nXue, Nianwen. 2008. Labeling Chinese predicates with\nsemantic roles. Computational linguistics. 34(2):225-\n255.\nYi, Szu-ting, Edward Loper, Martha Palmer. 2007. Can\nSemantic Roles Generalize Across Genres? In Pro-\nceedings of Human Language Technologies and The\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics, Rochester,\nNY, USA.\nZhang, Min, Wanxiang Che, Ai Ti Aw, Chew Lim Tan,\nGuodong Zhou, Ting Liu, Sheng Li. 2007. A Gram-\nmar-driven Convolution Tree Kernel for Semantic\nRole Classification, in Proceedings of the 45th An-\nnual Meeting of the Association of Computational\nLinguistics, Prague, Czech Republic.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.4594875","#text":"\nProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 324?333,\nHonolulu, October 2008. c?2008 Association for Computational Linguistics\n"},{"#tail":"\n","@confidence":"0.994208137931034","#text":"\nIn recent years, with the development of Chi-\nnese semantically annotated corpus, such as\nChinese Proposition Bank and Normalization\nBank, the Chinese semantic role labeling\n(SRL) task has been boosted. Similar to Eng-\nlish, the Chinese SRL can be divided into two\ntasks: semantic role identification (SRI) and\nclassification (SRC). Many features were in-\ntroduced into these tasks and promising re-\nsults were achieved. In this paper, we mainly\nfocus on the second task: SRC. After exploit-\ning the linguistic discrepancy between num-\nbered arguments and ARGMs, we built a se-\nmantic role classifier based on a hierarchical\nfeature selection strategy. Different from the\nprevious SRC systems, we divided SRC into\nthree sub tasks in sequence and trained models\nfor each sub task. Under the hierarchical ar-\nchitecture, each argument should first be de-\ntermined whether it is a numbered argument\nor an ARGM, and then be classified into fine-\ngained categories. Finally, we integrated the\nidea of exploiting argument interdependence\ninto our system and further improved the per-\nformance. With the novel method, the classi-\nfication precision of our system is 94.68%,\nwhich outperforms the strong baseline signifi-\ncantly. It is also the state-of-the-art on Chi-\nnese SRC.\n"},{"#tail":"\n","@confidence":"0.999132725","#text":"\nSemantic Role labeling (SRL) was first defined in\nGildea and Jurafsky (2002). The purpose of SRL\ntask is to identify and classify the semantic roles of\neach predicate in a sentence. The semantic roles\nare marked and each of them is assigned a tag\nwhich indicates the type of the semantic relation\nwith the related predicate. Typical tags include\nAgent, Patient, Source, etc. and some adjuncts\nsuch as Temporal, Manner, Extent, etc. Since the\narguments can provide useful semantic information,\nthe SRL is crucial to many natural language proc-\nessing tasks, such as Question and Answering (Na-\nrayanan and Harabagiu 2004), Information Extrac-\ntion (Surdeanu et al 2003), and Machine Transla-\ntion(Boas 2002). With the efforts of many re-\nsearchers (Carreras and M?rquez 2004, 2005, Mo-\nschitti 2004, Pradhan et al2005, Zhang et al2007),\ndifferent machine learning methods and linguistics\nresources are applied in this task, which has made\nSRL task progress fast.\nCompared to the research on English, the re-\nsearch on Chinese SRL is still in its infancy stage.\nPrevious work on Chinese SRL mainly focused on\nhow to transplant the machine learning methods\nwhich has been successful with English, such as\nSun and Jurafsky (2004), Xue and Palmer (2005)\nand Xue (2008). Sun and Jurafsky (2004) did the\npreliminary work on Chinese SRL without any\nlarge semantically annotated corpus of Chinese.\nThey just labeled the predicate-argument structures\nof ten specified verbs to a small collection of Chi-\nnese sentences, and used Support Vector Machines\nto identify and classify the arguments. This paper\nmade the first attempt on Chinese SRL and pro-\nduced promising results. After the PropBank (Xue\nand Palmer 2003) was built, Xue and Palmer (2005)\nand Xue (2008) have produced more complete and\nsystematic research on Chinese SRL.\nMoschitti et al (2005) has made some prelimi-\nnary attempt on the idea of hierarchical semantic\n"},{"#tail":"\n","@confidence":"0.997351659574468","#text":"\nrole labeling. However, without considerations on\nhow to utilize the characteristics of linguistically\nsimilar semantic roles, the purpose of the hierar-\nchical system is to simplify the classification proc-\ness to make it less time consuming. So the hierar-\nchical system in their paper performs a little worse\nthan the traditional SRL systems, although it is\nmore efficient.\nXue and Palmer (2004) did very encouraging\nwork on the feature calibration of semantic role\nlabeling. They found out that different features\nsuited for different sub tasks of SRL, i.e. semantic\nrole identification and classification. For semantic\nanalysis, developing features that capture the right\nkind of information is crucial. Experiments on\nChinese SRL (Xue and Palmer 2005, Xue 2008)\nreassured these findings.\nIn this paper, we mainly focus on the semantic\nrole classification (SRC) process. With the find-\nings about the linguistic discrepancy of different\nsemantic role groups, we try to build a 2-step se-\nmantic role classifier with hierarchical feature se-\nlection strategy. That means, for different sub tasks,\ndifferent models will be trained with different fea-\ntures. The purpose of this strategy is to capture the\nright kind of information of different semantic role\ngroups. It is hard to do manual selection of features\nsince there are too many feature templates which\nhas been proven to be useful in SRC; so, we de-\nsigned a simple feature selection algorithm to se-\nlect useful features automatically from a large set\nof feature templates. With this hierarchical feature\nselection architecture, our system can outperform\nprevious systems. The selected feature templates\nfor each process of SRC can in turn reassure the\nexistence of the linguistic discrepancy. At last, we\nalso integrate the idea of exploiting argument in-\nterdependence to make our system perform better.\nThe rest of the paper is organized as follows. In\nsection 2, the semantically annotated corpus - Chi-\nnese Propbank is discussed. The architecture of our\nmethod is described in section 3. The feature selec-\ntion strategy is discussed in section 4. The settings\nof experiments can be found in section 5. The re-\nsults of the experiments can be found in section 6,\nwhere we will try to make some linguistic explana-\ntions of the selected features. Section 7 is conclu-\n"},{"#tail":"\n","@confidence":"0.9859935","#text":"\nThe Chinese PropBank has labeled the predicate-\nargument structures of sentences from the Chinese\nTreeBank (Xue et al 2005). It is constituted of two\nparts. One is the labeled data, which indicates the\npositions of the predicates and its arguments in the\nChinese Treebank. The other is a dictionary which\n"},{"#tail":"\n","@confidence":"0.9485535","#text":"\nservice forthe insurance company now until\nUntil now, the insurance company has provided insurance services for the Sanxia Project.\n"},{"#tail":"\n","@confidence":"0.988895564102564","#text":"\nlists the frames of all the labeled predicates. Figure\n1 is an example from the PropBank1. We put the\nword-by-word translation and the translation of the\nwhole sentence below the example.\nIt is quite a complex sentence, as there are many\nsemantic roles in it. In this sentence, all the seman-\ntic roles of the verb ?? (provide) are presented in\nthe syntactic tree. We can separate the semantic\nroles into two groups.\nThe first group of semantic roles can be called\nthe core arguments, which capture the core rela-\ntions. In this sentence, there are three arguments of\nverb ?? (provide) in this sentence. ????\n(the insurance company) is labeled as ARG0,\nwhich is the proto-agent of the verb. Specifically to\nthe verb ?? (provide), it is the provider. ???\n? (insurance services) is the direct object of the\nverb, and it is the proto-patient, which is labeled as\nARG1. Specifically to the verb ?? (provide), it\nrepresents things provided. ????? (for the\nSanxia Project) is another kind of argument,\nwhich is labeled as ARG1, and it represents the\nreceiver.\nThe other group of semantic roles is called ad-\njuncts. They are always used to reveal the periph-\neral information. There are two adjuncts of the tar-\nget verb in this sentence: ???? (until recently)\nand ? (has), both of which are labeled as ARGM.\nHowever, the two ARGMs reveal information of\ndifferent aspects. Besides the ARGM tags, the sec-\nondary tags ?TMP? and ?ADV? are assigned to the\ntwo semantic roles respectively. ?TMP? indicates\nthat???? (until recently) is a modifier repre-\nsenting the temporal information, and ?ADV? in-\ndicates that? (has) is an adverbial modifier.\nIn the Chinese PropBank, the difference of the\ntwo groups is obvious. The core arguments are all\nlabeled with numbers, and they are also called the\nnumbered arguments. The numbers range from 0 to\n"},{"#tail":"\n","@confidence":"0.993896","#text":"\nmantic role classifier. We use ?hierarchical? to dis-\ntinguish our classifier from the previous ?flat? ones.\n"},{"#tail":"\n","@confidence":"0.999595266666667","#text":"\nThe purpose of the SRC task is to assign a tag to\nall the semantic roles which have been identified.\nThe tags include ARG0-4, and 17 kinds of\nARGMs (with functional tags). Previous SRC sys-\ntems treat all the tags equally, and view the SRC as\na multi-category classification task. However, we\nhave different opinions of the traditional architec-\nture.\nDue to the discussions in section 2, we noticed\nthat the semantic roles can be divided into two\ngroups naturally according to the different kinds of\nsemantic information represented by them. Here\nwe will make some linguistic analysis of the two\nsemantic role groups. Conversely to the process of\nthe syntactic realization of semantic roles, we want\nto find out what linguistic features make a con-\nstituent ARG0 instead of ARG1, or another con-\nstituent ARGM-TMP instead of ARGM-ADV, i.e.\nwhat features capture the most crucial information\nof the two groups.\nAs what we have assumed, the linguistic fea-\ntures which made a syntactic constituent labeled as\neither one of the core arguments or one of the ad-\njuncts varies greatly. Take the sentence in section 2\nas an example, even if the only information we\nhave about the phrase ???? (until now) is that\nit is an adjunct of the verb, we can almost confirm,\nno matter where this node takes place in the pars-\ning tree, this constituent will be labeled as ARGM-\nTMP. ? (has) is also the same. According to its\nmeaning, the only category can be assigned to it is\nARGM-ADV. But, things are quite different to the\ncore argument. In the same sentence, ????\n(the insurance company) is a good example. If we\nlimit our observation to the phrase itself, we can\nhardly assert that it is the ARG0 of the target verb.\nOnly when we extend our observation to the syn-\ntactic structure level, find out it is the subject of\nthis sentence, and the voice of the sentence is ac-\ntive, the semantic type of???? (the insurance\ncompany) is finally confirmed. If we have another\nsentence in which ???? (the insurance com-\npany) is not the subject, but rather the object, and\nthe target verb is ?? (set up), then it will proba-\nbly be labeled as ARG1.\n"},{"#tail":"\n","@confidence":"0.998830780487805","#text":"\nDue to the analysis above, we can conclude the\nlinguistic discrepancy of the two semantic role\ngroups as follows. Core arguments and adjuncts\nshare different kinds of inner linguistic consistency\nrespectively. For the core arguments, the specific\ntype cannot be determined with the information of\nthe arguments only. At this level, the core argu-\nments are dependent on other information except\nthe information about themselves. For example, the\ninformation of syntactic structures is crucial to the\ndetermination of the types of core arguments, and\ntrivial differences of the syntactic structures will\nlead to the different output. Because of this, we can\nsay that the core arguments are sensitive to the\nsyntactic structures. Compared to the core argu-\nments, adjuncts are the opposite. They are rela-\ntively independent on other information, since\nmost of the adjuncts can be easily classified just\nbased on the information about themselves2. And\nalthough the positions of the adjuncts in the syntac-\ntic structure can vary, the types of the adjuncts are\nfixed. In this sense, the adjuncts are insensitive to\nthe syntactic structures.\nAfter we made the linguistic discrepancy of the\ntwo semantic role groups, we can make a bold as-\nsumption that the differences of the two groups can\nbe reflected in the capability of different kinds of\nfeatures to capture the crucial information for the\ntwo groups. For example, the ?voice? features\nseems to be crucial to the core arguments but use-\nless to the adjuncts. This assumption provided us\nwith the idea of a hierarchical feature selection sys-\ntem.\nIn this system, we first classify the constituents\ninto two classes: core arguments and adjuncts. And\nthen, the system classifies core arguments and ad-\njuncts separately. For different subtasks we only\nselect the most useful features and discard the less\npertinent ones. We hope to take utilization of the\nmost crucial features to improve semantic role\nclassification.\n"},{"#tail":"\n","@confidence":"0.947283555555555","#text":"\nPrevious semantic role classifiers always did the\nclassification problem in one-step. However, in\nthis paper, we did SRC in two steps. The architec-\ntures of hierarchical semantic role classifiers can\n2 Extra features e.g. predicate may be still useful because that\nthe information, provided by the high-level description of self-\ndescriptive features, e.g. phrase type, are limited.\nbe found in figure 2, which is similar with that in\nMoschitti et al (2005).\n"},{"#tail":"\n","@confidence":"0.997399833333333","#text":"\nAs what has been shown in figure 2, a semantic\nrole will first be determined whether it is a num-\nbered argument or an ARGM by a binary-category\nclassifier. And, then if the semantic role is a num-\nbered argument, it will be determined by a 5-\ncategory classifier designed for ARGX, i.e. the\nnumbered arguments. If it is an ARGM, the func-\ntional tag will be assigned by a 17-category classi-\nfier built for ARGMs. Accordingly, with this hier-\narchical architecture, the SRC problem is divided\ninto 3 sub tasks, each of which has an independent\nclassifier.\n"},{"#tail":"\n","@confidence":"0.998594117647059","#text":"\nJiang et al (2005) has built a semantic role classi-\nfier exploiting the interdependence of semantic\nroles. It has turned the single point classification\nproblem into the sequence labeling problem with\nthe introduction of semantic context features. Se-\nmantic context features indicates the features ex-\ntracted from the arguments around the current one.\nWe can use window size to represent the scope of\nthe context. Window size [-m, n] means that, in the\nsequence that all the arguments has constructed,\nthe features of previous m and following n argu-\nments will be utilized for the classification of cur-\nrent semantic role. There are two kinds of argu-\nment sequences in Jiang et al (2005), and we only\ntest the linear sequence. Take the sentence in fig-\nure 1 as an example. The linear sequence of the\narguments in this sentence is: ????(until then),\n"},{"#tail":"\n","@confidence":"0.91453035","#text":"\n???? (the insurance company), ? (has), ?\n???? (for the Sanxia Project), ???? (in-\nsurance services). For the argument? (has), if the\nsemantic context window size is [-1,2], the seman-\ntic context features e.g. headword, phrase type and\netc. of ???? (the insurance company), ??\n??? (for the Sanxia Project) and ????\n(insurance services) will be utilized to serve the\nclassification task of? (has).\nWhile their paper has improved the SRC per-\nformance on English, it also has one potential dis-\nadvantage, which is that they didn?t separate the\ncore arguments and ARGMs. The influence and\nexplanations of this defect are presented in Section\n6. But in our hierarchical system, this problem can\nbe solved. Since in the first step, we have separated\nthe numbered arguments and ARGMs. We suppose\nthat with the separation of the two semantic role\ngroups, the system performance will be further im-\nproved.\n"},{"#tail":"\n","@confidence":"0.990249546666667","#text":"\nDue to what we have discussed in the section 3.1,\nwe need to select different features for the three\nsub task of SRC. In this paper, we did not make the\nselection manually; however, we make a simple\ngreedy strategy for feature selection to do it auto-\nmatically. Although the best solution may not be\nfound, automatic selection of features can try far\nmore combinations of feature templates than man-\nual selection. Because of this, this strategy possibly\ncan produce a better local optional solution.\nFirst, we built a pool of feature templates which\nhas proven to be useful on the SRC. Most of the\nfeature templates are standard, so only the new\nones will be explained. The candidate feature tem-\nplates include:\nVoice from Sun and Jurafsky (2004).\nHead word POS, Head Word of Prepositional\nPhrases, Constituent tree distance, from Pradhan\netc. (2004).\nPosition, subcat frame, phrase type, first word,\nlast word, subcat frame+, predicate, path, head\nword and its POS, predicate + head word, predi-\ncate + phrase type, path to BA and BEI, verb\nclass 3 , verb class + head word, verb class +\nphrase type, from Xue (2008).\n3 It is a bit different from Xue (2008), since we didn?t use the\nsyntactic alternation information.\npredicate POS, first word + last word, phrase\ntype of the sibling to the left, phrase type of the\nsibling to the right, verb + subcate frame+, verb\nPOS + subcat frame+, the amount of VPs in path,\nphrase type + phrase type of parent node, which\ncan be easily understood by name.\nvoice position, indicates whether the voice\nmarker (BA, BEI) is before or after the constituent\nin focus.\nsubcat frame*, the rule that expands the parent\nnode of the constituent in focus.\nsubcat frame@, the rule that expands the con-\nstituent in focus.\nlayer of the constituent in focus, the number of\nconstituents in the ascending part of the path sub-\ntracted by the number of those in the descending\npart of path, e.g. if the path is PP-BNF?VP?VP\n?VV, the feature extracted by this template will\nbe -1.\nSemCat (semantic category) of predicate, Sem-\nCat of first word, SemCat of head word, SemCat of\nlast word, SemCat of predicate + SemCat of first\nword, SemCat of predicate + SemCat of last word,\npredicate + SemCat of head word, SemCat of\npredicate + head word. The semantic categories of\nverbs and other words are extracted from the Se-\nmantic Knowledge-base of Contemporary Chinese\n(Wang et al 2003).\nverb AllFrameSets, the combination of all the\nframesets of a predicate.\nverb class + verb AllFrameSets, verb AllFra-\nmeSets + head word, verb AllFrameSets + phrase\ntype.\nThere are more than 40 feature templates, and it\nis quite difficult to traverse all the possible combi-\nnations and get the best one. So we use a greedy\nalgorithm to get an approximate optimal solution.\nThe feature selection algorithm is as follows.\nEach time we choose one of the feature templates\nand add it into the system. The one, after which is\nadded, the performance is the highest, will be cho-\nsen. Then we continue to choose feature templates\nuntil there are no one left. In the end, there are a\nseries of feature sets, which recorded the process\nof feature selection. Then we choose the feature set\nwhich can perform the best on development set.\nThe code of feature selection algorithm is designed\nin Figure 3.\n"},{"#tail":"\n","@confidence":"0.909239666666667","#text":"\nthe highest, will be chosen.\nFigure 3. the greedy feature selection algorithm\nTo make a comparison, we also built a tradi-\ntional 1-step semantic role classifier based on this\nfeature selection strategy. We will take this classi-\nfier as the baseline system.\n"},{"#tail":"\n","@confidence":"0.975539333333333","#text":"\nIn our SRL system, we use a Maximum Entropy\ntoolkit with tunable Gaussian Prior and L-BFGS\nparameter estimation, which is implemented by\n"},{"#tail":"\n","@confidence":"0.997187307692307","#text":"\nWe use Chinese PropBank 1.0 (LDC number:\nLDC2005T23) in our experiments. PropBank 1.0\nincludes the annotations for files chtb_001.fid to\nchtb_931.fid, or the first 250K words of the\nChinese TreeBank 5.1. For the experiments, the\ndata of PropBank is divided into three parts. 648\nfiles (from chtb_081 to chtb_899.fid) are used as\nthe training set. The development set includes 40\nfiles, from chtb_041.fid to chtb_080.fid. The test\nset includes 72 files, which are chtb_001 to\nchtb_041, and chtb_900 to chtb_931. We use the\nsame data setting with Xue (2008), however a bit\ndifferent from Xue and Palmer (2005).\n"},{"#tail":"\n","@confidence":"0.998789163265306","#text":"\nThe results of the feature selection are presented in\ntable1. In this table, ?Baseline? indicates the 1-step\narchitecture, and ?Hierarchical? indicates the ?hi-\nerarchical feature selection architecture? imple-\nmented in this paper. ?X_M?, ?ARGX? and\n?ARGM? indicate the three sub-procedures of the\nhierarchical architecture, which are ?ARGX and\nARGM separation?, ?ARGX classification?,\n?ARGM classification? respectively. ?Y? in the\ntable indicates that the feature template has been\nselected for the sub task.\nAccording to table 1, we can find some interest-\ning facts, which in turn prove what we found about\nsemantic role groups in section 3.1.\nIn table 1, feature templates related to the syn-\ntactic structure includes: voice-related group (voice,\nvoice information, path to BA and BEI), frame-\nrelated group (verb class, verb class + head word,\nverb class + phrase type, all frames of verb, verb\nclass + all frames of verb), the layer of argument,\nposition and 4 kinds of subcat frames. As we as-\nsumed before, these features are crucial to core\narguments but of little use to adjuncts. The results\nhave proven this assumption. Of the entire 14 syn-\ntactic structure-related feature templates, 8 were\nselected by the ARGX process but only 2 was se-\nlected by the ARGM process. The two exceptions\nshould be viewed as the result of random impact,\nwhich cannot be avoided in automatic feature se-\nlection.\nCompared with the different features selected by\nthese tasks, we can find other interesting results.\nFew of the features selected by the X_M process\nalso have related with the verb or the syntactic\nstructures, which is quite similar with the ARGM\nprocess. This is probably because most of ARGMs\nare easy to be identified without syntactic structure\ninformation, which makes the opposite of ARGMs,\ni.e. the ARGXs easy to be filtered. Besides, the\nfeatures selected by the baseline system have much\nin common with those selected by the ARGX\nprocess. This can be explained by the fact that both\nin the development and test set, the amount of core\narguments outperforms that of adjuncts. The pro-\nportions between core arguments and adjuncts are\n1.79:1 on the development set, and 1.63:1 on the\ntest set. Because of the bias, the baseline system\nwill tend to choose more syntactic structure-related\nfeatures to label core arguments precisely.\n"},{"#tail":"\n","@confidence":"0.9775499375","#text":"\nbaseline and hierarchical system\nWith this new architecture, we have achieved\nimprovement on the performance of the semantic\nrole classification, which can be found in table 2.\nOur classifier performs better both on the devel-\nopment and the test set. The labeled precision on\nthe development set is from 95.15% to 95.94%,\nand the test set is from 93.38% to 94.31%, with an\nERR (error reduction rate) of 14.05%. Both of the\nimprovements are statistically significant (?2 test\nwith p= 0.05). The errors of SRC have three ori-\ngins, which are correspondent to the three sub\ntasks of the hierarchical architecture. Detailed in-\nformation of the comparison between the two sys-\ntems can be found in table 3, which can tell us\nwhere the improvements come from.\n"},{"#tail":"\n","@confidence":"0.9979615","#text":"\nIn table 3, the percentages are calculated the\nway that the number of the errors was divided by\nthe number of the arguments in the test set.\nARGX/ARGM errors represent the errors that the\nsemantic roles are classified into wrong group, e.g.\nARGXs are labeled as ARGMs and vice versa. The\ninner errors represent the errors in a group, e.g.\nARG0 are labeled as ARG1. From this table, we\ncan find that ARGX is the most difficult task. X-M\nand ARGM are less challenging. Besides the rela-\ntively little error reduction in the ARGM process,\nthe greatest part of improvement comes from the\nprocess of the most difficult sub task: the ARGX\nsub task. It is a bit surprising that the first step of\nthe X_M in the hierarchical system process did not\nperform better than that in the baseline system.\n"},{"#tail":"\n","@confidence":"0.981672739130435","#text":"\nrespect to ARGMs and ARGXs, the hierarchical\nsystem outperforms the baseline system. Further-\nmore, the improvement on ARGXs is greater than\n4 From the name, TBERR possibly indicates the labeled errors\nin Chinese PropBank. However, we did not find any explana-\ntions, so we just put it here and group it to ARGM.\nthat of ARGMs. All types of numbered arguments\nget improvement in the hierarchical architecture,\nespecially ARG1, ARG4 and ARG3. Although the\nperformances of some types of the ARGMs de-\ncreased, the performances of all types of the\nARGMs which occurs more than 100 times in-\ncreased, including ADV (adverbials), LOC (loca-\ntives), MNR (manner markers) and TMP (temporal\nmarkers).\nAfter the hierarchical system was built, we tried\nto integrate the idea of exploiting argument inter-\ndependence into our system. We extract the seman-\ntic context features in a linear order, with the win-\ndow size from [0,0] to [-3,3]. Larger window sizes\nare of little value since too few arguments have\nmore than 6 other arguments in context. The re-\nsults are presented in table 5.\n"},{"#tail":"\n","@confidence":"0.95876321875","#text":"\nBase 93.38% 94.31%\n+window selection 93.38% 94.68%\nTable 5 integrating window selection into our system\n?Base? stands for the hierarchical system built\nabove, without semantic context features.\n?+window selection? indicates the new system\nwhich has utilized the idea of exploiting argument\ninterdependence. The best window sizes for the\nbaseline system, ARGX and ARGM processes in\nthe hierarchical system are [0,0], [-1,1], [0,0] re-\nspectively, which were determined by testing on\nthe development set. We can find that only for the\nARGX process, the semantic context features are\nuseful. For the baseline system and the ARGM\nprocess, exploiting argument interdependence does\nnot help improve the performance. This conclusion\nis different from Jiang et al (2004), but it can be\nexplained in the following way.\nIn fact, the interdependence only exists among\ncore arguments. For ARGMs, it is a different thing.\nAn ARGM cannot provide any information about\nthe type of the arguments close to it and the seman-\ntic context features does not help the classification\nof ARGMs. Also, take the sentence in section 2 as\nan example, the fact that ???? (until now) is\nARGM-TMP cannot raise the probability that??\n?? (the insurance company) is ARG0 or ? (has)\nis ARGM-ADV and vice versa. However, if we\nknow that ???? (the insurance company) is\nARG0, at least the phrase ???? (insurance\nservices) can never be ARG0. The semantic con-\ntext features extracted from or for ARGMs will do\n"},{"#tail":"\n","@confidence":"0.999917421052632","#text":"\nharm to the improvement of the system, since they\nare irrelative information. Because of the same rea-\nson, the performance of base system also decreased\nwhen semantic context features were extracted,\nsince the core arguments and the ARGMs are\nmixed together in the baseline system.\nBut for the ARGX sub task of our hierarchical\nsystem, since we have separated the numbered ar-\nguments and ARGMs first, the influences of\nARGMs can be eliminated. This made the interde-\npendence of core arguments can be directly ex-\nplored from the extraction of semantic context fea-\ntures. So the ARGX sub task is improved.\nTo prove that our method is effective, we also\nmake a comparison between the performances of\nour system and Xue and Palmer (2005), Xue\n(2008). Xue (2008) is the best SRL system until\nnow and it has the same data setting with ours. The\nresults are presented in Table 6.\n"},{"#tail":"\n","@confidence":"0.994396833333333","#text":"\nWe have to point out that all the three systems\nare based on Gold standard parsing. From the table\n6, we can find that our system is better than both of\nthe related systems. Our system has outperformed\nXue (2008) with a relative error reduction rate of\n9.8%.\n"},{"#tail":"\n","@confidence":"0.997853793103448","#text":"\nIn this paper, we have divided all the semantic\nroles into two groups according to their semantic\nrelations with the verb. After the grouping of the\nsemantic roles was made, we designed a hierarchi-\ncal semantic role classifier. To capture the accurate\ninformation of different semantic role groups, we\ndesigned a simple feature selection algorithm to\ncalibrate features for each sub task of SRC. It was\nvery encouraging that the hierarchical SRC system\noutperformed the strong baseline built with tradi-\ntional methods. And the selected features could be\nexplained, which in turn proves that the linguistic\ndiscrepancy of semantic role groups not only exists\nbut also can be captured. Then we integrated the\nidea of exploiting argument interdependence to\nfurther improve the performance of our system and\nexplained linguistically why the results of our sys-\ntem were different from the ones in previous re-\nsearch.\nAlthough we make discriminations of arguments\nand adjuncts, the analysis is still coarse-grained. Yi\net al (2007) has made the first attempt working on\nthe single semantic role level to make further im-\nprovement. However, the impact of this idea is\nlimited due to that the amount of the research tar-\nget, ARG2, is few in PropBank. What if we could\nextend the idea of hierarchical architecture to the\nsingle semantic role level? Would that help the\nimprovement of SRC?\n"},{"#tail":"\n","@confidence":"0.973229625","#text":"\nThis work was supported by National Natural Sci-\nence Foundation of China under Grant No.\n60303003 and National Social Science Foundation\nof China under Grant No. 06BYY048.\nWe want to thank Nianwen Xue, for his gener-\nous help at the beginning of this work. And thanks\nto the anonymous reviewers, for their valuable\ncomments on this paper.\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.9961135","#text":"\nInstitute of Computational Linguistics\nPeking University\n"},{"#tail":"\n","@confidence":"0.9952165","#text":"\nInstitute of Computational Linguistics\nPeking University\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.990737","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.996232","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.954048","@genericHeader":"method","#text":"\n2 The Chinese PropBank\n"},{"#tail":"\n","@confidence":"0.979236","@genericHeader":"method","#text":"\n4 Feature Selection Strategy\n"},{"#tail":"\n","@confidence":"0.955258","@genericHeader":"method","#text":"\n5 Experiment Settings\n"},{"#tail":"\n","@confidence":"0.999359","@genericHeader":"evaluation","#text":"\n6 Results and Discussion\n"},{"#tail":"\n","@confidence":"0.994769","@genericHeader":"conclusions","#text":"\n7 Conclusions and Future Work\n"},{"#tail":"\n","@confidence":"0.944772","@genericHeader":"acknowledgments","#text":"\nAcknowledgements\n"},{"#tail":"\n","@confidence":"0.990397","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.970948","#text":"\nTable 1. Feature selection results for the baseline and the hierarchical system\n"},{"#tail":"\n","@confidence":"0.999796","#text":"\nTable 2. Comparison of the performance between the\n"},{"#tail":"\n","@confidence":"0.698732","#text":"\nTable 3 Error rate analysis on the test set\n"},{"#tail":"\n","@confidence":"0.807569333333333","#text":"\nTable 4 Detailed labeled precision on the test set\nTable 4 presented the labeled precision of each\ntype of semantic role. It demonstrates that with\n"},{"#tail":"\n","@confidence":"0.974926","#text":"\nTable 6. Comparison with previous systems\n"}],"page":[{"#tail":"\n","@confidence":"0.99863","#text":"\n324\n"},{"#tail":"\n","@confidence":"0.462047","#text":"\nARG1\n"},{"#tail":"\n","@confidence":"0.998086","#text":"\n325\n"},{"#tail":"\n","@confidence":"0.997893","#text":"\n326\n"},{"#tail":"\n","@confidence":"0.985451","#text":"\n327\n"},{"#tail":"\n","@confidence":"0.998587","#text":"\n328\n"},{"#tail":"\n","@confidence":"0.99813","#text":"\n329\n"},{"#tail":"\n","@confidence":"0.997109","#text":"\n330\n"},{"#tail":"\n","@confidence":"0.982062","#text":"\n331\n"},{"#tail":"\n","@confidence":"0.966774","#text":"\n332\n"},{"#tail":"\n","@confidence":"0.999549","#text":"\n333\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.8925755","#text":"\nsions and future work.\nFigure 1. an example from PropBank\n"},{"#tail":"\n","@confidence":"0.7481255","#text":"\nFigure 2. The architecture of our hierarchical SRC\nsystem\n"}],"table":[{"#tail":"\n","@confidence":"0.990827714285714","#text":"\nHierarchical\nBaseline\nX_M ARGX ARGM\nFeature Name\nY predicate\nY Y predicate POS\nY Y first word\nY first word + last word\nY Y head word\nY head word POS\nY Y phrase type\nY Y phrase type + phrase type of parent node\nY phrase type of the sibling to the left\nY Y phrase type of the sibling to the right\nY Y position\nY voice\nY voice position\nY Y path to BA and BEI\nY Y Y verb class\nY verb class + head word\nY Y verb class + phrase type\nY Y verb AllFrameSets\nY Y verb class + verb AllFrameSets\nY subcat frame\nY subcat frame*\nY subcate frame@\nY subcat frame+\nY Y layer of the constituent in focus\nY Y Y predicate + head word\nY Y Y Y predicate + phrase type\nY Y Y SemCat of predicate\nY SemCat of first word\nY Y SemCat of last word\nY SemCat of predicate + SemCat of last word\nY Y SemCat of head word\n"},{"#tail":"\n","@confidence":"0.976642333333333","#text":"\nBaseline Hierarchical\nDEV 95.15% 95.94%\nTEST 93.38% 94.31%\n"},{"#tail":"\n","@confidence":"0.9494572","#text":"\nBaseline Hierarchical\nARGX/ARGM errors 1.66% 1.75%\ninner ARGX errors 3.59% 2.75%\ninner ARGM errors 1.37% 1.19%\nTOTAL 6.62% 5.69%\n"},{"#tail":"\n","@confidence":"0.995355391304348","#text":"\nBaseline Hierarchical Sum\nARG0 96.14% 96.58% 2046\nARG1 92.75% 94.60% 2428\nARG2 78.46% 78.85% 260\nARG3 60.00% 76.00% 25\nARG4 40.00% 100.00% 5\nARGM-ADV 96.64% 96.85% 1490\nARGM-ASP 100.00% 0.00% 1\nARGM-BNF 91.30% 86.96% 23\nARGM-CND 77.78% 77.78% 9\nARGM-CRD N/A N/A 0\nARGM-DGR N/A N/A 0\nARGM-DIR 54.84% 58.06% 31\nARGM-DIS 79.38% 79.38% 97\nARGM-EXT 50.00% 25.00% 8\nARGM-FRQ N/A N/A 0\nARGM-LOC 90.91% 92.21% 308\nARGM-MNR 89.92% 91.13% 248\nARGM-PRD N/A N/A 0\nARGM-PRP 97.83% 97.83% 46\nARGM-TMP 95.41% 96.30% 675\nARGM-TPC 33.33% 8.33% 12\nTBERR4 0.00% 0.00% 2\n"},{"#tail":"\n","@confidence":"0.7586705","#text":"\nX & P (2005) Xue(2008) Ours\n93.9% 94.1% 94.68%\n"}],"email":[{"#tail":"\n","@confidence":"0.986609","#text":"\nweiwei.ding.pku@gmail.com\n"},{"#tail":"\n","@confidence":"0.994168","#text":"\nchbb@pku.edu.cn\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.445503","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.8306295","#text":"Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 324?333, Honolulu, October 2008. c?2008 Association for Computational Linguistics"},"address":[{"#tail":"\n","@confidence":"0.999869","#text":"Beijing, 100871, China"},{"#tail":"\n","@confidence":"0.999948","#text":"Beijing, 100871, China"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.999936","#text":"Institute of Computational Linguistics Peking University"},{"#tail":"\n","@confidence":"0.999899","#text":"Institute of Computational Linguistics Peking University"}],"author":[{"#tail":"\n","@confidence":"0.988245","#text":"Weiwei Ding"},{"#tail":"\n","@confidence":"0.99007","#text":"Baobao Chang"}],"abstract":{"#tail":"\n","@confidence":"0.989179933333334","#text":"In recent years, with the development of Chinese semantically annotated corpus, such as Chinese Proposition Bank and Normalization Bank, the Chinese semantic role labeling (SRL) task has been boosted. Similar to English, the Chinese SRL can be divided into two tasks: semantic role identification (SRI) and classification (SRC). Many features were introduced into these tasks and promising results were achieved. In this paper, we mainly focus on the second task: SRC. After exploiting the linguistic discrepancy between numbered arguments and ARGMs, we built a semantic role classifier based on a hierarchical feature selection strategy. Different from the previous SRC systems, we divided SRC into three sub tasks in sequence and trained models for each sub task. Under the hierarchical architecture, each argument should first be determined whether it is a numbered argument or an ARGM, and then be classified into finegained categories. Finally, we integrated the idea of exploiting argument interdependence into our system and further improved the performance. With the novel method, the classification precision of our system is 94.68%, which outperforms the strong baseline significantly. It is also the state-of-the-art on Chinese SRC."},"title":{"#tail":"\n","@confidence":"0.984193","#text":"Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy"},"email":[{"#tail":"\n","@confidence":"0.999853","#text":"weiwei.ding.pku@gmail.com"},{"#tail":"\n","@confidence":"0.985193","#text":"chbb@pku.edu.cn"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the 17th international conference on Computational linguistics, Montreal, Canada."},"#text":"\n","marker":{"#tail":"\n","#text":"Baker, Fillmore, Lowe, 1998"},"location":{"#tail":"\n","#text":"Montreal, Canada."},"title":{"#tail":"\n","#text":"The Berkeley FrameNet project."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 17th international conference on Computational linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Collin F Baker"},{"#tail":"\n","#text":"Charles J Fillmore"},{"#tail":"\n","#text":"John B Lowe"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Boas, Hans C. 2002. Bilingual FrameNet dictionaries for machine translation. In Proceedings of LREC 2002, Las Palmas, Spain."},"#text":"\n","marker":{"#tail":"\n","#text":"Boas, 2002"},"location":{"#tail":"\n","#text":"Las Palmas,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and M?rquez 2004, 2005, Moschitti 2004, Pradhan et al2005, Zhang et al2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary wor","@endWordPosition":"370","@position":"2461","annotationId":"T1","@startWordPosition":"368","@citStr":"Boas 2002"}},"title":{"#tail":"\n","#text":"Bilingual FrameNet dictionaries for machine translation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of LREC 2002,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Hans C Boas"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Carreras, Xavier and Llu?s M?rquez. 2004. Introduction to the conll-2004 shared task: Semantic role labeling. In Proceedings of the Eighth Conference on Natural Language Learning, Boston, Massachusetts."},"#text":"\n","marker":{"#tail":"\n","#text":"Carreras, Mrquez, 2004"},"location":{"#tail":"\n","#text":"Boston, Massachusetts."},"title":{"#tail":"\n","#text":"Introduction to the conll-2004 shared task: Semantic role labeling."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Eighth Conference on Natural Language Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Xavier Carreras"},{"#tail":"\n","#text":"Llus Mrquez"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Carreras, Xavier and Llu?s M?rquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of the Nineth Conference on Natural Language Learning, Ann Arbor, Michigan."},"#text":"\n","marker":{"#tail":"\n","#text":"Carreras, Mrquez, 2005"},"location":{"#tail":"\n","#text":"Ann Arbor, Michigan."},"title":{"#tail":"\n","#text":"Introduction to the conll-2005 shared task: Semantic role labeling."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Nineth Conference on Natural Language Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Xavier Carreras"},{"#tail":"\n","#text":"Llus Mrquez"}]}},{"volume":{"#tail":"\n","#text":"linguistics."},"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"M?rquez, Llu?s, Xavier Carreras, Kenneth C. Litkowski, Suzanne Stevenson. 2008. Semantic Role Labeling: An Introduction to the Special Issue, Computational linguistics. 34(2):146-159.."},"journal":{"#tail":"\n","#text":"Computational"},"#text":"\n","pages":{"#tail":"\n","#text":"34--2"},"marker":{"#tail":"\n","#text":"Mrquez, Carreras, Litkowski, Stevenson, 2008"},"title":{"#tail":"\n","#text":"Semantic Role Labeling: An Introduction to the Special Issue,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Llus Mrquez"},{"#tail":"\n","#text":"Xavier Carreras"},{"#tail":"\n","#text":"Kenneth C Litkowski"},{"#tail":"\n","#text":"Suzanne Stevenson"}]}},{"date":{"#tail":"\n","#text":"2002"},"issue":{"#tail":"\n","#text":"3"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" tasks in sequence and trained models for each sub task. Under the hierarchical architecture, each argument should first be determined whether it is a numbered argument or an ARGM, and then be classified into finegained categories. Finally, we integrated the idea of exploiting argument interdependence into our system and further improved the performance. With the novel method, the classification precision of our system is 94.68%, which outperforms the strong baseline significantly. It is also the state-of-the-art on Chinese SRC. 1 Introduction Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002). The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine ","@endWordPosition":"269","@position":"1839","annotationId":"T2","@startWordPosition":"266","@citStr":"Gildea and Jurafsky (2002)"}},"title":{"#tail":"\n","#text":"Automatic labeling of semantic roles."},"volume":{"#tail":"\n","#text":"28"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3): 245-288."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"245--288"},"marker":{"#tail":"\n","#text":"Gildea, Jurafsky, 2002"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Daniel Gildea"},{"#tail":"\n","#text":"Daniel Jurafsky"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Jiang, Zheng Ping, Jia Li, Hwee Tou Ng. 2005. Semantic Argument Classification Exploiting Argument Interdependence. In 19th International Joint Conference on Artificial Intelligence. Edinburgh, Scotland."},"#text":"\n","marker":{"#tail":"\n","#text":"Jiang, Li, Ng, 2005"},"location":{"#tail":"\n","#text":"Edinburgh, Scotland."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" been shown in figure 2, a semantic role will first be determined whether it is a numbered argument or an ARGM by a binary-category classifier. And, then if the semantic role is a numbered argument, it will be determined by a 5- category classifier designed for ARGX, i.e. the numbered arguments. If it is an ARGM, the functional tag will be assigned by a 17-category classifier built for ARGMs. Accordingly, with this hierarchical architecture, the SRC problem is divided into 3 sub tasks, each of which has an independent classifier. 3.3 Integrating the Idea of Exploiting Argument Interdependence Jiang et al (2005) has built a semantic role classifier exploiting the interdependence of semantic roles. It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features. Semantic context features indicates the features extracted from the arguments around the current one. We can use window size to represent the scope of the context. Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n arguments will be utilized for the classification of current semantic role. ","@endWordPosition":"2335","@position":"14207","annotationId":"T3","@startWordPosition":"2332","@citStr":"Jiang et al (2005)"}},"title":{"#tail":"\n","#text":"Semantic Argument Classification Exploiting Argument Interdependence."},"booktitle":{"#tail":"\n","#text":"In 19th International Joint Conference on Artificial Intelligence."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Zheng Ping Jiang"},{"#tail":"\n","#text":"Jia Li"},{"#tail":"\n","#text":"Hwee Tou Ng"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Kingsbury, Paul and Martha Palmer. 2002. From TreeBank to PropBank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, Las Palmas, Spain."},"#text":"\n","marker":{"#tail":"\n","#text":"Kingsbury, Palmer, 2002"},"location":{"#tail":"\n","#text":"Las Palmas,"},"title":{"#tail":"\n","#text":"From TreeBank to PropBank."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 3rd International Conference on Language Resources and Evaluation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Paul Kingsbury"},{"#tail":"\n","#text":"Martha Palmer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Kipper, Karin, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, Austin, Texas, USA."},"#text":"\n","marker":{"#tail":"\n","#text":"Kipper, Dang, Palmer, 2000"},"location":{"#tail":"\n","#text":"Austin, Texas, USA."},"title":{"#tail":"\n","#text":"Class-based construction of a verb lexicon."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Karin Kipper"},{"#tail":"\n","#text":"Hoa Trang Dang"},{"#tail":"\n","#text":"Martha Palmer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Moschitti. Alessandro. 2004. A Study on Convolution Kernels for Shallow Statistic Parsing. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Barcelona, Spain."},"#text":"\n","marker":{"#tail":"\n","#text":"Alessandro, 2004"},"location":{"#tail":"\n","#text":"Barcelona,"},"title":{"#tail":"\n","#text":"A Study on Convolution Kernels for Shallow Statistic Parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Alessandro"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Moschitti, Alessandro, Ana-Maria Giuglea, Bonaventura Coppola, and Roberto Basili. 2005. Hierarchical semantic role labeling. In Proceedings of the Nineth Conference on Natural Language Learning, Ann Arbor, Michigan."},"#text":"\n","marker":{"#tail":"\n","#text":"Moschitti, Giuglea, Coppola, Basili, 2005"},"location":{"#tail":"\n","#text":"Ann Arbor, Michigan."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"(2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschitti et al (2005) has made some preliminary attempt on the idea of hierarchical semantic 324 role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for d","@endWordPosition":"554","@position":"3590","annotationId":"T4","@startWordPosition":"551","@citStr":"Moschitti et al (2005)"},{"#tail":"\n","#text":"features and discard the less pertinent ones. We hope to take utilization of the most crucial features to improve semantic role classification. 3.2 System Architecture Previous semantic role classifiers always did the classification problem in one-step. However, in this paper, we did SRC in two steps. The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g. phrase type, are limited. be found in figure 2, which is similar with that in Moschitti et al (2005). Figure 2. The architecture of our hierarchical SRC system As what has been shown in figure 2, a semantic role will first be determined whether it is a numbered argument or an ARGM by a binary-category classifier. And, then if the semantic role is a numbered argument, it will be determined by a 5- category classifier designed for ARGX, i.e. the numbered arguments. If it is an ARGM, the functional tag will be assigned by a 17-category classifier built for ARGMs. Accordingly, with this hierarchical architecture, the SRC problem is divided into 3 sub tasks, each of which has an independent class","@endWordPosition":"2215","@position":"13518","annotationId":"T5","@startWordPosition":"2212","@citStr":"Moschitti et al (2005)"}]},"title":{"#tail":"\n","#text":"Hierarchical semantic role labeling."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Nineth Conference on Natural Language Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alessandro Moschitti"},{"#tail":"\n","#text":"Ana-Maria Giuglea"},{"#tail":"\n","#text":"Bonaventura Coppola"},{"#tail":"\n","#text":"Roberto Basili"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Narayanan, Srini and Sanda Harabagiu. 2004.  Question answering based on semantic structures. In Proceedings of the 20th International Conference on Computational Linguistics, Geneva, Switzerland."},"#text":"\n","marker":{"#tail":"\n","#text":"Narayanan, Harabagiu, 2004"},"location":{"#tail":"\n","#text":"Geneva, Switzerland."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Introduction Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002). The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and M?rquez 2004, 2005, Moschitti 2004, Pradhan et al2005, Zhang et al2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue","@endWordPosition":"358","@position":"2379","annotationId":"T6","@startWordPosition":"354","@citStr":"Narayanan and Harabagiu 2004"}},"title":{"#tail":"\n","#text":"Question answering based on semantic structures."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 20th International Conference on Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Srini Narayanan"},{"#tail":"\n","#text":"Sanda Harabagiu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Pradhan, Sameer, Kadri Hacioglu, Valerie Kruglery, Wayne Ward, James H. Martin, Daniel Jurafskyz. 2004. Support vector learning for semantic argument classification. Machine Learning Journal, 60(1-3):11-39."},"journal":{"#tail":"\n","#text":"Machine Learning Journal,"},"#text":"\n","pages":{"#tail":"\n","#text":"60--1"},"marker":{"#tail":"\n","#text":"Pradhan, Hacioglu, Kruglery, Ward, Martin, Jurafskyz, 2004"},"title":{"#tail":"\n","#text":"Support vector learning for semantic argument classification."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sameer Pradhan"},{"#tail":"\n","#text":"Kadri Hacioglu"},{"#tail":"\n","#text":"Valerie Kruglery"},{"#tail":"\n","#text":"Wayne Ward"},{"#tail":"\n","#text":"James H Martin"},{"#tail":"\n","#text":"Daniel Jurafskyz"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Sun, Honglin and Daniel Jurafsky. 2004. Shallow Semantic Parsing of Chinese. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, Massachusetts."},"#text":"\n","marker":{"#tail":"\n","#text":"Sun, Jurafsky, 2004"},"location":{"#tail":"\n","#text":"Boston, Massachusetts."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"arayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and M?rquez 2004, 2005, Moschitti 2004, Pradhan et al2005, Zhang et al2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschi","@endWordPosition":"455","@position":"2974","annotationId":"T7","@startWordPosition":"452","@citStr":"Sun and Jurafsky (2004)"},{"#tail":"\n","#text":" this paper, we did not make the selection manually; however, we make a simple greedy strategy for feature selection to do it automatically. Although the best solution may not be found, automatic selection of features can try far more combinations of feature templates than manual selection. Because of this, this strategy possibly can produce a better local optional solution. First, we built a pool of feature templates which has proven to be useful on the SRC. Most of the feature templates are standard, so only the new ones will be explained. The candidate feature templates include: Voice from Sun and Jurafsky (2004). Head word POS, Head Word of Prepositional Phrases, Constituent tree distance, from Pradhan etc. (2004). Position, subcat frame, phrase type, first word, last word, subcat frame+, predicate, path, head word and its POS, predicate + head word, predicate + phrase type, path to BA and BEI, verb class 3 , verb class + head word, verb class + phrase type, from Xue (2008). 3 It is a bit different from Xue (2008), since we didn?t use the syntactic alternation information. predicate POS, first word + last word, phrase type of the sibling to the left, phrase type of the sibling to the right, verb + su","@endWordPosition":"2776","@position":"16843","annotationId":"T8","@startWordPosition":"2773","@citStr":"Sun and Jurafsky (2004)"}]},"title":{"#tail":"\n","#text":"Shallow Semantic Parsing of Chinese."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Honglin Sun"},{"#tail":"\n","#text":"Daniel Jurafsky"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Surdeanu, Mihai, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Ann Arbor, Michigan."},"#text":"\n","marker":{"#tail":"\n","#text":"Surdeanu, Harabagiu, Williams, Aarseth, 2003"},"location":{"#tail":"\n","#text":"Ann Arbor, Michigan."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ined in Gildea and Jurafsky (2002). The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and M?rquez 2004, 2005, Moschitti 2004, Pradhan et al2005, Zhang et al2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jur","@endWordPosition":"365","@position":"2425","annotationId":"T9","@startWordPosition":"362","@citStr":"Surdeanu et al 2003"}},"title":{"#tail":"\n","#text":"Using predicate-argument structures for information extraction."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mihai Surdeanu"},{"#tail":"\n","#text":"Sanda Harabagiu"},{"#tail":"\n","#text":"John Williams"},{"#tail":"\n","#text":"Paul Aarseth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Wang, Hui, Weidong Zhan, Shiwen Yu. 2003. The Specification of The Semantic Knowledge-base of Contemporary Chinese, In Journal of Chinese Language and Computing, 13(2):159-176."},"journal":{"#tail":"\n","#text":"In Journal of Chinese Language and Computing,"},"#text":"\n","pages":{"#tail":"\n","#text":"13--2"},"marker":{"#tail":"\n","#text":"Wang, Zhan, Yu, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cus, the number of constituents in the ascending part of the path subtracted by the number of those in the descending part of path, e.g. if the path is PP-BNF?VP?VP ?VV, the feature extracted by this template will be -1. SemCat (semantic category) of predicate, SemCat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word. The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese (Wang et al 2003). verb AllFrameSets, the combination of all the framesets of a predicate. verb class + verb AllFrameSets, verb AllFrameSets + head word, verb AllFrameSets + phrase type. There are more than 40 feature templates, and it is quite difficult to traverse all the possible combinations and get the best one. So we use a greedy algorithm to get an approximate optimal solution. The feature selection algorithm is as follows. Each time we choose one of the feature templates and add it into the system. The one, after which is added, the performance is the highest, will be chosen. Then we continue to choose","@endWordPosition":"3066","@position":"18489","annotationId":"T10","@startWordPosition":"3063","@citStr":"Wang et al 2003"}},"title":{"#tail":"\n","#text":"The Specification of The Semantic Knowledge-base of Contemporary Chinese,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hui Wang"},{"#tail":"\n","#text":"Weidong Zhan"},{"#tail":"\n","#text":"Shiwen Yu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Xue, Nianwen and Martha Palmer. 2003. Annotating the Propositions in the Penn Chinese Treebank. In Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan."},"#text":"\n","marker":{"#tail":"\n","#text":"Xue, Palmer, 2003"},"location":{"#tail":"\n","#text":"Sapporo, Japan."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"se SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschitti et al (2005) has made some preliminary attempt on the idea of hierarchical semantic 324 role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer","@endWordPosition":"531","@position":"3451","annotationId":"T11","@startWordPosition":"528","@citStr":"Xue and Palmer 2003"}},"title":{"#tail":"\n","#text":"Annotating the Propositions in the Penn Chinese Treebank."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nianwen Xue"},{"#tail":"\n","#text":"Martha Palmer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Xue, Nianwen and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Barcelona, Spain."},"#text":"\n","marker":{"#tail":"\n","#text":"Xue, Palmer, 2004"},"location":{"#tail":"\n","#text":"Barcelona,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"d Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschitti et al (2005) has made some preliminary attempt on the idea of hierarchical semantic 324 role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue and Palmer 2005, Xue 2008) reassured these findings. In this paper, we mainly focus on the semantic role classification (SRC) process. With the findings about the linguistic discrepancy of different semantic role groups, we try to build a 2-step semantic ro","@endWordPosition":"629","@position":"4058","annotationId":"T12","@startWordPosition":"626","@citStr":"Xue and Palmer (2004)"}},"title":{"#tail":"\n","#text":"Calibrating features for semantic role labeling."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nianwen Xue"},{"#tail":"\n","#text":"Martha Palmer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Xue, Nianwen and Martha Palmer. 2005. Automatic semantic role labeling for Chinese verbs. In 19th International Joint Conference on Artificial Intelligence. Edinburgh, Scotland."},"#text":"\n","marker":{"#tail":"\n","#text":"Xue, Palmer, 2005"},"location":{"#tail":"\n","#text":"Edinburgh, Scotland."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"04), Information Extraction (Surdeanu et al 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and M?rquez 2004, 2005, Moschitti 2004, Pradhan et al2005, Zhang et al2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschitti et al (2005) has ma","@endWordPosition":"459","@position":"2997","annotationId":"T13","@startWordPosition":"456","@citStr":"Xue and Palmer (2005)"},{"#tail":"\n","#text":"purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue and Palmer 2005, Xue 2008) reassured these findings. In this paper, we mainly focus on the semantic role classification (SRC) process. With the findings about the linguistic discrepancy of different semantic role groups, we try to build a 2-step semantic role classifier with hierarchical feature selection strategy. That means, for different sub tasks, different models will be trained with different features. The purpose of this strategy is to capture the right kind of information of different semantic role groups. It is hard to do manual selection of features since there are too many feature templates which ","@endWordPosition":"682","@position":"4416","annotationId":"T14","@startWordPosition":"679","@citStr":"Xue and Palmer 2005"},{"#tail":"\n","#text":"ent. 5.2 Data We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments. PropBank 1.0 includes the annotations for files chtb_001.fid to chtb_931.fid, or the first 250K words of the Chinese TreeBank 5.1. For the experiments, the data of PropBank is divided into three parts. 648 files (from chtb_081 to chtb_899.fid) are used as the training set. The development set includes 40 files, from chtb_041.fid to chtb_080.fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with Xue (2008), however a bit different from Xue and Palmer (2005). 6 Results and Discussion The results of the feature selection are presented in table1. In this table, ?Baseline? indicates the 1-step architecture, and ?Hierarchical? indicates the ?hierarchical feature selection architecture? implemented in this paper. ?X_M?, ?ARGX? and ?ARGM? indicate the three sub-procedures of the hierarchical architecture, which are ?ARGX and ARGM separation?, ?ARGX classification?, ?ARGM classification? respectively. ?Y? in the table indicates that the feature template has been selected for the sub task. According to table 1, we can find some interesting facts, which i","@endWordPosition":"3519","@position":"21049","annotationId":"T15","@startWordPosition":"3516","@citStr":"Xue and Palmer (2005)"},{"#tail":"\n","#text":"he performance of base system also decreased when semantic context features were extracted, since the core arguments and the ARGMs are mixed together in the baseline system. But for the ARGX sub task of our hierarchical system, since we have separated the numbered arguments and ARGMs first, the influences of ARGMs can be eliminated. This made the interdependence of core arguments can be directly explored from the extraction of semantic context features. So the ARGX sub task is improved. To prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer (2005), Xue (2008). Xue (2008) is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6. X & P (2005) Xue(2008) Ours 93.9% 94.1% 94.68% Table 6. Comparison with previous systems We have to point out that all the three systems are based on Gold standard parsing. From the table 6, we can find that our system is better than both of the related systems. Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%. 7 Conclusions and Future Work In this paper, we have divided all the semantic roles into two groups according to","@endWordPosition":"5064","@position":"30078","annotationId":"T16","@startWordPosition":"5061","@citStr":"Xue and Palmer (2005)"}]},"title":{"#tail":"\n","#text":"Automatic semantic role labeling for Chinese verbs."},"booktitle":{"#tail":"\n","#text":"In 19th International Joint Conference on Artificial Intelligence."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nianwen Xue"},{"#tail":"\n","#text":"Martha Palmer"}]}},{"volume":{"#tail":"\n","#text":"11"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Xue, Nianwen, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 11(2):207?238."},"journal":{"#tail":"\n","#text":"Natural Language Engineering,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Xue, Xia, Chiou, Palmer, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s. In section 2, the semantically annotated corpus - Chinese Propbank is discussed. The architecture of our method is described in section 3. The feature selection strategy is discussed in section 4. The settings of experiments can be found in section 5. The results of the experiments can be found in section 6, where we will try to make some linguistic explanations of the selected features. Section 7 is conclusions and future work. Figure 1. an example from PropBank 2 The Chinese PropBank The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank (Xue et al 2005). It is constituted of two parts. One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank. The other is a dictionary which IP ?? ?? ???? P NN NT NP-PN-SBJ VP PP-BNF VP VV NP-OBJ NP NN ?? ?? ?? f1 NN ????? ? AD NN P ARG2ADVP ARG0 PP-TMP ARGM-TMP has the Sanxia Project insurance provide ARGM-ADV ARG1 service forthe insurance company now until Until now, the insurance company has provided insurance services for the Sanxia Project. 325 lists the frames of all the labeled predicates. Figure 1 is an example from the PropBank1. We put the wor","@endWordPosition":"973","@position":"6174","annotationId":"T17","@startWordPosition":"970","@citStr":"Xue et al 2005"}},"title":{"#tail":"\n","#text":"The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nianwen Xue"},{"#tail":"\n","#text":"Fei Xia"},{"#tail":"\n","#text":"Fu dong Chiou"},{"#tail":"\n","#text":"Martha Palmer"}]}},{"volume":{"#tail":"\n","#text":"linguistics."},"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Xue, Nianwen. 2008. Labeling Chinese predicates with semantic roles. Computational linguistics. 34(2):225-255."},"journal":{"#tail":"\n","#text":"Computational"},"#text":"\n","pages":{"#tail":"\n","#text":"34--2"},"marker":{"#tail":"\n","#text":"Xue, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"n (Surdeanu et al 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and M?rquez 2004, 2005, Moschitti 2004, Pradhan et al2005, Zhang et al2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschitti et al (2005) has made some prelimi","@endWordPosition":"462","@position":"3012","annotationId":"T18","@startWordPosition":"461","@citStr":"Xue (2008)"},{"#tail":"\n","#text":"chical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue and Palmer 2005, Xue 2008) reassured these findings. In this paper, we mainly focus on the semantic role classification (SRC) process. With the findings about the linguistic discrepancy of different semantic role groups, we try to build a 2-step semantic role classifier with hierarchical feature selection strategy. That means, for different sub tasks, different models will be trained with different features. The purpose of this strategy is to capture the right kind of information of different semantic role groups. It is hard to do manual selection of features since there are too many feature templates which has been pr","@endWordPosition":"684","@position":"4427","annotationId":"T19","@startWordPosition":"683","@citStr":"Xue 2008"},{"#tail":"\n","#text":"t, we built a pool of feature templates which has proven to be useful on the SRC. Most of the feature templates are standard, so only the new ones will be explained. The candidate feature templates include: Voice from Sun and Jurafsky (2004). Head word POS, Head Word of Prepositional Phrases, Constituent tree distance, from Pradhan etc. (2004). Position, subcat frame, phrase type, first word, last word, subcat frame+, predicate, path, head word and its POS, predicate + head word, predicate + phrase type, path to BA and BEI, verb class 3 , verb class + head word, verb class + phrase type, from Xue (2008). 3 It is a bit different from Xue (2008), since we didn?t use the syntactic alternation information. predicate POS, first word + last word, phrase type of the sibling to the left, phrase type of the sibling to the right, verb + subcate frame+, verb POS + subcat frame+, the amount of VPs in path, phrase type + phrase type of parent node, which can be easily understood by name. voice position, indicates whether the voice marker (BA, BEI) is before or after the constituent in focus. subcat frame*, the rule that expands the parent node of the constituent in focus. subcat frame@, the rule that exp","@endWordPosition":"2840","@position":"17212","annotationId":"T20","@startWordPosition":"2839","@citStr":"Xue (2008)"},{"#tail":"\n","#text":"sification problem and it is quite efficient. 5.2 Data We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments. PropBank 1.0 includes the annotations for files chtb_001.fid to chtb_931.fid, or the first 250K words of the Chinese TreeBank 5.1. For the experiments, the data of PropBank is divided into three parts. 648 files (from chtb_081 to chtb_899.fid) are used as the training set. The development set includes 40 files, from chtb_041.fid to chtb_080.fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with Xue (2008), however a bit different from Xue and Palmer (2005). 6 Results and Discussion The results of the feature selection are presented in table1. In this table, ?Baseline? indicates the 1-step architecture, and ?Hierarchical? indicates the ?hierarchical feature selection architecture? implemented in this paper. ?X_M?, ?ARGX? and ?ARGM? indicate the three sub-procedures of the hierarchical architecture, which are ?ARGX and ARGM separation?, ?ARGX classification?, ?ARGM classification? respectively. ?Y? in the table indicates that the feature template has been selected for the sub task. According to ","@endWordPosition":"3510","@position":"20997","annotationId":"T21","@startWordPosition":"3509","@citStr":"Xue (2008)"},{"#tail":"\n","#text":"system also decreased when semantic context features were extracted, since the core arguments and the ARGMs are mixed together in the baseline system. But for the ARGX sub task of our hierarchical system, since we have separated the numbered arguments and ARGMs first, the influences of ARGMs can be eliminated. This made the interdependence of core arguments can be directly explored from the extraction of semantic context features. So the ARGX sub task is improved. To prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer (2005), Xue (2008). Xue (2008) is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6. X & P (2005) Xue(2008) Ours 93.9% 94.1% 94.68% Table 6. Comparison with previous systems We have to point out that all the three systems are based on Gold standard parsing. From the table 6, we can find that our system is better than both of the related systems. Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%. 7 Conclusions and Future Work In this paper, we have divided all the semantic roles into two groups according to their seman","@endWordPosition":"5066","@position":"30090","annotationId":"T22","@startWordPosition":"5065","@citStr":"Xue (2008)"}]},"title":{"#tail":"\n","#text":"Labeling Chinese predicates with semantic roles."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Nianwen Xue"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Yi, Szu-ting, Edward Loper, Martha Palmer. 2007. Can Semantic Roles Generalize Across Genres? In Proceedings of Human Language Technologies  and The Conference of the North American Chapter of the Association for Computational Linguistics, Rochester, NY, USA."},"#text":"\n","marker":{"#tail":"\n","#text":"Yi, Loper, Palmer, 2007"},"location":{"#tail":"\n","#text":"Rochester, NY, USA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" encouraging that the hierarchical SRC system outperformed the strong baseline built with traditional methods. And the selected features could be explained, which in turn proves that the linguistic discrepancy of semantic role groups not only exists but also can be captured. Then we integrated the idea of exploiting argument interdependence to further improve the performance of our system and explained linguistically why the results of our system were different from the ones in previous research. Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. Yi et al (2007) has made the first attempt working on the single semantic role level to make further improvement. However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the single semantic role level? Would that help the improvement of SRC? Acknowledgements This work was supported by National Natural Science Foundation of China under Grant No. 60303003 and National Social Science Foundation of China under Grant No. 06BYY048. We want to thank Nianwen Xue, for his generous help at the ","@endWordPosition":"5321","@position":"31617","annotationId":"T23","@startWordPosition":"5318","@citStr":"Yi et al (2007)"}},"title":{"#tail":"\n","#text":"Can Semantic Roles Generalize Across Genres?"},"booktitle":{"#tail":"\n","#text":"In Proceedings of Human Language Technologies and The Conference of the North American Chapter of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Szu-ting Yi"},{"#tail":"\n","#text":"Edward Loper"},{"#tail":"\n","#text":"Martha Palmer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Zhang, Min, Wanxiang Che, Ai Ti Aw, Chew Lim Tan, Guodong Zhou, Ting Liu, Sheng Li. 2007. A Grammar-driven Convolution Tree Kernel for Semantic Role Classification, in Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, Prague, Czech Republic."},"#text":"\n","marker":{"#tail":"\n","#text":"Zhang, Che, 2007"},"location":{"#tail":"\n","#text":"Prague, Czech Republic."},"title":{"#tail":"\n","#text":"Ai Ti Aw, Chew Lim Tan, Guodong Zhou, Ting Liu, Sheng Li."},"booktitle":{"#tail":"\n","#text":"in Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Min Zhang"},{"#tail":"\n","#text":"Wanxiang Che"}]}}]}}]}}
