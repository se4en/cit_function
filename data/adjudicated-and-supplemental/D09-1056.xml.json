{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","note":{"#tail":"\n","@confidence":"0.382014","#text":"\nProceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 534?542,\nSingapore, 6-7 August 2009.\n"},"listItem":{"#tail":"\n","@confidence":"0.353656142857143","#text":"\non Machine Learning. It provides a general im-\nplementation of linear chain Conditional Ran-\ndom Field sequence models and includes a model\ntrained on data from CoNLL, MUC6, MUC7, and\nACE newswire. Three types of entities were ex-\ntracted: person, location and organisation.\nOAK\n"},"figure":[{"#tail":"\n","@confidence":"0.870973","#text":"\nc?2009 ACL and AFNLP\nThe role of named entities in Web People Search\nJavier Artiles\nUNED NLP & IR group\nMadrid, Spain\njavart@bec.uned.es\nEnrique Amig\n?\no\nUNED NLP & IR group\nMadrid, Spain\nenrique@lsi.uned.es\nJulio Gonzalo\nUNED NLP & IR group\n"},{"#tail":"\n","@confidence":"0.895604285714286","#text":"\nOAK System\n5\n.\nStanford NE Recogniser\n6\nis a high-performance\nNamed Entity Recognition (NER) system based\n"},{"#tail":"\n","@confidence":"0.97674952","#text":"\nThat is:\nsim\ntoken\n(a, b) > sim\ntoken\n(c, d)\nThen for any other feature similarity\nsim\nx\n(a, b), we will talk about redundant samples\nwhen sim\nx\n(a, b) > sim\nx\n(c, d), non redundant\nsamples when sim\nx\n(a, b) < sim\nx\n(c, d), and\nnon informative samples when sim\nx\n(a, b) =\nsim\nx\n"},{"#tail":"\n","@confidence":"0.692723666666667","#text":"\nB-Cubed\nrun F-? =\n0.5\n"}],"address":{"#tail":"\n","@confidence":"0.672875","#text":"\nMadrid, Spain\n"},"equation":[{"#tail":"\n","@confidence":"0.99632825","#text":"\nt\nn\nn\nand R =\nt\nn\nt\n. We\n"},{"#tail":"\n","@confidence":"0.987874","#text":"\nPWA = Prob(x(a, a\n?\n) > x(c, d))\n"},{"#tail":"\n","@confidence":"0.757632","#text":"\n1\n. . . x\nn\n"},{"#tail":"\n","@confidence":"0.9762915","#text":"\nMaxPWA(X) =\nProb(?x ? X.x(a, a\n?\n) > x(c, d))\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.696555","#text":"\n2.2 Named entities in the WePS campaign\n"},{"#tail":"\n","@confidence":"0.974031","#text":"\n3.1 Data\n"},{"#tail":"\n","@confidence":"0.945945","#text":"\n3.2 Features\n"},{"#tail":"\n","@confidence":"0.968568","#text":"\n4.1 Reformulating WePS as a classification\n"},{"#tail":"\n","@confidence":"0.998944","#text":"\n4.2 Analysis of individual features\n"},{"#tail":"\n","@confidence":"0.999743","#text":"\n4.3 Analysis of feature combinations\n"}],"subsubsectionHeader":[{"#tail":"\n","@confidence":"0.510236","#text":"\nProgramme Committees in the Computer Science\n"},{"#tail":"\n","@confidence":"0.953104","#text":"\n4.2.1 Feature performance\n"},{"#tail":"\n","@confidence":"0.965652","#text":"\n4.2.2 Redundancy\n"},{"#tail":"\n","@confidence":"0.710706","#text":"\n4.3.1 Results on the clustering task\n"}],"footnote":{"#tail":"\n","@confidence":"0.240338","#text":"\nhttp://nlp.stanford.edu/software/CRF-NER.shtml\n"},"@confidence":"0.000000","#tail":"\n","reference":{"#tail":"\n","@confidence":"0.998712424242424","#text":"\nEneko Agirre and Philip Edmonds, editors. 2006.\nWord Sense Disambiguation: Algorithms and Appli-\ncations. Springer.\nReema Al-Kamha and David W. Embley. 2004.\nGrouping search-engine returned citations for\nperson-name queries. In WIDM ?04: Proceedings of\nthe 6th annual ACM international workshop on Web\ninformation and data management. ACM Press.\nEnrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-\nlisa Verdejo. 2008. A comparison of extrinsic\nclustering evaluation metrics based on formal con-\nstraints. Information Retrieval.\nJavier Artiles, Julio Gonzalo, and Felisa Verdejo. 2005.\nA testbed for people searching strategies in the\nwww. In SIGIR.\nJavier Artiles, Julio Gonzalo, and Satoshi Sekine.\n2007. The semeval-2007 weps evaluation: Estab-\nlishing a benchmark for the web people search task.\nIn Proceedings of the Fourth International Work-\nshop on Semantic Evaluations (SemEval-2007).\nACL.\nJavier Artiles, Julio Gonzalo, and Satoshi Sekine.\n2009. Weps 2 evaluation campaign: overview of\nthe web people search clustering task. In WePS 2\nEvaluation Workshop. WWW Conference 2009.\nAmit Bagga and Breck Baldwin. 1998. Entity-\nbased cross-document coreferencing using the vec-\ntor space model. In Proceedings of the 17th inter-\nnational conference on Computational linguistics.\nACL.\nMatthias Blume. 2005. Automatic entity disambigua-\ntion: Benefits to ner, relation extraction, link anal-\nysis, and inference. In International Conference on\nIntelligence Analysis.\nYing Chen and James H. Martin. 2007. Cu-comsem:\nExploring rich features for unsupervised web per-\nsonal name disambiguation. In Proceedings of the\nFourth International Workshop on Semantic Evalu-\nations. ACL.\nSilviu Cucerzan. 2007. Large scale named entity\ndisambiguation based on wikipedia data. In The\nEMNLP-CoNLL-2007.\nDavid del Valle-Agudo, C?esar de Pablo-S?anchez, and\nMar??a Teresa Vicente-D??ez. 2007. Uc3m-13: Dis-\nambiguation of person names based on the compo-\nsition of simple bags of typed terms. In Proceedings\nof the Fourth International Workshop on Semantic\nEvaluations. ACL.\nChung Heong Gooi and James Allan. 2004. Cross-\ndocument coreference on a large scale corpus. In\nHLT-NAACL.\nDmitri V. Kalashnikov, Stella Chen, Rabia Nuray,\nSharad Mehrotra, and Naveen Ashish. 2007. Dis-\nambiguation algorithm for people search on the web.\nIn Proc. of IEEE International Conference on Data\nEngineering (IEEE ICDE).\nBradley Malin. 2005. Unsupervised name disam-\nbiguation via social network similarity. In Workshop\non Link Analysis, Counterterrorism, and Security.\nGideon S. Mann and David Yarowsky. 2003. Unsuper-\nvised personal name disambiguation. In Proceed-\nings of the seventh conference on Natural Language\nLearning (CoNLL) at HLT-NAACL 2003. ACL.\nGideon S. Mann. 2006. Multi-Document Statistical\nFact Extraction and Fusion. Ph.D. thesis, Johns\nHopkins University.\nHien T. Nguyen and Tru H. Cao, 2008. Named En-\ntity Disambiguation: A Hybrid Statistical and Rule-\nBased Incremental Approach. Springer.\nOctavian Popescu and Bernardo Magnini. 2007. Irst-\nbp: Web people search using name entities. In Pro-\nceedings of the Fourth International Workshop on\nSemantic Evaluations. ACL.\nY. Ravin and Z. Kazi. 1999. Is hillary rodham clinton\nthe president? disambiguating names across docu-\nments. In Proceedings of the ACL ?99 Workshop\non Coreference and its Applications Association for\nComputational Linguistics.\nHoracio Saggion. 2008. Experiments on semantic-\nbased clustering for cross-document coreference. In\nInternational Joint Conference on Natural language\nProcessing.\nSatoshi Sekine. 2008. Extended named entity on-\ntology with attribute information. In Proceedings\nof the Sixth International Language Resources and\nEvaluation (LREC?08).\nAmanda Spink, Bernard Jansen, and Jan Pedersen.\n2004. Searching for people on web search engines.\nJournal of Documentation, 60:266 ? 278.\nKazunari Sugiyama and Manabu Okumura. 2007.\nTitpi: Web people search task using semi-supervised\nclustering approach. In Proceedings of the Fourth\nInternational Workshop on Semantic Evaluations.\nACL.\nXiaojun Wan, Jianfeng Gao, Mu Li, and Binggong\nDing. 2005. Person resolution in person search re-\nsults: Webhawk. In CIKM ?05: Proceedings of the\n14th ACM international conference on Information\nand knowledge management. ACM Press.\n"},"bodyText":[{"#tail":"\n","@confidence":"0.999503368421053","#text":"\nThe ambiguity of person names in the Web\nhas become a new area of interest for NLP\nresearchers. This challenging problem has\nbeen formulated as the task of clustering\nWeb search results (returned in response\nto a person name query) according to the\nindividual they mention. In this paper we\ncompare the coverage, reliability and in-\ndependence of a number of features that\nare potential information sources for this\nclustering task, paying special attention to\nthe role of named entities in the texts to\nbe clustered. Although named entities are\nused in most approaches, our results show\nthat, independently of the Machine Learn-\ning or Clustering algorithm used, named\nentity recognition and classification per se\nonly make a small contribution to solve the\nproblem.\n"},{"#tail":"\n","@confidence":"0.986372533333333","#text":"\nSearching the Web for names of people is a highly\nambiguous task, because a single name tends to\nbe shared by many people. This ambiguity has\nrecently become an active research topic and, si-\nmultaneously, in a relevant application domain for\nweb search services: Zoominfo.com, Spock.com,\n123people.com are examples of sites which per-\nform web people search, although with limited\ndisambiguation capabilities.\nA study of the query log of the AllTheWeb and\nAltavista search sites gives an idea of the relevance\nof the people search task: 11-17% of the queries\nwere composed of a person name with additional\nterms and 4% were identified as person names\n(Spink et al, 2004). According to the data avail-\nable from 1990 U.S. Census Bureau, only 90,000\ndifferent names are shared by 100 million people\n(Artiles et al, 2005). As the amount of informa-\ntion in the WWW grows, more of these people are\nmentioned in different web pages. Therefore, a\nquery for a common name in the Web will usually\nproduce a list of results where different people are\nmentioned.\nThis situation leaves to the user the task of find-\ning the pages relevant to the particular person he\nis interested in. The user might refine the original\nquery with additional terms, but this risks exclud-\ning relevant documents in the process. In some\ncases, the existence of a predominant person (such\nas a celebrity or a historical figure) makes it likely\nto dominate the ranking of search results, compli-\ncating the task of finding information about other\npeople sharing her name. The Web People Search\ntask, as defined in the first WePS evaluation cam-\npaign (Artiles et al, 2007), consists of grouping\nsearch results for a given name according to the\ndifferent people that share it.\nOur goal in this paper is to study which doc-\nument features can contribute to this task, and in\nparticular to find out which is the role that can be\nplayed by named entities (NEs): (i) How reliable\nis NEs overlap between documents as a source of\nevidence to cluster pages? (ii) How much recall\ndoes it provide? (iii) How unique is this signal?\n(i.e. is it redundant with other sources of informa-\ntion such as n-gram overlap?); and (iv) How sen-\nsitive is this signal to the peculiarities of a given\nNE recognition system, such as the granularity of\nits NE classification and the quality of its results?\nOur aim is to reach conclusions which are are\nnot tied to a particular choice of Clustering or Ma-\nchine Learning algorithms. We have taken two de-\ncisions in this direction: first, we have focused on\nthe problem of deciding whether two web pages\nrefer to the same individual or not (page corefer-\nence task). This is the kind of relatedness measure\nthat most clustering algorithms use, but in this way\nwe can factor out the algorithm and its parameter\nsettings. Second, we have developed a measure,\nMaximal Pairwise Accuracy (PWA) which, given\n"},{"#tail":"\n","@confidence":"0.999686928571428","#text":"\nan information source for the problem, estimates\nan upper bound for the performance of any Ma-\nchine Learning algorithm using this information.\nWe have used PWA as the basic metric to study the\nrole of different document features in solving the\ncoreference problem, and then we have checked\nthe predictive power of PWA with a Decision Tree\nalgorithm.\nThe remainder of the paper is organised as fol-\nlows. First, we examine the previous work in Sec-\ntion 2. Then we describe the our experimental set-\ntings (datasets and features we have used) in Sec-\ntion 3 and our empirical study in Section 4. The\npaper ends with some conclusions in Section 5.\n"},{"#tail":"\n","@confidence":"0.995547255319149","#text":"\nIn this section we will discuss (i) the state of the\nart in Web People Search in general, focusing on\nwhich features are used to solve the problem; and\n(ii) lessons learnt from the WePS evaluation cam-\npaign where most approaches to the problem have\nbeen tested and compared.\nThe disambiguation of person names in Web\nresults is usually compared to two other Natu-\nral Language Processing tasks: Word Sense Dis-\nambiguation (WSD) (Agirre and Edmonds, 2006)\nand Cross-document Coreference (CDC) (Bagga\nand Baldwin, 1998). Most of early research work\non person name ambiguity focuses on the CDC\nproblem or uses methods found in the WSD litera-\nture. It is only recently that the web name ambigu-\nity has been approached as a separate problem and\ndefined as an NLP task - Web People Search - on\nits own (Artiles et al, 2005; Artiles et al, 2007).\nTherefore, it is useful to point out some crucial\ndifferences between WSD, CRC and WePS:\n? WSD typically concentrates in the disam-\nbiguation of common words (nouns, verbs,\nadjectives) for which a relatively small num-\nber of senses exist, compared to the hun-\ndreds or thousands of people that can share\nthe same name.\n? WSD can rely on dictionaries to define the\nnumber of possible senses for a word. In the\ncase of name ambiguity no such dictionary\nis available, even though in theory there is an\nexact number of people that can be accounted\nas sharing the same name.\n? The objective of CDC is to reconstruct the\ncoreference chain for every mention of a per-\nson. In Web person name disambiguation it\nsuffices to group the documents that contain\nat least one mention to the same person.\nBefore the first WePS evaluation campaign in\n2007 (Artiles et al, 2007), research on the topic\nwas not based on a consistent task definition, and\nit lacked a standard manually annotated testbed.\nIn the WePS task, systems were given the top web\nsearch results produced by a person name query.\nThe expected output was a clustering of these re-\nsults, where each cluster should contain all and\nonly those documents referring to the same indi-\nvidual.\n"},{"#tail":"\n","@confidence":"0.998144555555556","#text":"\nMany different features have been used to repre-\nsent documents where an ambiguous name is men-\ntioned. The most basic is a Bag of Words (BoW)\nrepresentation of the document text. Within-\ndocument coreference resolution has been applied\nto produce summaries of text surrounding occur-\nrences of the name (Bagga and Baldwin, 1998;\nGooi and Allan, 2004). Nevertheless, the full\ndocument text is present in most systems, some-\ntimes as the only feature (Sugiyama and Okumura,\n2007) and sometimes in combination with others -\nsee for instance (Chen and Martin, 2007; Popescu\nand Magnini, 2007)-. Other representations use\nthe link structure (Malin, 2005) or generate graph\nrepresentations of the extracted features (Kalash-\nnikov et al, 2007).\nSome researchers (Cucerzan, 2007; Nguyen and\nCao, 2008) have explored the use of Wikipedia\ninformation to improve the disambiguation pro-\ncess. Wikipedia provides candidate entities that\nare linked to specific mentions in a text. The obvi-\nous limitation of this approach is that only celebri-\nties and historical figures can be identified in this\nway. These approaches are yet to be applied to the\nspecific task of grouping search results.\nBiographical features are strongly related to\nNEs and have also been proposed for this task\ndue to its high precision. Mann (2003) extracted\nthese features using lexical patterns to group pages\nabout the same person. Al-Kamha (2004) used a\nsimpler approach, based on hand coded features\n(e.g. email, zip codes, addresses, etc). In Wan\n(2005), biographical information (person name, ti-\ntle, organisation, email address and phone num-\nber) improves the clustering results when com-\nbined with lexical features (words from the doc-\n"},{"#tail":"\n","@confidence":"0.9996156","#text":"\nument) and NE (person, location, organisation).\nThe most used feature for the Web People\nSearch task, however, are NEs. Ravin (1999) in-\ntroduced a rule-based approach that tackles both\nvariation and ambiguity analysing the structure of\nnames. In most recent research, NEs (person, lo-\ncation and organisations) are extracted from the\ntext and used as a source of evidence to calculate\nthe similarity between documents -see for instance\n(Blume, 2005; Chen and Martin, 2007; Popescu\nand Magnini, 2007; Kalashnikov et al, 2007)-\n. For instance, Blume (2005) uses NEs coocur-\nring with the ambiguous mentions of a name as a\nkey feature for the disambiguation process. Sag-\ngion (2008) compared the performace of NEs ver-\nsus BoW features. In his experiments a only a\nrepresentation based on Organisation NEs outper-\nformed the word based approach. Furthermore,\nthis result is highly dependent on the choice of\nmetric weighting (NEs achieve high precision at\nthe cost of a low recall and viceversa for BoW).\nIn summary, the most common document repre-\nsentations for the problem include BoW and NEs,\nand in some cases biographical features extracted\nfrom the text.\n"},{"#tail":"\n","@confidence":"0.9800525","#text":"\nAmong the 16 teams that submitted results for the\nfirst WePS campaign, 10 of them\n"},{"#tail":"\n","@confidence":"0.983024684210526","#text":"\nused NEs in\ntheir document representation. This makes NEs\nthe second most common type of feature; only\nthe BoW feature was more popular. Other fea-\ntures used by the systems include noun phrases\n(Chen and Martin, 2007), word n-grams (Popescu\nand Magnini, 2007), emails and URLs (del Valle-\nAgudo et al, 2007), etc. In 2009, the second\nWePS campaign showed similar trends regarding\nthe use of NE features (Artiles et al, 2009).\nDue to the complexity of systems, the results\nof the WePS evaluation do not provide a direct\nanswer regarding the advantages of using NEs\nover other computationally lighter features such as\nBoW or word n-grams. But the WePS campaigns\ndid provide a useful, standardised resource to per-\nform the type of studies that were not possible be-\nfore. In the next Section we describe this dataset\nand how it has been adapted for our purposes.\n"},{"#tail":"\n","@confidence":"0.564609","#text":"\nBy team ID: CU-COMSEM, IRST-BP, PSNUS, SHEF,\nFICO, UNN, AUG, JHU1, DFKI2, UC3M13\n"},{"#tail":"\n","@confidence":"0.931209","#text":"\nWe have used the testbeds from WePS-1 (Artiles et\nal., 2007)\n"},{"#tail":"\n","@confidence":"0.672567","#text":"\nand WePS-2 (Artiles et al, 2009) eval-\nuation campaigns\n"},{"#tail":"\n","@confidence":"0.954895","#text":"\n.\nEach WePS dataset consists of 30 test cases: a\nrandom sample of 10 names from the US Cen-\nsus, 10 names from Wikipedia, and 10 names from\n"},{"#tail":"\n","@confidence":"0.999209611111111","#text":"\ndomain (ACL and ECDL). Each test case consists\nof, at most, 100 web pages from the top search\nresults of a web search engine, using a (quoted)\nperson name as query.\nFor each test case, annotators were asked to or-\nganise the web pages in groups where all docu-\nments refer to the same person. In cases where\na web page refers to more than one person us-\ning the same ambiguous name (e.g. a web page\nwith search results from Amazon), the document\nis assigned to as many groups as necessary. Doc-\numents were discarded when they did not contain\nany useful information about the person being re-\nferred.\nBoth the WePS-1 and WePS-2 testbeds have\nbeen used to evaluate clustering systems by WePS\ntask participants, and are now the standard testbed\nto test Web People Search systems.\n"},{"#tail":"\n","@confidence":"0.9954235","#text":"\nThe evaluated features can be grouped in four\nmain groups: token-based, n-grams, phrases and\nNEs. Wherever possible, we have generated lo-\ncal versions of these features that only consider\nthe sentences of the text that mention the ambigu-\nous person name\n"},{"#tail":"\n","@confidence":"0.992462","#text":"\n. Token-based features consid-\nered include document full text tokens, lemmas\n(using the OAK analyser, see below), title, snip-\npet (returned in the list of search results) and URL\n(tokenised using non alphanumeric characters as\nboundaries) tokens. English stopwords were re-\nmoved, including Web specific stopwords, as file\nand domain extensions, etc.\nWe generated word n-grams of length 2 to 5,\n"},{"#tail":"\n","@confidence":"0.9704365","#text":"\nThe WePS-1 corpus includes data from the Web03\ntestbed (Mann, 2006) which follows similar annotation\nguidelines, although the number of document per ambiguous\nname is more variable.\n"},{"#tail":"\n","@confidence":"0.790391","#text":"\nBoth corpora are available from the WePS website\nhttp://nlp.uned.es/weps\n"},{"#tail":"\n","@confidence":"0.991490666666667","#text":"\nA very sparse feature might never occur in a sentence\nwith the person name. In that cases there is no local version\nof the feature.\n"},{"#tail":"\n","@confidence":"0.999135636363636","#text":"\nusing the sentences found in the document text.\nPunctuation tokens (commas, dots, etc) were gen-\neralised as the same token. N-grams were dis-\ncarded when they were composed only of stop-\nwords or when they did not contain at least one\ntoken formed by alphanumeric characters (e.g. n-\ngrams like ?at the? or ?# @?). Noun phrases (us-\ning OAK analyser) were detected in the document\nand filtered in a similar way.\nNamed entities were extracted using two dif-\nferent tools: the Stanford NE Recogniser and the\n"},{"#tail":"\n","@confidence":"0.989880818181818","#text":"\nis a rule based English analyser that in-\ncludes many functionalities (POS tagger, stemmer,\nchunker, Named Entity (NE) tagger, dependency\nanalyser, parser, etc). It provides a fine grained\nNE recognition covering 100 different NE types\n(Sekine, 2008). Given the sparseness of most of\nthese fine-grained NE types, we have merged them\nin coarser groups: event, facility, location, person,\norganisation, product, periodx, timex and numex.\nWe have also used the results of a baseline\nNE recognition for comparison purposes. This\nmethod detects sequences of two or more upper-\ncased tokens in the text, and discards those that are\nfound lowercased in the same document or that are\ncomposed solely of stopwords.\nOther features are: emails, outgoing links found\nin the web pages and two boolean flags that in-\ndicate whether a pair of documents is linked or\nbelongs to the same domain. Because of their\nlow impact in the results these features haven?t re-\nceived an individual analysis, but they are included\nin the ?all features? combination in Figure 7.\n"},{"#tail":"\n","@confidence":"0.91946875","#text":"\nFrom the output of both systems we have discarded per-\nson NEs made of only one token (these are often first names\nthat significantly deteriorate the quality of the comparison be-\ntween documents).\n"},{"#tail":"\n","@confidence":"0.92098","#text":"\nhttp://nlp.cs.nyu.edu/oak . OAK was also used to detect\nnoun phrases and extract lemmas from the text.\n"},{"#tail":"\n","@confidence":"0.993395183673469","#text":"\ntask\nAs our goal is to study the impact of different fea-\ntures (information sources) in the task, a direct\nevaluation in terms of clustering has serious disad-\nvantages. Given the output of a clustering system\nit is not straightforward to assess why a document\nhas been assigned to a particular cluster. There are\nat least three different factors: the document sim-\nilarity function, the clustering algorithm and its\nparameter settings. Features are part of the doc-\nument similarity function, but its performance in\nthe clustering task depends on the other factors as\nwell. This makes it difficult to perform error anal-\nysis in terms of the features used to represent the\ndocuments.\nTherefore we have decided to transform the\nclustering problem into a classification problem:\ndeciding whether two documents refer to the same\nperson. Each pair of documents in a name dataset\nis considered a classification instance. Instances\nare labelled as coreferent (if they share the same\ncluster in the gold standard) or non coreferent (if\nthey do not share the same cluster). Then we\ncan evaluate the performance of each feature sep-\narately by measuring its ability to rank coreferent\npairs higher and non coreferent pairs lower. In the\ncase of feature combinations we can study them by\ntraining a classifier or using the maximal pairwise\naccuracy methods (explained in Section 4.3).\nEach instance (pair of documents) is repre-\nsented by the similarity scores obtained using dif-\nferent features and similarity metrics. We have\ncalculated for each feature three similarity met-\nrics: Dice?s coefficient, cosine (using standard\ntf.idf weighting) and a measure that simply counts\nthe size of the intersection set for a given feature\nbetween both documents. After testing these met-\nrics we found that Dice provides the best results\nacross different feature types. Differences be-\ntween Dice and cosine were consistent, although\nthey were not especially large. A possible expla-\nnation is that Dice does not take into account the\nredundancy of an n-gram or NE in the document,\nand the cosine distance does. This can be a cru-\ncial factor, for instance, in the document retrieval\nby topic; but it doesn?t seem to be the case when\ndealing with name ambiguity.\nThe resulting classification testbed consists of\n293,914 instances with the distribution shown in\n"},{"#tail":"\n","@confidence":"0.999553875","#text":"\nThere are two main aspects related with the use-\nfulness of a feature for WePS task. The first one is\nits performance. That is, to what extent the simi-\nlarity between two documents according to a fea-\nture implies that both mention the same person.\nThe second aspect is to what extent a feature is or-\nthogonal or redundant with respect to the standard\ntoken based similarity.\n"},{"#tail":"\n","@confidence":"0.990295923076923","#text":"\nAccording to the transformation of WePS cluster-\ning problem into a classification task (described\nin Section 4.1), we follow the next steps to study\nthe performance of individual features. First, we\ncompute the Dice coefficient similarity over each\nfeature for all document pairs. Then we rank the\ndocument pair instances according to these simi-\nlarities. A good feature should rank positive in-\nstances on top. If the number of coreferent pairs\nin the top n pairs is t\nn\nand the total number of\ncoreferent pairs is t, then P =\n"},{"#tail":"\n","@confidence":"0.9222005","#text":"\nplot the obtained precision/recall curves in Figures\n1, 2, 3 and 4.\nFrom the figures we can draw the following\nconclusions:\nFirst, considering subsets of tokens or lemma-\ntised tokens does not outperform the basic token\ndistance (figure 1 compares token-based features).\nWe see that only local and snippet tokens perform\nslightly better at low recall values, but do not go\nbeyond recall 0.3.\nSecond, shallow parsing or n-grams longer than\n2 do not seem to be effective, but using bi-grams\nimproves the results in comparison with tokens.\nFigure 2 compares n-grams of different sizes with\nnoun phrases and tokens. Overall, noun phrases\nhave a poor performance, and bi-grams give the\nbest results up to recall 0.7. Four-grams give\nslightly better precision but only reach 0.3 recall,\n"},{"#tail":"\n","@confidence":"0.992087583333333","#text":"\nThird, individual types of NEs do not improve\nover tokens. Figure 3 and Figure 4 display the\nresults obtained by the Stanford and OAK NER\ntools respectively. In the best case, Stanford per-\nson and organisation named entities obtain results\nthat match the tokens feature, but only at lower\nlevels of recall.\nFinally, using different NER systems clearly\nleads to different results. Surprisingly, the base-\nline NE system yields better results in a one to\none comparison, although it must be noted that\nthis baseline agglomerates different types of en-\n"},{"#tail":"\n","@confidence":"0.972436","#text":"\nwith the OAK NER tool\ntities that are separated in the case of Stanford and\nOAK, and this has a direct impact on its recall.\nThe OAK results are below the tokens and NE\nbaseline, possibly due to the sparseness of its very\nfine grained features. In NE types, cases such as\nperson and organisation results are still lower than\nobtained with Stanford.\n"},{"#tail":"\n","@confidence":"0.9891855","#text":"\nIn addition to performance, named entities (as well\nas other features) are potentially useful for the task\nonly if they provide information that complements\n(i.e. that does not substantially overlap) the basic\ntoken based metric. To estimate this redundancy,\nlet us consider all document tuples of size four <\na, b, c, d >. In 99% of the cases, token similarity is\ndifferent for < a, b > than for < c, d >. We take\ncombinations such that < a, b > are more similar\nto each other than < c, d > according to tokens.\n"},{"#tail":"\n","@confidence":"0.93997725","#text":"\n(c, d). If all samples are redundant or\nnon informative, then sim\nx\ndoes not provide\nadditional information for the classification task.\nFigure 5 shows the proportion of redundant, non\nredundant and non informative samples for sev-\neral similarity criteria, as compared to token-based\nsimilarity. In most cases NE based similarities\ngive little additional information: the baseline NE\nrecogniser, which has the largest independent con-\ntribution, gives additional information in less than\n20% of cases.\nIn summary, analysing individual features, the\nNEs do not outperform BoW in terms of the clas-\nsification task. In addition, NEs tend to be re-\ndundant regarding BoW. However, if we are able\nto combine optimally the contributions of the dif-\nferent features, the BoW approach could be im-\nproved. We address this issue in the next section.\n"},{"#tail":"\n","@confidence":"0.997681741935484","#text":"\nUp to now we have analysed the usefulness of in-\ndividual features for the WePS Task. However,\nthis begs to ask to what extent the NE features can\ncontribute to the task when they are combined to-\ngether and with token and n-gram based features.\nFirst, we use each feature combinations as the in-\nput for a Machine Learning algorithm. In particu-\nlar, we use a Decision Tree algorithm and WePS-1\ndata for training and WePS-2 data for testing. The\nDecision Tree algorithm was chosen because we\nhave a small set of features to train (similarity met-\nrics) and some of these features output Boolean\nvalues.\nResults obtained with this setup, however, can\nbe dependent on the choice of the ML approach.\nTo overcome this problem, in addition to the re-\nsults of a Decision Tree Machine Learning algo-\nrithm, we introduce a Maximal Pairwise Accuracy\n(MPA) measure that provides an upper bound for\nany machine learning algorithm using a feature\ncombination.\nWe can estimate the performance of an individ-\nual similarity feature x in terms of accuracy. It\nis considered a correct answer when the similarity\nx(a, a\n?\n) between two pages referring to the same\nperson is higher than the similarity x(b, c) between\ntwo pages referring to different people. Let us\ncall this estimation Pairwise Accuracy. In terms\nof probability it can be defined as:\n"},{"#tail":"\n","@confidence":"0.934310333333333","#text":"\nPWA is defined over a single feature (similar-\nity metric). When considering more than one sim-\nilarity measure, the results depend on how mea-\nsures are weighted. In that case we assume that\nthe best possible weighting is applied. When com-\nbining a set of features X = {x\n"},{"#tail":"\n","@confidence":"0.951567","#text":"\n}, a per-\nfect Machine Learning algorithm would learn to\nalways ?listen? to the features giving correct in-\nformation and ignore the features giving erroneous\ninformation. In other words, if at least one feature\ngives correct information, then the perfect algo-\nrithm would produce a correct output. This is what\nwe call the Maximal Pairwise Accuracy estimation\nof an upper bound for any ML system using the set\nof features X:\n"},{"#tail":"\n","@confidence":"0.89336375","#text":"\nThe upper bound (MaxPWA) of feature combi-\nnations happens to be highly correlated with the\nPWA obtained by the Decision Tree algorithm (us-\ning its confidence values as a similarity metric).\nFigure 6 shows this correlation for several features\ncombinations. This is an indication that the Deci-\nsion Tree is effectively using the information in the\nfeature set.\n"},{"#tail":"\n","@confidence":"0.992032666666667","#text":"\nlanguage processing machinery: tokens, url, title,\nsnippet, local tokens, n-grams and local n-grams;\nand (iii) just tokens. The results show that accord-\ning to both the Decision Tree results and the upper-\nbound (MaxPWA), adding new features to tokens\nimproves the classification. However, taking non-\nlinguistic features obtains similar results than tak-\ning all features. Our conclusion is that NE features\nare useful for the task, but do not seem to offer a\n"},{"#tail":"\n","@confidence":"0.999787","#text":"\ncompetitive advantage when compared with non-\nlinguistic features, and are more computationally\nexpensive. Note that we are using NE features in a\ndirect way: our results do not exclude the possibil-\nity of effectively exploiting NEs in more sophisti-\ncated ways, such as, for instance, exploiting the\nunderlying social network relationships between\nNEs in the texts.\n"},{"#tail":"\n","@confidence":"0.999318090909091","#text":"\nIn order to validate our results, we have tested\nwhether the classifiers learned with our feature\nsets lead to competitive systems for the full clus-\ntering task. In order to do so, we use the output of\nthe classifiers as similarity metrics for a particu-\nlar clustering algorithm, using WePS-1 to train the\nclassifiers and WePS-2 for testing.\nWe have used a Hierarchical Agglomerative\nClustering algorithm (HAC) with single linkage,\nusing the classifier?s confidence value in the nega-\ntive answer for each instance as a distance metric\n"},{"#tail":"\n","@confidence":"0.98476775","#text":"\nbetween document pairs. HAC is the algorithm\nused by some of the best performing systems in the\nWePS-2 evaluation. The distance threshold was\ntrained using the WePS-1 data. We report results\nwith the official WePS-2 evaluation metrics: ex-\ntended B-Cubed Precision and Recall (Amig?o et\nal., 2008).\nTwo Decision Tree models were evaluated: (i)\nML-ALL is a model trained using all the available\nfeatures (which obtains 0.76 accuracy in the clas-\nsification task) (ii) ML-NON LING was trained\nwith all the features except for OAK and Stanford\nNEs, noun phrases, lemmas and gazetteer features\n(which obtains 0.75 accuracy in the classification\ntask). These are the same classifiers considered in\nFigure 7.\nTable 2 shows the results obtained in the clus-\ntering task by the two DT models, together with\nthe four top scoring WePS-2 systems and the av-\nerage values for all WePS-2 systems. We found\nthat a ML based clustering using only non linguis-\ntic information slightly outperforms the best par-\nticipant in WePS-2. Surprisingly, adding linguis-\ntic information (NEs, noun phrases, etc.) has a\nsmall negative impact on the results (0.81 versus\n0.83), although the classifier with linguistic infor-\nmation was a bit better than the non-linguistic one.\nThis seems to be another indication that the use of\n"},{"#tail":"\n","@confidence":"0.9980982","#text":"\nThe DT classifier output consists of two confidence val-\nues, one for the positive and one for the negative answer, that\nadd up to 1.0 .\nnoun phrases and other linguistic features to im-\nprove the task is non-obvious to say the least.\n"},{"#tail":"\n","@confidence":"0.999840351351351","#text":"\nWe have presented an empirical study that tries to\ndetermine the potential role of several sources of\ninformation to solve the Web People Search clus-\ntering problem, with a particular focus on studying\nthe role of named entities in the task.\nTo abstract the study from the particular choice\nof a clustering algorithm and a parameter set-\nting, we have reformulated the problem as a co-\nreference classification task: deciding whether\ntwo pages refer to the same person or not. We\nhave also proposed the Maximal Pairwise Accu-\nracy estimation that establish an upper bound for\nthe results obtained by any Machine Learning al-\ngorithm using a particular set of features.\nOur results indicate that (i) NEs do not provide a\nsubstantial competitive advantage in the clustering\nprocess when compared to a rich combination of\nsimpler features that do not require linguistic pro-\ncessing (local, global and snippet tokens, n-grams,\netc.); (ii) results are sensitive to the NER system\nused: when using all NE features for training, the\nricher number of features provided by OAK seems\nto have an advantage over the simpler types in\nStanford NER and the baseline NER system.\nThis is not exactly a prescription against the use\nof NEs for Web People Search, because linguistic\nknowledge can be useful for other aspects of the\nproblem, such as visualisation of results and de-\nscription of the persons/clusters obtained: for ex-\nample, from a user point of view a network of the\nconnections of a person with other persons and or-\nganisations (which can only be done with NER)\ncan be part of a person?s profile and may help as\na summary of the cluster contents. But from the\nperspective of the clustering problem per se, a di-\nrect use of NEs and other linguistic features does\nnot seem to pay off.\n"},{"#tail":"\n","@confidence":"0.836303333333333","#text":"\nThis work has been partially supported by the\nRegional Government of Madrid, project MAVIR\nS0505-TIC0267.\n"}],"#text":"\n","sectionHeader":[{"#tail":"\n","@confidence":"0.981518","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998257","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.994281","@genericHeader":"method","#text":"\n2 Previous work\n"},{"#tail":"\n","@confidence":"0.672133","@genericHeader":"method","#text":"\n2.1 Features for Web People Search\n"},{"#tail":"\n","@confidence":"0.794545","@genericHeader":"method","#text":"\n3 Experimental settings\n"},{"#tail":"\n","@confidence":"0.964147","@genericHeader":"evaluation","#text":"\n4 Experiments and results\n"},{"#tail":"\n","@confidence":"0.998544","@genericHeader":"conclusions","#text":"\n5 Conclusions\n"},{"#tail":"\n","@confidence":"0.972899","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.904925","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.8550485","#text":"\nTable 1, where each instance is represented by 69\nfeatures.\n"},{"#tail":"\n","@confidence":"0.999699","#text":"\nTable 1: Distribution of classification instances\n"},{"#tail":"\n","@confidence":"0.998565","#text":"\nTable 2: Evaluation on the WePS-2 clustering task\n"}],"page":[{"#tail":"\n","@confidence":"0.998387","#text":"\n534\n"},{"#tail":"\n","@confidence":"0.995731","#text":"\n535\n"},{"#tail":"\n","@confidence":"0.672945","#text":"\n1\n"},{"#tail":"\n","@confidence":"0.632916","#text":"\n1\n"},{"#tail":"\n","@confidence":"0.86963","#text":"\n2\n"},{"#tail":"\n","@confidence":"0.956155","#text":"\n3\n"},{"#tail":"\n","@confidence":"0.985346","#text":"\n4\n"},{"#tail":"\n","@confidence":"0.972707","#text":"\n2\n"},{"#tail":"\n","@confidence":"0.97006","#text":"\n3\n"},{"#tail":"\n","@confidence":"0.983982","#text":"\n4\n"},{"#tail":"\n","@confidence":"0.995101","#text":"\n536\n"},{"#tail":"\n","@confidence":"0.975508","#text":"\n7\n"},{"#tail":"\n","@confidence":"0.937249","#text":"\n5\n"},{"#tail":"\n","@confidence":"0.704449","#text":"\n6\n"},{"#tail":"\n","@confidence":"0.969651","#text":"\n7\n"},{"#tail":"\n","@confidence":"0.996099","#text":"\n537\n"},{"#tail":"\n","@confidence":"0.998554","#text":"\n538\n"},{"#tail":"\n","@confidence":"0.989866","#text":"\n539\n"},{"#tail":"\n","@confidence":"0.888992","#text":"\n540\n"},{"#tail":"\n","@confidence":"0.971741","#text":"\n8\n"},{"#tail":"\n","@confidence":"0.971499","#text":"\n8\n"},{"#tail":"\n","@confidence":"0.990326","#text":"\n541\n"},{"#tail":"\n","@confidence":"0.997897","#text":"\n542\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.9239644","#text":"\nand three-grams do not give better precision than\nbi-grams.\nFigure 1: Precision/Recall curve of token-based\nfeatures\nFigure 2: Precision/Recall curve of word n-grams\n"},{"#tail":"\n","@confidence":"0.999946333333333","#text":"\nFigure 3: Precision/Recall curve of NEs obtained\nwith the Stanford NER tool\nFigure 4: Precision/Recall curve of NEs obtained\n"},{"#tail":"\n","@confidence":"0.842478","#text":"\nFigure 5: Independence of similarity criteria with\nrespect to the token based feature\n"},{"#tail":"\n","@confidence":"0.968243","#text":"\nFigure 6: Estimated PWA upper bound versus the\nreal PWA of decision trees trained with feature\ncombinations\nFigure 7: Maximal Pairwise Accuracy vs. results\nof a Decision Tree\n"},{"#tail":"\n","@confidence":"0.6174984","#text":"\nFigure 7 shows the PWA upper bound estima-\ntion and the actual PWA performance of a Deci-\nsion Tree ML algorithm for three combinations:\n(i) all features; (ii) non linguistic features, i.e.,\nfeatures which can be extracted without natural\n"}],"table":[{"#tail":"\n","@confidence":"0.95090575","#text":"\ntrue false total\nWePS1 61,290 122,437 183,727\nWePS2 54,641 55,546 110,187\nWePS1+WePS2 115,931 177,983 293,914\n"},{"#tail":"\n","@confidence":"0.940239","#text":"\nPre. Rec.\nML-NON LING .83 .91 .77\nS-1 .82 .87 .79\nML- ALL .81 .89 .76\nS-2 .81 .85 .80\nS-3 .81 .93 .73\nS-4 .72 .82 .66\nWePS-2 systems aver. .61 .74 .63\n"}],"email":{"#tail":"\n","@confidence":"0.915362","#text":"\njulio@lsi.uned.es\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.207344","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.931460666666667","#text":"Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 534?542, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP"},"address":[{"#tail":"\n","@confidence":"0.951716","#text":"Madrid, Spain"},{"#tail":"\n","@confidence":"0.939019","#text":"Madrid, Spain"},{"#tail":"\n","@confidence":"0.95502","#text":"Madrid, Spain"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.998292","#text":"UNED NLP & IR group"},{"#tail":"\n","@confidence":"0.425003"},{"#tail":"\n","@confidence":"0.990263","#text":"UNED NLP & IR group"},{"#tail":"\n","@confidence":"0.996952","#text":"UNED NLP & IR group"}],"author":[{"#tail":"\n","@confidence":"0.996808","#text":"Javier Artiles"},{"#tail":"\n","@confidence":"0.960823","#text":"Enrique Amig"},{"#tail":"\n","@confidence":"0.999455","#text":"Julio Gonzalo"}],"abstract":{"#tail":"\n","@confidence":"0.99871855","#text":"The ambiguity of person names in the Web has become a new area of interest for NLP researchers. This challenging problem has been formulated as the task of clustering Web search results (returned in response to a person name query) according to the individual they mention. In this paper we compare the coverage, reliability and independence of a number of features that are potential information sources for this clustering task, paying special attention to the role of named entities in the texts to be clustered. Although named entities are used in most approaches, our results show that, independently of the Machine Learning or Clustering algorithm used, named entity recognition and classification per se only make a small contribution to solve the problem."},"title":{"#tail":"\n","@confidence":"0.95715","#text":"The role of named entities in Web People Search"},"email":[{"#tail":"\n","@confidence":"0.954564","#text":"javart@bec.uned.es"},{"#tail":"\n","@confidence":"0.807062","#text":"o"},{"#tail":"\n","@confidence":"0.963394","#text":"enrique@lsi.uned.es"},{"#tail":"\n","@confidence":"0.990886","#text":"julio@lsi.uned.es"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"editor":{"#tail":"\n","#text":"Eneko Agirre and Philip Edmonds, editors."},"rawString":{"#tail":"\n","#text":"Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense Disambiguation: Algorithms and Applications. Springer."},"#text":"\n","marker":{"#tail":"\n","#text":"2006"},"publisher":{"#tail":"\n","#text":"Springer."},"title":{"#tail":"\n","#text":"Word Sense Disambiguation: Algorithms and Applications."},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Reema Al-Kamha and David W. Embley. 2004. Grouping search-engine returned citations for person-name queries. In WIDM ?04: Proceedings of the 6th annual ACM international workshop on Web information and data management. ACM Press."},"#text":"\n","marker":{"#tail":"\n","#text":"Al-Kamha, Embley, 2004"},"publisher":{"#tail":"\n","#text":"ACM Press."},"title":{"#tail":"\n","#text":"Grouping search-engine returned citations for person-name queries."},"booktitle":{"#tail":"\n","#text":"In WIDM ?04: Proceedings of the 6th annual ACM international workshop on Web information and data management."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Reema Al-Kamha"},{"#tail":"\n","#text":"David W Embley"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2008. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval."},"#text":"\n","marker":{"#tail":"\n","#text":"Amigo, Gonzalo, Artiles, Verdejo, 2008"},"title":{"#tail":"\n","#text":"A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Enrique Amigo"},{"#tail":"\n","#text":"Julio Gonzalo"},{"#tail":"\n","#text":"Javier Artiles"},{"#tail":"\n","#text":"Felisa Verdejo"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2005. A testbed for people searching strategies in the www. In SIGIR."},"#text":"\n","marker":{"#tail":"\n","#text":"Artiles, Gonzalo, Verdejo, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"taneously, in a relevant application domain for web search services: Zoominfo.com, Spock.com, 123people.com are examples of sites which perform web people search, although with limited disambiguation capabilities. A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al, 2004). According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (Artiles et al, 2005). As the amount of information in the WWW grows, more of these people are mentioned in different web pages. Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned. This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in. The user might refine the original query with additional terms, but this risks excluding relevant documents in the process. In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominat","@endWordPosition":"324","@position":"2006","annotationId":"T1","@startWordPosition":"321","@citStr":"Artiles et al, 2005"},{"#tail":"\n","#text":"ation campaign where most approaches to the problem have been tested and compared. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998). Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task - Web People Search - on its own (Artiles et al, 2005; Artiles et al, 2007). Therefore, it is useful to point out some crucial differences between WSD, CRC and WePS: ? WSD typically concentrates in the disambiguation of common words (nouns, verbs, adjectives) for which a relatively small number of senses exist, compared to the hundreds or thousands of people that can share the same name. ? WSD can rely on dictionaries to define the number of possible senses for a word. In the case of name ambiguity no such dictionary is available, even though in theory there is an exact number of people that can be accounted as sharing the same name. ? The objec","@endWordPosition":"944","@position":"5528","annotationId":"T2","@startWordPosition":"941","@citStr":"Artiles et al, 2005"}]},"title":{"#tail":"\n","#text":"A testbed for people searching strategies in the www."},"booktitle":{"#tail":"\n","#text":"In SIGIR."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Javier Artiles"},{"#tail":"\n","#text":"Julio Gonzalo"},{"#tail":"\n","#text":"Felisa Verdejo"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007. The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task."},"#text":"\n","marker":{"#tail":"\n","#text":"Artiles, Gonzalo, Sekine, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"s where different people are mentioned. This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in. The user might refine the original query with additional terms, but this risks excluding relevant documents in the process. In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name. The Web People Search task, as defined in the first WePS evaluation campaign (Artiles et al, 2007), consists of grouping search results for a given name according to the different people that share it. Our goal in this paper is to study which document features can contribute to this task, and in particular to find out which is the role that can be played by named entities (NEs): (i) How reliable is NEs overlap between documents as a source of evidence to cluster pages? (ii) How much recall does it provide? (iii) How unique is this signal? (i.e. is it redundant with other sources of information such as n-gram overlap?); and (iv) How sensitive is this signal to the peculiarities of a given N","@endWordPosition":"466","@position":"2819","annotationId":"T3","@startWordPosition":"463","@citStr":"Artiles et al, 2007"},{"#tail":"\n","#text":"most approaches to the problem have been tested and compared. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998). Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task - Web People Search - on its own (Artiles et al, 2005; Artiles et al, 2007). Therefore, it is useful to point out some crucial differences between WSD, CRC and WePS: ? WSD typically concentrates in the disambiguation of common words (nouns, verbs, adjectives) for which a relatively small number of senses exist, compared to the hundreds or thousands of people that can share the same name. ? WSD can rely on dictionaries to define the number of possible senses for a word. In the case of name ambiguity no such dictionary is available, even though in theory there is an exact number of people that can be accounted as sharing the same name. ? The objective of CDC is to reco","@endWordPosition":"948","@position":"5550","annotationId":"T4","@startWordPosition":"945","@citStr":"Artiles et al, 2007"},{"#tail":"\n","#text":"et al, 2009). Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams. But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before. In the next Section we describe this dataset and how it has been adapted for our purposes. 1 By team ID: CU-COMSEM, IRST-BP, PSNUS, SHEF, FICO, UNN, AUG, JHU1, DFKI2, UC3M13 3 Experimental settings 3.1 Data We have used the testbeds from WePS-1 (Artiles et al., 2007) 2 and WePS-2 (Artiles et al, 2009) evaluation campaigns 3 . Each WePS dataset consists of 30 test cases: a random sample of 10 names from the US Census, 10 names from Wikipedia, and 10 names from Programme Committees in the Computer Science domain (ACL and ECDL). Each test case consists of, at most, 100 web pages from the top search results of a web search engine, using a (quoted) person name as query. For each test case, annotators were asked to organise the web pages in groups where all documents refer to the same person. In cases where a web page refers to more than one person using the sa","@endWordPosition":"1826","@position":"10803","annotationId":"T5","@startWordPosition":"1823","@citStr":"Artiles et al., 2007"}]},"title":{"#tail":"\n","#text":"The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Javier Artiles"},{"#tail":"\n","#text":"Julio Gonzalo"},{"#tail":"\n","#text":"Satoshi Sekine"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). ACL."},"#text":"\n","marker":{"#tail":"\n"},"publisher":{"#tail":"\n","#text":"ACL."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)."},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2009. Weps 2 evaluation campaign: overview of the web people search clustering task. In WePS 2 Evaluation Workshop. WWW Conference 2009."},"#text":"\n","marker":{"#tail":"\n","#text":"Artiles, Gonzalo, Sekine, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" and NEs, and in some cases biographical features extracted from the text. 2.2 Named entities in the WePS campaign Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation. This makes NEs the second most common type of feature; only the BoW feature was more popular. Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del ValleAgudo et al, 2007), etc. In 2009, the second WePS campaign showed similar trends regarding the use of NE features (Artiles et al, 2009). Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams. But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before. In the next Section we describe this dataset and how it has been adapted for our purposes. 1 By team ID: CU-COMSEM, IRST-BP, PSNUS, SHEF, FICO, UNN, AUG, JHU1, DFKI2, UC3M13 3 Experimental settings 3.1 Data We have used the testbeds from WePS-1 (Artiles et a","@endWordPosition":"1721","@position":"10194","annotationId":"T6","@startWordPosition":"1718","@citStr":"Artiles et al, 2009"}},"title":{"#tail":"\n","#text":"Weps 2 evaluation campaign: overview of the web people search clustering task."},"booktitle":{"#tail":"\n","#text":"In WePS 2 Evaluation Workshop. WWW Conference"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Javier Artiles"},{"#tail":"\n","#text":"Julio Gonzalo"},{"#tail":"\n","#text":"Satoshi Sekine"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Amit Bagga and Breck Baldwin. 1998. Entitybased cross-document coreferencing using the vector space model. In Proceedings of the 17th international conference on Computational linguistics. ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Bagga, Baldwin, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ion 3 and our empirical study in Section 4. The paper ends with some conclusions in Section 5. 2 Previous work In this section we will discuss (i) the state of the art in Web People Search in general, focusing on which features are used to solve the problem; and (ii) lessons learnt from the WePS evaluation campaign where most approaches to the problem have been tested and compared. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998). Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task - Web People Search - on its own (Artiles et al, 2005; Artiles et al, 2007). Therefore, it is useful to point out some crucial differences between WSD, CRC and WePS: ? WSD typically concentrates in the disambiguation of common words (nouns, verbs, adjectives) for which a relatively small number of senses exist, compared to the hundreds or thousands of pe","@endWordPosition":"886","@position":"5231","annotationId":"T7","@startWordPosition":"883","@citStr":"Bagga and Baldwin, 1998"},{"#tail":"\n","#text":"annotated testbed. In the WePS task, systems were given the top web search results produced by a person name query. The expected output was a clustering of these results, where each cluster should contain all and only those documents referring to the same individual. 2.1 Features for Web People Search Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are lin","@endWordPosition":"1223","@position":"7133","annotationId":"T8","@startWordPosition":"1220","@citStr":"Bagga and Baldwin, 1998"}]},"title":{"#tail":"\n","#text":"Entitybased cross-document coreferencing using the vector space model."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 17th international conference on Computational linguistics. ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Amit Bagga"},{"#tail":"\n","#text":"Breck Baldwin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Matthias Blume. 2005. Automatic entity disambiguation: Benefits to ner, relation extraction, link analysis, and inference. In International Conference on Intelligence Analysis."},"#text":"\n","marker":{"#tail":"\n","#text":"Blume, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rmation (person name, title, organisation, email address and phone number) improves the clustering results when combined with lexical features (words from the doc535 ument) and NE (person, location, organisation). The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al, 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion (2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW). In summary, the most common document representations ","@endWordPosition":"1509","@position":"8947","annotationId":"T9","@startWordPosition":"1508","@citStr":"Blume, 2005"}},"title":{"#tail":"\n","#text":"Automatic entity disambiguation: Benefits to ner, relation extraction, link analysis, and inference."},"booktitle":{"#tail":"\n","#text":"In International Conference on Intelligence Analysis."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Matthias Blume"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Ying Chen and James H. Martin. 2007. Cu-comsem: Exploring rich features for unsupervised web personal name disambiguation. In Proceedings of the Fourth International Workshop on Semantic Evaluations. ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Chen, Martin, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" referring to the same individual. 2.1 Features for Web People Search Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping sear","@endWordPosition":"1261","@position":"7364","annotationId":"T10","@startWordPosition":"1258","@citStr":"Chen and Martin, 2007"},{"#tail":"\n","#text":"on name, title, organisation, email address and phone number) improves the clustering results when combined with lexical features (words from the doc535 ument) and NE (person, location, organisation). The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al, 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion (2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW). In summary, the most common document representations for the problem include","@endWordPosition":"1513","@position":"8970","annotationId":"T11","@startWordPosition":"1510","@citStr":"Chen and Martin, 2007"}]},"title":{"#tail":"\n","#text":"Cu-comsem: Exploring rich features for unsupervised web personal name disambiguation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourth International Workshop on Semantic Evaluations. ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ying Chen"},{"#tail":"\n","#text":"James H Martin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Silviu Cucerzan. 2007. Large scale named entity disambiguation based on wikipedia data. In The EMNLP-CoNLL-2007."},"#text":"\n","marker":{"#tail":"\n","#text":"Cucerzan, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ntation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results. Biographical features are strongly related to NEs and have also been proposed for this task due to its high precision. Mann (2003) extracted these features using lexical patterns to group pages abo","@endWordPosition":"1290","@position":"7573","annotationId":"T12","@startWordPosition":"1289","@citStr":"Cucerzan, 2007"}},"title":{"#tail":"\n","#text":"Large scale named entity disambiguation based on wikipedia data."},"booktitle":{"#tail":"\n","#text":"In The EMNLP-CoNLL-2007."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Silviu Cucerzan"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"David del Valle-Agudo, C?esar de Pablo-S?anchez, and Mar??a Teresa Vicente-D??ez. 2007. Uc3m-13: Disambiguation of person names based on the composition of simple bags of typed terms. In Proceedings of the Fourth International Workshop on Semantic Evaluations. ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Valle-Agudo, de Pablo-Sanchez, Vicente-Dez, 2007"},"title":{"#tail":"\n","#text":"Uc3m-13: Disambiguation of person names based on the composition of simple bags of typed terms."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourth International Workshop on Semantic Evaluations. ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David del Valle-Agudo"},{"#tail":"\n","#text":"Cesar de Pablo-Sanchez"},{"#tail":"\n","#text":"Mara Teresa Vicente-Dez"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Chung Heong Gooi and James Allan. 2004. Crossdocument coreference on a large scale corpus. In HLT-NAACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Gooi, Allan, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" WePS task, systems were given the top web search results produced by a person name query. The expected output was a clustering of these results, where each cluster should contain all and only those documents referring to the same individual. 2.1 Features for Web People Search Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mention","@endWordPosition":"1227","@position":"7156","annotationId":"T13","@startWordPosition":"1224","@citStr":"Gooi and Allan, 2004"}},"title":{"#tail":"\n","#text":"Crossdocument coreference on a large scale corpus."},"booktitle":{"#tail":"\n","#text":"In HLT-NAACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chung Heong Gooi"},{"#tail":"\n","#text":"James Allan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Dmitri V. Kalashnikov, Stella Chen, Rabia Nuray, Sharad Mehrotra, and Naveen Ashish. 2007. Disambiguation algorithm for people search on the web. In Proc. of IEEE International Conference on Data Engineering (IEEE ICDE)."},"#text":"\n","marker":{"#tail":"\n","#text":"Kalashnikov, Chen, Nuray, Mehrotra, Ashish, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"e most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results. Biographical features are strongly related to NEs and have also been proposed for this task due to its high precision. Mann (2003) extracted these features using l","@endWordPosition":"1286","@position":"7539","annotationId":"T14","@startWordPosition":"1282","@citStr":"Kalashnikov et al, 2007"},{"#tail":"\n","#text":"one number) improves the clustering results when combined with lexical features (words from the doc535 ument) and NE (person, location, organisation). The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al, 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion (2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW). In summary, the most common document representations for the problem include BoW and NEs, and in some cases biographical features","@endWordPosition":"1521","@position":"9023","annotationId":"T15","@startWordPosition":"1518","@citStr":"Kalashnikov et al, 2007"}]},"title":{"#tail":"\n","#text":"Disambiguation algorithm for people search on the web. In"},"booktitle":{"#tail":"\n","#text":"Proc. of IEEE International Conference on Data Engineering (IEEE ICDE)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dmitri V Kalashnikov"},{"#tail":"\n","#text":"Stella Chen"},{"#tail":"\n","#text":"Rabia Nuray"},{"#tail":"\n","#text":"Sharad Mehrotra"},{"#tail":"\n","#text":"Naveen Ashish"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Bradley Malin. 2005. Unsupervised name disambiguation via social network similarity. In Workshop on Link Analysis, Counterterrorism, and Security."},"#text":"\n","marker":{"#tail":"\n","#text":"Malin, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results. Biographical features are strongly related to NEs and have also been proposed","@endWordPosition":"1273","@position":"7453","annotationId":"T16","@startWordPosition":"1272","@citStr":"Malin, 2005"}},"title":{"#tail":"\n","#text":"Unsupervised name disambiguation via social network similarity."},"booktitle":{"#tail":"\n","#text":"In Workshop on Link Analysis, Counterterrorism, and Security."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Bradley Malin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Gideon S. Mann and David Yarowsky. 2003. Unsupervised personal name disambiguation. In Proceedings of the seventh conference on Natural Language Learning (CoNLL) at HLT-NAACL 2003. ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Mann, Yarowsky, 2003"},"publisher":{"#tail":"\n","#text":"ACL."},"title":{"#tail":"\n","#text":"Unsupervised personal name disambiguation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the seventh conference on Natural Language Learning (CoNLL) at HLT-NAACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Gideon S Mann"},{"#tail":"\n","#text":"David Yarowsky"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"2006"},"institution":{"#tail":"\n","#text":"Johns Hopkins University."},"rawString":{"#tail":"\n","#text":"Gideon S. Mann. 2006. Multi-Document Statistical Fact Extraction and Fusion. Ph.D. thesis, Johns Hopkins University."},"#text":"\n","marker":{"#tail":"\n","#text":"Mann, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ver possible, we have generated local versions of these features that only consider the sentences of the text that mention the ambiguous person name 4 . Token-based features considered include document full text tokens, lemmas (using the OAK analyser, see below), title, snippet (returned in the list of search results) and URL (tokenised using non alphanumeric characters as boundaries) tokens. English stopwords were removed, including Web specific stopwords, as file and domain extensions, etc. We generated word n-grams of length 2 to 5, 2 The WePS-1 corpus includes data from the Web03 testbed (Mann, 2006) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable. 3 Both corpora are available from the WePS website http://nlp.uned.es/weps 4 A very sparse feature might never occur in a sentence with the person name. In that cases there is no local version of the feature. 536 using the sentences found in the document text. Punctuation tokens (commas, dots, etc) were generalised as the same token. N-grams were discarded when they were composed only of stopwords or when they did not contain at least one token formed by alphanumeric characters (e","@endWordPosition":"2127","@position":"12540","annotationId":"T17","@startWordPosition":"2126","@citStr":"Mann, 2006"}},"title":{"#tail":"\n","#text":"Multi-Document Statistical Fact Extraction and Fusion."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Gideon S Mann"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Hien T. Nguyen and Tru H. Cao, 2008. Named Entity Disambiguation: A Hybrid Statistical and RuleBased Incremental Approach. Springer."},"#text":"\n","marker":{"#tail":"\n","#text":"Nguyen, Cao, 2008"},"publisher":{"#tail":"\n","#text":"Springer."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ocument text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results. Biographical features are strongly related to NEs and have also been proposed for this task due to its high precision. Mann (2003) extracted these features using lexical patterns to group pages about the same person. Al-","@endWordPosition":"1294","@position":"7596","annotationId":"T18","@startWordPosition":"1291","@citStr":"Nguyen and Cao, 2008"}},"title":{"#tail":"\n","#text":"Named Entity Disambiguation: A Hybrid Statistical and RuleBased Incremental Approach."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hien T Nguyen"},{"#tail":"\n","#text":"Tru H Cao"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Octavian Popescu and Bernardo Magnini. 2007. Irstbp: Web people search using name entities. In Proceedings of the Fourth International Workshop on Semantic Evaluations. ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Popescu, Magnini, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"individual. 2.1 Features for Web People Search Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results. Biographical fea","@endWordPosition":"1265","@position":"7392","annotationId":"T19","@startWordPosition":"1262","@citStr":"Popescu and Magnini, 2007"},{"#tail":"\n","#text":"ation, email address and phone number) improves the clustering results when combined with lexical features (words from the doc535 ument) and NE (person, location, organisation). The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al, 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion (2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW). In summary, the most common document representations for the problem include BoW and NEs, and in some c","@endWordPosition":"1517","@position":"8997","annotationId":"T20","@startWordPosition":"1514","@citStr":"Popescu and Magnini, 2007"}]},"title":{"#tail":"\n","#text":"Irstbp: Web people search using name entities."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourth International Workshop on Semantic Evaluations. ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Octavian Popescu"},{"#tail":"\n","#text":"Bernardo Magnini"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Y. Ravin and Z. Kazi. 1999. Is hillary rodham clinton the president? disambiguating names across documents. In Proceedings of the ACL ?99 Workshop on Coreference and its Applications Association for Computational Linguistics."},"#text":"\n","marker":{"#tail":"\n","#text":"Ravin, Kazi, 1999"},"title":{"#tail":"\n","#text":"Is hillary rodham clinton the president? disambiguating names across documents."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL ?99 Workshop on Coreference and its Applications Association for Computational Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Y Ravin"},{"#tail":"\n","#text":"Z Kazi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Horacio Saggion. 2008. Experiments on semanticbased clustering for cross-document coreference. In International Joint Conference on Natural language Processing."},"#text":"\n","marker":{"#tail":"\n","#text":"Saggion, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ed feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al, 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion (2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW). In summary, the most common document representations for the problem include BoW and NEs, and in some cases biographical features extracted from the text. 2.2 Named entities in the WePS campaign Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 us","@endWordPosition":"1548","@position":"9175","annotationId":"T21","@startWordPosition":"1546","@citStr":"Saggion (2008)"}},"title":{"#tail":"\n","#text":"Experiments on semanticbased clustering for cross-document coreference."},"booktitle":{"#tail":"\n","#text":"In International Joint Conference on Natural language Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Horacio Saggion"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Satoshi Sekine. 2008. Extended named entity ontology with attribute information. In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08)."},"#text":"\n","marker":{"#tail":"\n","#text":"Sekine, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tanford NE Recogniser 6 is a high-performance Named Entity Recognition (NER) system based on Machine Learning. It provides a general implementation of linear chain Conditional Random Field sequence models and includes a model trained on data from CoNLL, MUC6, MUC7, and ACE newswire. Three types of entities were extracted: person, location and organisation. OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc). It provides a fine grained NE recognition covering 100 different NE types (Sekine, 2008). Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex. We have also used the results of a baseline NE recognition for comparison purposes. This method detects sequences of two or more uppercased tokens in the text, and discards those that are found lowercased in the same document or that are composed solely of stopwords. Other features are: emails, outgoing links found in the web pages and two boolean flags that indicate whether a pair of documents is linked or bel","@endWordPosition":"2366","@position":"13992","annotationId":"T22","@startWordPosition":"2365","@citStr":"Sekine, 2008"}},"title":{"#tail":"\n","#text":"Extended named entity ontology with attribute information."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Satoshi Sekine"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Amanda Spink, Bernard Jansen, and Jan Pedersen. 2004. Searching for people on web search engines. Journal of Documentation, 60:266 ? 278."},"journal":{"#tail":"\n","#text":"Journal of Documentation,"},"#text":"\n","pages":{"#tail":"\n","#text":"60--266"},"marker":{"#tail":"\n","#text":"Spink, Jansen, Pedersen, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" ambiguous task, because a single name tends to be shared by many people. This ambiguity has recently become an active research topic and, simultaneously, in a relevant application domain for web search services: Zoominfo.com, Spock.com, 123people.com are examples of sites which perform web people search, although with limited disambiguation capabilities. A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al, 2004). According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (Artiles et al, 2005). As the amount of information in the WWW grows, more of these people are mentioned in different web pages. Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned. This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in. The user might refine the original query with additional terms, but this risks excluding relevant docu","@endWordPosition":"299","@position":"1860","annotationId":"T23","@startWordPosition":"296","@citStr":"Spink et al, 2004"}},"title":{"#tail":"\n","#text":"Searching for people on web search engines."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Amanda Spink"},{"#tail":"\n","#text":"Bernard Jansen"},{"#tail":"\n","#text":"Jan Pedersen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Kazunari Sugiyama and Manabu Okumura. 2007. Titpi: Web people search task using semi-supervised clustering approach. In Proceedings of the Fourth International Workshop on Semantic Evaluations. ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Sugiyama, Okumura, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ustering of these results, where each cluster should contain all and only those documents referring to the same individual. 2.1 Features for Web People Search Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998; Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with others - see for instance (Chen and Martin, 2007; Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al, 2007). Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in thi","@endWordPosition":"1247","@position":"7281","annotationId":"T24","@startWordPosition":"1244","@citStr":"Sugiyama and Okumura, 2007"}},"title":{"#tail":"\n","#text":"Titpi: Web people search task using semi-supervised clustering approach."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourth International Workshop on Semantic Evaluations. ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kazunari Sugiyama"},{"#tail":"\n","#text":"Manabu Okumura"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding. 2005. Person resolution in person search results: Webhawk. In CIKM ?05: Proceedings of the 14th ACM international conference on Information and knowledge management. ACM Press."},"#text":"\n","marker":{"#tail":"\n","#text":"Wan, Gao, Li, Ding, 2005"},"publisher":{"#tail":"\n","#text":"ACM Press."},"title":{"#tail":"\n","#text":"Person resolution in person search results: Webhawk."},"booktitle":{"#tail":"\n","#text":"In CIKM ?05: Proceedings of the 14th ACM international conference on Information and knowledge management."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Xiaojun Wan"},{"#tail":"\n","#text":"Jianfeng Gao"},{"#tail":"\n","#text":"Mu Li"},{"#tail":"\n","#text":"Binggong Ding"}]}}]}}]}}
