form of the same sentence) while (Mel?c?uk, 1988) presents sixty paraphrastic rules designed to account for paraphrastic relations between sentences. More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al, 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts. Such machine learning approaches have k
ed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al, 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts. Such machine learning approaches have known pros and cons. On the one hand, they produce large scale resources at little man labour cost. On the other hand, the degree of descriptive abstraction offered by the list of inference or paraphrase rules they output is low. We chose to investigate
hrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al, 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts. Such machine learning approaches have known pros and cons. On the one hand, they produce large scale resources at little man labour cost. On the other hand, the degree of descriptive abstraction offered by the list of inference or paraphrase rules they output is low. We chose to investigate an alternative research dir
n corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al, 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts. Such machine learning approaches have known pros and cons. On the one hand, they produce large scale resources at little man labour cost. On the other hand, the degree of descriptive abstraction offered by the list of inference or paraphrase rules they output is low. We chose to investigate an alternative research direction by aiming to develop a ?paraphrastic grammar? that is, a grammar which captures the paraphrastic relations between linguistic structur
tle man labour cost. On the other hand, the degree of descriptive abstraction offered by the list of inference or paraphrase rules they output is low. We chose to investigate an alternative research direction by aiming to develop a ?paraphrastic grammar? that is, a grammar which captures the paraphrastic relations between linguistic structures1 . Based on a computational grammar that associates natural language expressions with both a syntactic and a semantic representation, a paraphrastic gram1As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar (Candito, 1999) thus ensuring an additional level of abstraction. The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units. This specification is then compiled to automatically produce a specific grammar. mar is a grammar that moreover associates paraphrases with the same semantic representation. That is, contrary to machine learning based approaches which relate paraphrases via sentence patterns, the paraphrastic grammar approach relates paraphrases via a common semantic representation.
me?trable This tool is parameterisable. Cet outil peut e?tre parame?tre? This tool can be parameterised. 3 Developing a paraphrase testsuite Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar. Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?) While corpus driven efforts along the PARSEVAL lines (Black et al, 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old HewlettPackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al, 1987); and more recently, the much m
n efforts along the PARSEVAL lines (Black et al, 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old HewlettPackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al, 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998). Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a se
ethod consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old HewlettPackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al, 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998). Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology. In a first phase, we concentrate on simple, non-recursive predicate/argument structure. Given such a structure, the
 family, we attempt to find examples with distinct aspectual categories (state, accomplishment and process). Finally, given a WN family and an aspectual category, items will vary with respect to the arity of the main predicate and the types of their arguments e.g., predicates of arity one (run, cost, sleep), of arity two with non propositional arguments (eat, hit, dug), of arity two with a propositional argument (say, promise etc.), etc. 4 A paraphrastic grammar ?Semantic grammars? already exist which describe not only the syntax but also the semantics of natural language. Thus for instance, (Copestake and Flickinger, 2000; Copestake et al, 2001) describes a Head Driven Phrase Structure Grammar (HPSG) which supports the parallel construction of a phrase structure (or derived) tree and of a semantic representation and (Dalrymple, 1999) show how to equip Lexical Functional grammar (LFG) with a glue semantics. These grammars are both efficient and large scale in that they cover an important fragment of the natural language they describe and can be processed by parsers and generators in almost real time. For instance, the LFG grammar parses sentences from the Wall Street Journal and the ERG HPSG grammar will produc
ples with distinct aspectual categories (state, accomplishment and process). Finally, given a WN family and an aspectual category, items will vary with respect to the arity of the main predicate and the types of their arguments e.g., predicates of arity one (run, cost, sleep), of arity two with non propositional arguments (eat, hit, dug), of arity two with a propositional argument (say, promise etc.), etc. 4 A paraphrastic grammar ?Semantic grammars? already exist which describe not only the syntax but also the semantics of natural language. Thus for instance, (Copestake and Flickinger, 2000; Copestake et al, 2001) describes a Head Driven Phrase Structure Grammar (HPSG) which supports the parallel construction of a phrase structure (or derived) tree and of a semantic representation and (Dalrymple, 1999) show how to equip Lexical Functional grammar (LFG) with a glue semantics. These grammars are both efficient and large scale in that they cover an important fragment of the natural language they describe and can be processed by parsers and generators in almost real time. For instance, the LFG grammar parses sentences from the Wall Street Journal and the ERG HPSG grammar will produce semantic representatio
 the types of their arguments e.g., predicates of arity one (run, cost, sleep), of arity two with non propositional arguments (eat, hit, dug), of arity two with a propositional argument (say, promise etc.), etc. 4 A paraphrastic grammar ?Semantic grammars? already exist which describe not only the syntax but also the semantics of natural language. Thus for instance, (Copestake and Flickinger, 2000; Copestake et al, 2001) describes a Head Driven Phrase Structure Grammar (HPSG) which supports the parallel construction of a phrase structure (or derived) tree and of a semantic representation and (Dalrymple, 1999) show how to equip Lexical Functional grammar (LFG) with a glue semantics. These grammars are both efficient and large scale in that they cover an important fragment of the natural language they describe and can be processed by parsers and generators in almost real time. For instance, the LFG grammar parses sentences from the Wall Street Journal and the ERG HPSG grammar will produce semantic representations for about 83 per cent of the utterances in a corpus of some 10 000 utterances varying in length between one and thirty words. Parsing times vary between a few ms for short sentences and sev
operation used in phrase structure grammars while adjunction is an operation which inserts an auxiliary tree into a derived tree. To account for the effect of these insertions, two feature structures (called top and bottom) are associated with each tree node in FTAG. The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The language chosen for semantic representation is a flat semantics along the line of (Bos, 1995; Copestake et al, 1999; Copestake et al, 2001). However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n(x1, . . . , xn) where P n is a predicate of arity n and xi is either a constant or a unification variabl
sed in phrase structure grammars while adjunction is an operation which inserts an auxiliary tree into a derived tree. To account for the effect of these insertions, two feature structures (called top and bottom) are associated with each tree node in FTAG. The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The language chosen for semantic representation is a flat semantics along the line of (Bos, 1995; Copestake et al, 1999; Copestake et al, 2001). However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n(x1, . . . , xn) where P n is a predicate of arity n and xi is either a constant or a unification variable whose value will be i
 grammars while adjunction is an operation which inserts an auxiliary tree into a derived tree. To account for the effect of these insertions, two feature structures (called top and bottom) are associated with each tree node in FTAG. The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The language chosen for semantic representation is a flat semantics along the line of (Bos, 1995; Copestake et al, 1999; Copestake et al, 2001). However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n(x1, . . . , xn) where P n is a predicate of arity n and xi is either a constant or a unification variable whose value will be instantiated during proce
than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n(x1, . . . , xn) where P n is a predicate of arity n and xi is either a constant or a unification variable whose value will be instantiated during processing. Semantic construction proceeds from the derived tree (Gardent and Kallmeyer, 2003) rather than ? as is more common in TAG ? from the derivation tree. This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation. The association between tree nodes and unification variables encodes the syntax/semantics interface ? it specifies which node in the tree provides the value for which variable in the final semantic representation. As trees combine during derivation, (i) variables are unified ? both in the tree and in the associated semantic
 to the paraphrases given in example 1. To produce an identical semantic representation of these three sentences, we first need to ensure that synonyms be assigned the same concept. That is, we need to fix a concept inventory and to use this inventory in a consistent way in particular, by assigning synonyms the same concept. For non predicative units, we use WordNet synset numbers or when working within a restricted domain with a well defined thesaurus, the descriptors of that thesaurus. To represent the semantics of predicative units, we use FrameNet inventory of frames and frame elements (C.Johnson et al, 2002). FrameNet is an online lexical resource for English based on the principles of Frame Semantics. In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame. Finally each frame is associated with a set of target words, the words that evoke that frame. Thus FrameNet associates synonyms with an identical concept namely, the frame evoked by those synonyms. We make use of this feature and instead of choosing our own semantic predicates and relations, dr
tance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeille? FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem, we are currently working on developing a metagrammar in the sense of (Candito, 1999). This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way. For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur. But additionnally there will be semantic classes such as, ?binary predicate of semantic type X? which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments), BINARY NPRED (the class of b
sses (e.g., ?binary predicate of semantic type X?), we can in this way capture both intra and intercategorial paraphrasing links in a general way. Constructing paraphrastic sets. Depending on the type of paraphrastic means involved, constructing a paraphrastic set (the set of all lexical items related by a paraphrastic link be it parallel, shuffling or definitional) is more or less easy as resources for that specific means may or may not be readily available. Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al, 1993; Lin, 1998). techniques. For intercategorial synonymy involving a derivational morphology link, some resources are available which however are only partial in that they only store morphological families that is, sets of items that are morphologically related. Lexical semantics information still need to be include
ll lexical items related by a paraphrastic link be it parallel, shuffling or definitional) is more or less easy as resources for that specific means may or may not be readily available. Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al, 1993; Lin, 1998). techniques. For intercategorial synonymy involving a derivational morphology link, some resources are available which however are only partial in that they only store morphological families that is, sets of items that are morphologically related. Lexical semantics information still need to be included. Intercategorial synonymy not involving a derivational morphology link has been little studied and resources are lacking. However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources. For shuffling paraphrases, frenc
ted by a paraphrastic link be it parallel, shuffling or definitional) is more or less easy as resources for that specific means may or may not be readily available. Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al, 1993; Lin, 1998). techniques. For intercategorial synonymy involving a derivational morphology link, some resources are available which however are only partial in that they only store morphological families that is, sets of items that are morphologically related. Lexical semantics information still need to be included. Intercategorial synonymy not involving a derivational morphology link has been little studied and resources are lacking. However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources. For shuffling paraphrases, french alternatio
al synonymy involving a derivational morphology link, some resources are available which however are only partial in that they only store morphological families that is, sets of items that are morphologically related. Lexical semantics information still need to be included. Intercategorial synonymy not involving a derivational morphology link has been little studied and resources are lacking. However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources. For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns. S GNG ? V GAdvM ? coute GNX S:Commerce GAdvY D NX ? (S,G):goods cher la (S,M):money Y:High NX croisiere X:Cruise Figure 3: La croisie`re cou?te cher S GNG ? VSup? GN a D? NGMG
 Lexical semantics information still need to be included. Intercategorial synonymy not involving a derivational morphology link has been little studied and resources are lacking. However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources. For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns. S GNG ? V GAdvM ? coute GNX S:Commerce GAdvY D NX ? (S,G):goods cher la (S,M):money Y:High NX croisiere X:Cruise Figure 3: La croisie`re cou?te cher S GNG ? VSup? GN a D? NGMGNX cout D NX ? D S:Commerce la un (S,M):money NX (S,G):goods croisiere N X:Cruise ? NY Adj eleve Y:High Figure 4: La croisie`re a un cou?t e?leve? 5 Conclusion Besides the development and evaluation of a core p
rces are lacking. However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources. For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns. S GNG ? V GAdvM ? coute GNX S:Commerce GAdvY D NX ? (S,G):goods cher la (S,M):money Y:High NX croisiere X:Cruise Figure 3: La croisie`re cou?te cher S GNG ? VSup? GN a D? NGMGNX cout D NX ? D S:Commerce la un (S,M):money NX (S,G):goods croisiere N X:Cruise ? NY Adj eleve Y:High Figure 4: La croisie`re a un cou?t e?leve? 5 Conclusion Besides the development and evaluation of a core paraphrastic testsuite and grammar for French, we plan to investigate two main issues. First, how precisely should a metagrammar be structured to best describe a 
