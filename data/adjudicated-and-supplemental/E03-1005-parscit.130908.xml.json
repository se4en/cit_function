{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Bonnema, R., R. Bod, and R. Scha, 1997. A DOP Model for Semantic Interpretation, Proceedings ACL/EACL-97, Madrid, Spain."},"#text":"\n","marker":{"#tail":"\n","#text":"Bonnema, Bod, Scha, 1997"},"location":{"#tail":"\n","#text":"Madrid,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"l time. However, as mentioned above, efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential. In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse. 3 A New Notion of the Best Parse Tree 3.1 Two Criteria for the Best Parse Tree: Likelihood vs. Simplicity Most DOP models, such as in Bod (1993), Goodman (1996), Bonnema et al. (1997), Sima'an (2000) and Collins & Duffy (2002), use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence. We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2. In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shor","@endWordPosition":"2659","@position":"16123","annotationId":"T1","@startWordPosition":"2656","@citStr":"Bonnema et al. (1997)"}},"title":{"#tail":"\n","#text":"A DOP Model for Semantic Interpretation,"},"booktitle":{"#tail":"\n","#text":"Proceedings ACL/EACL-97,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Bonnema"},{"#tail":"\n","#text":"R Bod"},{"#tail":"\n","#text":"R Scha"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Charniak, E. 2000. A Maximum-Entropy-Inspired Parser. Proceedings ANLP-NAACL'2000, Seattle, Washington."},"#text":"\n","marker":{"#tail":"\n","#text":"Charniak, 2000"},"location":{"#tail":"\n","#text":"Seattle, Washington."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"rcolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998). And Collins (2000) argues for &quot;keeping track of counts of arbitrary fragments within parse trees&quot;, which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992). Thus the major innovations of DOP are: 1. the use of corpus fragments rather than grammar rules, 19 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a). 1.2 DOP1 in Retrospective One instantiation o","@endWordPosition":"457","@position":"2884","annotationId":"T2","@startWordPosition":"456","@citStr":"Charniak 2000"},{"#tail":"\n","#text":"uction in error rate over the model in Collins (1999) on the WSJ. Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part of this paper, we extend our experiments with a new notion of the best parse tree. Most previous notions of best parse tree in 20 DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation. We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone. Compared to Bod (2001), our","@endWordPosition":"1139","@position":"7180","annotationId":"T3","@startWordPosition":"1138","@citStr":"Charniak (2000)"},{"#tail":"\n","#text":"with Good-Turing (Bod 1998: 85 - 87). We used &quot;evalb&quot;4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999). We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems. 4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996, Charniak 1997, Collins 1999 and Charniak 2000). Parser LP LR 40 words Co1196 86.3 85.8 Char97 87.4 87.5 Co1199 88.7 88.5 Char00 90.1 90.1 Bod01 90.3 90.1 Bon99 86.7 86.0 100 words Co1196 85.7 85.3 Char97 86.6 86.7 Co1199 88.3 88.1 Char00 89.5 89.6 Bod01 89.7 89.5 Bon99 86.2 85.6 Table 1. Bod (2001) and Bonnema et al. (1999) compared to other parsers While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average","@endWordPosition":"3492","@position":"21288","annotationId":"T4","@startWordPosition":"3491","@citStr":"Charniak 2000"},{"#tail":"\n","#text":".3 20 90.3 90.1 88.9 88.7 50 89.5 89.2 89.3 89.0 100 88.1 87.5 89.7 89.4 1,000 87.4 87.0 89.7 89.4 Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity - DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1. Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, th","@endWordPosition":"3954","@position":"23899","annotationId":"T5","@startWordPosition":"3953","@citStr":"Charniak (2000)"}]},"title":{"#tail":"\n","#text":"A Maximum-Entropy-Inspired Parser."},"booktitle":{"#tail":"\n","#text":"Proceedings ANLP-NAACL'2000,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Charniak, E. 2000. A Maximum-Entropy-Inspired Parser. Proceedings ANLP-NAACL'2000, Seattle, Washington."},"#text":"\n","marker":{"#tail":"\n","#text":"Charniak, 2000"},"location":{"#tail":"\n","#text":"Seattle, Washington."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"rcolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998). And Collins (2000) argues for &quot;keeping track of counts of arbitrary fragments within parse trees&quot;, which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992). Thus the major innovations of DOP are: 1. the use of corpus fragments rather than grammar rules, 19 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a). 1.2 DOP1 in Retrospective One instantiation o","@endWordPosition":"457","@position":"2884","annotationId":"T6","@startWordPosition":"456","@citStr":"Charniak 2000"},{"#tail":"\n","#text":"uction in error rate over the model in Collins (1999) on the WSJ. Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part of this paper, we extend our experiments with a new notion of the best parse tree. Most previous notions of best parse tree in 20 DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation. We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone. Compared to Bod (2001), our","@endWordPosition":"1139","@position":"7180","annotationId":"T7","@startWordPosition":"1138","@citStr":"Charniak (2000)"},{"#tail":"\n","#text":"with Good-Turing (Bod 1998: 85 - 87). We used &quot;evalb&quot;4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999). We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems. 4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996, Charniak 1997, Collins 1999 and Charniak 2000). Parser LP LR 40 words Co1196 86.3 85.8 Char97 87.4 87.5 Co1199 88.7 88.5 Char00 90.1 90.1 Bod01 90.3 90.1 Bon99 86.7 86.0 100 words Co1196 85.7 85.3 Char97 86.6 86.7 Co1199 88.3 88.1 Char00 89.5 89.6 Bod01 89.7 89.5 Bon99 86.2 85.6 Table 1. Bod (2001) and Bonnema et al. (1999) compared to other parsers While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average","@endWordPosition":"3492","@position":"21288","annotationId":"T8","@startWordPosition":"3491","@citStr":"Charniak 2000"},{"#tail":"\n","#text":".3 20 90.3 90.1 88.9 88.7 50 89.5 89.2 89.3 89.0 100 88.1 87.5 89.7 89.4 1,000 87.4 87.0 89.7 89.4 Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity - DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1. Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, th","@endWordPosition":"3954","@position":"23899","annotationId":"T9","@startWordPosition":"3953","@citStr":"Charniak (2000)"}]},"title":{"#tail":"\n","#text":"A Maximum-Entropy-Inspired Parser."},"booktitle":{"#tail":"\n","#text":"Proceedings ANLP-NAACL'2000,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Charniak, E. 2000. A Maximum-Entropy-Inspired Parser. Proceedings ANLP-NAACL'2000, Seattle, Washington."},"#text":"\n","marker":{"#tail":"\n","#text":"Charniak, 2000"},"location":{"#tail":"\n","#text":"Seattle, Washington."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"rcolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998). And Collins (2000) argues for &quot;keeping track of counts of arbitrary fragments within parse trees&quot;, which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992). Thus the major innovations of DOP are: 1. the use of corpus fragments rather than grammar rules, 19 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a). 1.2 DOP1 in Retrospective One instantiation o","@endWordPosition":"457","@position":"2884","annotationId":"T10","@startWordPosition":"456","@citStr":"Charniak 2000"},{"#tail":"\n","#text":"uction in error rate over the model in Collins (1999) on the WSJ. Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part of this paper, we extend our experiments with a new notion of the best parse tree. Most previous notions of best parse tree in 20 DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation. We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone. Compared to Bod (2001), our","@endWordPosition":"1139","@position":"7180","annotationId":"T11","@startWordPosition":"1138","@citStr":"Charniak (2000)"},{"#tail":"\n","#text":"with Good-Turing (Bod 1998: 85 - 87). We used &quot;evalb&quot;4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999). We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems. 4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996, Charniak 1997, Collins 1999 and Charniak 2000). Parser LP LR 40 words Co1196 86.3 85.8 Char97 87.4 87.5 Co1199 88.7 88.5 Char00 90.1 90.1 Bod01 90.3 90.1 Bon99 86.7 86.0 100 words Co1196 85.7 85.3 Char97 86.6 86.7 Co1199 88.3 88.1 Char00 89.5 89.6 Bod01 89.7 89.5 Bon99 86.2 85.6 Table 1. Bod (2001) and Bonnema et al. (1999) compared to other parsers While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average","@endWordPosition":"3492","@position":"21288","annotationId":"T12","@startWordPosition":"3491","@citStr":"Charniak 2000"},{"#tail":"\n","#text":".3 20 90.3 90.1 88.9 88.7 50 89.5 89.2 89.3 89.0 100 88.1 87.5 89.7 89.4 1,000 87.4 87.0 89.7 89.4 Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity - DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1. Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, th","@endWordPosition":"3954","@position":"23899","annotationId":"T13","@startWordPosition":"3953","@citStr":"Charniak (2000)"}]},"title":{"#tail":"\n","#text":"A Maximum-Entropy-Inspired Parser."},"booktitle":{"#tail":"\n","#text":"Proceedings ANLP-NAACL'2000,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Collins, M. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies, Proceedings ACL'96, Santa Cruz, Ca."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1996"},"location":{"#tail":"\n","#text":"Santa Cruz, Ca."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"dically different from all other statistical parsing models at the time. Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g. in Fujisaki et al. 1989; Black et al. 1992, 1993; Briscoe and I Thanks to Ivan Sag for this pun. Waegner 1992; Pereira and Schabes 1992). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), Johnson (1998), Chiang (2000), and many others. The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and ","@endWordPosition":"309","@position":"1907","annotationId":"T14","@startWordPosition":"308","@citStr":"Collins (1996"},{"#tail":"\n","#text":" how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part of this paper, we extend our experiments with a new notion of the best parse tree. Most previous notions of best parse tree in 20 DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation. We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone. Compared to Bod (2001), our results show an 11% improvement in terms of relative error reduction and a speedup which reduces ","@endWordPosition":"1154","@position":"7278","annotationId":"T15","@startWordPosition":"1153","@citStr":"Collins (1996)"},{"#tail":"\n","#text":"hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 - 87). We used &quot;evalb&quot;4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999). We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems. 4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996, Charniak 1997, Collins 1999 and Charniak 2000). Parser LP LR 40 words Co1196 86.3 85.8 Char97 87.4 87.5 Co1199 88.7 88.5 Char00 90.1 90.1 Bod01 90.3 90.1 Bon99 86.7 86.0 100 words Co1196 85.7 85.3 Char97 86.6 86.7 Co1199 88.3 88.1 Char00 89.5 89.6 Bod01 89.7 89.5 Bon99 86.2 85.6 Table 1. Bod (2001) and Bonnema et al. (1999) compared to other parsers While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). As to the processing time, the PCFG reduction parses","@endWordPosition":"3485","@position":"21240","annotationId":"T16","@startWordPosition":"3484","@citStr":"Collins 1996"}]},"title":{"#tail":"\n","#text":"A New Statistical Parser Based on Bigram Lexical Dependencies,"},"booktitle":{"#tail":"\n","#text":"Proceedings ACL'96,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Collins, M. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies, Proceedings ACL'96, Santa Cruz, Ca."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1996"},"location":{"#tail":"\n","#text":"Santa Cruz, Ca."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"dically different from all other statistical parsing models at the time. Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g. in Fujisaki et al. 1989; Black et al. 1992, 1993; Briscoe and I Thanks to Ivan Sag for this pun. Waegner 1992; Pereira and Schabes 1992). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), Johnson (1998), Chiang (2000), and many others. The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and ","@endWordPosition":"309","@position":"1907","annotationId":"T17","@startWordPosition":"308","@citStr":"Collins (1996"},{"#tail":"\n","#text":" how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part of this paper, we extend our experiments with a new notion of the best parse tree. Most previous notions of best parse tree in 20 DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation. We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone. Compared to Bod (2001), our results show an 11% improvement in terms of relative error reduction and a speedup which reduces ","@endWordPosition":"1154","@position":"7278","annotationId":"T18","@startWordPosition":"1153","@citStr":"Collins (1996)"},{"#tail":"\n","#text":"hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 - 87). We used &quot;evalb&quot;4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999). We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems. 4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996, Charniak 1997, Collins 1999 and Charniak 2000). Parser LP LR 40 words Co1196 86.3 85.8 Char97 87.4 87.5 Co1199 88.7 88.5 Char00 90.1 90.1 Bod01 90.3 90.1 Bon99 86.7 86.0 100 words Co1196 85.7 85.3 Char97 86.6 86.7 Co1199 88.3 88.1 Char00 89.5 89.6 Bod01 89.7 89.5 Bon99 86.2 85.6 Table 1. Bod (2001) and Bonnema et al. (1999) compared to other parsers While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). As to the processing time, the PCFG reduction parses","@endWordPosition":"3485","@position":"21240","annotationId":"T19","@startWordPosition":"3484","@citStr":"Collins 1996"}]},"title":{"#tail":"\n","#text":"A New Statistical Parser Based on Bigram Lexical Dependencies,"},"booktitle":{"#tail":"\n","#text":"Proceedings ACL'96,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Collins, M. 2000. Discriminative Reranking for Natural Language Parsing, Proceedings ICML2000, Stanford, Ca."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 2000"},"location":{"#tail":"\n","#text":"Stanford, Ca."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998). And Collins (2000) argues for &quot;keeping track of counts of arbitrary fragments within parse trees&quot;, which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992). Thus the major innovations of DOP are: 1. the use of corpus fragments rather than grammar rules, 19 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a). 1.2 DOP1 in Retrospective One instantiation of DOP which has received considerab","@endWordPosition":"462","@position":"2919","annotationId":"T20","@startWordPosition":"461","@citStr":"Collins (2000)"},{"#tail":"\n","#text":" over the model in Collins (1999) on the WSJ. Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part of this paper, we extend our experiments with a new notion of the best parse tree. Most previous notions of best parse tree in 20 DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation. We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone. Compared to Bod (2001), our results show an 11","@endWordPosition":"1142","@position":"7199","annotationId":"T21","@startWordPosition":"1141","@citStr":"Collins (2000)"},{"#tail":"\n","#text":"able 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity - DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1. Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential proces","@endWordPosition":"3969","@position":"23998","annotationId":"T22","@startWordPosition":"3968","@citStr":"Collins (2000)"}]},"title":{"#tail":"\n","#text":"Discriminative Reranking for Natural Language Parsing,"},"booktitle":{"#tail":"\n","#text":"Proceedings ICML2000,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Collins, M. 2000. Discriminative Reranking for Natural Language Parsing, Proceedings ICML2000, Stanford, Ca."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 2000"},"location":{"#tail":"\n","#text":"Stanford, Ca."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998). And Collins (2000) argues for &quot;keeping track of counts of arbitrary fragments within parse trees&quot;, which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992). Thus the major innovations of DOP are: 1. the use of corpus fragments rather than grammar rules, 19 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a). 1.2 DOP1 in Retrospective One instantiation of DOP which has received considerab","@endWordPosition":"462","@position":"2919","annotationId":"T23","@startWordPosition":"461","@citStr":"Collins (2000)"},{"#tail":"\n","#text":" over the model in Collins (1999) on the WSJ. Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part of this paper, we extend our experiments with a new notion of the best parse tree. Most previous notions of best parse tree in 20 DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation. We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone. Compared to Bod (2001), our results show an 11","@endWordPosition":"1142","@position":"7199","annotationId":"T24","@startWordPosition":"1141","@citStr":"Collins (2000)"},{"#tail":"\n","#text":"able 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity - DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1. Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential proces","@endWordPosition":"3969","@position":"23998","annotationId":"T25","@startWordPosition":"3968","@citStr":"Collins (2000)"}]},"title":{"#tail":"\n","#text":"Discriminative Reranking for Natural Language Parsing,"},"booktitle":{"#tail":"\n","#text":"Proceedings ICML2000,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Collins M. and N. Duffy, 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. Proceedings ACL'2002, Philadelphia, PA."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, Duffy, 2002"},"location":{"#tail":"\n","#text":"Philadelphia, PA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ring the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998). And Collins (2000) argues for &quot;keeping track of counts of arbitrary fragments within parse trees&quot;, which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992). Thus the major innovations of DOP are: 1. the use of corpus fragments rather than grammar rules, 19 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a). 1.2 DOP1 in Retrospective One instantiation of DOP which has received considerable interest is the model known as DOP12 (Bod 1992). DOP1 combines subtrees from a treebank by means of node-substitution and computes the prob","@endWordPosition":"485","@position":"3061","annotationId":"T26","@startWordPosition":"482","@citStr":"Collins and Duffy (2002)"},{"#tail":"\n","#text":"more subtrees, and therefore more probability to larger subtrees. As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains. Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003). Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ. Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times","@endWordPosition":"1021","@position":"6427","annotationId":"T27","@startWordPosition":"1018","@citStr":"Collins & Duffy (2002)"},{"#tail":"\n","#text":"ient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential. In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse. 3 A New Notion of the Best Parse Tree 3.1 Two Criteria for the Best Parse Tree: Likelihood vs. Simplicity Most DOP models, such as in Bod (1993), Goodman (1996), Bonnema et al. (1997), Sima'an (2000) and Collins & Duffy (2002), use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence. We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2. In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training su","@endWordPosition":"2666","@position":"16166","annotationId":"T28","@startWordPosition":"2663","@citStr":"Collins & Duffy (2002)"}]},"title":{"#tail":"\n","#text":"New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron."},"booktitle":{"#tail":"\n","#text":"Proceedings ACL'2002,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Collins"},{"#tail":"\n","#text":"N Duffy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Goodman, J. 1996. Efficient Algorithms for Parsing the DOP Model, Proceedings Empirical Methods in Natural Language Processing, Philadelphia, PA."},"#text":"\n","marker":{"#tail":"\n","#text":"Goodman, 1996"},"location":{"#tail":"\n","#text":"Philadelphia, PA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"g the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations. Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002). Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar. While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the &quot;maximum constituents parse&quot;, i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b, 2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by traini","@endWordPosition":"720","@position":"4519","annotationId":"T29","@startWordPosition":"719","@citStr":"Goodman (1996"},{"#tail":"\n","#text":"e probabilities of its distinct derivations. Let tid be the i-th subtree in the derivation d that produces tree T, then the probability of T is given by P(7) = Edrli P(tid) Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees. A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account. Fortunately, there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities, as shown by Goodman (1996, 2002). Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002). Goodman assigns every node in every tree a unique number which is called its address. The notation A@k denotes the node at address k where A is the nonterminal labeling that node. A new nonterminal is created for each node in the training data. This nonterminal is called A k. Nonterminals of this form are called &quot;interior&quot; nonterminals, while the original nonterminals in the parse trees are called &quot;exterior&quot; nontermimals. Let aj represent the number of subtrees headed by the node A@j. Let a represe","@endWordPosition":"1525","@position":"9383","annotationId":"T30","@startWordPosition":"1524","@citStr":"Goodman (1996"},{"#tail":"\n","#text":"or the node in figure 1, the following eight PCFG rules are generated, where the number in parentheses following a rule is its probability. A1 \u2014BC Mad A BC (11a) Aj BkC (bklaj) A \u2014> BkC (bkla) Aj BC1 (cilaj) A BC1 (cila) Aj BkCi (bkcilaj) A BkCi (bkcila) Figure 2. PCFG-reduction of DOP1 Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a. And subderivations headed by A1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1/a1 (Goodman 1996). Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability. This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG. Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree. But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constitue","@endWordPosition":"1914","@position":"11697","annotationId":"T31","@startWordPosition":"1913","@citStr":"Goodman 1996"},{"#tail":"\n","#text":"ees in polynomial time. However, as mentioned above, efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential. In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse. 3 A New Notion of the Best Parse Tree 3.1 Two Criteria for the Best Parse Tree: Likelihood vs. Simplicity Most DOP models, such as in Bod (1993), Goodman (1996), Bonnema et al. (1997), Sima'an (2000) and Collins & Duffy (2002), use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence. We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2. In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tre","@endWordPosition":"2655","@position":"16100","annotationId":"T32","@startWordPosition":"2654","@citStr":"Goodman (1996)"}]},"title":{"#tail":"\n","#text":"Efficient Algorithms for Parsing the DOP Model,"},"booktitle":{"#tail":"\n","#text":"Proceedings Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Goodman"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Goodman, J. 1996. Efficient Algorithms for Parsing the DOP Model, Proceedings Empirical Methods in Natural Language Processing, Philadelphia, PA."},"#text":"\n","marker":{"#tail":"\n","#text":"Goodman, 1996"},"location":{"#tail":"\n","#text":"Philadelphia, PA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"g the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations. Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002). Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar. While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the &quot;maximum constituents parse&quot;, i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b, 2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by traini","@endWordPosition":"720","@position":"4519","annotationId":"T33","@startWordPosition":"719","@citStr":"Goodman (1996"},{"#tail":"\n","#text":"e probabilities of its distinct derivations. Let tid be the i-th subtree in the derivation d that produces tree T, then the probability of T is given by P(7) = Edrli P(tid) Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees. A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account. Fortunately, there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities, as shown by Goodman (1996, 2002). Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002). Goodman assigns every node in every tree a unique number which is called its address. The notation A@k denotes the node at address k where A is the nonterminal labeling that node. A new nonterminal is created for each node in the training data. This nonterminal is called A k. Nonterminals of this form are called &quot;interior&quot; nonterminals, while the original nonterminals in the parse trees are called &quot;exterior&quot; nontermimals. Let aj represent the number of subtrees headed by the node A@j. Let a represe","@endWordPosition":"1525","@position":"9383","annotationId":"T34","@startWordPosition":"1524","@citStr":"Goodman (1996"},{"#tail":"\n","#text":"or the node in figure 1, the following eight PCFG rules are generated, where the number in parentheses following a rule is its probability. A1 \u2014BC Mad A BC (11a) Aj BkC (bklaj) A \u2014> BkC (bkla) Aj BC1 (cilaj) A BC1 (cila) Aj BkCi (bkcilaj) A BkCi (bkcila) Figure 2. PCFG-reduction of DOP1 Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a. And subderivations headed by A1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1/a1 (Goodman 1996). Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability. This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG. Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree. But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constitue","@endWordPosition":"1914","@position":"11697","annotationId":"T35","@startWordPosition":"1913","@citStr":"Goodman 1996"},{"#tail":"\n","#text":"ees in polynomial time. However, as mentioned above, efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential. In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse. 3 A New Notion of the Best Parse Tree 3.1 Two Criteria for the Best Parse Tree: Likelihood vs. Simplicity Most DOP models, such as in Bod (1993), Goodman (1996), Bonnema et al. (1997), Sima'an (2000) and Collins & Duffy (2002), use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence. We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2. In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tre","@endWordPosition":"2655","@position":"16100","annotationId":"T36","@startWordPosition":"2654","@citStr":"Goodman (1996)"}]},"title":{"#tail":"\n","#text":"Efficient Algorithms for Parsing the DOP Model,"},"booktitle":{"#tail":"\n","#text":"Proceedings Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Goodman"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Goodman, J. 1996. Efficient Algorithms for Parsing the DOP Model, Proceedings Empirical Methods in Natural Language Processing, Philadelphia, PA."},"#text":"\n","marker":{"#tail":"\n","#text":"Goodman, 1996"},"location":{"#tail":"\n","#text":"Philadelphia, PA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"g the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations. Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002). Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar. While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the &quot;maximum constituents parse&quot;, i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b, 2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by traini","@endWordPosition":"720","@position":"4519","annotationId":"T37","@startWordPosition":"719","@citStr":"Goodman (1996"},{"#tail":"\n","#text":"e probabilities of its distinct derivations. Let tid be the i-th subtree in the derivation d that produces tree T, then the probability of T is given by P(7) = Edrli P(tid) Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees. A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account. Fortunately, there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities, as shown by Goodman (1996, 2002). Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002). Goodman assigns every node in every tree a unique number which is called its address. The notation A@k denotes the node at address k where A is the nonterminal labeling that node. A new nonterminal is created for each node in the training data. This nonterminal is called A k. Nonterminals of this form are called &quot;interior&quot; nonterminals, while the original nonterminals in the parse trees are called &quot;exterior&quot; nontermimals. Let aj represent the number of subtrees headed by the node A@j. Let a represe","@endWordPosition":"1525","@position":"9383","annotationId":"T38","@startWordPosition":"1524","@citStr":"Goodman (1996"},{"#tail":"\n","#text":"or the node in figure 1, the following eight PCFG rules are generated, where the number in parentheses following a rule is its probability. A1 \u2014BC Mad A BC (11a) Aj BkC (bklaj) A \u2014> BkC (bkla) Aj BC1 (cilaj) A BC1 (cila) Aj BkCi (bkcilaj) A BkCi (bkcila) Figure 2. PCFG-reduction of DOP1 Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a. And subderivations headed by A1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1/a1 (Goodman 1996). Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability. This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG. Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree. But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constitue","@endWordPosition":"1914","@position":"11697","annotationId":"T39","@startWordPosition":"1913","@citStr":"Goodman 1996"},{"#tail":"\n","#text":"ees in polynomial time. However, as mentioned above, efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential. In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse. 3 A New Notion of the Best Parse Tree 3.1 Two Criteria for the Best Parse Tree: Likelihood vs. Simplicity Most DOP models, such as in Bod (1993), Goodman (1996), Bonnema et al. (1997), Sima'an (2000) and Collins & Duffy (2002), use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence. We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2. In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tre","@endWordPosition":"2655","@position":"16100","annotationId":"T40","@startWordPosition":"2654","@citStr":"Goodman (1996)"}]},"title":{"#tail":"\n","#text":"Efficient Algorithms for Parsing the DOP Model,"},"booktitle":{"#tail":"\n","#text":"Proceedings Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Goodman"}}},{"volume":{"#tail":"\n","#text":"19"},"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Marcus, M., B. Santorini and M. Marcinkiewicz, 1993. Building a Large Annotated Corpus of English: the Penn Treebank, Computational Linguistics 19(2)."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Marcus, Santorini, Marcinkiewicz, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"n as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn, and since 0<p<1, the derivation with the fewest subtrees has the greatest probability. For SL-DOP and LS-DOP, we first compute either n likeliest or n simplest trees by means of Viterbi optimization. Next, we either select the simplest tree among the n likeliest ones (for SL - DOP) or the likeliest tree among the n simplest ones (for LS-DOP). In our experiments, n will never be larger than 1,000. 4 Experiments For our experiments we used the standard division of the WSJ (Marcus et al. 1993), with sections 2 through 21 for training (approx. 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks. Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing). We employed the same unknown (category) word model as in Bod (2001), based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 - 87). We used &quot;evalb&quot;","@endWordPosition":"3315","@position":"20127","annotationId":"T41","@startWordPosition":"3312","@citStr":"Marcus et al. 1993"}},"title":{"#tail":"\n","#text":"Building a Large Annotated Corpus of English: the Penn Treebank,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Marcus"},{"#tail":"\n","#text":"B Santorini"},{"#tail":"\n","#text":"M Marcinkiewicz"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Sima'an, K. 2000. Tree-gram Parsing: Lexical Dependencies and Structural Relations, Proceedings ACL'2000, Hong Kong, China."},"#text":"\n","marker":{"#tail":"\n","#text":"Sima'an, 2000"},"location":{"#tail":"\n","#text":"Hong Kong, China."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tioned above, efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential. In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse. 3 A New Notion of the Best Parse Tree 3.1 Two Criteria for the Best Parse Tree: Likelihood vs. Simplicity Most DOP models, such as in Bod (1993), Goodman (1996), Bonnema et al. (1997), Sima'an (2000) and Collins & Duffy (2002), use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence. We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2. In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation ","@endWordPosition":"2661","@position":"16139","annotationId":"T42","@startWordPosition":"2660","@citStr":"Sima'an (2000)"}},"title":{"#tail":"\n","#text":"Tree-gram Parsing: Lexical Dependencies and Structural Relations,"},"booktitle":{"#tail":"\n","#text":"Proceedings ACL'2000,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"K Sima'an"}}}]}}}}
