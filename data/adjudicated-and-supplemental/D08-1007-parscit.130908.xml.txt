r 2008. c�2008 Association for Computational Linguistics occurrence model on two tasks: identifying objects of verbs in an unseen corpus and finding pronominal antecedents in coreference data. 2 Related Work Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996). For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Usually, the classes are from WordNet (Miller et al., 1990), although they can also be inferred from clustering (Rooth et al., 1999). Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models. Another line of research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, PrSI
 from observed text. Dagan et al. (1999) define the similarity-weighted probability, PrSIM, to be: �PrSIM(n|v) � Sim(v′, v)Pr(n|v′) (1) v′∈SIMS(v) where Sim(v′, v) returns a real-valued similarity between two verbs v′ and v (normalized over all pair similarities in the sum). In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, SIMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)’s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as “[X wins Y] ⇒ [X plays Y]” are only valid for certain arguments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminative t
r arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, SIMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)’s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as “[X wins Y] ⇒ [X plays Y]” are only valid for certain arguments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminative techniques are widely used in NLP and have been applied to the related tasks of word prediction and language modeling. Even-Zohar and Roth (2000) use a classifier to predict the most likely word to fill a position in a sentence (in their experiments: a verb) from a set of candidates (sets of verbs), by inspecting the context of the target toke
ce word order) to create a neighborhood of implicit negative evidence. We create negatives by substitution rather than perturbation, and use corpuswide statistics to choose our negative instances. 3 Methodology 3.1 Creating Examples To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text. To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data. We measure this association using pointwise Mutual Information (MI) (Church and Hanks, 1990). The MI between a 60 verb predicate, v, and its object argument, n, is: Pr(v, n) Pr(n|v) MI(v, n) =log Pr(v)Pr(n) =log Pr(n) (2) If MI>0, the probability v and n occur together is greater than if they were independently distributed. We create sets of positive and negative examples separately for each predicate, v. First, we extract all pairs where MI(v, n)>T as positives. For each positive, we create pseudo-negative examples, (v, n′), by pairing v with a new argument, n′, that either has MI below the threshold or did not occur with v in the corpus. We require each negative n′ to have a simila
 they rarely contain digits, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n′) where v, n is a predicateargument pair observed in the corpus and v, n′ has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not 
ts, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n′) where v, n is a predicateargument pair observed in the corpus and v, n′ has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of 
m of attribute-value features. Let every feature Oi be of the form Oi(v, n) = (v = v� n f(n)). That is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f(n). For example, a feature for a verb-object pair might be, “the verb is eat and the object is lower-case.” In this representation, features for one predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)’s conditional probability scores for pseudodisambiguation of (v, n, n′) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007). 61 classifier for each predicate independently. The prediction becomes yv = Av · 4)v(n), where Av are the learned weights corresponding to predicate v and all features 4)v(n)=f(n) depend on the argument only. Some predicate partitions 
e predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)’s conditional probability scores for pseudodisambiguation of (v, n, n′) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007). 61 classifier for each predicate independently. The prediction becomes yv = Av · 4)v(n), where Av are the learned weights corresponding to predicate v and all features 4)v(n)=f(n) depend on the argument only. Some predicate partitions may have insufficient examples for training. Also, a predicate may occur in test data that was unseen during training. To handle these instances, we decided to cluster lowfrequency predicates. In our experiments assigning SP to verb-object pairs, we cluster all verbs that have less than 250 positive examples, using clusters generated by the CBC algorithm (Pante
ude Probj(n|v′) features for every verb that occurs more than 10 times in our corpus. Avv′ may be positive or negative, depending on the relation between v′ and v. We also include features for the probability of the noun occurring as the subject of other verbs, Prs,,bj(n|v′). For example, nouns that can be the object of eat will also occur as the subject of taste and contain. Other contexts, such as adjectival and nominal predicates, could also aid the prediction, but have not yet been investigated. The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir (2005). They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation. Our approach, on the other hand, discriminatively sets millions of individual similarity values. Like Weeds and Weir (2005), our similarity values are asymmetric. 3.3.2 String-based We include several simple character-based features of the noun string: the number of tokens, the case, and whether it contains digits, hyphens, an apostrophe, or other punctuation. We also include a feature for the first and last token, and fire indicator features if any token in the noun occurs on in-hou
, 2002) on a 10 GB corpus, giving 3620 clusters. If a noun belongs in a cluster, a corresponding feature fires. If a noun is in none of the clusters, a no-class feature fires. As an example, CBC cluster 1891 contains: sidewalk, driveway, roadway, footpath, bridge, highway, road, runway, street, alley, path, Interstate, ... In our training data, we have examples like widen highway, widen road and widen motorway. If we 62 see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).2 This data provides counts for pairs such as “Edwin Moses, hurdler” and “William Farley, industrialist.” We have features for all concepts and therefore learn their association with each verb. 4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set t
.. In our training data, we have examples like widen highway, widen road and widen motorway. If we 62 see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).2 This data provides counts for pairs such as “Edwin Moses, hurdler” and “William Farley, industrialist.” We have features for all concepts and therefore learn their association with each verb. 4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative arg
eriments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a l
Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature. Whe
cations: max at j=2) on the macroaveraged score across all development partitions. Note that we can not use the development set to optimize T and K because the development examples are obtained after setting these values. 4.2 Feature weights It is interesting to inspect the feature weights returned by our system. In particular, the weights on the verb co-occurrence features (Section 3.3.1) provide a high-quality, argument-specific similarityranking of other verb contexts. The DSP parameters for eat, for example, place high weight on features like Pr(nlbraise), Pr(nlration), and Pr(nlgarnish). Lin (1998a)’s similar word list for eat misses these but includes sleep (ranked 6) and sit (ranked 14), because these have similar subjects to eat. Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP„oo, on the verb join,3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a): participate 0.164, lead 0.150, return to 0.148, say 0.143, rejoin 0.142, sign 
igh for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations. 4.3 Pseudodisambiguation We first evaluate DSP on disambiguating positives from pseudo-negatives, comparing to recently3Which all correspond to nouns occurring in the object position of the verb (e.g. Probj(n|lead)), except “launch (subj)” which corresponds to Pr3ubj(n|launch). 63 System MacroAvg MicroAvg F Pairwise P R F P R Acc Cov Dagan et al. (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98 Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83 Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57 DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00 DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)’s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Er
71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)’s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Erk (2007): �MISIM(n, v) = log Sim(n′, n) Pr(v, n′) n′∈SIMS(n) Pr(v)Pr(n′) We gather similar words using Lin (1998a), mining similar verbs from a comparable-sized parsed corpus, and collecting similar nouns from a broader 10 GB corpus of English text.4 We also use Keller and Lapata (2003)’s approach to obtaining web-counts. Rather than mining parse trees, this technique retrieves counts for the pattern “V Det N” in raw online text, where V is any inflection of the verb, Det is the, a, or the empty string, and N is the singular or plural form of the noun. We compute a web-based MI by collecting Pr(n, v), Pr(n), and Pr(v) using all inflections, except we only use the root form of the noun. Rather than using
 it. We tested both and adopt the latter because it resulted in better performance on our development set. 5Available from the LDC as LDC2006T13. This collection was generated from approximately 1 trillion tokens of online text. Unfortunately, tokens appearing less than 200 times have been mapped to the (UNK) symbol, and only N-grams appearing more than 40 times are included. Unlike results from search engines, however, experiments with this corpus are replicable. not be able to provide a score for each example. The similarity-smoothed examples will be undefined if SIMS(w) is empty. Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. As a reasonable default for these cases, we assign them a negative decision. We evaluate disambiguation using precision (P), recall (R), and their harmonic mean, F-Score (F). Table 1 gives the results of our comparison. In the MacroAvg results, we weight each example equally. For MicroAvg, we weight each example by the frequency of the noun. To more directly compare with previous work, we also reproduced Pairwise Disambiguation by randomly pairing each positive with one of the negatives and then evaluating each system by the per
accepts far more pairs than MI (73% vs. 44%), even far more than a system that accepts any previously observed verb-object combination as plausible (57%). Recall is higher on more frequent verb-object pairs, but 70% of the pairs occurred only once in the corpus. Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modified KN-smoothing (Chen and Goodman, 1998), the recall of MI>0 on SJM only increases from 44.1% to 44.9%, still far below DSP. Frequency-based models have fundamentally low coverage. As furimportant than the property of being an entity” (Resnik, 1996). DSPall Erk (2007) Keller and Lapata (2003) F-Score 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 65 Verb Plaus./Implaus. Resnik Dagan et al. Erk MI DSP see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02 read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/— 2.12/-0.65 find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81 hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67 write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31 urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/— -0.34/-0.62 warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/— 2.00/-0.99 
d with a dash (—). Only DSP is completely defined and completely correct. 0 0.2 0.4 0.6 0.8 1 Recall Figure 2: Pronoun resolution precision-recall on MUC. ther evidence, if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation experiment (Section 4.3), MI>0 gets a MacroAvg precision of 86% but a MacroAvg recall of only 12%.9 4.6 Pronoun Resolution Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004). We study the cases where a 9Recall that even the Keller and Lapata (2003) system, built on the world’s largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). pronoun is the direct object of a verb predicate, v. A pronoun’s antecedent must obey v’s selectional preferences. If we have a better model of SP, we should be able to better select pronoun antecedents. We parsed the MUC-7 (1997) coreference corpus and extracted all pronouns in a direct object relation. For each pronoun, p, modified by a verb, v, we extracted all preceding nouns within the current or previous sentence. Thir
