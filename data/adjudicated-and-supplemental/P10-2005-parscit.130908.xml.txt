nments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. Most of the research on alignment combination i
on) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance than the baseline. More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model). In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. There is no need for a pre-determined threshold as used in (Huang, 2009). Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners. Our strategy is to diversify and th
ning data for alignment training. This is similar to the stemming method, but is more heuristicsbased, and does not rely on a set of available affixes. With the same motivation, we keep the first 4 characters of each English and Pashto word to generate one more alternative for the word alignment. 3 Confidence-Based Alignment Combination Now we describe the algorithm to combine multiple sets of word alignments based on weighted confidence scores. Suppose aijk is an alignment link in the i-th set of alignments between the j-th source word and k-th target word in sentence pair (S,T). Similar to (Huang, 2009), we define the confidence of aijk as 'c(aijk|S, T) = qs2t(aijk|S,T)qt2s(aijk|T,S), (1) S S CC S NP VP NP VP PRP VBP NP PRP$ NNS E: they are your employees and you know them well P: hQvy stAsO kArvAl dy Av tAsO hQvy smh pOEnB E’: they your employees are and you them well know NP ADVP VBP PRP RB 23 where the source-to-target link posterior probability qs2t(aijk|S, T) = EkK (tk s9) A , ′=1 pi (tk′ |sj ) (2) and the target-to-source link posterior probability qt2s(aijk|T, S) is defined similarly. pi(tk|sj) is the lexical translation probability between source word sj and target word tk in the i-t
