y useful tool in the translator?s armory. TM systems store a set of ?source, target? translation pairs in their databases. If a new input string cannot be found exactly in the translation database, a search is conducted for close (or ?fuzzy?) matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation. From this description, it should be clear that TM systems do not translate: Indeed, some researchers consider them to be little more than a search-and-replace engine, albeit a rather sophisticated one (Macklovitch and Russell 2000). We can illustrate this with respect to the TM entries in (1), taken from the Canadian Hansards: (1) a. While most were critical, some contributions were thoughtful and constructive =? La plupart ont formule? des critiques, mais certains ont fait des observations re?fle?chies et constructives. b. Others were plain meanspirited and some contained errors of fact =? D?autres discours comportaient des propos mesquins et me?me des erreurs de fait. ? School of Computing, Dublin 9, Ireland. E-mail: away@computing.dcu.ie ? School of Computing, Dublin 9, Ireland. E-mail: ngough@computing.dcu.ie 422 Co
tem can produce translations itself by automatically combining chunks from different translation examples stored in its memories. In Section 2, we describe how we automatically obtain a hierarchy of lexical resources that are used sequentially by our EBMT system, wEBMT, to translate new input. The primary resource gathered is a ?phrasal lexicon,? constructed by extracting over 200,000 phrases from the Penn Treebank and having them translated into French by three Web-based machine translation (MT) systems. Each set of translations is stored separately, and for each set the ?marker hypothesis? (Green 1979) is used to segment the phrasal lexicon into a ?marker lexicon.? The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ?marked? for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes. That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment. Consider the following example, selected at random from the Wall Street Journal section of the Penn-II Treebank: (5) The Dearborn, Mich., energy co
ners and one with a possessive pronoun. The sets of determiners and possessive pronouns are both very small. Furthermore, there are four prepositional phrases, and the set of prepositions is similarly small. A further assumption that could be made is that all words that end with -ed are verbs, such as stopped in (5). The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different language-related tasks, including ? language learning (Green 1979; Mori and Moeser 1983; Morgan, Meier, and Newport 1989) ? monolingual grammar induction (Juola 1998) ? grammar optimization (Juola 1994) ? insights into universal grammar (Juola 1998) ? machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance. Green?s (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguisti
 with a possessive pronoun. The sets of determiners and possessive pronouns are both very small. Furthermore, there are four prepositional phrases, and the set of prepositions is similarly small. A further assumption that could be made is that all words that end with -ed are verbs, such as stopped in (5). The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different language-related tasks, including ? language learning (Green 1979; Mori and Moeser 1983; Morgan, Meier, and Newport 1989) ? monolingual grammar induction (Juola 1998) ? grammar optimization (Juola 1994) ? insights into universal grammar (Juola 1998) ? machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance. Green?s (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist. The 424 
y small. Furthermore, there are four prepositional phrases, and the set of prepositions is similarly small. A further assumption that could be made is that all words that end with -ed are verbs, such as stopped in (5). The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different language-related tasks, including ? language learning (Green 1979; Mori and Moeser 1983; Morgan, Meier, and Newport 1989) ? monolingual grammar induction (Juola 1998) ? grammar optimization (Juola 1994) ? insights into universal grammar (Juola 1998) ? machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance. Green?s (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist. The 424 Computational Linguistics Volume 29, Number 3 research of Mori and Moeser (1983
 prepositional phrases, and the set of prepositions is similarly small. A further assumption that could be made is that all words that end with -ed are verbs, such as stopped in (5). The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different language-related tasks, including ? language learning (Green 1979; Mori and Moeser 1983; Morgan, Meier, and Newport 1989) ? monolingual grammar induction (Juola 1998) ? grammar optimization (Juola 1994) ? insights into universal grammar (Juola 1998) ? machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance. Green?s (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist. The 424 Computational Linguistics Volume 29, Number 3 research of Mori and Moeser (1983) showed a similar effect due to cas
uld be made is that all words that end with -ed are verbs, such as stopped in (5). The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different language-related tasks, including ? language learning (Green 1979; Mori and Moeser 1983; Morgan, Meier, and Newport 1989) ? monolingual grammar induction (Juola 1998) ? grammar optimization (Juola 1994) ? insights into universal grammar (Juola 1998) ? machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance. Green?s (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist. The 424 Computational Linguistics Volume 29, Number 3 research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated t
showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis. Juola?s (1994, 1998) work on grammar optimization and induction shows that context-free grammars can be converted to ?marker-normal form.? However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless, Juola (1998, page 23) observes that ?a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.? Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun?case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the ?source, target? chunks a
pping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.? Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun?case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the ?source, target? chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard ?word-level? translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input. In Section 3, we describe in detail the segmentation process, together with the procedure whereby target chunks are combined to produce candidate translations. In Section 4, we report initially on two experiments in which we test different versions of our EBMT system against test sets of NPs 
ed within (part of) the same chunk as their subject NPs. However, given that we translate phrases rather than sentences, it is a considerable problem for our approach, yet one that we overcome satisfactorily. In further work, if we were to store the translations of the VPs with their dummy subject NPs in a sentential lexicon and derive all marker lexicons from this database, the problem of subject-verb agreement would be largely overcome. 426 Computational Linguistics Volume 29, Number 3 2. Deriving Translation Resources from Web-Based MT Systems All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.3 Kay and Ro?scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the ?source, target? words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derive
nguistics Volume 29, Number 3 2. Deriving Translation Resources from Web-Based MT Systems All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.3 Kay and Ro?scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the ?source, target? words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersso
d Way (2003), are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.3 Kay and Ro?scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the ?source, target? words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such 
ure on trying to establish subsentential translations from a bilingual corpus.3 Kay and Ro?scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the ?source, target? words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of 
and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: ? Learnability (Zernik and Dyer 1987) ? Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996) ? Speech generation (Rayner and Carter 1997) ? Localization (Scha?ler 1996) More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at a subsentential level, while Carl, Way, and Scha?ler (2002) and Scha?ler, Way, and Carl (2003, pages 108?109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment. This, they suggest, may result in a paradigm shift f
ents in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: ? Learnability (Zernik and Dyer 1987) ? Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996) ? Speech generation (Rayner and Carter 1997) ? Localization (Scha?ler 1996) More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at a subsentential level, while Carl, Way, and Scha?ler (2002) and Scha?ler, Way, and Carl (2003, pages 108?109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment. This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technol
portant factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: ? Learnability (Zernik and Dyer 1987) ? Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996) ? Speech generation (Rayner and Carter 1997) ? Localization (Scha?ler 1996) More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at a subsentential level, while Carl, Way, and Scha?ler (2002) and Scha?ler, Way, and Carl (2003, pages 108?109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment. This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential a
r less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: ? Learnability (Zernik and Dyer 1987) ? Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996) ? Speech generation (Rayner and Carter 1997) ? Localization (Scha?ler 1996) More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at a subsentential level, while Carl, Way, and Scha?ler (2002) and Scha?ler, Way, and Carl (2003, pages 108?109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment. This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from 
and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: ? Learnability (Zernik and Dyer 1987) ? Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996) ? Speech generation (Rayner and Carter 1997) ? Localization (Scha?ler 1996) More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at a subsentential level, while Carl, Way, and Scha?ler (2002) and Scha?ler, Way, and Carl (2003, pages 108?109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment. This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from ?source, target? phrasal segments, and from there they suggest that ?it i
lution via the recombination element of EBMT systems such as those described in [Carl and Way 2003].? In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems. From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized, following a methodology based on Block (2000), to generate the ?generalized marker lexicon.? Finally, as a result of the 3 We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/?veronis/biblios/ptp.htm. 427 Way and Gough wEBMT methodology chosen, we automatically derive a fourth resource, namely, a ?word-level lexicon.? 2.1 The Phrasal Lexicon Our phrasal lexicon was built by selecting a set of 218,697 English noun phrases and verb phrases from the Penn Treebank. We identified all rule types occurring 1,000 or more times and eliminated those that 
le. If the URL takes the form of a query, then the document retrieved is the result of the query, namely, the translated Web page. Once this is obtained, it is a simple process to retrieve the French translations and associate them with their English source equivalents. 4 http://www.freetranslation.com 5 http://trans.voila.fr 6 http://www.logomedia.net 428 Computational Linguistics Volume 29, Number 3 2.2 The Marker Lexicons Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of ?source, target? chunks. Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English ?? French and English ?? Urdu. For the English ?? French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English ?? Urdu, Juola (1997, page 213) notes that ?the system learned the original training corpus . . . perfectly and could reproduce it without errors?; that is, it scored 100% accuracy when tested against the training corpus. On novel test sentences, he g
nal Linguistics Volume 29, Number 3 2.2 The Marker Lexicons Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of ?source, target? chunks. Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English ?? French and English ?? Urdu. For the English ?? French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English ?? Urdu, Juola (1997, page 213) notes that ?the system learned the original training corpus . . . perfectly and could reproduce it without errors?; that is, it scored 100% accuracy when tested against the training corpus. On novel test sentences, he gives results of 72% correct translation. In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English ?? German on a test set of 791 sentences from CorelDRAW manuals. As in METLA and Gaijin, we exploit lists of known marker words for each language to indicate the start and end of segments. For English, our source langua
 show the viability of this approach for English ?? French and English ?? Urdu. For the English ?? French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English ?? Urdu, Juola (1997, page 213) notes that ?the system learned the original training corpus . . . perfectly and could reproduce it without errors?; that is, it scored 100% accuracy when tested against the training corpus. On novel test sentences, he gives results of 72% correct translation. In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English ?? German on a test set of 791 sentences from CorelDRAW manuals. As in METLA and Gaijin, we exploit lists of known marker words for each language to indicate the start and end of segments. For English, our source language, we use the sets of marker words in (13): (13) <DET> {the, a, an, those, these, . . . } <PREP> {in, on, out, with, from, to, under, . . . } <QUANT> {all, some, few, many, . . . } <CONJ> {and, or, . . . } <POSS> {my, your, our,. . . } <PRON> {I, you, he, she, it,. . . } A similar set (14) was produced for French,
ntage of the assumption that where a chunk contains just one non?marker word in both source and target, these words are translations of each other. Where a marker-headed pair contains just two words, as in (16), for instance, we can extract the word-level translations in (23): (23) <QUANT> all : tous <PREP> of : d? <LEX> uses : usages <LEX> asbestos : asbeste That is, using the marker hypothesis method of segmentation, smaller aligned segments can be extracted from the phrasal lexicon without recourse to any detailed parsing techniques or complex co-ocurrence measures. 431 Way and Gough wEBMT Juola (1994, 1997) assumes that words ending in -ed are verbs. However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category. Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides. That is, for a rule in the Penn Treebank VP ?? VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle. Given this information, in such cases we tag such words w
 a present participle. Given this information, in such cases we tag such words with the <LEX> tag. Taking expanding the board to 14 members ?? augmente le conseil a` 14 membres as an example, we extract the chunks in (24): (24) <DET> the board : le conseil <DET> the : le <PREP> to <QUANT> 14 members : a` 14 membres <QUANT> 14 members : 14 membres <LEX> expanding : augmente <LEX> board : conseil <PREP> to : a` <LEX> members : membres We ignore here the trivially true lexical chunk ?<QUANT> 14 : 14.? In a final processing stage, we generalize over the marker lexicon following a process found in Block (2000). In Block?s approach, word alignments are assigned probabilities by means of a statistical word alignment tool. In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each ?source, target? segment. Block distinguishes chunks from ?patterns,? as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks. Once chunks are derived from ?source, target? alignments, patterns are computed from the derived chunks by means of the following algorithm: ?for each pair of chunk pairs 
 is what you wanted] ? 432 Computational Linguistics Volume 29, Number 3 Using the algorithm described above, the patterns in (26) are derived from the chunks in (25): (26) ? [V ist], [V is] ? ? [das V], [which V] ? ? [das V was], [which V what] ? ... ? [V ist was Sie], [V is what you] ? ... ? [das ist was V wollten], [which is what V wanted] ? ... Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and Gu?venir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into thei
s in (25): (26) ? [V ist], [V is] ? ? [das V], [which V] ? ? [das V was], [which V what] ? ... ? [V ist was Sie], [V is what you] ? ... ? [das ist was V wollten], [which is what V wanted] ? ... Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and Gu?venir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into their generalized equivalents, as in (28): (28) <DET> good man : bon homme That is, where Block (2000) substitutes variables for various words in his templates, we replace 
V is] ? ? [das V], [which V] ? ? [das V was], [which V what] ? ... ? [V ist was Sie], [V is what you] ? ... ? [das ist was V wollten], [which is what V wanted] ? ... Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and Gu?venir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into their generalized equivalents, as in (28): (28) <DET> good man : bon homme That is, where Block (2000) substitutes variables for various words in his templates, we replace certain lexic
, [which V] ? ? [das V was], [which V what] ? ... ? [V ist was Sie], [V is what you] ? ... ? [das ist was V wollten], [which is what V wanted] ? ... Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and Gu?venir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into their generalized equivalents, as in (28): (28) <DET> good man : bon homme That is, where Block (2000) substitutes variables for various words in his templates, we replace certain lexical items with thei
pproaches include those of Cicekli and Gu?venir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into their generalized equivalents, as in (28): (28) <DET> good man : bon homme That is, where Block (2000) substitutes variables for various words in his templates, we replace certain lexical items with their marker tag. Given that examples such as ??<DET> a : un? are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme. We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in
7) into their generalized equivalents, as in (28): (28) <DET> good man : bon homme That is, where Block (2000) substitutes variables for various words in his templates, we replace certain lexical items with their marker tag. Given that examples such as ??<DET> a : un? are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme. We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples. 2.3 Summary In sum, we automatically create four knowledge sources: ? the original ?source,target? phrasal translation pairs ? the marker lexicon (cf. (16)) ? the generalized marker lexicon (cf. (28)) ? the word-level lexicon (cf. (24)) 433 Way and Gough wEBMT When matching the input to the corpus, we search for chunks in the order given here, that is, from specific examples (those containing more
20 over both knowledge sources. Similarly, if we wish to consider translations produced by all three MT systems, then we add the weights of common translations and divide the weights of all proposed translations by six. When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf. Sato and Nagao 1990; Veale and Way 1997; Carl 1999).7 As an example, consider the translation into French of the house collapsed. Assume the conditional probabilities in (33): 7 Note that approaches that prefer the greatest context to be taken into account are not limited to EBMT. Research in the area of data-oriented parsing (cf. Bod, Scha, and Sima?an, 2003) also shows that unless the corpus is inherently biased, derivations constructed using the smallest number of subtrees have a higher probability than those built with a larger number of smaller subtrees. 436 Computational Linguistics Volume 29, Number 3 (33
e sources. Similarly, if we wish to consider translations produced by all three MT systems, then we add the weights of common translations and divide the weights of all proposed translations by six. When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf. Sato and Nagao 1990; Veale and Way 1997; Carl 1999).7 As an example, consider the translation into French of the house collapsed. Assume the conditional probabilities in (33): 7 Note that approaches that prefer the greatest context to be taken into account are not limited to EBMT. Research in the area of data-oriented parsing (cf. Bod, Scha, and Sima?an, 2003) also shows that unless the corpus is inherently biased, derivations constructed using the smallest number of subtrees have a higher probability than those built with a larger number of smaller subtrees. 436 Computational Linguistics Volume 29, Number 3 (33) a. P(la maison | t
, if we wish to consider translations produced by all three MT systems, then we add the weights of common translations and divide the weights of all proposed translations by six. When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf. Sato and Nagao 1990; Veale and Way 1997; Carl 1999).7 As an example, consider the translation into French of the house collapsed. Assume the conditional probabilities in (33): 7 Note that approaches that prefer the greatest context to be taken into account are not limited to EBMT. Research in the area of data-oriented parsing (cf. Bod, Scha, and Sima?an, 2003) also shows that unless the corpus is inherently biased, derivations constructed using the smallest number of subtrees have a higher probability than those built with a larger number of smaller subtrees. 436 Computational Linguistics Volume 29, Number 3 (33) a. P(la maison | the house) = 
ut with an associated weight and ranked by the system. We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks. That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. We have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm (Littlestone and Warmuth 1992). 4. Experiments and System Evaluation We report here on a number of experiments using test sets of 200 sentences and 500 noun phrases. Some typical examples from the two test sets are given in (39): (39) Noun phrases: ? the heavy use of management fees last year ? an increase through issues of new shares and convertible bonds ? a space-based defense shield for official acts by the congressman Sentences: ? The bright red one interferes with the genes that are responsible for collecting pollen. ? A more recent novel permitted the new basket product. ? The area with the museums and the charities
marker word in the string is the and its translation can be one of le, la, l?, or les, depending on the context. The system simply attaches the translation with the highest weight to the existing chunk ordinateurs personnels to produce the mistranslation in (50): (50) *la ordinateurs personnels The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. However, rather than output this wrong translation directly, we use a post hoc validation and (if required) correction process based on Grefenstette (1999). Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the ?best? translation and have its morphological variants searched for on-line. In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l?ordinateurs personnels. Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uni
ank using just 59 of its 29,000 rule types. These phrases were then translated automatically by three on-line MT systems. These translations gave rise to a number of automatically constructed linguistic resources: (1) the original ?source,target? phrasal translation pairs, (2) the marker lexicon, (3) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system, seeded with input from multiple translation systems, with a postvalidation process via the Web (amounting to an n-gram target language model), in effect forms a multiengine MT system as described by Frederking and Nirenburg (1994), Frederking et al (1994), and Hogan and Frederking (1998). 454 Computational Linguistics Volume 29, Number 3 eralized lexicon, and (4) the word-level lexicon. When the system is confronted with new input, these knowledge sources are searched in turn for matching chunks, and the target language chunks are combined to create translation candidates. We presented a number of experiments that showed how the system fared when confronted with NPs and sentences. For the test set of 500 NPs, we obtained translations in 96% of cases, with 77.8% of the 500 NPs being translated correctly. For sentences, 
ule types. These phrases were then translated automatically by three on-line MT systems. These translations gave rise to a number of automatically constructed linguistic resources: (1) the original ?source,target? phrasal translation pairs, (2) the marker lexicon, (3) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system, seeded with input from multiple translation systems, with a postvalidation process via the Web (amounting to an n-gram target language model), in effect forms a multiengine MT system as described by Frederking and Nirenburg (1994), Frederking et al (1994), and Hogan and Frederking (1998). 454 Computational Linguistics Volume 29, Number 3 eralized lexicon, and (4) the word-level lexicon. When the system is confronted with new input, these knowledge sources are searched in turn for matching chunks, and the target language chunks are combined to create translation candidates. We presented a number of experiments that showed how the system fared when confronted with NPs and sentences. For the test set of 500 NPs, we obtained translations in 96% of cases, with 77.8% of the 500 NPs being translated correctly. For sentences, we obtained translations 
slated automatically by three on-line MT systems. These translations gave rise to a number of automatically constructed linguistic resources: (1) the original ?source,target? phrasal translation pairs, (2) the marker lexicon, (3) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system, seeded with input from multiple translation systems, with a postvalidation process via the Web (amounting to an n-gram target language model), in effect forms a multiengine MT system as described by Frederking and Nirenburg (1994), Frederking et al (1994), and Hogan and Frederking (1998). 454 Computational Linguistics Volume 29, Number 3 eralized lexicon, and (4) the word-level lexicon. When the system is confronted with new input, these knowledge sources are searched in turn for matching chunks, and the target language chunks are combined to create translation candidates. We presented a number of experiments that showed how the system fared when confronted with NPs and sentences. For the test set of 500 NPs, we obtained translations in 96% of cases, with 77.8% of the 500 NPs being translated correctly. For sentences, we obtained translations in 92% of cases, with a completel
