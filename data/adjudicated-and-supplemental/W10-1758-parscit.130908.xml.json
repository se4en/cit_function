{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"David Chiang, Yuval Marton, and Philis Resnik. 2008. Online Large-Margin Training of Syntactic and Structural Translation Features. In Proceedings of the Conference on Empirical Methods in Natural Langauge Processing (EMNLP-2008), pages 224\u2013 233, October."},"#text":"\n","pages":{"#tail":"\n","#text":"224--233"},"marker":{"#tail":"\n","#text":"Chiang, Marton, Resnik, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"feature weights can dramatically reduce scores on evaluation metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005). When we recently added new features to the CMU-EBMT translation system (Brown, 1996; Brown, 2008)1, in addition to splitting a number of composite features into their components, our previous method of parameter tuning via coordinate ascent2 became impractical. With now more than 50 features partaking in the scoring model, MERT no longer seemed a good choice, as the common wisdom is that it is not able to reliably optimize more than about 20 features (Chiang et al., 2008). We had been using coordinate ascent because of a need to tune a substantial number of parameters which are not directly part of the log-linear model which can be tuned by MERT or similar methods. Our system generates a translation lattice by runtime lookup in the training corpus rather than using a precomputed phrase table, so important parameters include \u2022 the size of the sample of retrieved training instances for a given input phrase which are aligned, \u2022 the weight of source features for ranking training instances during sampling, and \u2022 the minimum alignment score to accept a translation i","@endWordPosition":"329","@position":"2160","annotationId":"T1","@startWordPosition":"326","@citStr":"Chiang et al., 2008"},{"#tail":"\n","#text":"ented two objective functions which operate on individual sentences without regard for choices made on other sentences. When the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable. For example, when using BLEU, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single-sentence objective functions. Our plan is to implement a windowed or moving-average version of BLEU as in (Chiang et al., 2008). We also plan to further speed up the tuning process by parallelizing the decoding of the sentences in the tuning set. As we have used a semi-batch update method which leaves the decoder\u2019s weights unchanged for an entire pass through the tuning set, there is no data dependency between individual sentences, allowing them to be decoded in par390 allel. The perceptron algorithm itself remains sequential, but as it is three orders of magnitude faster than the decoding, this will have negligible impact on overall speedup factors until hundreds of CPUs are used for simultaneous decoding. References","@endWordPosition":"4633","@position":"28295","annotationId":"T2","@startWordPosition":"4630","@citStr":"Chiang et al., 2008"}]},"title":{"#tail":"\n","#text":"Online Large-Margin Training of Syntactic and Structural Translation Features."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Conference on Empirical Methods in Natural Langauge Processing (EMNLP-2008),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Chiang"},{"#tail":"\n","#text":"Yuval Marton"},{"#tail":"\n","#text":"Philis Resnik"}]}}}}}}
