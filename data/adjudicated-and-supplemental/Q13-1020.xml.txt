es on the tree nodes. Experimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model
xperimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monoling
 show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rathe
ee translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapp
em using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al, 2006, 2009; Quirk et al, 2005; Galley et al, 2004, 2006; Marcu et al, 2006; Shen et al, 2008; Zhang et al, 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between langua
y built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. This indicates that parse trees are usually not the optimal choice for training tree-based translation models (Wang et al, 2010). Based on the above analysis, we can conclude that the tree structure that is independent from Tree-bank resources and simultaneously considers the bilingual mapping inside the bilingual sentence pairs would be a good choice for building treebased translation models. Therefore, complying with the above conditions, we propose an unsupervised tree structure for treebased translation models in this study. In the structures, tree nodes are labeled by combining the word classes of their boundary words rather than by syntactic labels, such as NP, VP. Furthermore, using these node labels, we design 
ently. The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in
on process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG
w direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on 
 build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing
trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space 
 sentences for syntactic pre-reordering. Our previous work (Zhai et al, 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on p
to the model more freely and effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on genera
effectively. Blunsom et al (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervis
SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended 
Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word clas
cause we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG tr
 the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution gram
thod to infer an STSG by exploring the space of alignments based on parse trees. Liu et al (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution grammar (STSG) between source strings and target tree fragments. We take STSG as the generation grammar to match the translation m
 for two word strings. They are labeled in the form of C1+C2, where C1 and C2 are the word classes of the two words separately. Accordingly, multi-word non-terminals represent the strings containing more than two words. They are labeled as C1?Cn, demanding that the word classes of the leftmost word and the rightmost word are C1 and Cn, respectively. We use POS tag to play the role of word class4. For example, the head node of the rule in Figure 1 is a multi-word non-terminal PRP?RB. It requires that the POS tags of the leftmost and rightmost word must be PRP and RB, respectively. Xiong et al. (2006) showed that the boundary word is an effective indicator for phrase reordering. Thus, we believe that combining the word class of boundary words can denote the whole phrase well. PRP...RB we PRP VBP:x0 RB:x1 VBP+RB ?? x1 x0 wo-men Figure 1. An example of an STSG production rule. Each production rule in P consists of a source string and a target tree fragment. In the target tree fragment, each internal node is labeled with a nonterminal in tN , and each leaf node is labeled with either a target word in t? or a non-terminal in tN . The source string in a production rule comprises source words an
power on tree construction than SCFG. In a STSG-based U-tree or a STSG rule, although not linguistically informed, the nodes labeled by POS tags are also effective on distinguishing different ones. However, with SCFG, we have to discard all the internal nodes (i.e., flattening the Utrees or rules) to express the same sequence, leading to a poor ability of distinguishing different U-trees and production rules. Thus, using STSG, we can build more specific U-trees for translation. In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar (Blunsom et al 2009)5. This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models. 5 In (Blunsom et al, 2009), for Chinese-to-English translation, the Bayesian SCFG grammar only outperform the heuristic SCFG grammar by 0.1 BLEU points on NIST MT 2004 and 0.6 BLEU points on NIST MT 2005 in the NEWS domain. 245 4 Bayesian Model In this section, we present a Bayesian model to learn STSG defined in sectio
in ir . Thanks to the exchangeability of the model, all permutations of the rules are actually equiprobable. This means that we can compute the probability of each rule based on the previous and subsequent rules (i.e. consider each rule as the last one). This characteristic allows us to design an efficient Gibbs sampling algorithm to train the Bayesian model. 4.1 Base Distribution The base distribution 0 ( | )P r N is designed to assign prior probabilities to the STSG production rules. Because each rule r consists of a target tree fragment frag and a source string str in the model, we follow Cohn and Blunsom (2009) and decompose the prior probability 0 ( | )P r N into two factors as follows: 0 ( | ) ( | ) ( | )P r N P frag N P str frag ? (4) where ( | )P frag N is the probability of producing the target tree fragment frag. To generate frag, Cohn and Blunsom (2009) used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al, 2007; Zhang et al, 2011a). The generation process starts at root
frag and a source string str in the model, we follow Cohn and Blunsom (2009) and decompose the prior probability 0 ( | )P r N into two factors as follows: 0 ( | ) ( | ) ( | )P r N P frag N P str frag ? (4) where ( | )P frag N is the probability of producing the target tree fragment frag. To generate frag, Cohn and Blunsom (2009) used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al, 2007; Zhang et al, 2011a). The generation process starts at root node N. At first, root node N is expanded into two child nodes. Then, each newly generated node will be checked to expand into two new child nodes with probability pexpand. This process repeats until all the new non-terminal nodes are checked. Obviously, pexpand controls the scale of tree fragments, where a large pexpand corresponds to large fragments 6. The new terminal nodes (words) are drawn uniformly from the target-side vocabulary, and the nonterminal nodes are created by asking two questions as follows: 1) What type is the node
mine that the left child of PRP?RB is a one-word non-terminal (labeled with PRP); b. Expand PRP and generate the word ?we? for PRP; 6 In our experiment, we set pexpand to 1/3 to encourage small tree fragments. 246 c. Determine that the right child of PRP?RB is a two-word non-terminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the two-word nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB. ( | )P str frag in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by (Blunsom et al, 2009) and (Cohn and Blunsom, 2009), we define ( | )P str frag as follows: var 1 1 1 ( | ) ( ;1) | | poisson sw sw sw c c s i P str frag P c c i u u? ? (5) where csw is the number of words in the source string. ?s means the source vocabulary set. Further, cvar denotes the number of variables, which is determined by the tree fragment frag. As shown in Equation(5), we first determine how many source words to generate using a Poisson distribution Ppoisson(csw;1), which imposes a stable preference for short source strings. Then, we draw each source word from a uniform distribution over ?s. Afterwards, 
f PRP?RB is a one-word non-terminal (labeled with PRP); b. Expand PRP and generate the word ?we? for PRP; 6 In our experiment, we set pexpand to 1/3 to encourage small tree fragments. 246 c. Determine that the right child of PRP?RB is a two-word non-terminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the two-word nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB. ( | )P str frag in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by (Blunsom et al, 2009) and (Cohn and Blunsom, 2009), we define ( | )P str frag as follows: var 1 1 1 ( | ) ( ;1) | | poisson sw sw sw c c s i P str frag P c c i u u? ? (5) where csw is the number of words in the source string. ?s means the source vocabulary set. Further, cvar denotes the number of variables, which is determined by the tree fragment frag. As shown in Equation(5), we first determine how many source words to generate using a Poisson distribution Ppoisson(csw;1), which imposes a stable preference for short source strings. Then, we draw each source word from a uniform distribution over ?s. Afterwards, we insert the variables into 
 string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word ??? (wo-men) ?; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously obtain large rules with enough context information by co
one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word ??? (wo-men) ?; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously obtain large rules with enough context information by composition. We will show the effectiveness of our U-tr
ning by Gibbs Sampling In this section, we introduce a collapsed Gibbs sampler, which enables us to train the Bayesian model efficiently. 5.1 Initialization State At first, we use random binary trees to initialize the sampler. To get the initial U-trees, we recursively and randomly segment a sentence into two parts and simultaneously create a tree node to dominate each part. The created tree nodes are labeled by the non-terminals described in section 3. Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules7 in terms of frontier nodes (Galley et al, 2004). Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler. Differently, Cohn and Blunsom (2009) designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will
 each part. The created tree nodes are labeled by the non-terminals described in section 3. Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules7 in terms of frontier nodes (Galley et al, 2004). Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler. Differently, Cohn and Blunsom (2009) designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics. For each training instance (a pair of source sentence and target U-tree structure), the extracted GHKM minimal translation rules compose a unique STSG derivation9. Moreover, all the rules developed from the training data constitute an initial STSG for the Gibbs sampler. 7 We attach the unaligned word to the lowest frontier node that 
word to the lowest frontier node that can cover it in terms of word alignment. 8 The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM). Actually, the frequent AEs also greatly impair the conventional TM. Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs. Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs. Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules (Galley et al, 2004) here to reduce the complexity of the sampler. 247 jin-tian jian-mianwo-men zai-ci PRP+VBP today NN we PRP meet VBP again RB ?? ?? ?? ?? PRP...RB NN...RB Figure 2. Illustration of an initial U-tree structure. The bold italic nodes with shadows are frontier nodes. Under this initial STSG, the sampler modifies the initial U-trees (initial sample) to create a series of new ones (new samples) by the Gibbs operators. Consequently, new STSGs are created based on the new U-trees simultaneously and used for the next sampling operation. Repeatedly and after a number of iterations, we can obtain the fin
ts state to the right state ? , and transform the U-tree to Figure 3(b) accordingly. jian-mianwo-men zai-ci s-node we PRP meet VBP again RB ?? ?? ?? PRP...RB PRP+VBP jian-mianwo-men zai-ci s-node we PRP meet VBP again RB ?? ?? ?? PRP...RB VBP+RB (b) ?=1(a) ?=0 Rotate Figure 3. Illustration of the Rotate operator. In the figure, (a) and (b) denote the s-node?s left state and right state respectively. The bold italic nodes with shadows in the figure are frontier nodes. Obviously, towards an s-node for sampling, the two values of ? would define two different U-trees. Using the GHKM algorithm (Galley et al 2004), we can get two different STSG derivations from the two U-trees based on the fixed word alignment. Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node?s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node. For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (? ) defines only one rule: 0 2 1 0 1 2 : ... ( ( : : ) : ) leftr x x x PRP RB PRP VBP x PRP x VBP x RB o  Differently, in Fig
 can also take more nodes as a unit for sampling, but this would make the algorithm much more complex. 249 the training data. For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set. We use MERT (Och, 2004) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to len
t set, and use the NIST MT04 and MT05 data as the test set. We use MERT (Och, 2004) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of th
g set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees
ality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees to build another s2t s
alty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al, 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the 
for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set ? to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the trans
). The system is implemented based on (Galley et al, 2006) and (Marcu et al 2006). In the system, we extract both the minimal GHKM rules (Galley et al, 2004), and the rules of SPMT Model 1 (Galley et al, 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al, 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al, 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set ? to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 From 
and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set ? to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 From (Zollmann and Vogel, 2011), we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags. Thus, to be convenient, we only conduct experiments with the SAMT system. 6.2 Analysis of The Gibbs Sampler To evaluate the effectiveness of the Gibbs sampler, we explore the change of the training data?s likelihood with increasing sampling iterations. 1.239E+08 1.243E+08 1.247E+08 1.251E+08 1.255E+08 1.259E+08 100 200 300 400 500 600 700 800 900 1000 Number of Sampling Iterations N e g a ti v e -L o g L ik e li h o o d random 1 random 2 random 3 Figure 5. Histograms of the training d
