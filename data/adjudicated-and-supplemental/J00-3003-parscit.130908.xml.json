{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Brill, Eric. 1993. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of the ARPA Workshop on Human Language Technology, Plainsboro, NJ, March."},"#text":"\n","marker":{"#tail":"\n","#text":"Brill, 1993"},"location":{"#tail":"\n","#text":"Plainsboro, NJ,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"h as question-answer pairs. All the work mentioned so far uses statistical models of various kinds. As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way. However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by Samuel, Carberry, and Vijay-Shanker (1998) is transformation-based learning (Brill 1993). Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed. How does the approach presented here differ from these various earlier models, particularly those based on HMMs? Apart from corpus and tag set differences, our approach differs primarily in that it generalizes the simple HMM approach to cope with new kinds of problems, based on the Bayes network repr","@endWordPosition":"13048","@position":"82515","annotationId":"T1","@startWordPosition":"13047","@citStr":"Brill 1993"}},"title":{"#tail":"\n","#text":"Automatic grammar induction and parsing free text: A transformation-based approach."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ARPA Workshop on Human Language Technology,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Eric Brill"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1988"},"rawString":{"#tail":"\n","#text":"Church, Kenneth Ward. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Second Conference on Applied Natural Language Processing, pages 136-143, Austin, TX."},"#text":"\n","pages":{"#tail":"\n","#text":"136--143"},"marker":{"#tail":"\n","#text":"Church, 1988"},"location":{"#tail":"\n","#text":"Austin, TX."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"(the forward-backward algorithm) The Viterbi algorithm for HMMs (Viterbi 1967) finds the globally most probable state sequence. When applied to a discourse model with locally decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA 348 Stolcke et al. Dialogue Act Modeling sequence with the highest posterior probability: if = argmax P(1.11E) (4) The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(U11E) for each i = 1,. . . ,n. We can compute the per-utterance posterior DA probabilities by summing: P(uIE) -= P(UE) (5) U: u,=u where the summation is over all sequences U whose ith element matches the label in question. The summation i","@endWordPosition":"4733","@position":"30088","annotationId":"T2","@startWordPosition":"4732","@citStr":"Church 1988"}},"title":{"#tail":"\n","#text":"A stochastic parts program and noun phrase parser for unrestricted text."},"booktitle":{"#tail":"\n","#text":"In Second Conference on Applied Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Kenneth Ward Church"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Dermatas, Evangelos and George Kokkinakis. 1995. Automatic stochastic tagging of natural language texts. Computational Linguistics, 21(2):137-163."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"21--2"},"marker":{"#tail":"\n","#text":"Dermatas, Kokkinakis, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ly decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA 348 Stolcke et al. Dialogue Act Modeling sequence with the highest posterior probability: if = argmax P(1.11E) (4) The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(U11E) for each i = 1,. . . ,n. We can compute the per-utterance posterior DA probabilities by summing: P(uIE) -= P(UE) (5) U: u,=u where the summation is over all sequences U whose ith element matches the label in question. The summation is efficiently carried out by the forward-backward algorithm for HMMs (Baum et al. 1970).3 For zeroth-order (unigram) discourse grammars, Viterbi decoding and forwardbackward decoding necess","@endWordPosition":"4765","@position":"30277","annotationId":"T3","@startWordPosition":"4762","@citStr":"Dermatas and Kokkinakis 1995"}},"title":{"#tail":"\n","#text":"Automatic stochastic tagging of natural language texts."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Evangelos Dermatas"},{"#tail":"\n","#text":"George Kokkinakis"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Hirschberg, Julia B. and Diane J. Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3):501-530."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"19--3"},"marker":{"#tail":"\n","#text":"Hirschberg, Litman, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" how the knowledge sources of words and prosody are modeled, and what automatic DA labeling results were obtained using each of the knowledge sources in turn. Finally, we present results for a combination of all knowledge sources. DA labeling accuracy results should be compared to a baseline (chance) accuracy of 35%, the relative frequency of the most frequent DA type (STATEMENT) in our test set.4 5.1 Dialogue Act Classification Using Words DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases (Hirschberg and Litman 1993) can serve as explicit indicators of discourse structure. Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams &quot;<start> do you&quot; occur in YES-NO-QUESTIONS. To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type. 5.1.1 Classification from True Words. Assuming that the true (hand-transcribed) words of utterances are given as ","@endWordPosition":"6023","@position":"38464","annotationId":"T4","@startWordPosition":"6020","@citStr":"Hirschberg and Litman 1993"}},"title":{"#tail":"\n","#text":"Empirical studies on the disambiguation of cue phrases."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Julia B Hirschberg"},{"#tail":"\n","#text":"Diane J Litman"}]}},{"date":{"#tail":"\n","#text":"1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"y independent of these choices. Finally, we developed a principled way of incorporating DA modeling into the probability model of a continuous speech recognizer, by constraining word hypotheses using the discourse context. However, the approach gives only a small reduction in word error on our corpus, which can be attributed to a preponderance of a single dialogue act type (statements). Note The research described here is based on a project at the 1997 Workshop on Innovative Techniques in LVCSR at the Center for Speech and Language Processing at Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998). The DA-labeled Switchboard transcripts as well as other project-related publications are available at http://www.colorado. edu/ling/jurafsky/ws97/. Acknowledgments We thank the funders, researchers, and support staff of the 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shirirtg, and Chuck Wooters. Additional support came from the NSF via grants IRI-9619921 and IRI-9314967, and from the UK Engineering and Physical Science Research Council (grant GR/J55106). Thanks to Mitch Weintraub, to Susann LuperFoy, Nigel Ward, James Allen, Jul","@endWordPosition":"14128","@position":"89682","annotationId":"T5","@startWordPosition":"14125","@citStr":"Jurafsky et al. 1998"}},"title":{"#tail":"\n","#text":"Lexical, prosodic, and syntactic cues for dialog acts."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Jurafsky, Daniel, Elizabeth E. Shriberg, Barbara Fox, and Traci Curl. 1998. Lexical, prosodic, and syntactic cues for dialog acts. In Proceedings of ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers, pages 114-120. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"114--120"},"marker":{"#tail":"\n","#text":"Jurafsky, Shriberg, Fox, Curl, 1998"},"publisher":{"#tail":"\n","#text":"Association"},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Daniel Jurafsky"},{"#tail":"\n","#text":"Elizabeth E Shriberg"},{"#tail":"\n","#text":"Barbara Fox"},{"#tail":"\n","#text":"Traci Curl"}]}}]}}}}
