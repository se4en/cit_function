from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.]1 Let us take a look from the social perspective. [If parents cannot affor
 why she supports abortion, namely it’s a woman’s right to abort and unwanted babies are threat to their parents’ future, which are mentioned in the first and third sentences in the post respectively. Our goal in this paper is to examine post- and sentence-level reason classification (RC) in ideological debates. Many online debaters use emotional languages, which may involve sarcasm and insults, to express their points, thereby making RC and SC in ideological debates potentially more challenging than that in other debate settings such as congressional debates and company-internal discussions (Walker et al., 2012). Besides examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 751 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (1) determining the reason(s) associated with each of its sentences (if any), and then (2) taking the union of the set of reasons associated with all of its sentences to be the set of reasons associated with the
as a feature. The second variant is the same as the first except that the head (i.e., the first argument in a relation) is replaced by its part-of-speech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or −). Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR (Das et al., 2010). Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and (2) an unordered word pair in which the words are taken from two frame elements of f. A frame-pair feature is represented as a word pair corresponding to the names of two frames and encodes whether the target word of the first frame
aining post and is represented by all but the quotation and positional features used to train the Baseline RC system, since these two feature types are sentencebased rather than post-based. After training, the resulting classifier can be used to stance-label a post independently of the other posts. In P2, on the other hand, we recast SC as a sequence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence. This choice is motivated by an observation we made previously (Hasan and Ng, 2013a): since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.3 As our sequence learner, we employ a maximum entropy Markov model (MEMM) (McCallum et al., 2000). Given an input post sequence PS = (p1, p2, . . . , pn), the MEMM finds the most probable stance sequence S = (s1, s2, ... , sn) by computing P(S|PS), where: n P(S|PS) = Y P(sk|sk−1,pk) (1) k=1 This probability can be computed efficiently via dynamic programming (DP), using a modified version of the Viterbi algorithm (Viterbi, 1967). There is a caveat
