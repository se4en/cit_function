{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Saˇsa Hasan and Hermann Ney. 2009. Comparison of extended lexicon models in search and rescoring for smt. In NAACL \u201909: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 17\u201320."},"#text":"\n","pages":{"#tail":"\n","#text":"17--20"},"marker":{"#tail":"\n","#text":"Hasan, Ney, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"es of the target-side are well-formed (WF) dependency structure, but this filtering led to degradation in translation performance. They obtained improvements by adding an additional dependency language model. The basic difference of our method from (Shen et al., 2008) is that we keep rules that both sides should be relaxed-wellformed dependency structure, not just the target side. Besides, our system complexity is not increased because no additional language model is introduced. The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Ta","@endWordPosition":"717","@position":"4644","annotationId":"T1","@startWordPosition":"714","@citStr":"Hasan and Ney, 2009"}},"title":{"#tail":"\n","#text":"Comparison of extended lexicon models in search and rescoring for smt."},"booktitle":{"#tail":"\n","#text":"In NAACL \u201909: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Saˇsa Hasan"},{"#tail":"\n","#text":"Hermann Ney"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In EMNLP \u201909: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72\u201380."},"#text":"\n","pages":{"#tail":"\n","#text":"72--80"},"marker":{"#tail":"\n","#text":"Shen, Xu, Zhang, Matsoukas, Weischedel, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"he log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information. 3 Relaxed-well-formed Dependency Structure Dependency models have recently gained considerable interest in SMT (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency tree can represent richer structural information. It reveals long-distance relation between words and directly models the semantic structure of a sentence without any constituent labels. Figure 2 shows an example of a d","@endWordPosition":"803","@position":"5178","annotationId":"T2","@startWordPosition":"800","@citStr":"Shen et al., 2009"}},"title":{"#tail":"\n","#text":"Effective use of linguistic and contextual information for statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In EMNLP \u201909: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Libin Shen"},{"#tail":"\n","#text":"Jinxi Xu"},{"#tail":"\n","#text":"Bing Zhang"},{"#tail":"\n","#text":"Spyros Matsoukas"},{"#tail":"\n","#text":"Ralph Weischedel"}]}}]}}}}
