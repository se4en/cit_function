s state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance. The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000). The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution. 1 Introduction Much research in natural language processing has gone into the development of rule-based machine learning algorithms. These algorithms are attractive because they often capture the linguistic features of a corpus in a small and concise set of rules
ormed multiple times during the update process, and the modification results in a significant reduction in running time. The disadvantage of this method consists in the system having an unrealistically high memory requirement. For example, a transformation-based text chunker training upon a modestly-sized corpus of 200,000 words has approximately 2 million rules active at each iteration. The additional memory space required to store the lists of pointers associated with these rules is about 450 MB, which is a rather large requirement to add to a system.l 2.1.2 The ICA Approach The ICA system (Hepple, 2000) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance. To achieve the speedup, the ICA system disallows any interaction between the learned rules, by enforcing the following two assumptions: • Sample Independence a state change in a sample (e.g. a change in the current partof-speech tag of a word) does not change the context of surrounding samples. This is certainly the case in tasks such as prepositional phrase attachment, where samples are mutually inde
d(r) = jG(r)j. • B (r) = fs 2 Sjp,(s) = true and C[s] =6 t, and C[s] = T [s]g the samples on which the rule applies and changes the classification from correct to incorrect; similarly, bad(r) = jB(r)j. Given a newly learned rule b that is to be applied to S, the goal is to identify the rules r for which at least one of the sets G (r) , B (r) is modified by the application of rule b. Obviously, if both sets are not modified when applying rule b, then the value of the objective function for rule r remains unchanged. 2The algorithm was implemented by the the authors, following the description in Hepple (2000). The presentation is complicated by the fact that, in many NLP tasks, the samples are not independent. For instance, in POS tagging, a sample is dependent on the classification of the preceding and succeeding 2 samples (this assumes that there exists a natural ordering of the samples in S). Let V (s) denote the &quot;vicinity&quot; of a sample the set of samples on whose classification the sample s might depend on (for consistency, s 2 V (s)); if samples are independent, then V (s) = fsg. 3.1 Generating the Rules Let s be a sample on which the best rule b applies (i.e. [b (s)] =6 C [s]). We need to ide
re not independent. The second task, prepositional phrase attachment, has examples which are independent from each other. The last task is text chunking, where both independence and commitment assumptions do not seem to be valid. A more detailed description of each task, data and the system parameters are presented in the following subsections. Four algorithms are compared during the following experiments: • The regular TBL, as described in section 2; • An improved version of TBL, which makes extensive use of indexes to speed up the rules' update; • The FastTBL algorithm; • The ICA algorithm (Hepple, 2000). 4.1 Part-of-Speech Tagging The goal of this task is to assign to each word in the given sentence a tag corresponding to its part of speech. A multitude of approaches have been proposed to solve this problem, including transformation-based learning, Maximum Entropy models, Hidden Markov models and memory-based approaches. The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998). The training set contained approximately 1M words and the test set approximately 200k words. Table 1 presents the results of the experiment4.
n 2; • An improved version of TBL, which makes extensive use of indexes to speed up the rules' update; • The FastTBL algorithm; • The ICA algorithm (Hepple, 2000). 4.1 Part-of-Speech Tagging The goal of this task is to assign to each word in the given sentence a tag corresponding to its part of speech. A multitude of approaches have been proposed to solve this problem, including transformation-based learning, Maximum Entropy models, Hidden Markov models and memory-based approaches. The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998). The training set contained approximately 1M words and the test set approximately 200k words. Table 1 presents the results of the experiment4. All the algorithms were trained until a rule with a score of 2 was reached. The FastTBL algorithm performs very similarly to the regular TBL, while running in an order of magnitude faster. The two assumptions made by the ICA algorithm result in considerably less training time, but the performance is also degraded (the difference in performance is statistically significant, as determined by a signed test, at a significance level of 0.001). Also present 
