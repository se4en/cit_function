ar style of linguistic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to defining a language (as a set of strings), deep gra
stic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to defining a language (as a set of strings), deep grammars relate 
 (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to defining a language (as a set of strings), deep grammars relate strings to informati
 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research 
trings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia 1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii 2003], LFG [Cahill et al. 2002b, 2004]). To a first approximation, these approaches can be classified as “conversion”- or “annotation”-based. TAG-based approaches convert 1 Our use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often finite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, fine-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) outpu
d Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches convert trees into CCG derivations from which CCG categories can be extracted. HPSGand LFG-based grammar induction methods automatically annotate treebank trees with (typed) attribute-value structure information for the extraction of constraint-based grammars and lexical resources. Two recent papers (Preiss 2003; Kaplan et al. 2004) have started tying together the research strands just sketched: They use dependency-based parser evaluation to compare wide-coverage parsing systems using hand-crafted, deep, constraint-based grammars with systems based on a simple version of treebank-based deep grammar acquisition technology in the conversion paradigm. In the experiments, tree output generated by Collins’s Model 1, 2, and 3 (1999) and Charniak’s (2000) parsers, for example, are automatically translated into dependency structures and evaluated against gold-standard dependency banks. Preiss (2003) uses the
nning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches convert trees into CCG derivations from which CCG categories can be extracted. HPSGand LFG-based grammar induction methods automatically annotate treebank trees with (typed) attribute-value structure information for the extraction of constraint-based grammars and lexical resources. Two recent papers (Preiss 2003; Kaplan et al. 2004) have started tying together the research strands just sketched: They use dependency-based parser evaluation to compare wide-coverage parsing systems using hand-crafted, deep, constraint-based grammars with systems based on a simple version of treebank-based deep grammar acquisition technology in the conversion paradigm. In the experiments, tree output generated by Collins’s Model 1, 2, and 3 (1999) and Charniak’s (2000) parsers, for example, are automatically translated into dependency structures and evaluated against gold-standard dependency banks. Preiss (2003) uses the grammatical relation
lations as input to an anaphora resolution system. Preiss concluded that parser ranking using grammatical relations reflected the absolute ranking (between treebank-induced parsers) using traditional tree-based metrics, but that the difference between the performance of the parsing algorithms narrowed when they carried out the anaphora resolution task. Her results show that the hand-crafted deep unification parser (Briscoe and Carroll 1993) outperforms the machine-learned parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision and recall on grammatical relations.4 Kaplan et al. (2004) compare their deep, handcrafted, LFG-based XLE parsing system (Riezler et al. 2002) with Collins’s (1999) model 3 using a simple conversion-based approach, capturing dependencies from the tree output of the machine-learned parser, and evaluating both parsers against the PARC 700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep grammar outperforms the state-of-the-art treebank-based shallow parser on the level of dependency representation, at the price of a small decrease in parsing speed. Both Preiss (2003) and Kaplan et al. (2004) emphasize that they use rather b
ed out the anaphora resolution task. Her results show that the hand-crafted deep unification parser (Briscoe and Carroll 1993) outperforms the machine-learned parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision and recall on grammatical relations.4 Kaplan et al. (2004) compare their deep, handcrafted, LFG-based XLE parsing system (Riezler et al. 2002) with Collins’s (1999) model 3 using a simple conversion-based approach, capturing dependencies from the tree output of the machine-learned parser, and evaluating both parsers against the PARC 700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep grammar outperforms the state-of-the-art treebank-based shallow parser on the level of dependency representation, at the price of a small decrease in parsing speed. Both Preiss (2003) and Kaplan et al. (2004) emphasize that they use rather basic versions of the conversion-based deep grammar acquisition technology outlined herein. In this article we revisit the experiments carried out by Preiss and Kaplan et al., this time using the sophisticated and fine-grained treebank- and annotation-based, deep, probabilistic LFG grammar acquisition methodology d
ed precision and recall on grammatical relations.4 Kaplan et al. (2004) compare their deep, handcrafted, LFG-based XLE parsing system (Riezler et al. 2002) with Collins’s (1999) model 3 using a simple conversion-based approach, capturing dependencies from the tree output of the machine-learned parser, and evaluating both parsers against the PARC 700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep grammar outperforms the state-of-the-art treebank-based shallow parser on the level of dependency representation, at the price of a small decrease in parsing speed. Both Preiss (2003) and Kaplan et al. (2004) emphasize that they use rather basic versions of the conversion-based deep grammar acquisition technology outlined herein. In this article we revisit the experiments carried out by Preiss and Kaplan et al., this time using the sophisticated and fine-grained treebank- and annotation-based, deep, probabilistic LFG grammar acquisition methodology developed in Cahill et al. (2002b), Cahill et al. (2004), O’Donovan et al. (2004), and Burke (2006) with a number of surprising results: 1. Evaluating against the PARC 700 Dependency Bank (King et al. 2003) using a retrained ver
aplan et al., this time using the sophisticated and fine-grained treebank- and annotation-based, deep, probabilistic LFG grammar acquisition methodology developed in Cahill et al. (2002b), Cahill et al. (2004), O’Donovan et al. (2004), and Burke (2006) with a number of surprising results: 1. Evaluating against the PARC 700 Dependency Bank (King et al. 2003) using a retrained version of Bikel’s (2002) parser, the best automatically induced, deep LFG resources achieve an f-score of 82.73%. This is an improvement of 3.13 percentage points over the previously best published results established by Kaplan et al. (2004) who use a hand-crafted, wide-coverage, deep LFG and the XLE parsing system. This is also a 4 The numbers given are difficult to compare as the results for the Briscoe and Carroll (1993) parser were captured for a richer set of grammatical relations than those for Collins (1997) and Charniak (2000). 84 Cahill et al. Statistical Parsing Using Automatic Dependency Structures statistically significant improvement of 2.18 percentage points over the most recent improved results presented in this article for the XLE system. 2. Evaluating against the Carroll, Briscoe, and Sanfilippo (1998) CBS 500 go
based, automatically acquired, deep LFG resources achieve an f-score of 80.23%. This is a statistically significant improvement of 3.66 percentage points over Carroll and Briscoe (2002), who use a hand-crafted, wide-coverage, deep, unification grammar and the RASP parsing system. Evaluation results on a reannotated version (Briscoe and Carroll 2006) of the PARC 700 Dependency Bank were recently published in Clark and Curran (2007), reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and Carroll (2006) point out, these evaluations are not directly comparable with the Kaplan et al. (2004) style evaluation against the original PARC 700 Dependency Bank, because the annotation schemes are different. The article is structured as follows: In Section 2, we outline the automatic LFG f-structure annotation algorithm and the pipeline parsing architecture of Cahill et al. (2002b), Cahill et al. (2004), and Burke (2006). In Section 3, we present our experiment design. In Section 4, using the DCU 105 Dependency Bank as our development set, we evaluate a number of treebank-induced LFG parsing systems against the automatically generated Penn-II WSJ Section 22 Dependency Bank test set. We us
reward([subj,obj,obl]) .2000 reward([subj]),p) 1.0000 (together with the subject control equation described previously) turns the parseroutput proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in (Figure 9). The full pipeline parsing architecture with the LDD resolution (rather than the Traces component for LDD resolved Penn-II treebank trees) component (and the LDD path and subcategorization frame extraction) is given in Figure 7. The pipeline architecture supports flexible integration of treebank-based PCFGs or state-of-the-art, history-based, and lexicalized parsers (Collins 1999; Charniak 2000; Bikel 2002) and enables dependency-based evaluation of such parsers. 3. Experiment Design In our experiments we compare four history-based parsers for integration into the pipeline parsing architecture described in Section 2.3: • Collins’s 1999 Models 311 • Charniak’s 2000 maximum-entropy inspired parser12 11 Downloaded from ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz. 12 Downloaded from ftp://ftp.cs.brown.edu/pub/nlparser/. 97 Computational Linguistics Volume 34, Number 1 • Bikel’s 2002 emulation of Collins Model 213 • a retrained version of Bikel’s (2002) parser which
 34, Number 1 • Bikel’s 2002 emulation of Collins Model 213 • a retrained version of Bikel’s (2002) parser which retains Penn-II functional tags Input for Collins’s and Bikel’s parsers was pre-tagged using the MXPOST POS tagger (Ratnaparkhi 1996). Charniak’s parser provides its own POS tagger. The combined system of best history-based parser and automatic f-structure annotation is compared to two probabilistic parsing systems based on hand-crafted, wide-coverage, constraintbased, deep grammars: • the RASP parsing system (Carroll and Briscoe 2002) • the XLE parsing system (Riezler et al. 2002; Kaplan et al. 2004) Both hand-crafted grammars perform their own POS tagging, resolve LDDs, and associate strings with dependency relations (in the form of grammatical relations or LFG f-structures). We evaluate the parsers against a number of gold-standard dependency banks. We use the DCU 105 Dependency Bank (Cahill et al. 2002a) as our development set for the treebank-based LFG parsers. We use the f-structure annotation algorithm to automatically generate a gold-standard test set from the original Section 22 treebank trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best treebank-b
uate the parsers against a number of gold-standard dependency banks. We use the DCU 105 Dependency Bank (Cahill et al. 2002a) as our development set for the treebank-based LFG parsers. We use the f-structure annotation algorithm to automatically generate a gold-standard test set from the original Section 22 treebank trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments. Following the experimental setup in Kaplan et al. (2004), we use the Penn-II Section 23- based PARC 700 Dependency Bank (King et al. 2003) to evaluate the treebank-induced LFG resources against the hand-crafted XLE grammar and parsing system of Riezler et al. (2002) and Kaplan et al. Following Preiss (2003), we use the SUSANNE Based CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebankinduced LFG resources against the hand-crafted RASP grammar and parsing system (Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al. 2002). For each gold standard, our experiment design is as follows: We parse automatically tagged input14 sentences with the treebank- and machine-learning-bas
G parsers. We use the f-structure annotation algorithm to automatically generate a gold-standard test set from the original Section 22 treebank trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments. Following the experimental setup in Kaplan et al. (2004), we use the Penn-II Section 23- based PARC 700 Dependency Bank (King et al. 2003) to evaluate the treebank-induced LFG resources against the hand-crafted XLE grammar and parsing system of Riezler et al. (2002) and Kaplan et al. Following Preiss (2003), we use the SUSANNE Based CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebankinduced LFG resources against the hand-crafted RASP grammar and parsing system (Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al. 2002). For each gold standard, our experiment design is as follows: We parse automatically tagged input14 sentences with the treebank- and machine-learning-based parsers trained on WSJ Sections 02–21 in the pipeline architecture, pass the resulting parse trees to our automatic f-structure annotation algorithm, collect the f-str
tions, pass them to a constraint-solver which generates an f-structure, resolve long-distance dependencies at f-structure following Cahill et al. (2004) and convert the resulting LDDresolved f-structures into dependency representations using the formats and software of Crouch et al. (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations) and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS 500 evaluation). In the experiments we did not use any additional annotations such as -A (for argument) that can be generated by some of the history-based parsers (Collins 1999) as the f-structure annotation algorithm is designed for Penn-II trees (which do not contain such annotations). We also did not use the limited LDD resolution for whrelative clauses provided by Collins’s Model 3 as better results are achieved by LDD 13 This was developed at the University of Pennsylvania by Dan Bikel and is freely available to download from http://www.cis.upenn.edu/∼dbikel/software.html. 14 Tags were automatically assigned either by the parsers themselves or by the MXPOST tagger (Ratnaparkhi 1996). 98 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Tabl
rmalism Comparison of Treebank-Induced and Hand-Crafted Grammars From the experiments in Section 4, we choose the treebank-based LFG system using the retrained version of Bikel’s parser (which retains Penn-II functional tag labels) to compare against parsing systems using deep, hand-crafted, constraint-based grammars at the level of dependencies. We report on two experiments. In the first experiment (Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained parser-based LFG system against the hand-crafted, wide-coverage LFG and XLE parsing system (Riezler et al. 2002; Kaplan et al. 2004) on the PARC 700 Dependency Bank (King et al. 2003). In the second experiment (Section 5.2), we evaluate against the handcrafted, wide-coverage unification grammar and RASP parsing system of Carroll and Briscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998). 5.1 Evaluation against PARC 700 The PARC 700 Dependency Bank (King et al. 2003) provides dependency relations (including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of the Penn-II Treebank. In order to evaluate the parsers, we follow the experimental setup of Kaplan et al. (2004) wi
ed Grammars From the experiments in Section 4, we choose the treebank-based LFG system using the retrained version of Bikel’s parser (which retains Penn-II functional tag labels) to compare against parsing systems using deep, hand-crafted, constraint-based grammars at the level of dependencies. We report on two experiments. In the first experiment (Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained parser-based LFG system against the hand-crafted, wide-coverage LFG and XLE parsing system (Riezler et al. 2002; Kaplan et al. 2004) on the PARC 700 Dependency Bank (King et al. 2003). In the second experiment (Section 5.2), we evaluate against the handcrafted, wide-coverage unification grammar and RASP parsing system of Carroll and Briscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998). 5.1 Evaluation against PARC 700 The PARC 700 Dependency Bank (King et al. 2003) provides dependency relations (including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of the Penn-II Treebank. In order to evaluate the parsers, we follow the experimental setup of Kaplan et al. (2004) with a split of 560 dependency structures for the tes
ies and the f-structures generated in our approach as regards feature geometry, feature nomenclature, and the treatment of named entities. In order to evaluate against the PARC 700 test set, we automatically map the f-structures produced by our parsers to a format similar to that of the PARC 700 Dependency Bank. This is done with conversion software in a post-processing stage on the f-structure annotated trees (Figure 13). The conversion software is developed on the 140-sentence development set of the PARC 700, except for the Multi-Word Expressions section. Following the experimental setup of Kaplan et al. (2004), we mark up multi-word expression predicates based on the gold-standard PARC 700 Dependency Bank. Multi-Word Expressions The f-structure annotation algorithm analyzes the internal structure of all noun phrases fully. In Figure 14, for example, BT is analyzed as an ADJUNCT modifier of the head securities, whereas PARC 700 analyzes this and other (more complex) named entities as multi-word expression predicates. The conversion software transforms the output of the f-structure annotation algorithm into the multi-word expression predicate format. Feature Geometry In constructions such as Figure 2
onstructions in terms of hierarchically cascading XCOMPs, whereas in PARC 700 the temporal and aspectual information expressed by auxiliary verbs is represented in terms of a flat analysis and features (Figure 15). The conversion software automatically flattens the f-structures produced by the automatic annotation algorithm into the PARC-style encoding. For full details of the mapping, see Burke et al. (2004). In our parsing experiments, we used the most up-to-date version of the handcrafted, wide-coverage, deep LFG resources and XLE parsing system with improved results over those reported in Kaplan et al. (2004): This latest version achieves 80.55% f-score, a 0.95 percentage point improvement on the previous 79.6%. The XLE parsing system combines a large-scale, hand-crafted LFG for English and a statistical disambiguation component to choose the most likely analysis among those returned by the symbolic parser. The statistical component is a log-linear model trained on 10,000 partially labeled structures from the WSJ. The results of the parsing experiments are presented in Table 11. We also include a figure for the upper bound of each system.23 Using Bikel’s retrained parser, the treebank-based LFG sy
7. Conclusions Parser comparison is a non-trivial and time-consuming exercise. We extensively evaluated four machine-learning-based shallow parsers and two hand-crafted, widecoverage deep probabilistic parsers involving four gold-standard dependency banks, using the Approximate Randomization Test (Noreen 1989) to test for statistical significance. We used a sophisticated method for automatically producing deep dependency relations from Penn-II-style CFG-trees (Cahill et al. 2002b, 2004) to compare shallow parser output at the level of dependency relation and revisit experiments carried out by Preiss (2003) and Kaplan et al. (2004). Our main findings are twofold: 1. Using our CFG-tree-to-dependency annotation technology, together with treebank- and machine-learning-based parsers, we can outperform hand-crafted, wide-coverage, deep, probabilistic, constraint-based grammars and parsers. This result is surprising for two reasons. First, it is established against two externallyprovided dependency banks (the PARC 700 and the CBS 500 gold standards), originally designed using the hand-crafted, wide-coverage, probabilistic grammars for the XLE (Riezler et al. 2002) and RASP (Carroll and Briscoe 2002) p
