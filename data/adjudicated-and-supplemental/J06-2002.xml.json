{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.841035","#text":"\n1. Introduction: Vagueness of Gradable Adjectives\n"},{"#tail":"\n","@confidence":"0.8753995","#text":"\n(1) The (n) Adj(est) N (singular/plural)\n(2) The Adj(est) (n) N (singular/plural)\n"},{"#tail":"\n","@confidence":"0.9617415","#text":"\n(3) The large mouse (= the one whose size is 10 cm)\n(4) The two large mice (= the two whose sizes are 8 and 10 cm)\n"},{"#tail":"\n","@confidence":"0.8696535","#text":"\nvan Deemter GRE with Gradable Properties\n4. The Generation of Vague Descriptions\n"},{"#tail":"\n","@confidence":"0.85960625","#text":"\n(7) The tallest two of the smallest three mice.\n(8) The mice that are taller than 2 cm but shorter than 4 cm.\n(The latter may be better expressed as the mice that are between 2 and 4 cm tall.) Let us see\nhow the algorithm of the previous sections can be extended to these cases.\n"},{"#tail":"\n","@confidence":"0.963734","#text":"\n(a) La = ?size < 14 cm , height > 8 cm?, to be realized as, e.g., the mice\ntaller than 8 cm but smaller than 14 cm.\n(b) Lb = ?height > 8 cm, size > 6 cm, < 14 cm?, e.g., the mice that are taller\n"},{"#tail":"\n","@confidence":"0.970128714285714","#text":"\nGRE for Vague Descriptions (using IA):\n1. Construct KB using Attributes and Values, assigning numerical Values to\nGradable Attributes.\n2. Recompile the KB, replacing equalities by inequalities, for all gradable\nAttributes.\n3. Determine the preference order between the different groups of Attributes.\n(A safe approach is to give all gradable Attributes lower preference than\nall nongradable ones.)\n4. Run IAPlur (Section 3.2), resulting in a list of properties that jointly identify\nthe target.\n5. Apply inferences to the list of properties. For example, replace\ncombinations of inequalities by one exact Value; replace inequalities by\nproperties that involve a cardinality; and so on.\n6. Perform linguistic realization (Section 6).\n"},{"#tail":"\n","@confidence":"0.862757076923077","#text":"\nvan Deemter GRE with Gradable Properties\nPhase 2: Recompilation of the KB forces one to compare all domain elements with each\nother. This takes at most quadratic time (i.e., O(n2), where n is the number of elements\nin the domain). This can be done off-line, once and for all.\nPhase 4: Content Determination. The initial list of properties, which contains inequal-\nities (e.g., L = ?mouse > 5 cm?), is calculated by IAPlur. The algorithm has to take more\nAttribute/Value pairs into account as a result of the recompilation of the KB, but this\ndoes not change its theoretical complexity (using nv and na as variables): It is O(nvna).\nPhase 5: Inference. The only inference step described so far replaces an inequality (e.g.,\nheight > n cm) by a ?superlative? property (e.g., height = max2). This step requires\nno computation to speak of: For any given inequality that appears in the description,\nthe value of m can be read off the input to the generator in O(nd) steps, where nd is the\nnumber of distractors. (This comes down to counting the number of elements in the\n"},{"#tail":"\n","@confidence":"0.7868225","#text":"\nvan Deemter GRE with Gradable Properties\n6. Linguistic Realization\n"},{"#tail":"\n","@confidence":"0.873844142857143","#text":"\neasy to draw. We propose to think of it as separating the language-independent, logical\naspect of referring expressions generation from its language-dependent, linguistic as-\npect. Our algorithm suggests a distinction into three phases, the first two of which can\nbe thought of as part of CD:\n1. CD proper, that is, the production of a distinguishing list of properties L;\n2. An inference phase, during which the list L is transformed;\n3. A realization phase, during which the choice between base, superlative,\n"},{"#tail":"\n","@confidence":"0.9905975","#text":"\nadjective. Informally:\n1. The dichotomy constraint of Section 5 did not hold up well: Even when\ncomparing two things, the superlative form was often preferred over the\ncomparative.\n2. When base forms were used, the gap was almost invariably large.\n3. Yet, the Minimality constraint of Section 5 turned out to be difficult to\n"},{"#tail":"\n","@confidence":"0.958932875","#text":"\nGRE for Vague Descriptions (version not relying on IA):\n1. Construct KB using Attributes and Values, assigning numerical Values to\ngradable Attributes.\n2. Recompile the KB, replacing equalities by inequalities.\n3. Let G deliver an unordered set of properties which jointly distinguish the\ntarget if such a set exists. (One or more of these properties may be\ninequalities.)\n4. Impose a linear ordering on the properties produced by (3). (If one wishes\n"},{"#tail":"\n","@confidence":"0.997854666666667","#text":"\nvan Deemter GRE with Gradable Properties\n5. Apply inferences (in the style of Section 4.1) to the list of properties.\n6. Perform linguistic realization.\n"},{"#tail":"\n","@confidence":"0.994319","#text":"\n1. ?Vi ? V : Vi(x) > Vi(r) and\n2. ??Vj ? V : Vj(x) < Vj(r)\n"},{"#tail":"\n","@confidence":"0.88751","#text":"\nSection 2:\n1. the academic (Can only refer to a)\n2. both academics (Can only refer to {a, b})\n3. the three academics (Can only refer to {a, b, c})\n"},{"#tail":"\n","@confidence":"0.4749985","#text":"\nvan Deemter GRE with Gradable Properties\n10. Conclusion\n"}],"figure":[{"#tail":"\n","@confidence":"0.90885225","#text":"\n(i) ?size < 14 cm, height(x) = max2?,\n(?The tallest two of the mice that are smaller than 14 cm?)\n(ii) ?size(x) = min3, height(x) = max2?\n(?The tallest two of the smallest three mice?)\n"},{"#tail":"\n","@confidence":"0.80088375","#text":"\n[W] [W] [W] [W]\na b c d e f g h i\n^^^^^^^^^^\n   |\n"}],"author":{"#tail":"\n","@confidence":"0.926969","#text":"\nKees van Deemter?\n"},"equation":[{"#tail":"\n","@confidence":"0.976396375","#text":"\nL := \nC := Domain\nFor each Ai  A do\nVi = FindBestValue(S, Ai)\nIf S ? [[?Ai, Vi?]] & C ? [[?Ai, Vi?]] then do\nL := L ? {?Ai, Vi?}\nC := C ? [[?Ai, Vi?]]\nIf C = S then Return L\n"},{"#tail":"\n","@confidence":"0.999613","#text":"\ntype(c1) = type(c2) = type(c3) = type(c4) = mouse\ntype(p5) = rat\nsize(c1) = 6 cm\n"},{"#tail":"\n","@confidence":"0.998150666666667","#text":"\nsize(c2) = 10 cm\nsize(c3) = 12 cm\nsize(c4) = size(p5) = 14 cm\n"},{"#tail":"\n","@confidence":"0.826104333333333","#text":"\nC = {c1, ..., c4}.) Now size is taken into account, and size(x) = 14 cm singles out c4.\nThe resulting list is\nL = {mouse, 14 cm}\n"},{"#tail":"\n","@confidence":"0.667502","#text":"\nL1 = ?mouse, >10 cm?,\n"},{"#tail":"\n","@confidence":"0.984839","#text":"\nL2 = ?mouse, size(x) = max2?.\n"},{"#tail":"\n","@confidence":"0.954444","#text":"\nL3 = ?mouse, size(x) = max?.\n"},{"#tail":"\n","@confidence":"0.892744833333333","#text":"\nsize(c1) < 10 cm\nsize(c1), size(c2) < 12 cm\nsize(c1), size(c2), size(c3) < 14 cm\nsize(c4), size(p5) > 12 cm\nsize(c3), size(c4), size(p5) > 10 cm\nsize(c2), size(c3), size(c4), size(p5) > 6 cm\n"},{"#tail":"\n","@confidence":"0.99983875","#text":"\nheight(c1) = 7 cm\nheight(p5) = 8 cm\nheight(c3) = 9 cm\nheight(c2) = height(c4) = 10 cm.\n"},{"#tail":"\n","@confidence":"0.984917375","#text":"\ntype(c1) = type(c2) = type(c3) = type(c4) = mouse\ntype(p5) = rat\nheight(c1) < 8 cm\nheight(c1), height(p5) < 9 cm\nheight(c1), height(c3), height(p5) < 10 cm\nheight(c2), height(c4) > 9 cm\nheight(c2), height(c3), height(c4) > 8 cm\nheight(c2), height(c3), height(c4), height(p5) > 7 cm\n"},{"#tail":"\n","@confidence":"0.999993","#text":"\nL1 = ?size(x) = min3, height(x) = max?\nL2 = ?size(x) = min, height(x) = max?\n"},{"#tail":"\n","@confidence":"0.7416055","#text":"\nlist is\n1 2 1 7 7 1 1 3 1\n"},{"#tail":"\n","@confidence":"0.918905","#text":"\n1 2 1 (7) (7) 1 1 3 1\n"},{"#tail":"\n","@confidence":"0.9989976","#text":"\ntype(d1) = type(d2) = dog\ntype(c) = cat\ntype(s1) = type(s2) = type(s3) = shed\nsize(d1) = size(d2) = size(c) = 1m\nsize(s1) = 3m\nsize(s2) = 5m\nsize(s3) = 6m\ncontains-d1 = s1\ncontains-d2 = s2\ncontains-c = s3\n"},{"#tail":"\n","@confidence":"0.9997915","#text":"\nheight(a) = 5 m\nheight(b) = height(c) = 15 m\nwidth(a) = width(b) = 3 m\nwidth(c) = 2 m\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.980313","#text":"\n1.1 Vague Descriptions\n"},{"#tail":"\n","@confidence":"0.983483","#text":"\n1.2 Vagueness in NLG\n"},{"#tail":"\n","@confidence":"0.98412","#text":"\n1.3 Plan of This Article\n"},{"#tail":"\n","@confidence":"0.652548","#text":"\n2.1 Linguistic Motivation\n"},{"#tail":"\n","@confidence":"0.996846","#text":"\n2.2 Caveat: Full NP Anaphora\n"},{"#tail":"\n","@confidence":"0.996457","#text":"\n2.3 Caveat: Evaluative Adjectives\n"},{"#tail":"\n","@confidence":"0.999619","#text":"\n3.1 The Incremental Algorithm\n"},{"#tail":"\n","@confidence":"0.423603","#text":"\nReturn Failure\n"},{"#tail":"\n","@confidence":"0.999462","#text":"\n3.2 The Existing Treatment of Gradables\n"},{"#tail":"\n","@confidence":"0.997986","#text":"\n4.1 Expressing One Vague Property\n"},{"#tail":"\n","@confidence":"0.998906","#text":"\n4.2 Expressing Several Vague Properties\n"},{"#tail":"\n","@confidence":"0.999274","#text":"\n4.3 Computational Complexity\n"},{"#tail":"\n","@confidence":"0.861211","#text":"\n7.1 Human Speakers? Use of Vague Descriptions\n"},{"#tail":"\n","@confidence":"0.740519","#text":"\n7.2 Testing the Correctness of the Generated Expressions\n"},{"#tail":"\n","@confidence":"0.601817","#text":"\n7.3 Testing the Felicity of the Generated Expressions\n"},{"#tail":"\n","@confidence":"0.99614","#text":"\n8.1 Problems with Incrementality\n"},{"#tail":"\n","@confidence":"0.942255","#text":"\n8.2 Low Preference for Gradable Properties?\n"},{"#tail":"\n","@confidence":"0.962728","#text":"\n9.1 Relational Descriptions\n"},{"#tail":"\n","@confidence":"0.995338","#text":"\n9.2 Boolean Combinations\n"},{"#tail":"\n","@confidence":"0.71085","#text":"\n9.3 Multidimensionality\n9.3.1 Combinations of Adjectives. When objects are compared in terms of several\n"},{"#tail":"\n","@confidence":"0.501768","#text":"\n9.4 Salience as a Gradable Property\n"},{"#tail":"\n","@confidence":"0.722591","#text":"\n9.5 Beyond Vague Descriptions: Nouns, and Pointing\n"}],"subsubsectionHeader":[{"#tail":"\n","@confidence":"0.788331","#text":"\n4.1.1 Numerical Properties. We shall assume that gradable properties are stored in\n"},{"#tail":"\n","@confidence":"0.880907","#text":"\n4.1.2 Exploiting Numerical Properties, Singular. To (almost4) ensure that every descrip-\n"},{"#tail":"\n","@confidence":"0.58838","#text":"\n4.1.3 Exploiting Numerical Properties, Plural. If plural descriptions were generated us-\n"},{"#tail":"\n","@confidence":"0.461027","#text":"\n4.1.4 Ordering of Properties. Even if comparative properties are at the bottom of the\n"},{"#tail":"\n","@confidence":"0.982008","#text":"\n4.1.5 Beyond Content Determination (CD). Assuming the analysis of Section 2.1, the\n"},{"#tail":"\n","@confidence":"0.851295","#text":"\n4.2.1 Descriptions Using (In)equalities. When opposites are part of the KB, there is no\n"},{"#tail":"\n","@confidence":"0.992606","#text":"\n4.2.2 Adjectives in Superlative and Base Form. To generate descriptions like the ones\n"},{"#tail":"\n","@confidence":"0.517011","#text":"\n9.3.2 Multidimensional Adjectives (and Color). Multidimensionality can also slip in\n"},{"#tail":"\n","@confidence":"0.434985","#text":"\n9.4.2 Salience + Plurality = Ambiguity. It is now easy to see why plural descriptions\n"}],"footnote":[{"#tail":"\n","@confidence":"0.525896","#text":"\n? Computing Science Department, King?s College, University of Aberdeen, United Kingdom, E-mail:\nkvdeemter@csd.abdn.ac.uk.\n1 We take such adjectives to be ones that have comparative and superlative forms, and which can be\n"},{"#tail":"\n","@confidence":"0.822365","#text":"\n2 The reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or\nvertical distance, or some combination of dimensions (Kamp 1975; also Section 8.1 of the present article).\n"}],"title":{"#tail":"\n","@confidence":"0.84131","#text":"\nGenerating Referring Expressions\nthat Involve Gradable Properties\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.9590615","#text":"\nI thank Hua Cheng, Roger Evans, Albert\nGatt, Markus Guhe, Imtiaz Khan, Emiel\nKrahmer, Judith Masthoff, Chris Mellish,\nOystein Nilsen, Manfred Pinkal, Paul Piwek,\nEhud Reiter, Graeme Ritchie, Ielka van der\nSluis, Rosemary Stevenson, Matthew Stone,\n"},{"#tail":"\n","@confidence":"0.996987851851852","#text":"\nAho, Alfred V., John E. Hopcroft, and Jeffrey\nD. Ullman. 1983. Data Structures and\nAlgorithms. Addison-Wesley Publishing\nCompany, Reading, MA.\nAppelt, Doug. Planning English referring\nexpressions. Artificial Intelligence, 26:1?33.\nReprinted in: B. J. Grosz, K. Sparck Jones,\nand B. L. Webber, editors (1986). Readings\nin Natural Language Processing. Morgan\nKaufmann, Los Altos, CA.\nBarwise, Jon and John Perry. 1983. Situations\nand Attitudes. MIT Press, Cambridge, MA.\nBerlin, Brent and Paul Kay. 1969. Basic Color\nTerms. University of California Press,\nBerkeley.\nBerry, Dianne C., Peter R. Knapp, and Theo\nRaynor. 2002. Is 15 percent very common?\nInforming people about the risks of\nmedication side effects. International\nJournal of Pharmacy Practice, 10:145?151.\nBeun, Robbert-Jan and Anita Cremers. 1998.\nObject reference in a shared domain of\nconversation. Pragmatics and Cognition,\n6(1/2):121?152.\nBierwisch, Manfred. 1989. The semantics of\ngradation. In M. Bierwisch and E. Lang,\neditors, Dimensional Adjectives. Springer\nVerlag, Berlin, pages 71?261.\nDale, Robert. 1989. Cooking up referring\nexpressions. In Proceedings of the 27th\nAnnual Meeting of the Association for\nComputational Linguistics (ACL-89),\npages 68?75.\nDale, Robbert and Nickolas Haddock. 1991.\nGenerating referring expressions\ncontaining relations. Proceedings of the 5th\nConference of the European Chapter of the\nACL, EACL-91, pages 161?166, Berlin,\nGermany.\nDale, Robbert and Ehud Reiter. 1995.\nComputational interpretations of the\nGricean maximes in the generation of\nreferring expressions. Cognitive Science,\n18:233?263.\nDeVault, David and Matthew Stone. 2004.\nInterpreting vague utterances in context.\nIn Proceedings of COLING 2004,\npages 1247?1253, Geneva.\nEbeling, K. S. and S. A. Gelman. 1994.\nChildren?s use of context in interpreting\n?big? and ?little?. Child Development,\n65(4):1178-1192.\nFeldman, Allan M. 1980. Welfare Economics\nand Social Choice Theory. Kluwer, Boston.\n"},{"#tail":"\n","@confidence":"0.998973025210084","#text":"\nvan Deemter GRE with Gradable Properties\nFunakoshi, Kotaro, Satoru Watanabe, Naoko\nKuriyama, and Takenobu Takunaga.\n2004. Generating referring expressions\nusing perceptual groups. In Proceedings\nof 3rd International Conference on Natural\nLanguage Generation (INLG) 2004,\npages 51?60. Brockenhurst, UK.\nGaiffe, Bertrand and Laurent Romary. 1997.\nConstraints on the use of language,\ngesture, and speech for multimodal\ndialogues. In Proceedings of ACL Workshop\nReferring Phenomena in a Multimedia Context\nand Their Computational Treatment,\npages 94?98, Madrid, Spain.\nGoldberg, Eli, Norbert Driedger, and Richard\nKitteridge. 1994. Using natural-language\nprocessing to produce weather forecasts.\nIEEE Expert, 9(2):45?53.\nGorniak, Peter and Deb Roy. 2003.\nUnderstanding complex visually\nreferring utterances. In Proceedings of\nHLT-NAACL03 Workshop on Learning\nWord Meaning from Non-Linguistic Data,\npages 14?21. Edmonton, Canada,\nMay 2003.\nGrice, Paul. 1975. Logic and conversation. In\nP. Cole and J. Morgan, editors, Syntax and\nSemantics, volume 3, Speech Acts. Academic\nPress, New York, pages 43?58.\nHermann, Tony and Roland Deutsch. 1976.\nPsychologie der Objektbenennung. Huber\nVerlag, Bern.\nHyde, Dominic. 2002. Sorites Paradox.\nIn Edward Zalta, editor, The Stanford\nEncyclopedia of Philosophy (Fall 2002\nEdition), http://plato.stanford.edu/\narchives/fall2002/entries/sorites-\nparadox/.\nJames, Glyn, David Burley, Dick Clements,\nPhil Dyke, John Searl, and Jerry Wright.\n1996. Modern Engineering Mathematics,\nsecond edition. Addison-Wesley Longman\nLtd., Harlow, UK.\nKamp, Hans. 1975. Two theories about\nadjectives. In E. Keenan, editor, Semantics\nfor Natural Language. Cambridge\nUniversity Press, Cambridge, UK.\nKennedy, Christopher. 1999. Projecting the\nAdjective: The syntax and Semantics of\nGradability and Comparison. Ph.D. thesis,\nUC Santa Cruz.\nKrahmer, Emiel and Marie?t Theune. 2002.\nEfficient context-sensitive generation of\nreferring expressions. In K. van Deemter\nand R. Kibble, editors, Information Sharing:\nReference and Presupposition in Language\nGeneration and Interpretation, CSLI\nPublications, CSLI, Stanford,\npages 223?264.\nKrahmer, Emiel and Ielka van der Sluis. 2003.\nA new model for generating multimodal\nreferring expressions. In Proceedings of 9th\nEuropean Workshop on Natural Language\nGeneration (ENLG-2003), pages 47?54,\nBudapest.\nKyburg, Alice and Michael Morreau.\n2000. Fitting Words: vague language in\ncontext. Linguistics and Philosophy,\n23:577?597.\nLevelt, William J. M. 1989. Speaking: From\nIntention to Articulation. MIT Press,\nCambridge, MA.\nMalouf, Rob. 2000. The order of prenominal\nadjectives in natural language generation.\nIn Proceedings of ACL-2000, pages 85?92,\nHong Kong.\nMasthoff, Judith. 2004. Group modeling:\nSelecting a sequence of television items to\nsuit a group of viewers. User Modeling and\nUser Adapted Interaction, 14:37?85.\nMellish, Chris. 2000. Understanding\nshortcuts in NLG systems. In Proceedings of\nWorkshop ?Impacts in Natural Language\nGeneration?, pages 43?50, Dagstuhl,\nGermany.\nNash, John. 1950. The bargaining problem.\nEconometrica, 18:155?162.\nPeccei, Jean Stillwell. 1994. Child Language,\nRoutledge.\nPechmann, Thomas. 1989. Incremental\nspeech production and referential\noverspecification. Linguistics, 27:98?110.\nPinkal, Manfred. 1979. How to refer with\nvague descriptions. In R. Ba?uerle, U. Egli,\nand A. von Stechow, editors, Semantics\nfrom Different Points of View. Springer\nVerlag, Berlin, pages 32?50.\nQuirk, Randolph, Sidney Greenbaum,\nGeoffrey Leech, and Jan Svartvik. 1972. A\nGrammar of Contemporary English.\nLongman, Harlow, Essex.\nQuirk, Randolph, Sidney Greenbaum,\nGeoffrey Leech, and Jan Svartvik. 1985. A\nComprehensive Grammar of the English.\nLongman, Harlow, Essex.\nRasmusen, Eric. 1989. Games and Information:\nAn Introduction to Game Theory. Blackwell\nPublishing.\nReiter, Ehud and Robert Dale. 2000. Building\nNatural Language Generation Systems.\nCambridge University Press, Cambridge,\nUK.\nReiter, Ehud and Somayajulu (Yaji) Sripada.\n2002. Should corpora texts be gold\nstandards for NLG? In Proceedings of\nSecond International Conference on Natural\nLanguage Generation (INLG-2002),\npages 97?104, New York.\n"},{"#tail":"\n","@confidence":"0.999616711864407","#text":"\nComputational Linguistics Volume 32, Number 2\nRosch, Eleanor. 1975. Cognitive reference\npoints. Cognitive Psychology, 7:532?547.\nRosch, Eleanor. 1978. Principles of\ncategorization. In E. Rosch and B. Lloyd,\neditors, Cognition and Categorization,\nLawrence Erlbaum, Hillsdale, NJ,\npages 27?48.\nSedivy, Julie, Michael Tanenhaus, Craig\nChambers, and Gregory Carlson. 1999.\nAchieving incremental semantic\ninterpretation through contextual\nrepresentation. Cognition, 71:109?147.\nShaw, James and Vasileios Hatzivassiloglou.\n1999. Ordering among premodifiers. In\nProceedings of ACL99, pages 135?143,\nUniversity of Maryland, College Park.\nSonnenschein, Susan. 1982. The effects of\nredundant communications on listeners:\nWhen more is less. Child Development,\n53:717?729.\nSripada, Yaji, Ehud Reiter, and Ian Davy.\n2003. SumTime-Mousam: Configurable\nmarine weather forecast generator. Expert\nUpdate, 6(3):4?10.\nStone, Matthew. 2000. On identifying sets. In\nProceedings of INLG-2000, pages 116?123,\nMitzpe Ramon, Israel.\nStone, Matthew and Bonnie Webber. 1998.\nTextual Economy through close coupling\nof syntax and semantics. In Proceedings of\nINLG-1998, pages 178?187.\nTho?risson, Kristinn R. 1994. Simulated\nperceptual grouping: An application to\nhuman-computer interaction. In\nProceedings of 6th Annual Conference of the\nCognitive Science Society, pages 876?881.\nToogood, John H. 1980. What do we mean by\n?usually?? Lancet, 1:1094.\nvan Deemter, Kees. 2000. Generating vague\ndescriptions. In Proceedings of International\nConference on Natural Language Generation\n(INLG-2000), pages 179?185, Mitzpe\nRamon, Israel.\nvan Deemter, Kees. 2002. Generating\nreferring expressions: Boolean extensions\nof the incremental algorithm.\nComputational Linguistics, 28(1):37?52.\nvan Deemter, Kees. 2004. Finetuning an\nNLG system through experiments with\nhuman subjects: The case of vague\ndescriptions. In Proceedings of 3rd\nInternational Conference on Natural\nLanguage Generation (INLG-04),\npages 31?40, Brockenhurst, UK.\nvan Deemter, Kees and Jan Odijk. 1997.\nContext modeling and the generation of\nspoken discourse. Speech Communication,\n21:101?121.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.993647571428571","#text":"\nThis article examines the role of gradable properties in referring expressions from the perspective\nof natural language generation. First, we propose a simple semantic analysis of vague de-\nscriptions (i.e., referring expressions that contain gradable adjectives) that reflects the context-\ndependent meaning of the adjectives in them. Second, we show how this type of analysis can\ninform algorithms for the generation of vague descriptions from numerical data. Third, we ask\nwhen such descriptions should be used. The article concludes with a discussion of salience and\npointing, which are analyzed as if they were gradable adjectives.\n"},{"#tail":"\n","@confidence":"0.991334210526316","#text":"\nVague or gradable expressions pose problems to models of language, caused by their\ncontext dependence, and by the fact that they are applicable to different degrees. This\narticle focuses on gradable adjectives, also called degree adjectives.1 More specifically,\nwe shall explore how referring expressions containing gradable adjectives can be pro-\nduced by a Natural Language Generation (NLG) program. Following Pinkal (1979), such\nexpressions will be called vague descriptions even though, as we shall see, the vagueness\nof the adjective does not extend to the description as a whole. It will be useful to gen-\neralize over different forms of the adjective, covering the superlative form (e.g., largest)\nand the comparative form (larger), as well as the positive or base form (large) of the\nadjective. Vague descriptions are worth studying because they use vagueness in a\ncomparatively transparent way, often combining clarity of reference with indeterminacy\nof meaning; as a result, they allow us to make inroads into the difficult area of research\non vagueness. Generation offers an interesting perspective because it forces one to ask\nwhen it is a good idea to use these descriptions, in addition to asking what they mean.\nGradability is especially widespread in adjectives. A search of the British National\nCorpus (BNC), for example, shows at least seven of the ten most frequent adjectives\n(last, other, new, good, old, great, high, small, different, large) to be gradable. Children use\nvague adjectives among their first dozens of words (Peccei 1994) and understand some\nof their intricacies as early as their 24th month (Ebeling and Gelman 1994). These\n"},{"#tail":"\n","@confidence":"0.873712888888889","#text":"\npremodified by intensifiers such as very (Quirk et al 1972, Section 5.4).\nSubmission received: 7 July 2004; revised submission received: 19 October 2005; accepted for\npublication: 24 November 2005.\n? 2006 Association for Computational Linguistics\nComputational Linguistics Volume 32, Number 2\nintricacies include what Ebeling and Gelman call perceptual context dependence, as\nwhen a set of objects is perceptually available and the adjective is applied to an element\nor subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are\nvisible).\n"},{"#tail":"\n","@confidence":"0.999580424242424","#text":"\nSome NLG systems produce gradable adjectives. The FOG weather-forecast system, for\nexample, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output\n(Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not\nappear to have generic rules governing the use of gradable notions: it does not compute\nthe meaning of a vague term based on the context, but uses fixed boundary values\ninstead. A more flexible approach is used by Reiter and Sripada (2002), where users can\nspecify boundary values for attributes like rainfall, specifying, for example, rain counts\nas moderate above 7 mm/h, as heavy above 20 mm/h, and so on. A third approach was\nimplemented in Dial Your Disc (DYD), where the extension of a gradable adjective like\nfamous was computed rather than specified by hand (van Deemter and Odijk 1997). To\ndetermine, for example, whether one of Mozart?s piano sonatas could be called a famous\nsonata, the system looked up the number x of compact disc recordings of this sonata (as\nlisted in an encyclopedia) and compared it to the average number y of CD recordings of\neach of Mozart?s sonatas. The sonata was called a famous sonata if x >> y. Like DYD, the\nwork reported in this article will abandon the use of fixed boundary values for gradable\nadjectives, letting these values depend on the context in which the adjective is used.\nSometimes we are forced to be vague because the information we have (e.g., based on\nperception or verbal reports) is itself inexact. Such cases can be modeled by letting NLG\nsystems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We\nshall focus on the more challenging case where the output of the generator is less precise\nthan the input, as is the case in FOG and DYD. This can be a hazardous affair, since vague\nexpressions tend to be interpreted in different ways by different people (Toogood 1980),\nsometimes in stark contrast with the intention of the speaker/writer (Berry, Knapp,\nand Raynor 2002). We shall therefore focus?unlike earlier computational accounts?on\nvague descriptions, that is, vague expressions in definite descriptions. Here, the context\ntends to obliterate the vagueness associated with the adjective. Suppose you enter a\nvet?s surgery in the company of two dogs: a big one on a leash, and a tiny one in your\narms. The vet asks ?Who?s the patient??, and you answer ?the big dog.? This answer\nwill allow the vet to pick out the patient just as reliably as if you had said ?the one on\nthe leash?; the fact that big is a vague term is irrelevant. You omit the exact size of the\ndog, just like some of its other properties (e.g., the leash), because they do not improve\nthe description. This shows how vague properties can contribute to the precise task of\nidentifying a referent.\n"},{"#tail":"\n","@confidence":"0.999627","#text":"\nWe will show how existing algorithms for the generation of referring expressions (GRE)\ncan do justice to gradable properties, whether they originate from the gradable adjec-\ntives in a vague description, or from some entirely different source (such as the degree of\nsalience of the referent). Considerable attention will be paid to the many open questions\nin this area, which will have to be resolved before NLG can be said to contain a proper\ntreatment of vague expressions. We start with two preliminary sections, containing a\nsemantic analysis of vague descriptions (Section 2) and a version of the Incremental\n"},{"#tail":"\n","@confidence":"0.9927435","#text":"\nvan Deemter GRE with Gradable Properties\nAlgorithm that generates references to sets (Section 3). Section 4 describes the core of\none particular algorithm for generating vague descriptions. Section 5 discusses prag-\nmatic constraints that let such an algorithm avoid descriptions that are semantically\ncorrect but clumsy. Section 6 discusses linguistic realization. Section 7 summarizes\nsome empirical results. Section 8 explores non-incremental versions of our algorithm.\nSection 9 shows how our approach can be extended to include nouns, salience, and\npointing. Section 10 sums up our main findings.\n"},{"#tail":"\n","@confidence":"0.907047","#text":"\nWe shall be studying vague descriptions of various forms: They may or may not contain\na numeral n (positioned before or after the adjective); and the gradable adjective (Adj)\nmay at least be in base (large) or superlative form (largest):\n"},{"#tail":"\n","@confidence":"0.943179","#text":"\nIf Adj is in the base form, we focus on the word order (1); if Adj is superlative, we focus\non (2). (Little will hinge on this decision.) We are limiting ourselves to referential uses of\nthese expressions, excluding cases like This must be the largest tree in the world, in which\nthe expression ascribes a property to an already-identified object. Likewise, we exclude\nintensional ones (e.g., Consider the smallest element of this set, in a mathematical proof,\nwhen the identity of the element may not be known).\nMany different analyses are possible of what it means to be large: larger than\naverage, larger than most, larger than some given baseline, and so on. It is doubtful that\nany one of these analyses makes sense for all definite descriptions. To see this, consider\na domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of\n"},{"#tail":"\n","@confidence":"0.987098545454545","#text":"\nClearly, what it takes for the adjective to be applicable has not been cast in stone,\nbut is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may\nset the standards higher (cf., Kennedy 1999). The numeral (whether it is implicit, as\nin (3), or explicit) can be construed as allowing the reader to draw inferences about\nthe standards employed (Kyburg and Morreau 2000; DeVault and Stone 2004): (3), for\nexample, implies a standard that counts 10 cm as large and 8 cm as not large. Our\nown proposal will abstract away from the effects of linguistic context. We shall ask\nhow noun phrases like the ones in (3) and (4) can be generated, without asking how\nthey constrain, and are constrained by, other uses of large and related words. This will\nallow us to make the following simplification: In a definite description that expresses\nonly properties that are needed for singling out a referent, we take the base form of\n"},{"#tail":"\n","@confidence":"0.985422363636364","#text":"\nComputational Linguistics Volume 32, Number 2\nthe adjective to be semantically equivalent to the superlative form (and, analogously, the\ncomparative):\nThe n large mice = The largest n mice\nThe large mice = The largest mice\nThe large mouse = The largest mouse.\nViewed in this way, gradable adjectives are an extreme example of the ?efficiency of\nlanguage? (Barwise and Perry 1983): Far from meaning something concrete like ?larger\nthan 8 cm??a concept that would have very limited applicability?or even something\nmore general like ?larger than the average N,? a word like large is applicable across a\nwide range of different situations.\n"},{"#tail":"\n","@confidence":"0.998647636363636","#text":"\nHaving said this, there are pragmatic differences between the base form and the superla-\ntive (Section 5). For example, the equivalence does not take anaphoric uses into account,\nsuch as when the large mouse is legitimized by the fact that the mouse has been called\nlarge before, as in\n(5) I was transfixed by a large mouse on the chimney; then suddenly, dozens\nof mice were teeming on the ground. The large mouse was running away.\nwhere the mouse on the chimney may be smaller than those on the ground. We focus on\nEbeling and Gelman?s (1994) perceptual context dependence (Section 1), pretending that\nthe only contextually relevant factor is the ?comparison set?: those elements of the noun\ndenotation that are perceptually available. We disregard functional context dependence,\nas when the small hat is the one too small to fit on your head.\n"},{"#tail":"\n","@confidence":"0.990177692307692","#text":"\nWhat we said above has also disregarded elements of the ?global? (i.e., not immediately\navailable) context. For some adjectives, including the ones that Bierwisch (1989) called\nevaluative (as opposed to dimensional), this is clearly inadequate. He argued that evalu-\native adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid,\netc.) can be recognized by the way in which they compare with antonyms. For example\n(after Bierwisch 1989),\n(6a) Hans is taller than Fritz ? Fritz is shorter than Hans.\n(6b) Hans is smarter than Fritz ? Fritz is more stupid than Hans.\nWe could require that the referent of an evaluative description fall into the correct\nsegment of the relevant dimension. (For Fritz to be the stupid man, it is not enough\nfor him to be the least intelligent male in the local context; he also has to be a fairly\nstupid specimen in his own right.) If this is done, it is not evident that dimensional\nadjectives should be treated differently: If Hans?s and Fritz?s heights are 210 and\n"},{"#tail":"\n","@confidence":"0.984618962962963","#text":"\nvan Deemter GRE with Gradable Properties\n205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if\nHans is the only other man in the local context (but see Sedivy et al 1999, discussed in\nSection 7.2). Be this as it may, we shall henceforth focus on local context, assuming that\nadditional requirements on the global context can be made if necessary.\nWith these qualifications in place, let us say more precisely what we will assume\nthe different types of expressions to mean. For ease of reading, concrete examples (e.g.,\nlarge) will replace abstract labels (e.g., ?Adj?), but the analysis is meant to be general.\nThe largest n mouse/mice; The n large mice. Imagine a set C of contextually relevant\nanimals. Then these noun phrases (NPs) presuppose that there is a subset S of C that\ncontains n elements, all of which are mice, and such that (1) C?S =  (i.e., not all\nelements of C are elements of S) and (2) every mouse in C?S (i.e., every contextually\nrelevant mouse not in S) is smaller than every mouse in S. If such a set S exists then the\nNP denotes S. The case where n = 1, realized as The large(st) mouse, falls out\nautomatically.\nThe large(st) mice. This account can be extended to cover cases of the form the\n[Adj]-(est) [Npl] (pl = plural), where the numeral n is suppressed: They will be taken to\nbe ambiguous between all expressions the [Adj]-(est) n [Npl], where n > 1. Sometimes,\nthis leaves only one possibility. For instance, in a domain where there are five mice, of\nsizes 4, 4, 4, 5, and 6 cm, the only possible value of n is 2, causing the NP to denote the\ntwo mice of 5 and 6 cm in size.\nPragmatic refinements are discussed in Section 5. Our analysis is limited to NPs that\ncontain only one vague adjective. Doubly-graded descriptions tend to cause ambiguity,\nsince they involve a trade-off between several dimensions. An NP like the tall fat giraffe,\nfor example, might describe a referent that is neither the tallest nor the fattest giraffe,\nas long as a combination of height and fatness singles it out. Some of the problems that\ncome up in such cases will be discussed in Section 9.1.\n"},{"#tail":"\n","@confidence":"0.976217666666667","#text":"\nArguably the most fundamental task in the generation of referring expressions (GRE),\ncontent determination (CD) requires finding a set of properties that jointly identify the in-\ntended referent. Various CD algorithms have been proposed, most of which approximate\nthe minimal number of properties that are needed to identify the target. Approxima-\ntions differ in terms of their computational complexity and the degree to which they\nmatch the way in which people use referring expressions (see Dale and Reiter [1995] for\na survey). As we shall see in Section 8, any one of these algorithms could be used as\na basis for our task. For concreteness, we focus here on Dale and Reiter?s Incremental\nAlgorithm (IA). We shall use a form of the IA that can refer to sets as well as individuals,\nas long as the sets are individuated via their elements (i.e., distributively, as opposed to\ncollectively, cf., Stone [2000]). This version of the IA will be called IAPlur. (For motivation\nand extensions, see van Deemter 2000, 2002.)\n"},{"#tail":"\n","@confidence":"0.99117575","#text":"\nPut simply, IA accumulates semantic properties until the target objects are the only ones\nin the domain of which all the accumulated properties are true. This can be done by\narranging the properties in a list and by checking, for each property in the list, whether\nit is useful (in the sense that it removes one or more distractors); if a property is useful, it\n"},{"#tail":"\n","@confidence":"0.988229625","#text":"\nComputational Linguistics Volume 32, Number 2\nis included in the description, after which the next property is given the same treatment.\nThis process of checking and including goes on until the target objects are the only ones\nof which all the properties in the list are true (i.e., until there are no distractors left).\nFor reasons that will become apparent later, we complicate matters slightly: Fol-\nlowing Dale and Reiter, we view each property as consisting of an Attribute (e.g.,\ncolor) and a Value (e.g., white), written ?Attribute,Value?. (Attributes can be viewed\nas grouping together a number of related properties.) Attributes are ordered in a\nlist A, and this preference order determines the order in which properties are examined\n(and possibly added to the description) by the algorithm. Suppose S is the target set,\nand C the set of all objects that play a role at a given stage of the algorithm (we call\nthese the confusables). The algorithm iterates through A; for each Attribute, it checks\nwhether, by specifying a Value for it, one can rule out at least one member of C that has\nnot yet been ruled out; if so then the Attribute is added to a set L, with the best possible\nValue (as determined by FindBestValue). Confusables that are ruled out are removed\nfrom C. The expansion of L and the contraction of C continue until C = S:\n"},{"#tail":"\n","@confidence":"0.998892142857143","#text":"\nFindBestValue selects the ?best value? from among the Values of a given Attribute,\nassuming that these are linearly ordered in terms of specificity. The function selects the\nValue that removes most distractors, but in case of a tie, the least specific contestant\nis chosen, as long as it is not less specific than the basic-level Value (i.e., the most\ncommonly occurring and psychologically most fundamental level, Rosch 1978). IAPlur\ncan refer to individuals as well as sets, since reference to a target individual r can be\nmodeled as reference to the singleton set {r}.\n"},{"#tail":"\n","@confidence":"0.991918615384615","#text":"\nIAPlur deals with vague properties in essentially the same way as FOG: Attributes like\nsize are treated as if they were not context dependent: Their Values always apply to the\nsame objects, regardless of what other properties occur in the description. In this way,\nIA could never describe the same animal as the large chihuahua and the small brown dog,\nfor example. This approach does not do justice to gradable adjectives, whether they are\nused in the base form, the superlative, or the comparative. Suppose, for example, one set\na fixed quantitative boundary, making the word large true of everything above it, and\nfalse of everything below it. Then IA would tend to have little use for this property\nat all since, presumably, every chihuahua would be small and every alsatian large,\nmaking each of the combinations {large, chihuahua} (which denotes the empty set) and\n{large, alsatian} (the set of all alsatians) useless. In other words, existing treatments of\ngradables in GRE fail to take the ?efficiency of language? into account (Barwise and\nPerry 1983; see our Section 2).\n"},{"#tail":"\n","@confidence":"0.992210833333333","#text":"\nWe now turn to the question of how vague descriptions may be generated from nu-\nmerical data. We focus on semantic issues, postponing discussion of pragmatics until\nSection 5, and linguistic realization until Section 6. We shall make occasional reference\nto a PROLOG program called VAGUE, designed by Richard Power, which implements a\nversion of the algorithm described in this section. Code and documentation for VAGUE\ncan be found at http://www.csd.abdn.ac.uk/?kvdeemte/vague.html.\n"},{"#tail":"\n","@confidence":"0.971408444444445","#text":"\nthe Knowledge Base (KB) as Attributes with (decimal) numerical Values, where the\nnumbers can be the result of physical measurements. We will sometimes speak of\nthese numerical Values as if they represented exact Values even though they typically\nrepresent approximations.3 For concreteness, we shall take them to be of the form n cm,\nwhere n is a positive real number. For example,\ntype = rodent, mouse\ncolor = black, blue, yellow\nsize = 3 cm, 4 cm, ..., 10 cm.\nMaking use of this KB, the IA is able to generate a description involving a list of\nproperties like L = {yellow, mouse, 9 cm}, for example, exploiting the Attribute size.\nThe result could be the NP The 9-cm yellow mouse, for example. The challenge formulated\nin Section 1, however, is to avoid unnecessary precision by avoiding numerical values\nunless they are necessary for the individuation of the target. This challenge will be\nanswered using a replacement strategy. Numerical Values such as 9 cm, in L, will be\nreplaced by a superlative Value (?being the unique largest element of C?) whenever all\ndistractors happen to have a smaller size. This list can then be realized in several ways,\nusing either the superlative, the comparative, or the base form (e.g., the largest yellow\nmouse, the larger yellow mouse, or the large yellow mouse).\n"},{"#tail":"\n","@confidence":"0.94362425","#text":"\ntion contains a property expressible as a noun, we shall assume that the type Attribute\nis more highly preferred than all others. Suppose also, for now, that properties related\nto size are less preferred than others. As a result, all other properties that turn up in the\nNP are already in the list L when size is added. Suppose the target is c4:\n"},{"#tail":"\n","@confidence":"0.73463","#text":"\nbe described by the GRE algorithm, since it determines which objects count as having the same size.\n"},{"#tail":"\n","@confidence":"0.683395","#text":"\ntype-related property if none is present yet (cf., Dale and Reiter 1995). VAGUE uses both of these devices.\n"},{"#tail":"\n","@confidence":"0.35002","#text":"\nComputational Linguistics Volume 32, Number 2\n"},{"#tail":"\n","@confidence":"0.995414","#text":"\nSince gradable properties are (for now at least) assumed to be dispreferred, the first\nproperty that makes it into L is ?mouse,? which removes p5 from the context set. (Result:\n"},{"#tail":"\n","@confidence":"0.947682375","#text":"\nThis might be considered the end of the matter, since the target has been singled out. But\nwe are interested in alternative lists, to enable later modules to use gradable adjectives.\nOne way in which such a list can be computed is as follows. Given that 14 cm happens to\nbe the greatest size of any mouse, size(x) = 14 cm can be replaced, in L, by the property\nof ?being the sole object larger than all other elements of C? (notation: size(x) = max1;\nnote that C is the set of mice). Since this property is only applicable because of the\nproperties earlier-introduced into L, it becomes essential that L is an ordered list:\nL = ?mouse, size(x) = max1? (?the largest mouse?)\n"},{"#tail":"\n","@confidence":"0.926163125","#text":"\ning the replacement strategy sketched above, it would be impossible to characterize sets\nwhose elements have different sizes. To make this possible, we have to use inequalities,\nthat is, Values of the form ?> ?? or ?< ??, instead of Values of the form ?= ??. Therefore,\nwe compile the KB into a more elaborate form by replacing equalities by inequalities of\nthe form size(x) > ? or size(x) < ?. The new KB can be limited to relevant inequalities\nonly: for every n such that the old KB contains an equality of the form size(x) = n cm,\nthe new KB contains all those inequalities whose truth follows from the equalities in the\nold KB. For example,\nsize(c4), size(p5) > 12 cm\nsize(c3), size(c4), size(p5) > 10 cm\nsize(c2), size(c3), size(c4), size(p5) > 6 cm,\nwhere ?size? is an Attribute, ?> 12 cm?, ?> 10 cm?, and ?> 6 cm? are Values, and c2, c3,\nc4, c5, p5 are domain objects of which a given ?Attribute, Value? combination is true. The\nprocedure is analogous to the treatment of negations and disjunctions in van Deemter\n(2002): Properties that are implicit in the KB are made available for GRE.\nThe representation of inequalities is not entirely trivial. For one thing, it is conve-\nnient to view properties of the form size(x) < ? as belonging to a different Attribute\nthan those of the form size(x) > ?, because this causes the Values of an Attribute to be\nlinearly ordered: Being larger than 12 cm implies being larger than 10 cm, and so on.\nMore importantly, it will now become normal for an object to have many Values for the\nsame Attribute; c4, for example, has the Values > 6 cm, > 10 cm, and > 12 cm. Each of\nthese Values has equal status, so the notion of a basic-level Value cannot play a role (cf.,\nDale and Reiter 1995). If we abstract away from the role of basic-level Values, then Dale\nand Reiter?s FindBestValue chooses the most general Value that removes the maximal\n"},{"#tail":"\n","@confidence":"0.982698","#text":"\nvan Deemter GRE with Gradable Properties\nnumber of distractors, as we have seen. The problem at hand suggests a simpler ap-\nproach that will always prefer logically stronger inequalities over logically weaker ones,\neven when they do not remove more distractors.5 (Thus, size(x) > m is preferred over\nsize(x) > n iff m > n; conversely, size(x) < m is preferred over size(x) < n iff m < n.)\nThis is reflected by the order in which the properties are listed above: Once a size-\nrelated property is selected, later size-related properties do not remove any distractors\nand will therefore not be included in the description.\nLet us return to our example. Suppose the target set S is {c3, c4}. The KB models\nits two elements as having different sizes (12 cm and 14 cm, respectively), hence they\ndo not share a property of the form size(x) = ?. They do, however, share the property\nsize(x) > 10 cm. This property is exploited by IAPlur to construct the list\n"},{"#tail":"\n","@confidence":"0.999478","#text":"\nfirst selecting the property ?mouse,? then the property size(x) > 10 cm. (The property\nsize(x) > 12 cm is attempted first but rejected.) Since L succeeds in distinguishing\nthe two target elements, it follows that they are the only mice greater than 10 cm.\nConsequently, this inequality can be replaced by the property ?being a set of cardinality\n2, whose elements are larger than all others? (notation: size(x) = max2), leading to NPs\nsuch as the largest (two) mice:\n"},{"#tail":"\n","@confidence":"0.985871142857143","#text":"\nNote that size(x) = max2 is true of a pair of mice: Strictly speaking, the step from L1 to L2\ntranslates a distributive property (?being larger than 10 cm?) into a collective one. The\ncase in which the numeral is 1 corresponds with the singular (e.g., the largest mouse).\nOptionally, we can go a step further and replace size(x) = max2 by the less specified\nproperty size(x) = max, which abbreviates ?being a set of cardinality greater than 1, all\nof whose elements are larger than all other elements in C.? The result may be realized\nas the largest mice.\n"},{"#tail":"\n","@confidence":"0.941531071428571","#text":"\npreference order, while stronger inequalities precede weaker ones, the order is not fixed\ncompletely. Suppose, for example, that the KB contains information about height as\nwell as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c)\nwidth > x, and (d) width < x. Which of these should come first? Hermann and Deutsch\n(1976; also reported in Levelt 1989) show that greater differences are most likely to be\nchosen, presumably because they are more striking. In experiments involving candles\nof different heights and widths, if the referent is both the tallest and the fattest candle,\nsubjects tended to say ?the tall candle? when the tallest candle is much taller than all\nothers whereas the same candle is only slightly wider than the others; if the reverse is\nthe case, the preference switches to ?the fat candle.? Hermann and Deutsch?s findings\nmay be implemented as follows. First, the Values of the different Attributes should be\nnormalized to make them comparable. Second, preference order should be calculated\n5 A statement p is logically stronger than q if p has q as a logical consequence (i.e., p |= q), whereas the\nreverse is not true (i.e., q |= p).\n"},{"#tail":"\n","@confidence":"0.9772212","#text":"\nComputational Linguistics Volume 32, Number 2\ndynamically (i.e., based on the current value of C, and taking the target into account),\npreferring larger gaps over smaller ones. (It is possible, e.g., that width is most suitable\nfor singling out a black cat, but height for singling out a white cat.) The rest of the\nalgorithm remains unchanged.\n"},{"#tail":"\n","@confidence":"0.9617805","#text":"\nn large mouse/mice is semantically equivalent to the n largest mouse/mice. Consequently,\nthere is no need to distinguish between the two at the level of CD. Representations like\nthe ones in L2 and L3 are neutral between the superlative and the base form. Pragmatic\nconstraints determine which of these expressions [the (n) largest, the (n) larger, the (n)\nlarge] is most appropriate in a given situation (Section 5).\n4.1.6 Inference. The replacement strategy, whereby one list of properties is transformed\ninto another, is essentially a simple kind of logical inference. L1 and L2, for instance, are\nguaranteed to single out the same set, given that exactly two mice are larger than 10 cm;\ngiven the content of the KB, the two lists are co-extensive. Once the numeral is dropped,\nhowever, as in L3, there is real loss of information: L3 can be used for characterizing\na number of sets, including the one characterized by L2. In any case, the properties in\nthese lists are logically distinct, so the choice between them belongs to CD.\n"},{"#tail":"\n","@confidence":"0.985436","#text":"\nIf the KB contains several gradable Attributes, a description can make use of several of\nthem, as in example (7). Even if only one gradable Attribute is represented, descriptions\nmay contain different adjectives, expressing opposites, as in example (8).\n"},{"#tail":"\n","@confidence":"0.9665965","#text":"\nneed for representing equalities separately, since they arise automatically, as combina-\ntions of opposites. Every equality of the form ?size(x) = m cm? is equivalent to the com-\nbination of a property of the form ?size(x) > i cm? and one of the form ?size(x) < j cm.?\nGiven the content of the following KB, for example, saying that the size of an object is\nbetween 6 and 12 cm amounts to saying that its size is 10 cm, and this is implemented\nby adding appropriate transformations to the generator.\n"},{"#tail":"\n","@confidence":"0.938395","#text":"\nvan Deemter GRE with Gradable Properties\nDifferent measures have to be taken when several vague Attributes are involved. Sup-\npose height has these Values:\n"},{"#tail":"\n","@confidence":"0.897824","#text":"\nAfter recompiling these into the form of inequalities (reiterating types):\n"},{"#tail":"\n","@confidence":"0.97160225","#text":"\nSuppose the target set is {c2, c3}. The algorithm will first select the property mouse,\nsince crisp properties are more preferred than vague ones (Result: C = {c1, c2, c3, c4}).\nThe sequel depends on preference order. Omitting the property of being a mouse for\nbrevity, possible results include the following:\n"},{"#tail":"\n","@confidence":"0.843709666666667","#text":"\nthan 8 cm and sized between 6 and 14 cm.\nAnalogous to Section 4.1, one might stop here. But there is scope here for logical\ninference, even more so than before; likewise, there are pitfalls, more than before.\n"},{"#tail":"\n","@confidence":"0.987661333333333","#text":"\nin examples (7) and (8), we need to transform a comparative property into a superlative\nproperty, moving from properties of the form ?height> x? to properties of the form ?the\ntallest n.? This can be done in different ways. For example, La may give rise to\n"},{"#tail":"\n","@confidence":"0.977729142857143","#text":"\nComputational Linguistics Volume 32, Number 2\nOnce we know which of these outcomes is preferable, the algorithm may be fine-tuned.\n(If brevity is an issue, e.g., then one might let a generation program vary the preference\norder used by the IA, then choose the outcome that is shortest.) The transformations\ndescribed so far rest on logical equivalence (modulo the KB). If numerals are omitted as\nwell, the result is usually no longer equivalent of course, and the description is at risk\nof becoming almost entirely uninformative (e.g., L2):\n"},{"#tail":"\n","@confidence":"0.989108","#text":"\nThe algorithm outlined in this and the previous section can be summarized as follows:\n"},{"#tail":"\n","@confidence":"0.98846875","#text":"\nIf gradable properties are less preferred than crisp ones (point 3) then this algorithm will\nonly use gradable properties if an entirely crisp distinguishing description is impossible.\nThis may well cause gradable properties to be underused. For this and other reasons,\nwe shall consider non-incremental versions of these ideas in Section 8.\n"},{"#tail":"\n","@confidence":"0.9984156","#text":"\nWe will examine the worst-case complexity of interpretation as well as generation to\nshed some light on the hypothesis that vague descriptions are more difficult to process\nthan others because they involve a comparison between objects (Beun and Cremers\n1998, Krahmer and Theune 2002). Before we do this, consider the tractability of the\noriginal IA. If the running time of FindBestValue(r, Ai) is a constant times the number of\nValues of the Attribute Ai, then the worst-case running time of IA (and IAPlur) is O(nvna),\nwhere na equals the number of Attributes in the language and nv the average number\nof Values of all Attributes. This is because, in the worst case, all Values of all Attributes\nneed to be attempted (van Deemter 2002). As for the new algorithm, we focus on the\ncrucial phases 2, 4, and 5.\n"},{"#tail":"\n","@confidence":"0.995128060606061","#text":"\nextension of the inequality.) Therefore, if the number of inequalities in the description is\nni then the complexity is O(ndni).\nThus, the complexity of GRE in the gradable case is determined by three steps: The\nfirst is quadratic and can be performed off-line, the second has a worst-case running\ntime of O(nvna), and the third one has a worst-case running time of O(ndni). Thus,\ngradable GRE takes only polynomial time, and if we focus on the part that cannot be\ndone off-line, it takes only linear time. In other words, gradable GRE does take more\ntime than nongradable GRE, but the difference seems modest.\nThe intuition that vague descriptions are more difficult than others is also confirmed\n(though again only to a modest extent) when we focus on the hearer. First, consider a\nnon-vague description consisting of a combination of n properties, P1, ..., Pn. To discover\nits referent, the denotation of the Boolean expression P1 ? .. ? Pn needs to be calculated,\nwhich takes just n?1 calculations of the form\nIntersect ?P1? ? ... ? ?Pi?1? (a set that has been computed already)\nwith ?Pi? (the extension of the next property in the description).\nIf computing the intersection of two sets takes constant time then this makes the com-\nplexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of\nproperties used. In a vague description, the property last added to the description is\ncontext dependent. Worst case, calculating the set corresponding with such a property,\nof the form size(x) = maxm, for example, involves sorting the distractors as to their\nsize, which may amount to O(n2d) or O(nd log nd) calculations (depending on the sorting\nalgorithm: cf. [Aho et al 1983] Chapter 8). Once again, the most time-consuming\npart of the calculation can be performed off-line, since it is the same for all referring\nexpressions.\nThus, the worst-case time complexity of interpretation is as follows: The part that can\nbe computed off-line takes O(nd log nd) calculations. The part that has to be computed\nfor each referring expression separately takes O(nd) calculations. Once again, there is a\ndifference with the nongradable case, but the difference is modest, especially regarding\nthe part that cannot be done off-line. One should bear in mind that worst-case theo-\nretical complexity is not always a good measure of the time that a program takes in\nthe kinds of cases that occur most commonly, let alne the difficulty for a person. For\nexample, it seems likely that hearers and speakers will have most difficulty dealing with\ndifferences that are too small to be obvious (e.g., two mice that are very similar in size).\n"},{"#tail":"\n","@confidence":"0.998842456521739","#text":"\nNLG has to do more than select a distinguishing description (i.e., one that unambigu-\nously denotes its referent; Dale 1989): The selected expression should also be felicitous.\nConsider the question, discussed in the philosophical logic literature, of whether it is\nlegitimate, for a gradable adjective, to distinguish between ?observationally indifferent?\nentities: Suppose two objects x and y, are so similar that it is impossible to distinguish\ntheir sizes; can it ever be reasonable to say that x is large and y is not? A positive\nanswer would not be psychologically plausible, since x and y are indistinguishable;\nbut a negative answer would prohibit any binary distinction between objects that are\nlarge and objects that are not, given that one can always construct objects x and y, one of\nwhich falls just below the divide while the other falls just above it. This is the strongest\nversion of the sorites paradox (e.g., Hyde 2002).\nOur approach to vague descriptions allows a subtle response: that the offending\nstatement may be correct yet infelicitous. This shifts the problem from asking when\nvague descriptions are ?correct? to the question of when they are used felicitously.\nFelicity is naturally thought of as a gradable concept. There is therefore no need for a\ngenerator to demarcate precisely between felicitous and infelicitous expressions, as long\nas all the utterances generated are felicitous enough. When in doubt, a generator should\navoid the expression in question. If x and y are mice of sizes 10 and 9.9 cm, for example,\nthen it is probably better to describe x as the largest mouse than as the large mouse.\nPrior to carrying out the experiments to be reported in Section 7, we believed that\nthe following constraints should be taken into account:\nSmall Gaps. Expressions of the form the (n) large [N] are infelicitous when the gap\nbetween (1) the smallest element of the designated set S (henceforth, s?) and (2) the\nlargest N smaller than all elements of S (henceforth, s+) is small in comparison with the\nother gaps (Thorisson 1994; Funakoshi et al 2004). If this gap is so small as to make the\ndifference between the sizes of s? and s+ impossible to perceive, then the expression is\nalso infelicitous.\nDichotomy. When separating one single referent from one distractor, the comparative\nform is often said to be favored (Use the comparative form to compare two things). We\nexpected this to generalize to situations where all the referents are of one size, and all\nthe distractors of another.\nMinimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference\nshould be given to the base form. In English, where the base form is morphologically\nsimpler than the other two, this rule could be argued to follow from Gricean principles\n(Grice 1975).\nTo keep matters simple, linguistic realization could choose the base form if and only if\nthe gap between s? and s+ surpasses a certain value, which is specified interactively by\nthe user. (This approach was chosen for the VAGUE program.)\nAs for the presence/absence of the numeral in the description, there appear to be\ndifferent ?believable? patterns of linguistic behavior. A cautious generator might only\nomit the numeral when the pragmatic principles happen to enforce a specific extension\n(e.g., the large mice, when the mice are sized 3, 2.8, 2.499, and 2.498 cm). This would allow\nthe generator to use vague expressions, but only where they result in a description that\nis itself unambiguous.\nWe shall see in Section 7 that it has not been easy to confirm the pragmatic con-\nstraints of the present section experimentally.\n"},{"#tail":"\n","@confidence":"0.979665333333333","#text":"\nSome recent GRE algorithms have done away with the separation between content de-\ntermination and linguistic realization, interleaving the two processes instead (Stone and\nWebber 1998; Krahmer and Theune 2002). We have separated the two phases because,\nin the case of vague descriptions, interleaving would tend to be difficult. Consider, for\ninstance, the list of properties L = ?size > 3 cm, size < 9 cm?. If interleaving forced\nus to realize the two properties in L one by one, then it would no longer be possible to\ncombine them into, for example, the largest mouse but one (if the facts in the KB support\nit), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size\n< 9 cm). Clearly, sophisticated use of gradable adjectives requires a separation between\nCD and linguistic realization, unless one is willing to complicate linguistic realization\nconsiderably.\nHaving said this, the distinction between CD and linguistic realization is not always\n"},{"#tail":"\n","@confidence":"0.971949173913043","#text":"\nand comparative forms is made, among other things.\nOne area of current interest concerns the left-to-right arrangement of premodifying\nadjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work\nin this area is often based on assigning adjectives to a small number of categories\n(e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives? relative\nposition. Interestingly, vague properties tend to be realized before others. Quirk et al\n(1985), for example, report that ?adjectives denoting size, length, and height normally\nprecede other nonderived adjectives? (e.g., the small round table is usually preferred to\nthe round small table).\nSemantically, this does not come as a surprise. In a noun phrase of the form the three\nsmall(-est) [N], for example, the words preceding N select the three smallest elements of\n[N]. It follows that, to denote the three smallest elements of the set of round tables, the\nonly option is to say the three small round tables, rather than the three round small tables.\nThe latter would mean something else, namely, the three round ones among the n small(est)\ntables (where n is not specified). It actually seems quite possible to say this, but only\nwhen some set of small tables is contextually salient (e.g., I don?t mean those small tables,\nI mean the three round ones). Given that n is unspecified, the noun phrase would tend to\nbe very unclear in any other context.\nThe VAGUE program follows Quirk?s rule by realizing gradable properties before\nnongradable ones, choosing some simple (and sometimes stilted) syntactic patterns.\n7. Empirical Grounding\nA full validation of a GRE program that generates vague descriptions would address\nthe following questions: (1) When is it natural to generate a vague description (i.e., a\n"},{"#tail":"\n","@confidence":"0.961891333333333","#text":"\nComputational Linguistics Volume 32, Number 2\nqualitative description as opposed to a purely quantitative one)? (2) Given that a vague\ndescription is used, which form of the description is most natural? and (3) Are the\ngenerated descriptions properly understood by hearers and readers? Much is unknown,\nbut we shall summarize the available results in these three areas very briefly, referring\nreaders to the literature for details.\n"},{"#tail":"\n","@confidence":"0.974930710526315","#text":"\nCommon sense (as well as the Gricean maxims; Grice 1975) suggests that vague de-\nscriptions are preferred by speakers over quantitative ones whenever the additional\ninformation provided by a quantitative description is irrelevant to the purpose of the\ncommunication. We are not aware of any empirical validation of this idea, but the fact\nthat vague descriptions are frequent is fairly well documented. Dale and Reiter (1995),\nfor example, discussed the transcripts of a dialogue between people who assemble\na piece of garden furniture (originally recorded by Candy Sidner). They found that,\nwhile instructional texts tended to use numerical descriptions like the 3 14 ? bolt, human\nassemblers ?unless they were reading or discussing the written instructions, in all cases used\nrelative modifiers, such as the long bolt? (Dale and Reiter 1995).6\nOur own experiments (van Deemter 2004) point in the same direction. In one ex-\nperiment, for example, 34 students at the University of Brighton were shown six pieces\nof paper, each of which showed two isosceles and approximately equilateral triangles.\nTriangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively. On\neach sheet, one of the two triangles had been circled with a pencil. We asked subjects to\nimagine themselves on the phone to someone who held a copy of the same sheet, but\nnot necessarily with the same orientation (e.g., possibly upside down), and to complete\nthe answers in the following:\nQ: Which triangle on this sheet\nwas circled?\nA: The ............ triangle.\nThis setup was used for testing a number of hypotheses. What is relevant for current\npurposes is that all except one subject used qualitative size-related descriptions (the big\ntriangle, the largest triangle, etc.) in the majority of cases. As many as 27 of the 34 subjects\nused such descriptions in all cases.\nIt seems likely that qualitative descriptions would be less frequent if speakers\nwere offered an easy way to determine the relevant measurements (e.g., if a ruler was\nprovided). As it was, subjects went for the easy option, relying on a comparison of sizes\nrather than on an estimation of their absolute values. Further experiments are needed\nbefore we can say with more confidence under what circumstances vague descriptions\nare favored over absolute ones.\nIt is normally perhaps unlikely that people produce language on the basis of the\nkind of numerical representations that our algorithm has used as input. Although psy-\nchological plausibility is not our aim, it is worth noting that the inequalities computed\nas step 2 of the algorithm of Section 4 might be psychologically more plausible, since\nthey are essentially no more than comparisons between objects.\n6 Presumably, Beun and Cremers (1998) found vague adjectives to be rare because, in their experiments,\nreferents could always be identified using nongradable dimensions.\n"},{"#tail":"\n","@confidence":"0.456842","#text":"\nvan Deemter GRE with Gradable Properties\n"},{"#tail":"\n","@confidence":"0.984233260869565","#text":"\nSedivy et al (1999) asked subjects to identify the target of a vague description in a visual\nscene. Consider the tall cup. The relevant scene would contain three distractors: (1) a less\ntall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind\nof object that previous studies had shown to be intermediate in height (e.g., a pitcher\nthat, while being taller than both cups, was neither short nor tall for a pitcher), and (3)\na different type of object to which the adjective is inapplicable (e.g., a door key). Across\nthe different conditions under which the experiment was done (e.g., allowing subjects\nto study the domain before or after the onset of speech), it was found not to matter\nmuch whether the adjective applied ?intrinsically? to the target object (i.e., whether the\ntarget was tall for a cup): Hearers identifed the target without problems in both types\nof situations. The time subjects took before looking at the target for the first time was\nmeasured, and although these latency times were somewhat greater when the referent\nwere not intrinsically tall than when they were, the average difference was tiny at 554\nversus 538 miliseconds. Since latency times are thought to be sensitive to most of the\nproblems that hearers may have in processing a text, these results suggest that, for\ndimensional adjectives, it is forgivable to disregard global context.\nTo get an idea of whether our plural descriptions are understood correctly by human\nreaders, we showed subjects sequences of numbers, exactly two of which appeared in\nbrackets, along with the following instructions:\nSuppose you want to inform a hearer *which numbers in\na given list appear in brackets*, where the hearer\nknows what the numbers are, but not which of them appear\nin brackets. Forexample, the hearer knows that the\n"},{"#tail":"\n","@confidence":"0.9726015","#text":"\nYou, as a speaker, know that only the two occurrences\nof the number 7 appear in brackets:\n"},{"#tail":"\n","@confidence":"0.998517111111111","#text":"\nOur question to you is: Would it be *correct* to convey\nthis information by saying ??The two high numbers appear\nin brackets???\nThe outcomes of the experiment suggested that readers understand plural vague de-\nscriptions in accordance with the semantics of Section 2 (van Deemter 2000). In other\nwords, they judged the description to be correct if and only if the two highest numbers\nin the sequence appeared in brackets.\nAssessing the evidence, it seems that vague descriptions are largely unproblematic\nfrom the point of view of interpretation.\n"},{"#tail":"\n","@confidence":"0.9819975","#text":"\nHow can we choose between the different forms that a vague description can take?\nReiter and Sripada (2002) showed that the variation in corpora based on expert authors\n"},{"#tail":"\n","@confidence":"0.9114565","#text":"\nComputational Linguistics Volume 32, Number 2\ncan be considerable, especially in their use of vague expressions (e.g., by evening, by late\nevening, around midnight). We confirmed these findings using experiments with human\nsubjects (van Deemter 2004), focusing on the choice between the different forms of the\n"},{"#tail":"\n","@confidence":"0.963968333333334","#text":"\nconfirm: Even when the gap was large, base forms were often dispreferred.\nThe validity of these results can be debated (van Deemter 2004) but, taking them at face\nvalue, one could base different generation strategies on them. For example, one might\nuse the superlative all the time, since this was?surprisingly?the most frequent form\noverall. Based on point (2), however, one might also defend using the base form when-\never the gap is large enough (as was done in the VAGUE program). Future experiments\nshould allow us to refine this position, perhaps depending on factors such as genre,\ncommunicative goal, and type of audience.\n8. Incrementality: Help or Hindrance?\nThe account sketched in Section 4 was superimposed on an incremental GRE algorithm,\npartly because incrementality is well established in this area (Appelt 1985; Dale and\nReiter 1995). But IA may be replaced by any other reasonable7 GRE algorithm, for\nexample, one that always exactly minimizes the number of properties expressed, or\none that always ?greedily? selects the property that removes the maximum number of\ndistractors. Let G be any such GRE algorithm, then we can proceed as follows:\n"},{"#tail":"\n","@confidence":"0.771804166666667","#text":"\nto generate the same descriptions as in Sections 4.1 and 4.2, then\ninequalities go last.) Delete any inequalities that do not remove any\ndistractors.\n7 Concretely, we require of a reasonable GRE algorithm that it avoid combining logically comparable\ninequalities, such as size(x) > 10 and size(x) > 20, inside one description. All GRE algorithms\nthat we know of fulfill this requirement.\n"},{"#tail":"\n","@confidence":"0.954414","#text":"\nImposing a linear order (4) is a necessary preparation for (5) because the super-\nlative properties resulting from (5), unlike the inequalities resulting from (4), are\ncontext dependent. For example, ?mouse, size(x) = max2? (the largest two mice,\n{c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two\nelements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the\nshort(est) black mouse if there is only one black mouse, because this might invite false\nimplicatures.\n"},{"#tail":"\n","@confidence":"0.997879666666666","#text":"\nWhile IA is generally thought to be consistent with findings on human language pro-\nduction (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982),\nthe hypothesis that incrementality is a good model of human GRE seems unfalsifiable\nuntil a preference order is specified for the properties on which it operates. (Wildly\nredundant descriptions can result if the ?wrong? preference order are chosen.) We shall\nsee that vague descriptions pose particular challenges to incrementality.\nOne question emerges when the IA is combined with findings on word order and\nincremental interpretation. If human speakers and/or writers perform CD incrementally,\nthen why are properties not expressed in the same order in which they were selected?\nThis question is especially pertinent in the case of vague expressions, since gradable\nproperties are selected last, but realized first (Section 6). This means that the linguistic\nrealization cannot start until CD is concluded, contradicting eye-tracking experiments\nsuggesting that speakers start speaking while still scanning distractors (Pechmann\n1989). A similar problem is discussed in the psycholinguistics of interpretation (Sedivy\net al 1999): Interpretation is widely assumed to proceed incrementally, but vague de-\nscriptions resist strict incrementality, since an adjective in a vague description can only\nbe fully interpreted when its comparison set is known. Sedivy and colleagues resolve\nthis quandary by allowing a kind of revision, whereby later words allow hearers to\nrefine their interpretation of gradable adjectives. Summarizing the situation in gener-\nation and interpretation, it is clear that the last word on incrementality has not been\nsaid.\n"},{"#tail":"\n","@confidence":"0.999714666666667","#text":"\nIt has been argued that, in an incremental approach, gradable properties should be\ngiven a low preference ranking because they are difficult to process (Krahmer and\nTheune 2002). We have seen in Section 4.3 that generation and interpretation of vague\ndescriptions does have a slightly higher computational complexity than that of non-\nvague descriptions. Yet, by giving gradable properties a low ranking, we might cause\nthe algorithm to underuse them, for example, in situations where gradable properties\nare highly relevant to the purpose of the discourse (e.g., a fist fight between people of\nvery different sizes). Luckily, there are no semantic or algorithmic reasons for giving\ngradables a low ranking. Let us see how things would work if they were ranked more\nhighly.\nSuppose comparative properties do not go to the end of the preference list. After\ntransformation into superlative properties, this alternative preference ranking could\n"},{"#tail":"\n","@confidence":"0.770015444444444","#text":"\nComputational Linguistics Volume 32, Number 2\nlead to a list like ?mouse, size(x) = min4, brown, weight(x) = max2?, where two or-\ndinary properties are separated by a superlative one. A direct approach to realization\nmight word this as the two heaviest brown ones among the smallest four mice. To avoid such\nawkward expressions, one can change the order of properties after CD (mirroring step\n4 above), moving the inequalities to the end of the list before they are transformed into\nthe appropriate superlatives. The effect would be to boost the number of occurrences of\ngradable properties in generated descriptions while keeping CD incremental.\n9. Extensions of the Approach\n"},{"#tail":"\n","@confidence":"0.9961894","#text":"\nSome generalizations of our method are fairly straightforward. For example, consider\na relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as\nin the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is\nnot difficult once relational descriptions are integrated with a standard GRE algorithm\n(Krahmer and Theune 2002, Section 8.6.2): Suppose an initial description is generated\ndescribing the set of all those dogs that are in sheds over a given size (say, size 5); if this\ndescription happens to distinguish an individual dog then this legitimizes the use of the\nnoun phrase the dog in the large shed. Note that this is felicitous even if the shed is not\nthe largest one in the domain, as is true for d2 in the following situation (contains-a=b\nmeans that a is contained by b):\n"},{"#tail":"\n","@confidence":"0.999302333333333","#text":"\nIn other words, the dog in the large shed denotes ?the dog such that there is no other shed\nthat is equally large or larger and that contains a dog?. Note that it would be odd, in the\nabove-sketched situation, to say the dog in the largest shed.\n"},{"#tail":"\n","@confidence":"0.919861","#text":"\nGeneralizations to complex Boolean descriptions involving negation and disjunction\n(van Deemter 2004) appear to be largely straightforward, except for issues to do with\n"},{"#tail":"\n","@confidence":"0.820146","#text":"\nvan Deemter GRE with Gradable Properties\nopposites and markedness. For example, the generator will have to decide whether to\nsay the patients that are old or the patients that are not young.\n"},{"#tail":"\n","@confidence":"0.994957285714286","#text":"\ndimensions, these dimensions can be weighed in different ways (e.g., Rasmusen 1989).\nLet us focus on references to an individual referent r, starting with a description that\ncontains more than one gradable adjective. The NP the tall fat giraffe, for example,\ncan safely refer to an element b in a situation like the one below, where b is the only\nelement that exceeds all distractors with respect to some dimension (a different one\nfor a than for c, as it happens) while not being exceeded by any distractors in any\ndimension:\n"},{"#tail":"\n","@confidence":"0.959196666666667","#text":"\nCases like this would be covered if the decision-theoretic property of Pareto optimality\n(e.g., Feldman 1980) was used as the sole criterion: Formally, an object r ? C has a\nPareto-optimal combination of Values V iff there is no other x ? C such that\n"},{"#tail":"\n","@confidence":"0.99566525","#text":"\nIn our example, b is the only object that has a Pareto-optimal combination of Values,\npredicting correctly that b can be called the tall fat giraffe. It seems likely, however, that\npeople use doubly graded descriptions more liberally. For example, if the example is\nmodified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still\nbe the only reasonable referent of the tall fat giraffe. Many alternative strategies are pos-\nsible. The Nash arbitration plan, for example, would allow a doubly graded description\nwhenever the product of the Values for the referent r exceeds that of all distractors (Nash\n1950; cf. Gorniak and Roy 2003; Thorisson 1994, for other plans).\n"},{"#tail":"\n","@confidence":"0.998783545454546","#text":"\nthrough the backdoor. Consider big, for example, when applied to 3D shapes. If there\nexists a formula for mapping three dimensions into one (e.g., length ? width ? height)\nthen the result is one dimension (overall-size), and the algorithm of Section 4 can be\napplied verbatim. But if big is applied to a person then it is far from clear that there is one\ncanonical formula for mapping the different dimensions of your body into one overall\ndimension, and this complicates the situation. Similar things hold for multifaceted\nproperties like intelligence (Kamp 1975).\nColor terms are a case apart. If color is modeled in terms of saturation, hue, and lumi-\nnosity, for instance, then an object a may be classified as greener than b on one dimension\n(e.g., saturation), but less green than b on another (e.g., hue). This would considerably\ncomplicate the application of our algorithm to color terms, which is otherwise mostly\n"},{"#tail":"\n","@confidence":"0.994423368421053","#text":"\nComputational Linguistics Volume 32, Number 2\nstraighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs,\nwould refer to the one that is closest to prototypical green.) A further complication\nis that different speakers can regard very different values as prototypical, making it\ndifficult to assess which of two objects is greener even on one dimension (Berlin and Kay\n1969, pages 10?12). (Ideally, GRE should also take into account that the meaning of color\nwords can differ across different types of referent. Red as in red hair, e.g., differs from\nred as in red chair.)\nDifferent attitudes towards multidimensionality are possible. One possibility is to\nbe cautious and to keep aiming for distinguishing descriptions in the strict sense. In\nthis case, the program should limit the use of vague descriptions to situations where\nthere exists a referent that has a Pareto-optimal combination of Values. Alternatively,\none could allow referring expressions to be ambiguous. It would be consistent with this\nattitude, for example, to map multiple dimensions into one overall dimension, perhaps\nby borrowing from principles applied in perceptual grouping, where different perceptual\ndimensions are mapped into one (e.g., Thorisson 1994). The empirical basis of this line\nof work, however, is still somewhat weak, so the risk of referential unclarity looms large.\nAlso, this attitude would go against the spirit of GRE, where referring expressions have\nalways been assumed to be distinguishing.\n"},{"#tail":"\n","@confidence":"0.976947125","#text":"\nWe shall see that a natural treatment of salience falls automatically out of our treatment\nof vague descriptions. As we shall see, this will allow us to simplify the structure of GRE\nalgorithms, and it will explain why many definite descriptions that look as if they were\ndistinguishing descriptions are actually ambiguous.\n9.4.1 A New Perspective on Salience. Krahmer and Theune (2002) have argued that\nDale and Reiter?s (1995) dichotomy between salient and nonsalient objects (where the\nobjects in the domain are the salient ones) should be replaced by an account that takes\ndegrees of salience into account: No object can be too unsalient to be referred to, as long\nas the right properties are available. In effect, this proposal (which measured salience\nnumerically) analyzes the black mouse as denoting the unique most salient object in the\ndomain that is both black and a mouse. Now suppose we let GRE treat salience just like\nother gradable Attributes. Suppose there are ten mice, five of which are black, whose\ndegrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other\nobjects in the domain (cats, white mice) all have a higher salience. Then our algorithm\nmight generate this list of properties:\nL = ?mouse, black, salience > 4?.\nThis is a distinguishing description for the black mouse whose salience is 5: the most\nsalient black mouse. The simpler description the black mouse can be derived by stipulating\nthat the property of being most salient can be left implicit in English. The salience\nAttribute has to be taken into account by CD, however, and this can be ensured in various\nways. For example, instead of testing whether C ? [[?Ai, Vi?]] = {r}, one tests whether r\nis the most salient element of C ? [[?Ai, Vi?]]. Alternatively, the algorithm might proceed\nas usual, performing the usual test (involving C ? [[?Ai, Vi?]] = {r}) but starting with\na reduced domain, consisting of the things that are at least as salient as the target r:\n"},{"#tail":"\n","@confidence":"0.339033666666667","#text":"\nvan Deemter GRE with Gradable Properties\nDomain := {x ? Domain: salience(x) ? r}. The two approaches are equivalent in many\nsituations.\n"},{"#tail":"\n","@confidence":"0.994738228571429","#text":"\nare often ambiguous. Taking salience into account as suggested above, the singular the\nblack mouse can only refer to the most salient mouse. But the mice can refer to the most\nsalient two (sized 5 and 4), the most salient three (sized 5, 4, and 3), or to all of them. To\ndisambiguate the description, something like a number can be used (e.g., the two mice),\njust as in the case of vague descriptions.\nWhen salience is combined with other gradable notions, the likelihood of unclarity\nis even greater. Consider the large(st) dog. Our analysis predicts ambiguity when size\nand salience do not go hand in hand.\nType: d1 (dog), d2 (dog), d3 (dog), d4 (dog), c5 (cat)\nSize: d1 (20 cm), d2 (50 cm), d3 (70 cm), d4 (60 cm), c5 (50 cm)\nSalience: d1 (6), d2 (4), d3 (3), d4 (5), c5 (6).\nIf we are interested in the three most salient dogs (d1, d2, and d4) then the large(est) dog\ndesignates d4, but if we are interested in the four most salient ones (d1, d2, d3, and d4),\nthen it designates d3, for example. In other words, the description is ambiguous between\nd3 and d4, depending on whether we attach greater importance to salience or size. This\nis borne out by our generation algorithm. Consider the simpler of the two treatments of\nsalience, for example, which starts out with a reduced domain. If d4 is the target then the\nreduced domain (consisting of all things at least as salient as the target) is {d1, d2, d4, c5};\ndog narrows this down to {d1, d2, d4}, after which size = max1 generates the large dog.\nBut if d3 is the target then the same procedure applies, this time starting with the full\ndomain (since no element is less salient that d3) and the same description is generated\nto refer to a different animal. For a reader, clearly, salience and gradable adjectives are\na problematic combination. This should come as no surprise, since salience itself is a\ngradable property, and combinations of gradable properties are always problematic, as\nwe have seen in the previous section.\n9.4.3 Salience as a Multidimensional Property. Note that salience itself is multidimen-\nsional. Consider two people talking about the railway station, when one railway station\nis near but of only minor importance (e.g., only few trains stop there), while another is\nfurther afield but of greater significance for travel. In such a situation, it can be unclear\nwhich of the two railway stations is intended. Without more empirical research, we\ncannot know how people combine salience with other dimensions.\nGRE has usually assumed that distinguishing descriptions are the norm, but once\nsalience is taken into account (especially in combination with plurals and/or other\ngradable dimensions) it becomes difficult to generate descriptions that are immune to\nbeing misunderstood.\n"},{"#tail":"\n","@confidence":"0.94505","#text":"\n9.5.1 Nouns. Two other generalizations are worth mentioning. The first involves a\nclass of descriptions that do not involve any overt gradable adjectives. Color terms,\nfor example (cf., Section 9.1), are applicable to different degrees, and the same is true\n"},{"#tail":"\n","@confidence":"0.9731345","#text":"\nComputational Linguistics Volume 32, Number 2\nfor many other nouns, such as girl, which involves a vaguely defined age. Similar\nclaims can be made about less obvious cases. Consider a gathering containing one\nfamous professor (a), one junior lecturer (b), one Ph.D. student (c), and a policewoman\n(e). Then the word academic might denote (a), but also (b) or (c). Accordingly, each\nof the following referring expressions appears viable, mirroring examples 3 and 4 of\n"},{"#tail":"\n","@confidence":"0.995484","#text":"\nThese descriptions are easily generated on the basis of a KB that involves Values rep-\nresenting degrees of being an academic, the more so because our approach generalizes\nto ordinal measurements (except for Small Gaps (Section 5), which requires an inter-\nval or ratio scale, since it involves an assessment of the size of the gap between\nValues). Note that this treatment could cover all those nouns that are used with various\ndegrees of strictness. It is difficult to say how many nouns fall in this category, but the\nphenomenon is believed to be widespread. This is, for example, one of the central tenets\nof Prototype Theory (Rosch 1975). An uncomfortable consequence of these observations\nis that it is no longer obvious which words denote a crisp property, and which a gradable\nproperty. (For example, it is not clear whether GRE should treat academic as gradable.)\n9.5.2 Pointing. To show that vagueness is also inherent in multimodal communication,\nimagine the same gathering, but with some more people present. Suppose someone\npoints at the centre of the gathering. (See below, where W denotes women.) If the distance\nbetween pointer and pointee is considerable then the boundaries of the region pointed\nto are not exactly defined: e is definitely pointed at, but d and f might be doubtful:\n"},{"#tail":"\n","@confidence":"0.995909636363636","#text":"\nHere, {e} is a possible referent, and so are {d, e, f} and perhaps {c, d, e, f, g}. The set {d, f}\nis not, since there is a gap between its two elements. If precise pointing is represented\nas a crisp property whose denotation equals the set of elements pointed at (Krahmer\nand Van der Sluis 2003), then vague pointing can be incorporated in our algorithm\nby representing it by a gradable property: We let the generator use a KB that involves\nnumerical degrees of being pointed at, where this degree is highest for e, next highest\nfor d and f , and so on. If this is done, the generator can generate these two women, along\nwith a pointing gesture like the one in our example, to refer to {e, f}. No changes to\nthe algorithm of Section 4 are necessary. A variant of this approach arises if pointing\nis modeled as a way of establishing degrees of salience (i.e., the closer to the center of\npointing, the higher the value for the attribute SALIENCE) in the style of Section 9.2.\n"},{"#tail":"\n","@confidence":"0.994333564102564","#text":"\nIf the usefulness of NLG resides in its ability to present data in human-accessible form,\nthen vagueness must surely be one of its central instruments, because it allows the\nsuppression of irrelevant detail. In principle, this might be done by providing the gen-\nerator with vague input?in which case no special algorithms are needed?but suitably\ncontextualized vague input is often not available (Mellish 2000). The only practical\nalternative is to provide the generator with ?crisp? (i.e., quantitative) input, allowing the\ngenerator to be hooked on to a general-purpose database. It is this avenue that we have\nexplored in this article, in combination with various (incremental and other) approaches\nto GRE.\nFar from being a peculiarity of a few adjectives, vagueness is widespread. We\nbelieve that our approach can be applied to a variety of situations in which vagueness\naffects referring expressions including, for example,\n color terms (Section 9.3);\n nouns that allow different degrees of strictness (Section 9.5);\n degrees of salience (Section 9.4); and\n imprecise pointing (Section 9.5).\nOn the other hand, we have also met some considerable obstacles on our way:\nExpressive choice (Sections 4 and 7). By enabling the generator to produce more\nreferring expressions, we have made it harder to choose between them. For example,\nwhen is a qualitative description preferable over a quantitative one? At a more detailed\nlevel, the generator must choose between descriptions like the heaviest two of the\nsmallest three mice, the mice that weigh between 40 and 60 grams, and so on, each of which\nmay single out the same individuals. Section 7.3 has summarized some experimental\nevidence related to such choices, focusing on the different forms of the adjective, but the\nevidence is far from conclusive. Much is still unknown, differences between speakers\nabound, and the experimental methodology for advancing the state of the art in this\narea is not without its problems (van Deemter 2004).\nArchitecture (Section 6). The inference rules that were necessary to convert one list\nof properties into another do not sit comfortably within the received NLG pipeline\nmodel (e.g., Reiter and Dale 2000). An example of such an inference rule is the one\nthat transforms a list of the form ?mouse, >10 cm? into one of the form ?mouse,\nsize(x) = max2? if only two mice are larger than 10 cm. The same issues also make\nit difficult to interleave CD and linguistic realization as proposed by various authors,\nbecause properties may need to be combined before they are expressed.\nIncrementality (Section 8). Gradable adjectives complicate the notion of incrementality,\nin generation as well as interpretation. Focusing on generation, for example, they force\nus to reexamine the idea that properties can be put into words more or less as soon\nas they have been selected by content determination (even apart from the issue noted\nunder Architecture).\n"},{"#tail":"\n","@confidence":"0.99428","#text":"\nComputational Linguistics Volume 32, Number 2\nAdjectives and presuppositions (Section 2). Our generation-oriented perspective\nsheds some doubt on Bierwisch?s (1989) claim that dimensional adjectives are\ninsensitive to standards provided by the global context: If a man?s height is 205 cm,\nthen surely no local context can make it felicitous (as opposed to just humorous) to\nrefer to him as the short man. A related issue that we have not touched upon is the fact\nthat adjectives are often used partly to pass judgement: One and the same car might\nbe designated as the expensive car by a hesitant customer and as the luxury car by an\neager salesman: Even if expense and luxury go hand in hand, the two adjectives have\ndifferent connotations, and this is something that a generator would ideally be aware of.\nMultidimensionality (Section 9.3). We know roughly how to deal with one gradable\ndimension: the short man, for example, is the shortest man around. But in practice, we\noften juggle several dimensions. This happens, for example, when two adjectives are\nused (the short thin man), or when salience is taken into account (e.g., the short man,\nwhen the shortest man is not the most salient one), threatening to make irrefutably\ndistinguishing descriptions something of an exception. (For a study of approaches to\nmultidimensionality in a different area, see Masthoff 2004.) At some point, GRE may\nhave to abandon the strategy of aiming for unambiguous descriptions in all situations.\n"},{"#tail":"\n","@confidence":"0.932447875","#text":"\nand Sebastian Varges for helpful comments. I\nam especially grateful to Richard Power, for\ninspiration as well as for implementing the\nVAGUE program at great speed. Thanks are\ndue to four anonymous reviewers for some\nvery substantial contributions. This work has\nbeen supported by the EPSRC (GR/S13330,\nTUNA project).\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.974582","#text":"\nUniversity of Aberdeen\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.835912","@genericHeader":"method","#text":"\n2. The Meaning of Vague Descriptions\n"},{"#tail":"\n","@confidence":"0.424386","@genericHeader":"method","#text":"\n3. Generation of Crisp Descriptions\n"},{"#tail":"\n","@confidence":"0.64173","@genericHeader":"method","#text":"\n3 The degree of precision of the measurement (James et al 1996, Section 1.5) determines which objects can\n"},{"#tail":"\n","@confidence":"0.40253","@genericHeader":"method","#text":"\n4 To turn this likelihood into a certainty, one can add a test at the end of the algorithm, which adds a\n"},{"#tail":"\n","@confidence":"0.899985","@genericHeader":"method","#text":"\n5. Pragmatic Constraints\n"},{"#tail":"\n","@confidence":"0.981752","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.984202","@genericHeader":"references","#text":"\nReferences\n"}],"page":[{"#tail":"\n","@confidence":"0.998596","#text":"\n196\n"},{"#tail":"\n","@confidence":"0.999267","#text":"\n197\n"},{"#tail":"\n","@confidence":"0.997031","#text":"\n198\n"},{"#tail":"\n","@confidence":"0.998511","#text":"\n199\n"},{"#tail":"\n","@confidence":"0.979306","#text":"\n200\n"},{"#tail":"\n","@confidence":"0.994019","#text":"\n201\n"},{"#tail":"\n","@confidence":"0.987511","#text":"\n202\n"},{"#tail":"\n","@confidence":"0.993407","#text":"\n203\n"},{"#tail":"\n","@confidence":"0.992913","#text":"\n204\n"},{"#tail":"\n","@confidence":"0.987671","#text":"\n205\n"},{"#tail":"\n","@confidence":"0.998384","#text":"\n206\n"},{"#tail":"\n","@confidence":"0.982018","#text":"\n207\n"},{"#tail":"\n","@confidence":"0.988517","#text":"\n208\n"},{"#tail":"\n","@confidence":"0.978546","#text":"\n209\n"},{"#tail":"\n","@confidence":"0.878466","#text":"\n210\n"},{"#tail":"\n","@confidence":"0.98525","#text":"\n211\n"},{"#tail":"\n","@confidence":"0.982655","#text":"\n212\n"},{"#tail":"\n","@confidence":"0.993165","#text":"\n213\n"},{"#tail":"\n","@confidence":"0.995352","#text":"\n214\n"},{"#tail":"\n","@confidence":"0.988666","#text":"\n215\n"},{"#tail":"\n","@confidence":"0.995641","#text":"\n216\n"},{"#tail":"\n","@confidence":"0.985984","#text":"\n217\n"},{"#tail":"\n","@confidence":"0.976996","#text":"\n218\n"},{"#tail":"\n","@confidence":"0.974165","#text":"\n219\n"},{"#tail":"\n","@confidence":"0.83702","#text":"\n220\n"},{"#tail":"\n","@confidence":"0.917744","#text":"\n221\n"},{"#tail":"\n","@confidence":"0.997949","#text":"\n222\n"}],"table":{"#tail":"\n","@confidence":"0.328462","#text":"\nComputational Linguistics Volume 32, Number 2\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.920026","#tail":"\n","@no":"0","#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.990817","#text":"University of Aberdeen"},"author":{"#tail":"\n","@confidence":"0.997924","#text":"Kees van_Deemter"},"abstract":{"#tail":"\n","@confidence":"0.989907","#text":"This article examines the role of gradable properties in referring expressions from the perspective of natural language generation. First, we propose a simple semantic analysis of vague descriptions (i.e., referring expressions that contain gradable adjectives) that reflects the contextdependent meaning of the adjectives in them. Second, we show how this type of analysis can inform algorithms for the generation of vague descriptions from numerical data. Third, we ask when such descriptions should be used. The article concludes with a discussion of salience and pointing, which are analyzed as if they were gradable adjectives."},"title":{"#tail":"\n","@confidence":"0.998846","#text":"Generating Referring Expressions that Involve Gradable Properties"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"date":{"#tail":"\n","#text":"1983"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" ?Pi? (the extension of the next property in the description). If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used. In a vague description, the property last added to the description is context dependent. Worst case, calculating the set corresponding with such a property, of the form size(x) = maxm, for example, involves sorting the distractors as to their size, which may amount to O(n2d) or O(nd log nd) calculations (depending on the sorting algorithm: cf. [Aho et al 1983] Chapter 8). Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions. Thus, the worst-case time complexity of interpretation is as follows: The part that can be computed off-line takes O(nd log nd) calculations. The part that has to be computed for each referring expression separately takes O(nd) calculations. Once again, there is a difference with the nongradable case, but the difference is modest, especially regarding the part that cannot be done off-line. One should bear in mind that worst-case theoretical co","@endWordPosition":"6562","@position":"38580","annotationId":"T1","@startWordPosition":"6559","@citStr":"Aho et al 1983"}},"title":{"#tail":"\n","#text":"Data Structures and Algorithms."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Aho, Alfred V., John E. Hopcroft, and Jeffrey D. Ullman. 1983. Data Structures and Algorithms. Addison-Wesley Publishing Company, Reading, MA. Appelt, Doug. Planning English referring expressions. Artificial Intelligence, 26:1?33."},"journal":{"#tail":"\n","#text":"Artificial Intelligence,"},"#text":"\n","pages":{"#tail":"\n","#text":"26--1"},"marker":{"#tail":"\n","#text":"Aho, Hopcroft, Ullman, 1983"},"publisher":{"#tail":"\n","#text":"Addison-Wesley Publishing Company,"},"location":{"#tail":"\n","#text":"Reading, MA."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alfred V Aho"},{"#tail":"\n","#text":"John E Hopcroft"},{"#tail":"\n","#text":"Jeffrey D Ullman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1986"},"editor":{"#tail":"\n","#text":"Reprinted in: B. J. Grosz, K. Sparck Jones, and B. L. Webber, editors"},"rawString":{"#tail":"\n","#text":"Reprinted in: B. J. Grosz, K. Sparck Jones, and B. L. Webber, editors (1986). Readings in Natural Language Processing. Morgan Kaufmann, Los Altos, CA."},"#text":"\n","marker":{"#tail":"\n","#text":"1986"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann,"},"location":{"#tail":"\n","#text":"Los Altos, CA."},"booktitle":{"#tail":"\n","#text":"Readings in Natural Language Processing."},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"1983"},"rawString":{"#tail":"\n","#text":"Barwise, Jon and John Perry. 1983. Situations and Attitudes. MIT Press, Cambridge, MA."},"#text":"\n","marker":{"#tail":"\n","#text":"Barwise, Perry, 1983"},"publisher":{"#tail":"\n","#text":"MIT Press,"},"location":{"#tail":"\n","#text":"Cambridge, MA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":", we take the base form of 2 The reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or vertical distance, or some combination of dimensions (Kamp 1975; also Section 8.1 of the present article). 197 Computational Linguistics Volume 32, Number 2 the adjective to be semantically equivalent to the superlative form (and, analogously, the comparative): The n large mice = The largest n mice The large mice = The largest mice The large mouse = The largest mouse. Viewed in this way, gradable adjectives are an extreme example of the ?efficiency of language? (Barwise and Perry 1983): Far from meaning something concrete like ?larger than 8 cm??a concept that would have very limited applicability?or even something more general like ?larger than the average N,? a word like large is applicable across a wide range of different situations. 2.2 Caveat: Full NP Anaphora Having said this, there are pragmatic differences between the base form and the superlative (Section 5). For example, the equivalence does not take anaphoric uses into account, such as when the large mouse is legitimized by the fact that the mouse has been called large before, as in (5) I was transfixed by a larg","@endWordPosition":"1669","@position":"10203","annotationId":"T2","@startWordPosition":"1666","@citStr":"Barwise and Perry 1983"},{"#tail":"\n","#text":"are used in the base form, the superlative, or the comparative. Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it. Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless. In other words, existing treatments of gradables in GRE fail to take the ?efficiency of language? into account (Barwise and Perry 1983; see our Section 2). 200 van Deemter GRE with Gradable Properties 4. The Generation of Vague Descriptions We now turn to the question of how vague descriptions may be generated from numerical data. We focus on semantic issues, postponing discussion of pragmatics until Section 5, and linguistic realization until Section 6. We shall make occasional reference to a PROLOG program called VAGUE, designed by Richard Power, which implements a version of the algorithm described in this section. Code and documentation for VAGUE can be found at http://www.csd.abdn.ac.uk/?kvdeemte/vague.html. 4.1 Express","@endWordPosition":"3225","@position":"19196","annotationId":"T3","@startWordPosition":"3222","@citStr":"Barwise and Perry 1983"}]},"title":{"#tail":"\n","#text":"Situations and Attitudes."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jon Barwise"},{"#tail":"\n","#text":"John Perry"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1969"},"rawString":{"#tail":"\n","#text":"Berlin, Brent and Paul Kay. 1969. Basic Color Terms. University of California Press, Berkeley."},"#text":"\n","marker":{"#tail":"\n","#text":"Berlin, Kay, 1969"},"publisher":{"#tail":"\n","#text":"University of California Press,"},"location":{"#tail":"\n","#text":"Berkeley."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue). This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly 215 Computational Linguistics Volume 32, Number 2 straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.) A further complication is that different speakers can regard very different values as prototypical, making it difficult to assess which of two objects is greener even on one dimension (Berlin and Kay 1969, pages 10?12). (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair.) Different attitudes towards multidimensionality are possible. One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense. In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values. Alternatively, one could allow referring expressions to be ambiguous. ","@endWordPosition":"10764","@position":"64354","annotationId":"T4","@startWordPosition":"10761","@citStr":"Berlin and Kay 1969"}},"title":{"#tail":"\n","#text":"Basic Color Terms."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Brent Berlin"},{"#tail":"\n","#text":"Paul Kay"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Berry, Dianne C., Peter R. Knapp, and Theo Raynor. 2002. Is 15 percent very common? Informing people about the risks of medication side effects. International Journal of Pharmacy Practice, 10:145?151."},"journal":{"#tail":"\n","#text":"International Journal of Pharmacy Practice,"},"#text":"\n","pages":{"#tail":"\n","#text":"10--145"},"marker":{"#tail":"\n","#text":"Berry, Knapp, Raynor, 2002"},"title":{"#tail":"\n","#text":"Is 15 percent very common? Informing people about the risks of medication side effects."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dianne C Berry"},{"#tail":"\n","#text":"Peter R Knapp"},{"#tail":"\n","#text":"Theo Raynor"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Beun, Robbert-Jan and Anita Cremers. 1998. Object reference in a shared domain of conversation. Pragmatics and Cognition, 6(1/2):121?152."},"#text":"\n","pages":{"#tail":"\n","#text":"6--1"},"marker":{"#tail":"\n","#text":"Beun, Cremers, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"able properties are less preferred than crisp ones (point 3) then this algorithm will only use gradable properties if an entirely crisp distinguishing description is impossible. This may well cause gradable properties to be underused. For this and other reasons, we shall consider non-incremental versions of these ideas in Section 8. 4.3 Computational Complexity We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects (Beun and Cremers 1998, Krahmer and Theune 2002). Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, Ai) is a constant times the number of Values of the Attribute Ai, then the worst-case running time of IA (and IAPlur) is O(nvna), where na equals the number of Attributes in the language and nv the average number of Values of all Attributes. This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002). As for the new algorithm, we focus on the crucial phases 2, 4, and 5. 206 van Deemter GRE with Gradable Properties Phas","@endWordPosition":"5990","@position":"35261","annotationId":"T5","@startWordPosition":"5987","@citStr":"Beun and Cremers 1998"},{"#tail":"\n","#text":"timation of their absolute values. Further experiments are needed before we can say with more confidence under what circumstances vague descriptions are favored over absolute ones. It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input. Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. 6 Presumably, Beun and Cremers (1998) found vague adjectives to be rare because, in their experiments, referents could always be identified using nongradable dimensions. 210 van Deemter GRE with Gradable Properties 7.2 Testing the Correctness of the Generated Expressions Sedivy et al (1999) asked subjects to identify the target of a vague description in a visual scene. Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pi","@endWordPosition":"8390","@position":"49811","annotationId":"T6","@startWordPosition":"8387","@citStr":"Beun and Cremers (1998)"}]},"title":{"#tail":"\n","#text":"Object reference in a shared domain of conversation. Pragmatics and Cognition,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Robbert-Jan Beun"},{"#tail":"\n","#text":"Anita Cremers"}]}},{"date":{"#tail":"\n","#text":"1989"},"editor":{"#tail":"\n","#text":"In M. Bierwisch and E. Lang, editors, Dimensional Adjectives."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"where the mouse on the chimney may be smaller than those on the ground. We focus on Ebeling and Gelman?s (1994) perceptual context dependence (Section 1), pretending that the only contextually relevant factor is the ?comparison set?: those elements of the noun denotation that are perceptually available. We disregard functional context dependence, as when the small hat is the one too small to fit on your head. 2.3 Caveat: Evaluative Adjectives What we said above has also disregarded elements of the ?global? (i.e., not immediately available) context. For some adjectives, including the ones that Bierwisch (1989) called evaluative (as opposed to dimensional), this is clearly inadequate. He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms. For example (after Bierwisch 1989), (6a) Hans is taller than Fritz ? Fritz is shorter than Hans. (6b) Hans is smarter than Fritz ? Fritz is more stupid than Hans. We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension. (For Fritz to be the stupid man, it is not enough for him t","@endWordPosition":"1886","@position":"11535","annotationId":"T7","@startWordPosition":"1885","@citStr":"Bierwisch (1989)"}},"title":{"#tail":"\n","#text":"The semantics of gradation."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Bierwisch, Manfred. 1989. The semantics of gradation. In M. Bierwisch and E. Lang, editors, Dimensional Adjectives. Springer Verlag, Berlin, pages 71?261."},"#text":"\n","pages":{"#tail":"\n","#text":"71--261"},"marker":{"#tail":"\n","#text":"Bierwisch, 1989"},"publisher":{"#tail":"\n","#text":"Springer Verlag,"},"location":{"#tail":"\n","#text":"Berlin,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Manfred Bierwisch"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Dale, Robert. 1989. Cooking up referring expressions. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL-89), pages 68?75."},"#text":"\n","pages":{"#tail":"\n","#text":"68--75"},"marker":{"#tail":"\n","#text":"Dale, 1989"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"done off-line. One should bear in mind that worst-case theoretical complexity is not always a good measure of the time that a program takes in the kinds of cases that occur most commonly, let alne the difficulty for a person. For example, it seems likely that hearers and speakers will have most difficulty dealing with differences that are too small to be obvious (e.g., two mice that are very similar in size). 207 Computational Linguistics Volume 32, Number 2 5. Pragmatic Constraints NLG has to do more than select a distinguishing description (i.e., one that unambiguously denotes its referent; Dale 1989): The selected expression should also be felicitous. Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between ?observationally indifferent? entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects th","@endWordPosition":"6748","@position":"39721","annotationId":"T8","@startWordPosition":"6747","@citStr":"Dale 1989"}},"title":{"#tail":"\n","#text":"Cooking up referring expressions."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL-89),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Robert Dale"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Dale, Robbert and Nickolas Haddock. 1991. Generating referring expressions containing relations. Proceedings of the 5th Conference of the European Chapter of the ACL, EACL-91, pages 161?166, Berlin, Germany."},"#text":"\n","pages":{"#tail":"\n","#text":"161--166"},"marker":{"#tail":"\n","#text":"Dale, Haddock, 1991"},"location":{"#tail":"\n","#text":"Berlin, Germany."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"t word this as the two heaviest brown ones among the smallest four mice. To avoid such awkward expressions, one can change the order of properties after CD (mirroring step 4 above), moving the inequalities to the end of the list before they are transformed into the appropriate superlatives. The effect would be to boost the number of occurrences of gradable properties in generated descriptions while keeping CD incremental. 9. Extensions of the Approach 9.1 Relational Descriptions Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2): Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed. Note that this is felicitous even if the shed is not the largest one i","@endWordPosition":"9960","@position":"59673","annotationId":"T9","@startWordPosition":"9957","@citStr":"Dale and Haddock 1991"}},"title":{"#tail":"\n","#text":"Generating referring expressions containing relations."},"booktitle":{"#tail":"\n","#text":"Proceedings of the 5th Conference of the European Chapter of the ACL, EACL-91,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Robbert Dale"},{"#tail":"\n","#text":"Nickolas Haddock"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Dale, Robbert and Ehud Reiter. 1995. Computational interpretations of the Gricean maximes in the generation of referring expressions. Cognitive Science, 18:233?263."},"journal":{"#tail":"\n","#text":"Cognitive Science,"},"#text":"\n","pages":{"#tail":"\n","#text":"18--233"},"marker":{"#tail":"\n","#text":"Dale, Reiter, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"less preferred than others. As a result, all other properties that turn up in the NP are already in the list L when size is added. Suppose the target is c4: type(c1) = type(c2) = type(c3) = type(c4) = mouse type(p5) = rat size(c1) = 6 cm 3 The degree of precision of the measurement (James et al 1996, Section 1.5) determines which objects can be described by the GRE algorithm, since it determines which objects count as having the same size. 4 To turn this likelihood into a certainty, one can add a test at the end of the algorithm, which adds a type-related property if none is present yet (cf., Dale and Reiter 1995). VAGUE uses both of these devices. 201 Computational Linguistics Volume 32, Number 2 size(c2) = 10 cm size(c3) = 12 cm size(c4) = size(p5) = 14 cm Since gradable properties are (for now at least) assumed to be dispreferred, the first property that makes it into L is ?mouse,? which removes p5 from the context set. (Result: C = {c1, ..., c4}.) Now size is taken into account, and size(x) = 14 cm singles out c4. The resulting list is L = {mouse, 14 cm} This might be considered the end of the matter, since the target has been singled out. But we are interested in alternative lists, to enable later","@endWordPosition":"3711","@position":"22104","annotationId":"T10","@startWordPosition":"3708","@citStr":"Dale and Reiter 1995"},{"#tail":"\n","#text":"ation of inequalities is not entirely trivial. For one thing, it is convenient to view properties of the form size(x) < ? as belonging to a different Attribute than those of the form size(x) > ?, because this causes the Values of an Attribute to be linearly ordered: Being larger than 12 cm implies being larger than 10 cm, and so on. More importantly, it will now become normal for an object to have many Values for the same Attribute; c4, for example, has the Values > 6 cm, > 10 cm, and > 12 cm. Each of these Values has equal status, so the notion of a basic-level Value cannot play a role (cf., Dale and Reiter 1995). If we abstract away from the role of basic-level Values, then Dale and Reiter?s FindBestValue chooses the most general Value that removes the maximal 202 van Deemter GRE with Gradable Properties number of distractors, as we have seen. The problem at hand suggests a simpler approach that will always prefer logically stronger inequalities over logically weaker ones, even when they do not remove more distractors.5 (Thus, size(x) > m is preferred over size(x) > n iff m > n; conversely, size(x) < m is preferred over size(x) < n iff m < n.) This is reflected by the order in which the properties ar","@endWordPosition":"4250","@position":"25049","annotationId":"T11","@startWordPosition":"4247","@citStr":"Dale and Reiter 1995"},{"#tail":"\n","#text":"aders? Much is unknown, but we shall summarize the available results in these three areas very briefly, referring readers to the literature for details. 7.1 Human Speakers? Use of Vague Descriptions Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication. We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. Dale and Reiter (1995), for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner). They found that, while instructional texts tended to use numerical descriptions like the 3 14 ? bolt, human assemblers ?unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt? (Dale and Reiter 1995).6 Our own experiments (van Deemter 2004) point in the same direction. In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each ","@endWordPosition":"8010","@position":"47480","annotationId":"T12","@startWordPosition":"8007","@citStr":"Dale and Reiter (1995)"},{"#tail":"\n","#text":"r example, one might use the superlative all the time, since this was?surprisingly?the most frequent form overall. Based on point (2), however, one might also defend using the base form whenever the gap is large enough (as was done in the VAGUE program). Future experiments should allow us to refine this position, perhaps depending on factors such as genre, communicative goal, and type of audience. 8. Incrementality: Help or Hindrance? The account sketched in Section 4 was superimposed on an incremental GRE algorithm, partly because incrementality is well established in this area (Appelt 1985; Dale and Reiter 1995). But IA may be replaced by any other reasonable7 GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always ?greedily? selects the property that removes the maximum number of distractors. Let G be any such GRE algorithm, then we can proceed as follows: GRE for Vague Descriptions (version not relying on IA): 1. Construct KB using Attributes and Values, assigning numerical Values to gradable Attributes. 2. Recompile the KB, replacing equalities by inequalities. 3. Let G deliver an unordered set of properties which jointly distinguish the","@endWordPosition":"9130","@position":"54277","annotationId":"T13","@startWordPosition":"9127","@citStr":"Dale and Reiter 1995"}]},"title":{"#tail":"\n","#text":"Computational interpretations of the Gricean maximes in the generation of referring expressions."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Robbert Dale"},{"#tail":"\n","#text":"Ehud Reiter"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"DeVault, David and Matthew Stone. 2004. Interpreting vague utterances in context."},"#text":"\n","marker":{"#tail":"\n","#text":"DeVault, Stone, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":". To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm) Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000; DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of 2 The reader is asked to focus on any reasonable size measurement, for examp","@endWordPosition":"1479","@position":"9083","annotationId":"T14","@startWordPosition":"1476","@citStr":"DeVault and Stone 2004"}},"title":{"#tail":"\n","#text":"Interpreting vague utterances in context."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David DeVault"},{"#tail":"\n","#text":"Matthew Stone"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In Proceedings of COLING 2004, pages 1247?1253, Geneva."},"#text":"\n","pages":{"#tail":"\n","#text":"1247--1253"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Geneva."},"booktitle":{"#tail":"\n","#text":"In Proceedings of COLING 2004,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Ebeling, K. S. and S. A. Gelman. 1994."},"#text":"\n","marker":{"#tail":"\n","#text":"Ebeling, Gelman, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean. Gradability is especially widespread in adjectives. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words (Peccei 1994) and understand some of their intricacies as early as their 24th month (Ebeling and Gelman 1994). These ? Computing Science Department, King?s College, University of Aberdeen, United Kingdom, E-mail: kvdeemter@csd.abdn.ac.uk. 1 We take such adjectives to be ones that have comparative and superlative forms, and which can be premodified by intensifiers such as very (Quirk et al 1972, Section 5.4). Submission received: 7 July 2004; revised submission received: 19 October 2005; accepted for publication: 24 November 2005. ? 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 2 intricacies include what Ebeling and Gelman call perceptual context dependence","@endWordPosition":"375","@position":"2462","annotationId":"T15","@startWordPosition":"372","@citStr":"Ebeling and Gelman 1994"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K S Ebeling"},{"#tail":"\n","#text":"S A Gelman"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"Children?s use of context in interpreting ?big? and ?little?. Child Development, 65(4):1178-1192."},"#text":"\n","pages":{"#tail":"\n","#text":"65--4"},"marker":{"#tail":"\n"},"title":{"#tail":"\n","#text":"Children?s use of context in interpreting ?big? and ?little?. Child Development,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"1980"},"rawString":{"#tail":"\n","#text":"Feldman, Allan M. 1980. Welfare Economics and Social Choice Theory. Kluwer, Boston."},"#text":"\n","marker":{"#tail":"\n","#text":"Feldman, 1980"},"publisher":{"#tail":"\n","#text":"Kluwer,"},"location":{"#tail":"\n","#text":"Boston."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"es to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: height(a) = 5 m height(b) = height(c) = 15 m width(a) = width(b) = 3 m width(c) = 2 m Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r ? C has a Pareto-optimal combination of Values V iff there is no other x ? C such that 1. ?Vi ? V : Vi(x) > Vi(r) and 2. ??Vj ? V : Vj(x) < Vj(r) In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall","@endWordPosition":"10373","@position":"62045","annotationId":"T16","@startWordPosition":"10372","@citStr":"Feldman 1980"}},"title":{"#tail":"\n","#text":"Welfare Economics and Social Choice Theory."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Allan M Feldman"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"van Deemter GRE with Gradable Properties Funakoshi, Kotaro, Satoru Watanabe, Naoko Kuriyama, and Takenobu Takunaga. 2004. Generating referring expressions using perceptual groups. In Proceedings of 3rd International Conference on Natural Language Generation (INLG) 2004, pages 51?60. Brockenhurst, UK."},"#text":"\n","pages":{"#tail":"\n","#text":"51--60"},"marker":{"#tail":"\n","#text":"van Deemter, 2004"},"publisher":{"#tail":"\n","#text":"Brockenhurst, UK."},"title":{"#tail":"\n","#text":"GRE with Gradable Properties Funakoshi, Kotaro, Satoru Watanabe, Naoko Kuriyama, and Takenobu Takunaga."},"booktitle":{"#tail":"\n","#text":"In Proceedings of 3rd International Conference on Natural Language Generation (INLG)"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"van Deemter"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Gaiffe, Bertrand and Laurent Romary. 1997. Constraints on the use of language, gesture, and speech for multimodal dialogues. In Proceedings of ACL Workshop Referring Phenomena in a Multimedia Context and Their Computational Treatment, pages 94?98, Madrid, Spain."},"#text":"\n","pages":{"#tail":"\n","#text":"94--98"},"marker":{"#tail":"\n","#text":"Gaiffe, Romary, 1997"},"location":{"#tail":"\n","#text":"Madrid,"},"title":{"#tail":"\n","#text":"Constraints on the use of language, gesture, and speech for multimodal dialogues."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL Workshop Referring Phenomena in a Multimedia Context and Their Computational Treatment,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Bertrand Gaiffe"},{"#tail":"\n","#text":"Laurent Romary"}]}},{"volume":{"#tail":"\n","#text":"9"},"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Goldberg, Eli, Norbert Driedger, and Richard Kitteridge. 1994. Using natural-language processing to produce weather forecasts. IEEE Expert, 9(2):45?53."},"journal":{"#tail":"\n","#text":"IEEE Expert,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Goldberg, Driedger, Kitteridge, 1994"},"title":{"#tail":"\n","#text":"Using natural-language processing to produce weather forecasts."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Eli Goldberg"},{"#tail":"\n","#text":"Norbert Driedger"},{"#tail":"\n","#text":"Richard Kitteridge"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Gorniak, Peter and Deb Roy. 2003."},"#text":"\n","marker":{"#tail":"\n","#text":"Gorniak, Roy, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan, for example, would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors (Nash 1950; cf. Gorniak and Roy 2003; Thorisson 1994, for other plans). 9.3.2 Multidimensional Adjectives (and Color). Multidimensionality can also slip in through the backdoor. Consider big, for example, when applied to 3D shapes. If there exists a formula for mapping three dimensions into one (e.g., length ? width ? height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situ","@endWordPosition":"10530","@position":"62903","annotationId":"T17","@startWordPosition":"10527","@citStr":"Gorniak and Roy 2003"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Peter Gorniak"},{"#tail":"\n","#text":"Deb Roy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Understanding complex visually referring utterances. In Proceedings of HLT-NAACL03 Workshop on Learning Word Meaning from Non-Linguistic Data, pages 14?21. Edmonton, Canada, May 2003."},"#text":"\n","pages":{"#tail":"\n","#text":"14--21"},"marker":{"#tail":"\n","#text":"2003"},"location":{"#tail":"\n","#text":"Edmonton, Canada,"},"title":{"#tail":"\n","#text":"Understanding complex visually referring utterances."},"booktitle":{"#tail":"\n","#text":"In Proceedings of HLT-NAACL03 Workshop on Learning Word Meaning from Non-Linguistic Data,"},"@valid":"true"},{"date":{"#tail":"\n","#text":"1975"},"editor":{"#tail":"\n","#text":"In P. Cole and J. Morgan, editors,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"o perceive, then the expression is also infelicitous. Dichotomy. When separating one single referent from one distractor, the comparative form is often said to be favored (Use the comparative form to compare two things). We expected this to generalize to situations where all the referents are of one size, and all the distractors of another. Minimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English, where the base form is morphologically simpler than the other two, this rule could be argued to follow from Gricean principles (Grice 1975). To keep matters simple, linguistic realization could choose the base form if and only if the gap between s? and s+ surpasses a certain value, which is specified interactively by the user. (This approach was chosen for the VAGUE program.) As for the presence/absence of the numeral in the description, there appear to be different ?believable? patterns of linguistic behavior. A cautious generator might only omit the numeral when the pragmatic principles happen to enforce a specific extension (e.g., the large mice, when the mice are sized 3, 2.8, 2.499, and 2.498 cm). This would allow the genera","@endWordPosition":"7191","@position":"42386","annotationId":"T18","@startWordPosition":"7190","@citStr":"Grice 1975"},{"#tail":"\n","#text":"estions: (1) When is it natural to generate a vague description (i.e., a 209 Computational Linguistics Volume 32, Number 2 qualitative description as opposed to a purely quantitative one)? (2) Given that a vague description is used, which form of the description is most natural? and (3) Are the generated descriptions properly understood by hearers and readers? Much is unknown, but we shall summarize the available results in these three areas very briefly, referring readers to the literature for details. 7.1 Human Speakers? Use of Vague Descriptions Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication. We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. Dale and Reiter (1995), for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner). They found that, while instructional texts tended to use numerical descriptions lik","@endWordPosition":"7954","@position":"47113","annotationId":"T19","@startWordPosition":"7953","@citStr":"Grice 1975"}]},"title":{"#tail":"\n","#text":"Logic and conversation."},"volume":{"#tail":"\n","#text":"volume"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Grice, Paul. 1975. Logic and conversation. In P. Cole and J. Morgan, editors, Syntax and Semantics, volume 3, Speech Acts. Academic Press, New York, pages 43?58."},"#text":"\n","pages":{"#tail":"\n","#text":"43--58"},"marker":{"#tail":"\n","#text":"Grice, 1975"},"publisher":{"#tail":"\n","#text":"Academic Press,"},"location":{"#tail":"\n","#text":"New York,"},"booktitle":{"#tail":"\n","#text":"Syntax and Semantics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Paul Grice"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1976"},"rawString":{"#tail":"\n","#text":"Hermann, Tony and Roland Deutsch. 1976. Psychologie der Objektbenennung. Huber Verlag, Bern."},"#text":"\n","marker":{"#tail":"\n","#text":"Hermann, Deutsch, 1976"},"publisher":{"#tail":"\n","#text":"Huber Verlag,"},"location":{"#tail":"\n","#text":"Bern."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"s ?being a set of cardinality greater than 1, all of whose elements are larger than all other elements in C.? The result may be realized as the largest mice. L3 = ?mouse, size(x) = max?. 4.1.4 Ordering of Properties. Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch (1976; also reported in Levelt 1989) show that greater differences are most likely to be chosen, presumably because they are more striking. In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say ?the tall candle? when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to ?the fat candle.? Hermann and Deutsch?s findings may be implemented as follows. First, the Values of the different Attribute","@endWordPosition":"4716","@position":"27694","annotationId":"T20","@startWordPosition":"4713","@citStr":"Hermann and Deutsch (1976"},{"#tail":"\n","#text":"ry preparation for (5) because the superlative properties resulting from (5), unlike the inequalities resulting from (4), are context dependent. For example, ?mouse, size(x) = max2? (the largest two mice, {c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two elements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the short(est) black mouse if there is only one black mouse, because this might invite false implicatures. 8.1 Problems with Incrementality While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates. (Wildly redundant descriptions can result if the ?wrong? preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same ord","@endWordPosition":"9443","@position":"56249","annotationId":"T21","@startWordPosition":"9440","@citStr":"Hermann and Deutsch 1976"}]},"title":{"#tail":"\n","#text":"Psychologie der Objektbenennung."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Tony Hermann"},{"#tail":"\n","#text":"Roland Deutsch"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"note":{"#tail":"\n","#text":"Sorites Paradox."},"rawString":{"#tail":"\n","#text":"Hyde, Dominic. 2002. Sorites Paradox."},"#text":"\n","marker":{"#tail":"\n","#text":"Hyde, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"?observationally indifferent? entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it. This is the strongest version of the sorites paradox (e.g., Hyde 2002). Our approach to vague descriptions allows a subtle response: that the offending statement may be correct yet infelicitous. This shifts the problem from asking when vague descriptions are ?correct? to the question of when they are used felicitously. Felicity is naturally thought of as a gradable concept. There is therefore no need for a generator to demarcate precisely between felicitous and infelicitous expressions, as long as all the utterances generated are felicitous enough. When in doubt, a generator should avoid the expression in question. If x and y are mice of sizes 10 and 9.9 cm, for","@endWordPosition":"6881","@position":"40534","annotationId":"T22","@startWordPosition":"6880","@citStr":"Hyde 2002"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dominic Hyde"}}},{"#tail":"\n","editor":{"#tail":"\n","#text":"In Edward Zalta, editor,"},"rawString":{"#tail":"\n","#text":"In Edward Zalta, editor, The Stanford Encyclopedia of Philosophy (Fall 2002 Edition), http://plato.stanford.edu/ archives/fall2002/entries/soritesparadox/."},"#text":"\n","marker":{"#tail":"\n"},"booktitle":{"#tail":"\n","#text":"The Stanford Encyclopedia of Philosophy (Fall 2002 Edition), http://plato.stanford.edu/ archives/fall2002/entries/soritesparadox/."},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"James, Glyn, David Burley, Dick Clements, Phil Dyke, John Searl, and Jerry Wright. 1996. Modern Engineering Mathematics, second edition. Addison-Wesley Longman Ltd., Harlow, UK."},"#text":"\n","marker":{"#tail":"\n","#text":"James, Burley, Clements, Dyke, Searl, Wright, 1996"},"publisher":{"#tail":"\n","#text":"Addison-Wesley Longman Ltd.,"},"location":{"#tail":"\n","#text":"Harlow, UK."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"llow mouse, or the large yellow mouse). 4.1.2 Exploiting Numerical Properties, Singular. To (almost4) ensure that every description contains a property expressible as a noun, we shall assume that the type Attribute is more highly preferred than all others. Suppose also, for now, that properties related to size are less preferred than others. As a result, all other properties that turn up in the NP are already in the list L when size is added. Suppose the target is c4: type(c1) = type(c2) = type(c3) = type(c4) = mouse type(p5) = rat size(c1) = 6 cm 3 The degree of precision of the measurement (James et al 1996, Section 1.5) determines which objects can be described by the GRE algorithm, since it determines which objects count as having the same size. 4 To turn this likelihood into a certainty, one can add a test at the end of the algorithm, which adds a type-related property if none is present yet (cf., Dale and Reiter 1995). VAGUE uses both of these devices. 201 Computational Linguistics Volume 32, Number 2 size(c2) = 10 cm size(c3) = 12 cm size(c4) = size(p5) = 14 cm Since gradable properties are (for now at least) assumed to be dispreferred, the first property that makes it into L is ?mouse,? wh","@endWordPosition":"3654","@position":"21783","annotationId":"T23","@startWordPosition":"3651","@citStr":"James et al 1996"}},"title":{"#tail":"\n","#text":"Modern Engineering Mathematics, second edition."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Glyn James"},{"#tail":"\n","#text":"David Burley"},{"#tail":"\n","#text":"Dick Clements"},{"#tail":"\n","#text":"Phil Dyke"},{"#tail":"\n","#text":"John Searl"},{"#tail":"\n","#text":"Jerry Wright"}]}},{"date":{"#tail":"\n","#text":"1975"},"editor":{"#tail":"\n","#text":"In E. Keenan, editor,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"rge. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of 2 The reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or vertical distance, or some combination of dimensions (Kamp 1975; also Section 8.1 of the present article). 197 Computational Linguistics Volume 32, Number 2 the adjective to be semantically equivalent to the superlative form (and, analogously, the comparative): The n large mice = The largest n mice The large mice = The largest mice The large mouse = The largest mouse. Viewed in this way, gradable adjectives are an extreme example of the ?efficiency of language? (Barwise and Perry 1983): Far from meaning something concrete like ?larger than 8 cm??a concept that would have very limited applicability?or even something more general like ?larger than the avera","@endWordPosition":"1599","@position":"9776","annotationId":"T24","@startWordPosition":"1598","@citStr":"Kamp 1975"},{"#tail":"\n","#text":" Color). Multidimensionality can also slip in through the backdoor. Consider big, for example, when applied to 3D shapes. If there exists a formula for mapping three dimensions into one (e.g., length ? width ? height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. Similar things hold for multifaceted properties like intelligence (Kamp 1975). Color terms are a case apart. If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue). This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly 215 Computational Linguistics Volume 32, Number 2 straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.) A further complication is that differ","@endWordPosition":"10638","@position":"63587","annotationId":"T25","@startWordPosition":"10637","@citStr":"Kamp 1975"}]},"title":{"#tail":"\n","#text":"Two theories about adjectives."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Kamp, Hans. 1975. Two theories about adjectives. In E. Keenan, editor, Semantics for Natural Language. Cambridge University Press, Cambridge, UK."},"#text":"\n","marker":{"#tail":"\n","#text":"Kamp, 1975"},"publisher":{"#tail":"\n","#text":"Cambridge University Press,"},"location":{"#tail":"\n","#text":"Cambridge, UK."},"booktitle":{"#tail":"\n","#text":"Semantics for Natural Language."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Hans Kamp"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"UC Santa Cruz."},"rawString":{"#tail":"\n","#text":"Kennedy, Christopher. 1999. Projecting the Adjective: The syntax and Semantics of Gradability and Comparison. Ph.D. thesis, UC Santa Cruz."},"#text":"\n","marker":{"#tail":"\n","#text":"Kennedy, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" means to be large: larger than average, larger than most, larger than some given baseline, and so on. It is doubtful that any one of these analyses makes sense for all definite descriptions. To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm) Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000; DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In ","@endWordPosition":"1446","@position":"8883","annotationId":"T26","@startWordPosition":"1445","@citStr":"Kennedy 1999"}},"title":{"#tail":"\n","#text":"Projecting the Adjective: The syntax and Semantics of Gradability and Comparison."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Christopher Kennedy"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"editor":{"#tail":"\n","#text":"In K. van Deemter and R. Kibble, editors,"},"rawString":{"#tail":"\n","#text":"Krahmer, Emiel and Marie?t Theune. 2002. Efficient context-sensitive generation of referring expressions. In K. van Deemter and R. Kibble, editors, Information Sharing: Reference and Presupposition in Language Generation and Interpretation, CSLI Publications, CSLI, Stanford, pages 223?264."},"#text":"\n","pages":{"#tail":"\n","#text":"223--264"},"marker":{"#tail":"\n","#text":"Krahmer, Theune, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"s preferred than crisp ones (point 3) then this algorithm will only use gradable properties if an entirely crisp distinguishing description is impossible. This may well cause gradable properties to be underused. For this and other reasons, we shall consider non-incremental versions of these ideas in Section 8. 4.3 Computational Complexity We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects (Beun and Cremers 1998, Krahmer and Theune 2002). Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, Ai) is a constant times the number of Values of the Attribute Ai, then the worst-case running time of IA (and IAPlur) is O(nvna), where na equals the number of Attributes in the language and nv the average number of Values of all Attributes. This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002). As for the new algorithm, we focus on the crucial phases 2, 4, and 5. 206 van Deemter GRE with Gradable Properties Phase 2: Recompilation of the ","@endWordPosition":"5994","@position":"35287","annotationId":"T27","@startWordPosition":"5991","@citStr":"Krahmer and Theune 2002"},{"#tail":"\n","#text":"c extension (e.g., the large mice, when the mice are sized 3, 2.8, 2.499, and 2.498 cm). This would allow the generator to use vague expressions, but only where they result in a description that is itself unambiguous. We shall see in Section 7 that it has not been easy to confirm the pragmatic constraints of the present section experimentally. 208 van Deemter GRE with Gradable Properties 6. Linguistic Realization Some recent GRE algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead (Stone and Webber 1998; Krahmer and Theune 2002). We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = ?size > 3 cm, size < 9 cm?. If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm). Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic rea","@endWordPosition":"7368","@position":"43494","annotationId":"T28","@startWordPosition":"7365","@citStr":"Krahmer and Theune 2002"},{"#tail":"\n","#text":"ty, since an adjective in a vague description can only be fully interpreted when its comparison set is known. Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said. 8.2 Low Preference for Gradable Properties? It has been argued that, in an incremental approach, gradable properties should be given a low preference ranking because they are difficult to process (Krahmer and Theune 2002). We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions. Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes). Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking. Let us see how things would work if they were ranked more highly. Suppo","@endWordPosition":"9710","@position":"58065","annotationId":"T29","@startWordPosition":"9707","@citStr":"Krahmer and Theune 2002"},{"#tail":"\n","#text":"re transformed into the appropriate superlatives. The effect would be to boost the number of occurrences of gradable properties in generated descriptions while keeping CD incremental. 9. Extensions of the Approach 9.1 Relational Descriptions Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2): Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed. Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b): type(d1) = type(d2) = dog type(c) = cat type(s1) = type(s2) = type(s3) = shed size(d1) = size(d2) = size(c) = 1m size(s1) = 3m size(s2) =","@endWordPosition":"10001","@position":"59916","annotationId":"T30","@startWordPosition":"9998","@citStr":"Krahmer and Theune 2002"},{"#tail":"\n","#text":" still somewhat weak, so the risk of referential unclarity looms large. Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing. 9.4 Salience as a Gradable Property We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions. As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous. 9.4.1 A New Perspective on Salience. Krahmer and Theune (2002) have argued that Dale and Reiter?s (1995) dichotomy between salient and nonsalient objects (where the objects in the domain are the salient ones) should be replaced by an account that takes degrees of salience into account: No object can be too unsalient to be referred to, as long as the right properties are available. In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice","@endWordPosition":"11003","@position":"65894","annotationId":"T31","@startWordPosition":"11000","@citStr":"Krahmer and Theune (2002)"}]},"title":{"#tail":"\n","#text":"Efficient context-sensitive generation of referring expressions."},"booktitle":{"#tail":"\n","#text":"Information Sharing: Reference and Presupposition in Language Generation and Interpretation, CSLI Publications, CSLI, Stanford,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Emiel Krahmer"},{"#tail":"\n","#text":"Mariet Theune"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Krahmer, Emiel and Ielka van der Sluis. 2003."},"#text":"\n","marker":{"#tail":"\n","#text":"Krahmer, van der Sluis, 2003"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Emiel Krahmer"},{"#tail":"\n","#text":"Ielka van der Sluis"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"A new model for generating multimodal referring expressions. In Proceedings of 9th European Workshop on Natural Language Generation (ENLG-2003), pages 47?54, Budapest."},"#text":"\n","pages":{"#tail":"\n","#text":"47--54"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Budapest."},"title":{"#tail":"\n","#text":"A new model for generating multimodal referring expressions."},"booktitle":{"#tail":"\n","#text":"In Proceedings of 9th European Workshop on Natural Language Generation (ENLG-2003),"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Kyburg, Alice and Michael Morreau. 2000. Fitting Words: vague language in context. Linguistics and Philosophy, 23:577?597."},"#text":"\n","pages":{"#tail":"\n","#text":"23--577"},"marker":{"#tail":"\n","#text":"Kyburg, Morreau, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"all definite descriptions. To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm) Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000; DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of 2 The reader is asked to focus on any reasonable si","@endWordPosition":"1475","@position":"9058","annotationId":"T32","@startWordPosition":"1472","@citStr":"Kyburg and Morreau 2000"}},"title":{"#tail":"\n","#text":"Fitting Words: vague language in context. Linguistics and Philosophy,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alice Kyburg"},{"#tail":"\n","#text":"Michael Morreau"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Levelt, William J. M. 1989. Speaking: From Intention to Articulation. MIT Press, Cambridge, MA."},"#text":"\n","marker":{"#tail":"\n","#text":"Levelt, 1989"},"publisher":{"#tail":"\n","#text":"MIT Press,"},"location":{"#tail":"\n","#text":"Cambridge, MA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":", all of whose elements are larger than all other elements in C.? The result may be realized as the largest mice. L3 = ?mouse, size(x) = max?. 4.1.4 Ordering of Properties. Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch (1976; also reported in Levelt 1989) show that greater differences are most likely to be chosen, presumably because they are more striking. In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say ?the tall candle? when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to ?the fat candle.? Hermann and Deutsch?s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make ","@endWordPosition":"4721","@position":"27725","annotationId":"T33","@startWordPosition":"4720","@citStr":"Levelt 1989"},{"#tail":"\n","#text":"ause the superlative properties resulting from (5), unlike the inequalities resulting from (4), are context dependent. For example, ?mouse, size(x) = max2? (the largest two mice, {c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two elements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the short(est) black mouse if there is only one black mouse, because this might invite false implicatures. 8.1 Problems with Incrementality While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates. (Wildly redundant descriptions can result if the ?wrong? preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which t","@endWordPosition":"9445","@position":"56262","annotationId":"T34","@startWordPosition":"9444","@citStr":"Levelt 1989"}]},"title":{"#tail":"\n","#text":"Speaking: From Intention to Articulation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"William J M Levelt"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Malouf, Rob. 2000. The order of prenominal adjectives in natural language generation."},"#text":"\n","marker":{"#tail":"\n","#text":"Malouf, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" expressions generation from its language-dependent, linguistic aspect. Our algorithm suggests a distinction into three phases, the first two of which can be thought of as part of CD: 1. CD proper, that is, the production of a distinguishing list of properties L; 2. An inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives? relative position. Interestingly, vague properties tend to be realized before others. Quirk et al (1985), for example, report that ?adjectives denoting size, length, and height normally precede other nonderived adjectives? (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small(-est) [N], for example, the words p","@endWordPosition":"7614","@position":"44982","annotationId":"T35","@startWordPosition":"7613","@citStr":"Malouf 2000"}},"title":{"#tail":"\n","#text":"The order of prenominal adjectives in natural language generation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Rob Malouf"}}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In Proceedings of ACL-2000, pages 85?92, Hong Kong."},"#text":"\n","pages":{"#tail":"\n","#text":"85--92"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Hong Kong."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL-2000,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Masthoff, Judith. 2004. Group modeling: Selecting a sequence of television items to suit a group of viewers. User Modeling and User Adapted Interaction, 14:37?85."},"#text":"\n","pages":{"#tail":"\n","#text":"14--37"},"marker":{"#tail":"\n","#text":"Masthoff, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"g that a generator would ideally be aware of. Multidimensionality (Section 9.3). We know roughly how to deal with one gradable dimension: the short man, for example, is the shortest man around. But in practice, we often juggle several dimensions. This happens, for example, when two adjectives are used (the short thin man), or when salience is taken into account (e.g., the short man, when the shortest man is not the most salient one), threatening to make irrefutably distinguishing descriptions something of an exception. (For a study of approaches to multidimensionality in a different area, see Masthoff 2004.) At some point, GRE may have to abandon the strategy of aiming for unambiguous descriptions in all situations. Acknowledgments I thank Hua Cheng, Roger Evans, Albert Gatt, Markus Guhe, Imtiaz Khan, Emiel Krahmer, Judith Masthoff, Chris Mellish, Oystein Nilsen, Manfred Pinkal, Paul Piwek, Ehud Reiter, Graeme Ritchie, Ielka van der Sluis, Rosemary Stevenson, Matthew Stone, and Sebastian Varges for helpful comments. I am especially grateful to Richard Power, for inspiration as well as for implementing the VAGUE program at great speed. Thanks are due to four anonymous reviewers for some very sub","@endWordPosition":"13067","@position":"78117","annotationId":"T36","@startWordPosition":"13066","@citStr":"Masthoff 2004"}},"title":{"#tail":"\n","#text":"Group modeling: Selecting a sequence of television items to suit a group of viewers. User Modeling and User Adapted Interaction,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Judith Masthoff"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Mellish, Chris. 2000. Understanding shortcuts in NLG systems. In Proceedings of Workshop ?Impacts in Natural Language Generation?, pages 43?50, Dagstuhl, Germany."},"#text":"\n","pages":{"#tail":"\n","#text":"43--50"},"marker":{"#tail":"\n","#text":"Mellish, 2000"},"location":{"#tail":"\n","#text":"Dagstuhl, Germany."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" degrees of salience (i.e., the closer to the center of pointing, the higher the value for the attribute SALIENCE) in the style of Section 9.2. 218 van Deemter GRE with Gradable Properties 10. Conclusion If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle, this might be done by providing the generator with vague input?in which case no special algorithms are needed?but suitably contextualized vague input is often not available (Mellish 2000). The only practical alternative is to provide the generator with ?crisp? (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE. Far from being a peculiarity of a few adjectives, vagueness is widespread. We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example,  color terms (Section 9.3);  nouns that allow different degrees of strictness (","@endWordPosition":"12433","@position":"74208","annotationId":"T37","@startWordPosition":"12432","@citStr":"Mellish 2000"}},"title":{"#tail":"\n","#text":"Understanding shortcuts in NLG systems."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Workshop ?Impacts in Natural Language Generation?,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Chris Mellish"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1950"},"rawString":{"#tail":"\n","#text":"Nash, John. 1950. The bargaining problem. Econometrica, 18:155?162."},"journal":{"#tail":"\n","#text":"Econometrica,"},"#text":"\n","pages":{"#tail":"\n","#text":"18--155"},"marker":{"#tail":"\n","#text":"Nash, 1950"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"he only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan, for example, would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors (Nash 1950; cf. Gorniak and Roy 2003; Thorisson 1994, for other plans). 9.3.2 Multidimensional Adjectives (and Color). Multidimensionality can also slip in through the backdoor. Consider big, for example, when applied to 3D shapes. If there exists a formula for mapping three dimensions into one (e.g., length ? width ? height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and","@endWordPosition":"10525","@position":"62877","annotationId":"T38","@startWordPosition":"10524","@citStr":"Nash 1950"}},"title":{"#tail":"\n","#text":"The bargaining problem."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"John Nash"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Peccei, Jean Stillwell. 1994. Child Language, Routledge."},"#text":"\n","marker":{"#tail":"\n","#text":"Peccei, 1994"},"publisher":{"#tail":"\n","#text":"Child Language, Routledge."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean. Gradability is especially widespread in adjectives. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words (Peccei 1994) and understand some of their intricacies as early as their 24th month (Ebeling and Gelman 1994). These ? Computing Science Department, King?s College, University of Aberdeen, United Kingdom, E-mail: kvdeemter@csd.abdn.ac.uk. 1 We take such adjectives to be ones that have comparative and superlative forms, and which can be premodified by intensifiers such as very (Quirk et al 1972, Section 5.4). Submission received: 7 July 2004; revised submission received: 19 October 2005; accepted for publication: 24 November 2005. ? 2006 Association for Computational Linguistics Computational Linguistics Vo","@endWordPosition":"359","@position":"2366","annotationId":"T39","@startWordPosition":"358","@citStr":"Peccei 1994"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Jean Stillwell Peccei"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Pechmann, Thomas. 1989. Incremental speech production and referential overspecification. Linguistics, 27:98?110."},"journal":{"#tail":"\n","#text":"Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"27--98"},"marker":{"#tail":"\n","#text":"Pechmann, 1989"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rlative properties resulting from (5), unlike the inequalities resulting from (4), are context dependent. For example, ?mouse, size(x) = max2? (the largest two mice, {c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two elements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the short(est) black mouse if there is only one black mouse, because this might invite false implicatures. 8.1 Problems with Incrementality While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates. (Wildly redundant descriptions can result if the ?wrong? preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were select","@endWordPosition":"9447","@position":"56277","annotationId":"T40","@startWordPosition":"9446","@citStr":"Pechmann 1989"}},"title":{"#tail":"\n","#text":"Incremental speech production and referential overspecification."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Thomas Pechmann"}}},{"date":{"#tail":"\n","#text":"1979"},"editor":{"#tail":"\n","#text":"In R. Ba?uerle, U. Egli, and A. von Stechow, editors,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rticle concludes with a discussion of salience and pointing, which are analyzed as if they were gradable adjectives. 1. Introduction: Vagueness of Gradable Adjectives 1.1 Vague Descriptions Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees. This article focuses on gradable adjectives, also called degree adjectives.1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following Pinkal (1979), such expressions will be called vague descriptions even though, as we shall see, the vagueness of the adjective does not extend to the description as a whole. It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the dif","@endWordPosition":"177","@position":"1236","annotationId":"T41","@startWordPosition":"176","@citStr":"Pinkal (1979)"}},"title":{"#tail":"\n","#text":"How to refer with vague descriptions."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Pinkal, Manfred. 1979. How to refer with vague descriptions. In R. Ba?uerle, U. Egli, and A. von Stechow, editors, Semantics from Different Points of View. Springer Verlag, Berlin, pages 32?50."},"#text":"\n","pages":{"#tail":"\n","#text":"32--50"},"marker":{"#tail":"\n","#text":"Pinkal, 1979"},"publisher":{"#tail":"\n","#text":"Springer Verlag,"},"location":{"#tail":"\n","#text":"Berlin,"},"booktitle":{"#tail":"\n","#text":"Semantics from Different Points of View."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Manfred Pinkal"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1972"},"rawString":{"#tail":"\n","#text":"Quirk, Randolph, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1972. A Grammar of Contemporary English. Longman, Harlow, Essex."},"journal":{"#tail":"\n","#text":"A Grammar of Contemporary English. Longman,"},"#text":"\n","marker":{"#tail":"\n","#text":"Quirk, Greenbaum, Leech, Svartvik, 1972"},"location":{"#tail":"\n","#text":"Harlow, Essex."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":", for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words (Peccei 1994) and understand some of their intricacies as early as their 24th month (Ebeling and Gelman 1994). These ? Computing Science Department, King?s College, University of Aberdeen, United Kingdom, E-mail: kvdeemter@csd.abdn.ac.uk. 1 We take such adjectives to be ones that have comparative and superlative forms, and which can be premodified by intensifiers such as very (Quirk et al 1972, Section 5.4). Submission received: 7 July 2004; revised submission received: 19 October 2005; accepted for publication: 24 November 2005. ? 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 2 intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible). 1.2 Vagueness in NLG Some NLG systems produce gradable adjectives. The FOG weather","@endWordPosition":"417","@position":"2749","annotationId":"T42","@startWordPosition":"414","@citStr":"Quirk et al 1972"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Randolph Quirk"},{"#tail":"\n","#text":"Sidney Greenbaum"},{"#tail":"\n","#text":"Geoffrey Leech"},{"#tail":"\n","#text":"Jan Svartvik"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"rawString":{"#tail":"\n","#text":"Quirk, Randolph, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English. Longman, Harlow, Essex."},"journal":{"#tail":"\n","#text":"A Comprehensive Grammar of the English. Longman,"},"#text":"\n","marker":{"#tail":"\n","#text":"Quirk, Greenbaum, Leech, Svartvik, 1985"},"location":{"#tail":"\n","#text":"Harlow, Essex."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives? relative position. Interestingly, vague properties tend to be realized before others. Quirk et al (1985), for example, report that ?adjectives denoting size, length, and height normally precede other nonderived adjectives? (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small(-est) [N], for example, the words preceding N select the three smallest elements of [N]. It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables. The latter would mean something else, nam","@endWordPosition":"7654","@position":"45258","annotationId":"T43","@startWordPosition":"7651","@citStr":"Quirk et al (1985)"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Randolph Quirk"},{"#tail":"\n","#text":"Sidney Greenbaum"},{"#tail":"\n","#text":"Geoffrey Leech"},{"#tail":"\n","#text":"Jan Svartvik"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Rasmusen, Eric. 1989. Games and Information: An Introduction to Game Theory. Blackwell Publishing."},"#text":"\n","marker":{"#tail":"\n","#text":"Rasmusen, 1989"},"publisher":{"#tail":"\n","#text":"Blackwell Publishing."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":", to say the dog in the largest shed. 9.2 Boolean Combinations Generalizations to complex Boolean descriptions involving negation and disjunction (van Deemter 2004) appear to be largely straightforward, except for issues to do with 214 van Deemter GRE with Gradable Properties opposites and markedness. For example, the generator will have to decide whether to say the patients that are old or the patients that are not young. 9.3 Multidimensionality 9.3.1 Combinations of Adjectives. When objects are compared in terms of several dimensions, these dimensions can be weighed in different ways (e.g., Rasmusen 1989). Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: height(a) = 5 m height(b) = height(c) = 15 m width(a) = width(b) = 3 m width(c) = 2 m Cases like this would be covered if the decision-theoretic property of","@endWordPosition":"10258","@position":"61406","annotationId":"T44","@startWordPosition":"10257","@citStr":"Rasmusen 1989"}},"title":{"#tail":"\n","#text":"Games and Information: An Introduction to Game Theory."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Eric Rasmusen"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Reiter, Ehud and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press, Cambridge, UK."},"#text":"\n","marker":{"#tail":"\n","#text":"Reiter, Dale, 2000"},"publisher":{"#tail":"\n","#text":"Cambridge University Press,"},"location":{"#tail":"\n","#text":"Cambridge, UK."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" so on, each of which may single out the same individuals. Section 7.3 has summarized some experimental evidence related to such choices, focusing on the different forms of the adjective, but the evidence is far from conclusive. Much is still unknown, differences between speakers abound, and the experimental methodology for advancing the state of the art in this area is not without its problems (van Deemter 2004). Architecture (Section 6). The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model (e.g., Reiter and Dale 2000). An example of such an inference rule is the one that transforms a list of the form ?mouse, >10 cm? into one of the form ?mouse, size(x) = max2? if only two mice are larger than 10 cm. The same issues also make it difficult to interleave CD and linguistic realization as proposed by various authors, because properties may need to be combined before they are expressed. Incrementality (Section 8). Gradable adjectives complicate the notion of incrementality, in generation as well as interpretation. Focusing on generation, for example, they force us to reexamine the idea that properties can be put","@endWordPosition":"12719","@position":"76000","annotationId":"T45","@startWordPosition":"12716","@citStr":"Reiter and Dale 2000"}},"title":{"#tail":"\n","#text":"Building Natural Language Generation Systems."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ehud Reiter"},{"#tail":"\n","#text":"Robert Dale"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Reiter, Ehud and Somayajulu (Yaji) Sripada. 2002. Should corpora texts be gold standards for NLG? In Proceedings of Second International Conference on Natural Language Generation (INLG-2002), pages 97?104, New York."},"#text":"\n","pages":{"#tail":"\n","#text":"97--104"},"marker":{"#tail":"\n","#text":"Reiter, Sripada, 2002"},"location":{"#tail":"\n","#text":"New York."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"n element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible). 1.2 Vagueness in NLG Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by Reiter and Sripada (2002), where users can specify boundary values for attributes like rainfall, specifying, for example, rain counts as moderate above 7 mm/h, as heavy above 20 mm/h, and so on. A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart?s piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number ","@endWordPosition":"579","@position":"3773","annotationId":"T46","@startWordPosition":"576","@citStr":"Reiter and Sripada (2002)"},{"#tail":"\n","#text":"ng ??The two high numbers appear in brackets??? The outcomes of the experiment suggested that readers understand plural vague descriptions in accordance with the semantics of Section 2 (van Deemter 2000). In other words, they judged the description to be correct if and only if the two highest numbers in the sequence appeared in brackets. Assessing the evidence, it seems that vague descriptions are largely unproblematic from the point of view of interpretation. 7.3 Testing the Felicity of the Generated Expressions How can we choose between the different forms that a vague description can take? Reiter and Sripada (2002) showed that the variation in corpora based on expert authors 211 Computational Linguistics Volume 32, Number 2 can be considerable, especially in their use of vague expressions (e.g., by evening, by late evening, around midnight). We confirmed these findings using experiments with human subjects (van Deemter 2004), focusing on the choice between the different forms of the adjective. Informally: 1. The dichotomy constraint of Section 5 did not hold up well: Even when comparing two things, the superlative form was often preferred over the comparative. 2. When base forms were used, the gap was a","@endWordPosition":"8883","@position":"52727","annotationId":"T47","@startWordPosition":"8880","@citStr":"Reiter and Sripada (2002)"}]},"title":{"#tail":"\n","#text":"Should corpora texts be gold standards for NLG?"},"booktitle":{"#tail":"\n","#text":"In Proceedings of Second International Conference on Natural Language Generation (INLG-2002),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ehud Reiter"},{"#tail":"\n","#text":"Somayajulu Sripada"}]}},{"volume":{"#tail":"\n","#text":"32"},"#tail":"\n","date":{"#tail":"\n","#text":"1975"},"institution":{"#tail":"\n","#text":"Computational Linguistics"},"rawString":{"#tail":"\n","#text":"Computational Linguistics Volume 32, Number 2 Rosch, Eleanor. 1975. Cognitive reference points. Cognitive Psychology, 7:532?547."},"#text":"\n","pages":{"#tail":"\n","#text":"7--532"},"marker":{"#tail":"\n","#text":"1975"},"location":{"#tail":"\n","#text":"Rosch, Eleanor."},"title":{"#tail":"\n","#text":"Cognitive reference points. Cognitive Psychology,"},"@valid":"true"},{"date":{"#tail":"\n","#text":"1978"},"editor":{"#tail":"\n","#text":"In E. Rosch and B. Lloyd, editors,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"C := Domain For each Ai  A do Vi = FindBestValue(S, Ai) If S ? [[?Ai, Vi?]] & C ? [[?Ai, Vi?]] then do L := L ? {?Ai, Vi?} C := C ? [[?Ai, Vi?]] If C = S then Return L Return Failure FindBestValue selects the ?best value? from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity. The function selects the Value that removes most distractors, but in case of a tie, the least specific contestant is chosen, as long as it is not less specific than the basic-level Value (i.e., the most commonly occurring and psychologically most fundamental level, Rosch 1978). IAPlur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}. 3.2 The Existing Treatment of Gradables IAPlur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description. In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example. This approach does not do justice to gradable ad","@endWordPosition":"3018","@position":"17951","annotationId":"T48","@startWordPosition":"3017","@citStr":"Rosch 1978"}},"title":{"#tail":"\n","#text":"Principles of categorization."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Rosch, Eleanor. 1978. Principles of categorization. In E. Rosch and B. Lloyd, editors, Cognition and Categorization, Lawrence Erlbaum, Hillsdale, NJ, pages 27?48."},"#text":"\n","pages":{"#tail":"\n","#text":"27--48"},"marker":{"#tail":"\n","#text":"Rosch, 1978"},"location":{"#tail":"\n","#text":"Hillsdale, NJ,"},"booktitle":{"#tail":"\n","#text":"Cognition and Categorization, Lawrence Erlbaum,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Eleanor Rosch"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Sedivy, Julie, Michael Tanenhaus, Craig Chambers, and Gregory Carlson. 1999. Achieving incremental semantic interpretation through contextual representation. Cognition, 71:109?147."},"journal":{"#tail":"\n","#text":"Cognition,"},"#text":"\n","pages":{"#tail":"\n","#text":"71--109"},"marker":{"#tail":"\n","#text":"Sedivy, Tanenhaus, Chambers, Carlson, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"eferent of an evaluative description fall into the correct segment of the relevant dimension. (For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans?s and Fritz?s heights are 210 and 198 van Deemter GRE with Gradable Properties 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see Sedivy et al 1999, discussed in Section 7.2). Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary. With these qualifications in place, let us say more precisely what we will assume the different types of expressions to mean. For ease of reading, concrete examples (e.g., large) will replace abstract labels (e.g., ?Adj?), but the analysis is meant to be general. The largest n mouse/mice; The n large mice. Imagine a set C of contextually relevant animals. Then these noun phrases (NPs) presuppose that there is a subset ","@endWordPosition":"2074","@position":"12599","annotationId":"T49","@startWordPosition":"2071","@citStr":"Sedivy et al 1999"},{"#tail":"\n","#text":"the kind of numerical representations that our algorithm has used as input. Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. 6 Presumably, Beun and Cremers (1998) found vague adjectives to be rare because, in their experiments, referents could always be identified using nongradable dimensions. 210 van Deemter GRE with Gradable Properties 7.2 Testing the Correctness of the Generated Expressions Sedivy et al (1999) asked subjects to identify the target of a vague description in a visual scene. Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key). Across the different conditions under which the experiment was done (e","@endWordPosition":"8427","@position":"50065","annotationId":"T50","@startWordPosition":"8424","@citStr":"Sedivy et al (1999)"},{"#tail":"\n","#text":"emental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected? This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989). A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known. Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said. 8.2 Low Preference for Gradable Properties? It has been argued that, in an incremental ","@endWordPosition":"9599","@position":"57331","annotationId":"T51","@startWordPosition":"9596","@citStr":"Sedivy et al 1999"}]},"title":{"#tail":"\n","#text":"Achieving incremental semantic interpretation through contextual representation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Julie Sedivy"},{"#tail":"\n","#text":"Michael Tanenhaus"},{"#tail":"\n","#text":"Craig Chambers"},{"#tail":"\n","#text":"Gregory Carlson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"University of Maryland, College Park."},"rawString":{"#tail":"\n","#text":"Shaw, James and Vasileios Hatzivassiloglou. 1999. Ordering among premodifiers. In Proceedings of ACL99, pages 135?143, University of Maryland, College Park."},"#text":"\n","pages":{"#tail":"\n","#text":"135--143"},"marker":{"#tail":"\n","#text":"Shaw, Hatzivassiloglou, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ent, logical aspect of referring expressions generation from its language-dependent, linguistic aspect. Our algorithm suggests a distinction into three phases, the first two of which can be thought of as part of CD: 1. CD proper, that is, the production of a distinguishing list of properties L; 2. An inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives? relative position. Interestingly, vague properties tend to be realized before others. Quirk et al (1985), for example, report that ?adjectives denoting size, length, and height normally precede other nonderived adjectives? (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small(-est) [N], for exampl","@endWordPosition":"7612","@position":"44968","annotationId":"T52","@startWordPosition":"7609","@citStr":"Shaw and Hatzivassiloglou 1999"}},"title":{"#tail":"\n","#text":"Ordering among premodifiers."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL99,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"James Shaw"},{"#tail":"\n","#text":"Vasileios Hatzivassiloglou"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1982"},"rawString":{"#tail":"\n","#text":"Sonnenschein, Susan. 1982. The effects of redundant communications on listeners: When more is less. Child Development, 53:717?729."},"#text":"\n","pages":{"#tail":"\n","#text":"53--717"},"marker":{"#tail":"\n","#text":"Sonnenschein, 1982"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ies resulting from (5), unlike the inequalities resulting from (4), are context dependent. For example, ?mouse, size(x) = max2? (the largest two mice, {c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two elements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the short(est) black mouse if there is only one black mouse, because this might invite false implicatures. 8.1 Problems with Incrementality While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates. (Wildly redundant descriptions can result if the ?wrong? preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected? This question is","@endWordPosition":"9449","@position":"56297","annotationId":"T53","@startWordPosition":"9448","@citStr":"Sonnenschein 1982"}},"title":{"#tail":"\n","#text":"The effects of redundant communications on listeners: When more is less. Child Development,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Susan Sonnenschein"}}},{"volume":{"#tail":"\n","#text":"6"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Sripada, Yaji, Ehud Reiter, and Ian Davy. 2003. SumTime-Mousam: Configurable marine weather forecast generator. Expert Update, 6(3):4?10."},"journal":{"#tail":"\n","#text":"Expert Update,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Sripada, Reiter, Davy, 2003"},"title":{"#tail":"\n","#text":"SumTime-Mousam: Configurable marine weather forecast generator."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yaji Sripada"},{"#tail":"\n","#text":"Ehud Reiter"},{"#tail":"\n","#text":"Ian Davy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Stone, Matthew. 2000. On identifying sets. In Proceedings of INLG-2000, pages 116?123, Mitzpe Ramon, Israel."},"#text":"\n","pages":{"#tail":"\n","#text":"116--123"},"marker":{"#tail":"\n","#text":"Stone, 2000"},"location":{"#tail":"\n","#text":"Mitzpe Ramon,"},"title":{"#tail":"\n","#text":"On identifying sets."},"booktitle":{"#tail":"\n","#text":"In Proceedings of INLG-2000,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Matthew Stone"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Stone, Matthew and Bonnie Webber. 1998."},"#text":"\n","marker":{"#tail":"\n","#text":"Stone, Webber, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"en to enforce a specific extension (e.g., the large mice, when the mice are sized 3, 2.8, 2.499, and 2.498 cm). This would allow the generator to use vague expressions, but only where they result in a description that is itself unambiguous. We shall see in Section 7 that it has not been easy to confirm the pragmatic constraints of the present section experimentally. 208 van Deemter GRE with Gradable Properties 6. Linguistic Realization Some recent GRE algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead (Stone and Webber 1998; Krahmer and Theune 2002). We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = ?size > 3 cm, size < 9 cm?. If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm). Clearly, sophisticated use of gradable adjectives requires a separation bet","@endWordPosition":"7364","@position":"43468","annotationId":"T54","@startWordPosition":"7361","@citStr":"Stone and Webber 1998"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matthew Stone"},{"#tail":"\n","#text":"Bonnie Webber"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"Textual Economy through close coupling of syntax and semantics. In Proceedings of INLG-1998, pages 178?187."},"#text":"\n","pages":{"#tail":"\n","#text":"178--187"},"marker":{"#tail":"\n"},"title":{"#tail":"\n","#text":"Textual Economy through close coupling of syntax and semantics."},"booktitle":{"#tail":"\n","#text":"In Proceedings of INLG-1998,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Tho?risson, Kristinn R. 1994. Simulated perceptual grouping: An application to human-computer interaction. In Proceedings of 6th Annual Conference of the Cognitive Science Society, pages 876?881."},"#text":"\n","pages":{"#tail":"\n","#text":"876--881"},"marker":{"#tail":"\n","#text":"Thorisson, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"or should avoid the expression in question. If x and y are mice of sizes 10 and 9.9 cm, for example, then it is probably better to describe x as the largest mouse than as the large mouse. Prior to carrying out the experiments to be reported in Section 7, we believed that the following constraints should be taken into account: Small Gaps. Expressions of the form the (n) large [N] are infelicitous when the gap between (1) the smallest element of the designated set S (henceforth, s?) and (2) the largest N smaller than all elements of S (henceforth, s+) is small in comparison with the other gaps (Thorisson 1994; Funakoshi et al 2004). If this gap is so small as to make the difference between the sizes of s? and s+ impossible to perceive, then the expression is also infelicitous. Dichotomy. When separating one single referent from one distractor, the comparative form is often said to be favored (Use the comparative form to compare two things). We expected this to generalize to situations where all the referents are of one size, and all the distractors of another. Minimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English, where the ","@endWordPosition":"7069","@position":"41657","annotationId":"T55","@startWordPosition":"7068","@citStr":"Thorisson 1994"},{"#tail":"\n","#text":"mal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan, for example, would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors (Nash 1950; cf. Gorniak and Roy 2003; Thorisson 1994, for other plans). 9.3.2 Multidimensional Adjectives (and Color). Multidimensionality can also slip in through the backdoor. Consider big, for example, when applied to 3D shapes. If there exists a formula for mapping three dimensions into one (e.g., length ? width ? height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. Similar t","@endWordPosition":"10532","@position":"62919","annotationId":"T56","@startWordPosition":"10531","@citStr":"Thorisson 1994"},{"#tail":"\n","#text":"ty are possible. One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense. In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values. Alternatively, one could allow referring expressions to be ambiguous. It would be consistent with this attitude, for example, to map multiple dimensions into one overall dimension, perhaps by borrowing from principles applied in perceptual grouping, where different perceptual dimensions are mapped into one (e.g., Thorisson 1994). The empirical basis of this line of work, however, is still somewhat weak, so the risk of referential unclarity looms large. Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing. 9.4 Salience as a Gradable Property We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions. As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are ac","@endWordPosition":"10894","@position":"65214","annotationId":"T57","@startWordPosition":"10893","@citStr":"Thorisson 1994"}]},"title":{"#tail":"\n","#text":"Simulated perceptual grouping: An application to human-computer interaction."},"booktitle":{"#tail":"\n","#text":"In Proceedings of 6th Annual Conference of the Cognitive Science Society,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Kristinn R Thorisson"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Lancet,"},"date":{"#tail":"\n","#text":"1980"},"rawString":{"#tail":"\n","#text":"Toogood, John H. 1980. What do we mean by ?usually?? Lancet, 1:1094."},"#text":"\n","pages":{"#tail":"\n","#text":"1--1094"},"marker":{"#tail":"\n","#text":"Toogood, 1980"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" adjectives, letting these values depend on the context in which the adjective is used. Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact. Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair, since vague expressions tend to be interpreted in different ways by different people (Toogood 1980), sometimes in stark contrast with the intention of the speaker/writer (Berry, Knapp, and Raynor 2002). We shall therefore focus?unlike earlier computational accounts?on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective. Suppose you enter a vet?s surgery in the company of two dogs: a big one on a leash, and a tiny one in your arms. The vet asks ?Who?s the patient??, and you answer ?the big dog.? This answer will allow the vet to pick out the patient just as reliably as if you had said ?the ","@endWordPosition":"822","@position":"5185","annotationId":"T58","@startWordPosition":"821","@citStr":"Toogood 1980"}},"title":{"#tail":"\n","#text":"What do we mean by ?usually??"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"John H Toogood"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"van Deemter, Kees. 2000. Generating vague descriptions. In Proceedings of International Conference on Natural Language Generation (INLG-2000), pages 179?185, Mitzpe Ramon, Israel."},"#text":"\n","pages":{"#tail":"\n","#text":"179--185"},"marker":{"#tail":"\n","#text":"van Deemter, 2000"},"location":{"#tail":"\n","#text":"Mitzpe Ramon,"},"title":{"#tail":"\n","#text":"Generating vague descriptions."},"booktitle":{"#tail":"\n","#text":"In Proceedings of International Conference on Natural Language Generation (INLG-2000),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Kees van Deemter"}}},{"volume":{"#tail":"\n","#text":"28"},"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"van Deemter, Kees. 2002. Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37?52."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"van Deemter, 2002"},"title":{"#tail":"\n","#text":"Generating referring expressions: Boolean extensions of the incremental algorithm."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Kees van Deemter"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"van Deemter, Kees. 2004. Finetuning an NLG system through experiments with human subjects: The case of vague descriptions. In Proceedings of 3rd International Conference on Natural Language Generation (INLG-04), pages 31?40, Brockenhurst, UK."},"#text":"\n","pages":{"#tail":"\n","#text":"31--40"},"marker":{"#tail":"\n","#text":"van Deemter, 2004"},"location":{"#tail":"\n","#text":"Brockenhurst, UK."},"title":{"#tail":"\n","#text":"Finetuning an NLG system through experiments with human subjects: The case of vague descriptions."},"booktitle":{"#tail":"\n","#text":"In Proceedings of 3rd International Conference on Natural Language Generation (INLG-04),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Kees van Deemter"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"van Deemter, Kees and Jan Odijk. 1997. Context modeling and the generation of spoken discourse. Speech Communication, 21:101?121."},"journal":{"#tail":"\n","#text":"Speech Communication,"},"#text":"\n","pages":{"#tail":"\n","#text":"21--101"},"marker":{"#tail":"\n","#text":"van Deemter, Odijk, 1997"},"title":{"#tail":"\n","#text":"Context modeling and the generation of spoken discourse."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kees van Deemter"},{"#tail":"\n","#text":"Jan Odijk"}]}}]}}]}}
