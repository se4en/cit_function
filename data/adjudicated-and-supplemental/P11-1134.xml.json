{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","note":{"#tail":"\n","@confidence":"0.616079","#text":"\nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1336?1345,\n"},"listItem":[{"#tail":"\n","@confidence":"0.7849835","#text":"\n1. use the paraphrase table in l2 to find para-\nphrases of phrases extracted from H;\n2. map them to entries in the phrase table, and ex-\ntract their equivalents in l1;\n1338\n3. use the paraphrase table in l1 to find para-\nphrases of the extracted fragments in l1;\n4. map such paraphrases to phrases in T.\n"},{"#tail":"\n","@confidence":"0.997759333333333","#text":"\n1. Exact: in the case that two phrases are identical\nat one of the three levels (token, lemma, stem);\n2. Lexical: in the case that two different phrases\n"},{"#tail":"\n","@confidence":"0.7141696","#text":"\nwith different thresholds.\n- Universal dictionary database8: 9,944 entries.\n- Wiktionary database9: 5,866 entries.\n- Omegawiki database10: 8,237 entries.\n- Wikipedia interlanguage links11: 7,425 entries.\n"}],"figure":{"#tail":"\n","@confidence":"0.5979525","#text":"\nYashar Mehdad\nFBK - irst and Uni. of Trento\nPovo (Trento), Italy\nmehdad@fbk.eu\nMatteo Negri\nFBK - irst\nPovo (Trento), Italy\nnegri@fbk.eu\nMarcello Federico\nFBK - irst\n"},"address":[{"#tail":"\n","@confidence":"0.4775","#text":"\nPortland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics\n"},{"#tail":"\n","@confidence":"0.429678","#text":"\nPovo (Trento), Italy\n"}],"equation":{"#tail":"\n","@confidence":"0.936176666666667","#text":"\nScoren =\nMn\nNn\n"},"subsectionHeader":[{"#tail":"\n","@confidence":"0.998855","#text":"\n3.1 Extracting Phrase and Paraphrase Tables\n"},{"#tail":"\n","@confidence":"0.999464","#text":"\n3.2 Phrasal Matching Method\n"},{"#tail":"\n","@confidence":"0.988321","#text":"\n4.1 Dataset\n"},{"#tail":"\n","@confidence":"0.974614","#text":"\n4.2 Knowledge sources\n"},{"#tail":"\n","@confidence":"0.977696","#text":"\n4.3 Results and Discussion\n"},{"#tail":"\n","@confidence":"0.965162","#text":"\n5.1 Dataset\n"},{"#tail":"\n","@confidence":"0.99427","#text":"\n5.2 Knowledge sources\n"},{"#tail":"\n","@confidence":"0.858051","#text":"\n5.3 Results and Discussion\n"}],"footnote":[{"#tail":"\n","@confidence":"0.926707","#text":"\n1http://www.statmt.org/wmt10/\n"},{"#tail":"\n","@confidence":"0.980205","#text":"\n2http://www.cs.cmu.edu/ alavie/METEOR\n"},{"#tail":"\n","@confidence":"0.999455333333333","#text":"\n3http://crowdflower.com/\n4https://www.mturk.com/mturk/\n5Workers? trustworthiness can be automatically determined\nby means of hidden gold units randomly inserted into jobs.\n6http://snowball.tartarus.org/\n7http://xdxf.revdanica.com/\n"},{"#tail":"\n","@confidence":"0.9968715","#text":"\n8http://www.dicts.info/\n9http://en.wiktionary.org/\n10http://www.omegawiki.org/\n11http://www.wikipedia.org/\n"}],"title":{"#tail":"\n","@confidence":"0.506394","#text":"\nUsing Bilingual Parallel Corpora\nfor Cross-Lingual Textual Entailment\n"},"@confidence":"0.000001","#tail":"\n","reference":{"#tail":"\n","@confidence":"0.994728276119403","#text":"\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe.\n1998. The Berkeley FrameNet project. Proceedings\nof COLING-ACL.\nColin Bannard and Chris Callison-Burch. 2005. Para-\nphrasing with Bilingual Parallel Corpora. Proceed-\nings of the 43rd Annual Meeting of the Association for\nComputational Linguistics (ACL 2005).\nRoy Bar-haim , Jonathan Berant , Ido Dagan , Iddo\nGreental , Shachar Mirkin , Eyal Shnarch , and Idan\nSzpektor. 2008. Efficient semantic deduction and ap-\nproximate matching over compact parse forests. Pro-\nceedings of the TAC 2008 Workshop on Textual Entail-\nment.\nLuisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang\nDang, and Danilo Giampiccolo. 2010. The Sixth\nPASCAL Recognizing Textual Entailment Challenge.\nProceedings of the the Text Analysis Conference (TAC\n2010).\nTimothy Chklovski and Patrick Pantel. 2004. Verbocean:\nMining the web for fine-grained semantic verb rela-\ntions. Proceedings of Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP-04).\nIdo Dagan and Oren Glickman. 2004. Probabilistic tex-\ntual entailment: Generic applied modeling of language\nvariability. Proceedings of the PASCAL Workshop of\nLearning Methods for Text Understanding and Min-\ning.\nIdo Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.\n2009. Recognizing textual entailment: Rational, eval-\nuation and approaches. Journal of Natural Language\nEngineering , Volume 15, Special Issue 04, pp i-xvii.\nMichael Denkowski and Alon Lavie. 2010. Extending\nthe METEOR Machine Translation Evaluation Metric\nto the Phrase Level. Proceedings of Human Language\nTechnologies (HLT-NAACL 2010).\nGeorgiana Dinu and Rui Wang. 2009. Inference Rules\nand their Application to Recognizing Textual Entail-\nment. Proceedings of the 12th Conference of the Eu-\nropean Chapter of the ACL (EACL 2009).\nClaudio Giuliano. 2007. jLSI a tool for la-\ntent semantic indexing. Software avail-\nable at http://tcc.itc.it/research/textec/tools-\nresources/jLSI.html.\nLidija Iordanskaja, Richard Kittredge, and Alain Polg re..\n1991. Lexical selection and paraphrase in a meaning\ntext generation model. Natural Language Generation\nin Articial Intelligence and Computational Linguistics.\nThorsten Joachims. 1999. Making large-scale support\nvector machine learning practical.\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu 2003.\nStatistical Phrase-Based Translation. Proceedings of\nHLT/NAACL.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran, Richard\nZens, Chris Dyer, Ondrej Bojar, Alexandra Con-\nstantin, and Evan Herbst. 2007. Moses: Open Source\nToolkit for Statistical Machine Translation. Proceed-\nings of the Conference of the Association for Compu-\ntational Linguistics (ACL).\nMilen Kouleykov and Bernardo Magnini. 2005. Tree\nedit distance for textual entailment. Proceedings of\nRALNP-2005, International Conference on Recent Ad-\nvances in Natural Language Processing.\nMilen Kouylekov, Yashar Mehdad, and Matteo Negri.\n2010. Mining Wikipedia for Large-Scale Repositories\nof Context-Sensitive Entailment Rules. Proceedings\nof the Language Resources and Evaluation Conference\n(LREC 2010).\nYashar Mehdad, Alessandro Moschitti and Fabio Mas-\nsimo Zanzotto. 2010. Syntactic/semantic structures\nfor textual entailment recognition. Proceedings of the\n11th Annual Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL HLT 2010).\nDekang Lin and Patrick Pantel. 2001. DIRT - Discovery\nof Inference Rules from Text.. Proceedings of ACM\nConference on Knowledge Discovery and Data Mining\n(KDD-01).\nKathleen R. McKeown, Regina Barzilay, David Evans,\nVasileios Hatzivassiloglou, Judith L. Klavans, Ani\nNenkova, Carl Sable, Barry Schiffman, and Sergey\nSigelman. 2002. Tracking and summarizing news on\na daily basis with Columbias Newsblaster. Proceed-\nings of the Human Language Technology Conference..\nYashar Mehdad, Matteo Negri, and Marcello Federico.\n2010. Towards Cross-Lingual Textual Entailment.\nProceedings of the 11th Annual Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics (NAACL HLT 2010).\nDan Moldovan and Adrian Novischi. 2002. Lexical\nchains for question answering. Proceedings of COL-\nING.\nMatteo Negri and Yashar Mehdad. 2010. Creating a Bi-\nlingual Entailment Corpus through Translations with\nMechanical Turk: $100 for a 10-day Rush. Proceed-\nings of the NAACL 2010 Workshop on Creating Speech\nand Language Data With Amazons Mechanical Turk .\nFranz Josef Och and Hermann Ney. 2003. A system-\natic comparison of various statistical alignment mod-\nels. Computational Linguistics, 29(1):1951.\n1344\nEmanuele Pianta, Luisa Bentivogli, and Christian Gi-\nrardi. 2002. MultiWordNet: Developing and Aligned\nMultilingual Database. Proceedings of the First Inter-\nnational Conference on Global WordNet.\nVasile Rus, Art Graesser, and Kirtan Desai 2005.\nLexico-Syntactic Subsumption for Textual Entailment.\nProceedings of RANLP 2005.\nHelmut Schmid 2005. Probabilistic Part-of-Speech Tag-\nging Using Decision Trees. Proceedings of the In-\nternational Conference on New Methods in Language\nProcessing.\nMarta Tatu andDan Moldovan. 2005. A semantic ap-\nproach to recognizing textual entailment. Proceed-\nings of the Human Language Technology Conference\nand Conference on Empirical Methods in Natural Lan-\nguage Processing (HLT/EMNLP 2005).\nMatthew Snover, Nitin Madnani, Bonnie Dorr, and\nRichard Schwartz. 2009. Fluency, Adequacy, or\nHTER? Exploring Different Human Judgments with\na Tunable MT Metric. Proceedings of WMT09.\nRui Wang and Yi Zhang,. 2009. Recognizing Tex-\ntual Relatedness with Predicate-Argument Structures.\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing (EMNLP 2009).\nKenji Yamada and Kevin Knight 2001. A Syntax-Based\nStatistical Translation Model. Proceedings of the Con-\nference of the Association for Computational Linguis-\ntics (ACL).\nShiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.\n2009. Extracting Paraphrase Patterns from Bilingual\nParallel Corpora. Journal of Natural Language Engi-\nneering , Volume 15, Special Issue 04, pp 503-526.\n"},"bodyText":[{"#tail":"\n","@confidence":"0.999232090909091","#text":"\nThis paper explores the use of bilingual par-\nallel corpora as a source of lexical knowl-\nedge for cross-lingual textual entailment. We\nclaim that, in spite of the inherent difficul-\nties of the task, phrase tables extracted from\nparallel data allow to capture both lexical re-\nlations between single words, and contextual\ninformation useful for inference. We experi-\nment with a phrasal matching method in or-\nder to: i) build a system portable across lan-\nguages, and ii) evaluate the contribution of\nlexical knowledge in isolation, without inter-\naction with other inference mechanisms. Re-\nsults achieved on an English-Spanish corpus\nobtained from the RTE3 dataset support our\nclaim, with an overall accuracy above average\nscores reported by RTE participants on mono-\nlingual data. Finally, we show that using par-\nallel corpora to extract paraphrase tables re-\nveals their potential also in the monolingual\nsetting, improving the results achieved with\nother sources of lexical knowledge.\n"},{"#tail":"\n","@confidence":"0.996131113636364","#text":"\nCross-lingual Textual Entailment (CLTE) has been\nproposed by (Mehdad et al, 2010) as an extension\nof Textual Entailment (Dagan and Glickman, 2004)\nthat consists in deciding, given two texts T and H in\ndifferent languages, if the meaning of H can be in-\nferred from the meaning of T. The task is inherently\ndifficult, as it adds issues related to the multilingual\ndimension to the complexity of semantic inference\nat the textual level. For instance, the reliance of cur-\nrent monolingual TE systems on lexical resources\n(e.g. WordNet, VerbOcean, FrameNet) and deep\nprocessing components (e.g. syntactic and semantic\nparsers, co-reference resolution tools, temporal ex-\npressions recognizers and normalizers) has to con-\nfront, at the cross-lingual level, with the limited\navailability of lexical/semantic resources covering\nmultiple languages, the limited coverage of the ex-\nisting ones, and the burden of integrating language-\nspecific components into the same cross-lingual ar-\nchitecture.\nAs a first step to overcome these problems,\n(Mehdad et al, 2010) proposes a ?basic solution?,\nthat brings CLTE back to the monolingual scenario\nby translating H into the language of T. Despite the\nadvantages in terms of modularity and portability of\nthe architecture, and the promising experimental re-\nsults, this approach suffers from one main limitation\nwhich motivates the investigation on alternative so-\nlutions. Decoupling machine translation (MT) and\nTE, in fact, ties CLTE performance to the availabil-\nity of MT components, and to the quality of the\ntranslations. As a consequence, on one side trans-\nlation errors propagate to the TE engine hampering\nthe entailment decision process. On the other side\nsuch unpredictable errors reduce the possibility to\ncontrol the behaviour of the engine, and devise ad-\nhoc solutions to specific entailment problems.\nThis paper investigates the idea, still unexplored,\nof a tighter integration of MT and TE algorithms and\ntechniques. Our aim is to embed cross-lingual pro-\ncessing techniques inside the TE recognition pro-\ncess in order to avoid any dependency on external\nMT components, and eventually gain full control of\nthe system?s behaviour. Along this direction, we\n"},{"#tail":"\n","@confidence":"0.981616925531915","#text":"\nstart from the acquisition and use of lexical knowl-\nedge, which represents the basic building block of\nany TE system. Using the basic solution proposed\nby (Mehdad et al, 2010) as a term of comparison,\nwe experiment with different sources of multilingual\nlexical knowledge to address the following ques-\ntions:\n(1) What is the potential of the existing mul-\ntilingual lexical resources to approach CLTE?\nTo answer this question we experiment with lex-\nical knowledge extracted from bilingual dictionar-\nies, and from a multilingual lexical database. Such\nexperiments show two main limitations of these re-\nsources, namely: i) their limited coverage, and ii)\nthe difficulty to capture contextual information when\nonly associations between single words (or at most\nnamed entities and multiword expressions) are used\nto support inference.\n(2) Does MT provide useful resources or tech-\nniques to overcome the limitations of existing re-\nsources? We envisage several directions in which\ninputs from MT research may enable or improve\nCLTE. As regards the resources, phrase and para-\nphrase tables extracted from bilingual parallel cor-\npora can be exploited as an effective way to cap-\nture both lexical relations between single words, and\ncontextual information useful for inference. As re-\ngards the algorithms, statistical models based on co-\noccurrence observations, similar to those used inMT\nto estimate translation probabilities, may contribute\nto estimate entailment probabilities in CLTE. Focus-\ning on the resources direction, the main contribu-\ntion of this paper is to show that the lexical knowl-\nedge extracted from parallel corpora allows to sig-\nnificantly improve the results achieved with other\nmultilingual resources.\n(3) In the cross-lingual scenario, can we achieve\nresults comparable to those obtained in mono-\nlingual TE? Our experiments show that, although\nCLTE seems intrinsically more difficult, the results\nobtained using phrase and paraphrase tables are bet-\nter than those achieved by average systems on mono-\nlingual datasets. We argue that this is due to the\nfact that parallel corpora are a rich source of cross-\nlingual paraphrases with no equivalents in monolin-\ngual TE.\n(4) Can parallel corpora be useful also for mono-\nlingual TE? To answer this question, we experiment\non monolingual RTE datasets using paraphrase ta-\nbles extracted from bilingual parallel corpora. Our\nresults improve those achieved with the most widely\nused resources in monolingual TE, namely Word-\nNet, Verbocean, and Wikipedia.\nThe remainder of this paper is structured as fol-\nlows. Section 2 shortly overviews the role of lexical\nknowledge in textual entailment, highlighting a gap\nbetween TE and CLTE in terms of available knowl-\nedge sources. Sections 3 and 4 address the first three\nquestions, giving motivations for the use of bilingual\nparallel corpora in CLTE, and showing the results of\nour experiments. Section 5 addresses the last ques-\ntion, reporting on our experiments with paraphrase\ntables extracted from phrase tables on the monolin-\ngual RTE datasets. Section 6 concludes the paper,\nand outlines the directions of our future research.\n2 Lexical resources for TE and CLTE\nAll current approaches to monolingual TE, ei-\nther syntactically oriented (Rus et al, 2005), or\napplying logical inference (Tatu and Moldovan,\n2005), or adopting transformation-based techniques\n(Kouleykov and Magnini, 2005; Bar-Haim et al,\n2008), incorporate different types of lexical knowl-\nedge to support textual inference. Such information\nranges from i) lexical paraphrases (textual equiva-\nlences between terms) to ii) lexical relations pre-\nserving entailment between words, and iii) word-\nlevel similarity/relatedness scores. WordNet, the\nmost widely used resource in TE, provides all the\nthree types of information. Synonymy relations\ncan be used to extract lexical paraphrases indicat-\ning that words from the text and the hypothesis en-\ntail each other, thus being interchangeable. Hy-\npernymy/hyponymy chains can provide entailment-\npreserving relations between concepts, indicating\nthat a word in the hypothesis can be replaced\nby a word from the text. Paths between con-\ncepts and glosses can be used to calculate simi-\nlarity/relatedness scores between single words, that\ncontribute to the computation of the overall similar-\nity between the text and the hypothesis.\nBesides WordNet, the RTE literature documents\nthe use of a variety of lexical information sources\n(Bentivogli et al, 2010; Dagan et al, 2009).\nThese include, just to mention the most popular\n"},{"#tail":"\n","@confidence":"0.998159","#text":"\nones, DIRT (Lin and Pantel, 2001), VerbOcean\n(Chklovski and Pantel, 2004), FrameNet (Baker et\nal., 1998), and Wikipedia (Mehdad et al, 2010;\nKouylekov et al, 2009). DIRT is a collection of sta-\ntistically learned inference rules, that is often inte-\ngrated as a source of lexical paraphrases and entail-\nment rules. VerbOcean is a graph of fine-grained\nsemantic relations between verbs, which are fre-\nquently used as a source of precise entailment rules\nbetween predicates. FrameNet is a knowledge-base\nof frames describing prototypical situations, and the\nrole of the participants they involve. It can be\nused as an alternative source of entailment rules,\nor to determine the semantic overlap between texts\nand hypotheses. Wikipedia is often used to extract\nprobabilistic entailment rules based word similar-\nity/relatedness scores.\nDespite the consensus on the usefulness of lexi-\ncal knowledge for textual inference, determining the\nactual impact of these resources is not straightfor-\nward, as they always represent one component in\ncomplex architectures that may use them in differ-\nent ways. As emerges from the ablation tests re-\nported in (Bentivogli et al, 2010), even the most\ncommon resources proved to have a positive impact\non some systems and a negative impact on others.\nSome previous works (Bannard and Callison-Burch,\n2005; Zhao et al, 2009; Kouylekov et al, 2009)\nindicate, as main limitations of the mentioned re-\nsources, their limited coverage, their low precision,\nand the fact that they are mostly suitable to capture\nrelations mainly between single words.\nAddressing CLTE we have to face additional and\nmore problematic issues related to: i) the stronger\nneed of lexical knowledge, and ii) the limited avail-\nability of multilingual lexical resources. As regards\nthe first issue, it?s worth noting that in the monolin-\ngual scenario simple ?bag of words? (or ?bag of n-\ngrams?) approaches are per se sufficient to achieve\nresults above baseline. In contrast, their applica-\ntion in the cross-lingual setting is not a viable so-\nlution due to the impossibility to perform direct lex-\nical matches between texts and hypotheses in differ-\nent languages. This situation makes the availability\nof multilingual lexical knowledge a necessary con-\ndition to bridge the language gap. However, with\nthe only exceptions represented by WordNet and\nWikipedia, most of the aforementioned resources\nare available only for English. Multilingual lexi-\ncal databases aligned with the EnglishWordNet (e.g.\nMultiWordNet (Pianta et al, 2002)) have been cre-\nated for several languages, with different degrees of\ncoverage. As an example, the 57,424 synsets of the\nSpanish section of MultiWordNet algned to English\ncover just around 50% of the WordNet?s synsets,\nthus making the coverage issue even more problem-\natic than for TE. As regards Wikipedia, the cross-\nlingual links between pages in different languages\noffer a possibility to extract lexical knowledge use-\nful for CLTE. However, due to their relatively small\nnumber (especially for some languages), bilingual\nlexicons extracted from Wikipedia are still inade-\nquate to provide acceptable coverage. In addition,\nfeaturing a bias towards named entities, the infor-\nmation acquired through cross-lingual links can at\nmost complement the lexical knowledge extracted\nfrom more generic multilingual resources (e.g bilin-\ngual dictionaries).\n"},{"#tail":"\n","@confidence":"0.976821238095238","#text":"\nBilingual parallel corpora represent a possible solu-\ntion to overcome the inadequacy of the existing re-\nsources, and to implement a portable approach for\nCLTE. To this aim, we exploit parallel data to: i)\nlearn alignment criteria between phrasal elements\nin different languages, ii) use them to automatically\nextract lexical knowledge in the form of phrase ta-\nbles, and iii) use the obtained phrase tables to create\nmonolingual paraphrase tables.\nGiven a cross-lingual T/H pair (with the text in\nl1 and the hypothesis in l2), our approach leverages\nthe vast amount of lexical knowledge provided by\nphrase and paraphrase tables to map H into T. We\nperform such mapping with two different methods.\nThe first method uses a single phrase table to di-\nrectly map phrases extracted from the hypothesis to\nphrases in the text. In order to improve our system?s\ngeneralization capabilities and increase the cover-\nage, the second method combines the phrase table\nwith two monolingual paraphrase tables (one in l1,\nand one in l2). This allows to:\n"},{"#tail":"\n","@confidence":"0.9954815","#text":"\nWith the second method, phrasal matches between\nthe text and the hypothesis are indirectly performed\nthrough paraphrases of the phrase table entries.\nThe final entailment decision for a T/H pair is as-\nsigned considering a model learned from the similar-\nity scores based on the identified phrasal matches.\nIn particular, ?YES? and ?NO? judgements are as-\nsigned considering the proportion of words in the\nhypothesis that are found also in the text. This way\nto approximate entailment reflects the intuition that,\nas a directional relation between the text and the hy-\npothesis, the full content of H has to be found in T.\n"},{"#tail":"\n","@confidence":"0.985345259259259","#text":"\nPhrase tables (PHT) contain pairs of correspond-\ning phrases in two languages, together with associa-\ntion probabilities. They are widely used in MT as a\nway to figure out how to translate input in one lan-\nguage into output in another language (Koehn et al,\n2003). There are several methods to build phrase ta-\nbles. The one adopted in this work consists in learn-\ning phrase alignments from a word-aligned bilingual\ncorpus. In order to build English-Spanish phrase ta-\nbles for our experiments, we used the freely avail-\nable Europarl V.4, News Commentary and United\nNations Spanish-English parallel corpora released\nfor the WMT101. We run TreeTagger (Schmid,\n1994) for tokenization, and used the Giza++ (Och\nand Ney, 2003) to align the tokenized corpora at\nthe word level. Subsequently, we extracted the bi-\nlingual phrase table from the aligned corpora using\nthe Moses toolkit (Koehn et al, 2007). Since the re-\nsulting phrase table was very large, we eliminated\nall the entries with identical content in the two lan-\nguages, and the ones containing phrases longer than\n5 words in one of the two sides. In addition, in or-\nder to experiment with different phrase tables pro-\nviding different degrees of coverage and precision,\nwe extracted 7 phrase tables by pruning the initial\none on the direct phrase translation probabilities of\n0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting\n"},{"#tail":"\n","@confidence":"0.9843422","#text":"\nphrase tables range from 76 to 48 million entries,\nwith an average of 3.9 words per phrase.\nParaphrase tables (PPHT) contain pairs of corre-\nsponding phrases in the same language, possibly as-\nsociated with probabilities. They proved to be use-\nful in a number of NLP applications such as natural\nlanguage generation (Iordanskaja et al, 1991), mul-\ntidocument summarization (McKeown et al, 2002),\nautomatic evaluation of MT (Denkowski and Lavie,\n2010), and TE (Dinu and Wang, 2009).\nOne of the proposed methods to extract para-\nphrases relies on a pivot-based approach using\nphrase alignments in a bilingual parallel corpus\n(Bannard and Callison-Burch, 2005). With this\nmethod, all the different phrases in one language that\nare aligned with the same phrase in the other lan-\nguage are extracted as paraphrases. After the extrac-\ntion, pruning techniques (Snover et al, 2009) can\nbe applied to increase the precision of the extracted\nparaphrases.\nIn our work we used available2 paraphrase\ndatabases for English and Spanish which have been\nextracted using the method previously outlined.\nMoreover, in order to experiment with different\nparaphrase sets providing different degrees of cov-\nerage and precision, we pruned the main paraphrase\ntable based on the probabilities, associated to its en-\ntries, of 0.1, 0.2 and 0.3. The number of phrase pairs\nextracted varies from 6 million to about 80000, with\nan average of 3.2 words per phrase.\n"},{"#tail":"\n","@confidence":"0.994676384615384","#text":"\nIn order to maximize the usage of lexical knowledge,\nour entailment decision criterion is based on similar-\nity scores calculated with a phrase-to-phrase match-\ning process.\nA phrase in our approach is an n-gram composed\nof up to 5 consecutive words, excluding punctua-\ntion. Entailment decisions are estimated by com-\nbining phrasal matching scores (Scoren) calculated\nfor each level of n-grams , which is the number\nof 1-grams, 2-grams,..., 5-grams extracted from H\nthat match with n-grams in T. Phrasal matches are\nperformed either at the level of tokens, lemmas, or\nstems, can be of two types:\n"},{"#tail":"\n","@confidence":"0.962335157894737","#text":"\ncan be mapped through entries of the resources\nused to bridge T and H (i.e. phrase tables, para-\nphrases tables, dictionaries or any other source\nof lexical knowledge).\nFor each phrase in H, we first search for exact\nmatches at the level of token with phrases in T. If\nno match is found at a token level, the other levels\n(lemma and stem) are attempted. Then, in case of\nfailure with exact matching, lexical matching is per-\nformed at the same three levels. To reduce redun-\ndant matches, the lexical matches between pairs of\nphrases which have already been identified as exact\nmatches are not considered.\nOnce matching for each n-gram level has been\nconcluded, the number of matches (Mn) and the\nnumber of phrases in the hypothesis (Nn) are used\nto estimate the portion of phrases in H that are\nmatched at each level (n). The phrasal matching\nscore for each n-gram level is calculated as follows:\n"},{"#tail":"\n","@confidence":"0.9931728","#text":"\nTo combine the phrasal matching scores obtained\nat each n-gram level, and optimize their relative\nweights, we trained a Support Vector Machine clas-\nsifier, SVMlight (Joachims, 1999), using each score\nas a feature.\n"},{"#tail":"\n","@confidence":"0.994051833333333","#text":"\nTo address the first two questions outlined in Sec-\ntion 1, we experimented with the phrase matching\nmethod previously described, contrasting the effec-\ntiveness of lexical information extracted from par-\nallel corpora with the knowledge provided by other\nresources used in the same way.\n"},{"#tail":"\n","@confidence":"0.994469272727273","#text":"\nThe dataset used for our experiments is an English-\nSpanish entailment corpus obtained from the orig-\ninal RTE3 dataset by translating the English hy-\npothesis into Spanish. It consists of 1600 pairs\nderived from the RTE3 development and test sets\n(800+800). Translations have been generated by\nthe CrowdFlower3 channel to Amazon Mechanical\nTurk4 (MTurk), adopting the methodology proposed\nby (Negri and Mehdad, 2010). The method relies\non translation-validation cycles, defined as separate\njobs routed to MTurk?s workforce. Translation jobs\nreturn one Spanish version for each hypothesis. Val-\nidation jobs ask multiple workers to check the cor-\nrectness of each translation using the original En-\nglish sentence as reference. At each cycle, the trans-\nlated hypothesis accepted by the majority of trust-\nful validators5 are stored in the CLTE corpus, while\nwrong translations are sent back to workers in a\nnew translation job. Although the quality of the re-\nsults is enhanced by the possibility to automatically\nweed out untrusted workers using gold units, we per-\nformed a manual quality check on a subset of the ac-\nquired CLTE corpus. The validation, carried out by\na Spanish native speaker on 100 randomly selected\npairs after two translation-validation cycles, showed\nthe good quality of the collected material, with only\n3 minor ?errors? consisting in controversial but sub-\nstantially acceptable translations reflecting regional\nSpanish variations.\nThe T-H pairs in the collected English-Spanish\nentailment corpus were annotated using TreeTagger\n(Schmid, 1994) and the Snowball stemmer6 with to-\nken, lemma, and stem information.\n"},{"#tail":"\n","@confidence":"0.978756111111111","#text":"\nFor comparison with the extracted phrase and para-\nphrase tables, we use a large bilingual dictionary\nand MultiWordNet as alternative sources of lexical\nknowledge.\nBilingual dictionaries (DIC) allow for precise\nmappings between words in H and T. To create\na large bilingual English-Spanish dictionary we\nprocessed and combined the following dictionaries\nand bilingual resources:\n"},{"#tail":"\n","@confidence":"0.9961687","#text":"\nThe resulting dictionary features 53,958 entries,\nwith an average length of 1.2 words.\nMultiWordNet (MWN) allows to extract mappings\nbetween English and Spanish words connected by\nentailment-preserving semantic relations. The ex-\ntraction process is dataset-dependent, as it checks\nfor synonymy and hyponymy relations only between\nterms found in the dataset. The resulting collection\nof cross-lingual words associations contains 36,794\npairs of lemmas.\n"},{"#tail":"\n","@confidence":"0.998401222222222","#text":"\nOur results are calculated over 800 test pairs of our\nCLTE corpus, after training the SVM classifier over\n800 development pairs. This section reports the\npercentage of correct entailment assignments (accu-\nracy), comparing the use of different sources of lex-\nical knowledge.\nInitially, in order to find a reasonable trade-off be-\ntween precision and coverage, we used the 7 phrase\ntables extracted with different pruning thresholds\n"},{"#tail":"\n","@confidence":"0.997328846153846","#text":"\nresources.\n(see Section 3.1). Figure 1 shows that with the prun-\ning threshold set to 0.05, we obtain the highest re-\nsult of 62.62% on the test set. The curve demon-\nstrates that, although with higher pruning thresholds\nwe retain more reliable phrase pairs, their smaller\nnumber provides limited coverage leading to lower\nresults. In contrast, the large coverage obtained with\nthe pruning threshold set to 0.01 leads to a slight\nperformance decrease due to probably less precise\nphrase pairs.\nOnce the threshold has been set, in order to\nprove the effectiveness of information extracted\nfrom bilingual corpora, we conducted a series of ex-\nperiments using the different resources mentioned in\nSection 4.2.\nAs it can be observed in Table 1, the highest\nresults are achieved using the phrase table, both\nalone and in combination with paraphrase tables\n(62.62% and 62.88% respectively). These results\nsuggest that, with appropriate pruning thresholds,\nthe large number and the longer entries contained\nin the phrase and paraphrase tables represent an ef-\nfective way to: i) obtain high coverage, and ii) cap-\nture cross-lingual associations between multiple lex-\nical elements. This allows to overcome the bias to-\nwards single words featured by dictionaries and lex-\nical databases.\nAs regards the other resources used for compari-\nson, the results show that dictionaries substantially\noutperform MWN. This can be explained by the\nlow coverage of MWN, whose entries also repre-\nsent weaker semantic relations (preserving entail-\nment, but with a lower probability to be applied)\nthan the direct translations between terms contained\nin the dictionary.\nOverall, our results suggest that the lexical knowl-\nedge extracted from parallel data can be successfully\nused to approach the CLTE task.\n"},{"#tail":"\n","@confidence":"0.97711180952381","#text":"\n5 Using parallel corpora for TE\nThis section addresses the third and the fourth re-\nsearch questions outlined in Section 1. Building\non the positive results achieved on the cross-lingual\nscenario, we investigate the possibility to exploit\nbilingual parallel corpora in the traditional monolin-\ngual scenario. Using the same approach discussed\nin Section 4, we compare the results achieved with\nEnglish paraphrase tables with those obtained with\nother widely used monolingual knowledge resources\nover two RTE datasets.\nFor the sake of completeness, we report in this\nsection also the results obtained adopting the ?basic\nsolution? proposed by (Mehdad et al, 2010). Al-\nthough it was presented as an approach to CLTE,\nthe proposed method brings the problem back to the\nmonolingual case by translating H into the language\nof T. The comparison with this method aims at ver-\nifying the real potential of parallel corpora against\nthe use of a competitive MT system (Google Trans-\nlate) in the same scenario.\n"},{"#tail":"\n","@confidence":"0.999533555555555","#text":"\nWe experiment with the original RTE3 and RTE5\ndatasets, annotated with token, lemma, and stem in-\nformation using the TreeTagger and the Snowball\nstemmer.\nIn addition to confront our method with the solu-\ntion proposed by (Mehdad et al, 2010) we translated\nthe Spanish hypotheses of our CLTE dataset into En-\nglish using Google Translate. The resulting dataset\nwas annotated in the same way.\n"},{"#tail":"\n","@confidence":"0.994839857142857","#text":"\nWe compared the results achieved with paraphrase\ntables (extracted with different pruning thresh-\nolds12) with those obtained using the three most\n12We pruned the paraphrase table (PPHT), with probabilities\nset to 0.1 (PPHT 0.1), 0.2 (PPHT 0.2), and 0.3 (PPHT 0.3)\nwidely used English resources for Textual Entail-\nment (Bentivogli et al, 2010), namely:\nWordNet (WN). WordNet 3.0 has been used\nto extract a set of 5396 pairs of words connected by\nthe hyponymy and synonymy relations.\nVerbOcean (VO). VerbOcean has been used\nto extract 18232 pairs of verbs connected by the\n?stronger-than? relation (e.g. ?kill? stronger-than\n?injure?).\nWikipedia (WIKI). We performed Latent Se-\nmantic Analysis (LSA) over Wikipedia using the\njLSI tool (Giuliano, 2007) to measure the relat-\nedness between words in the dataset. Then, we\nfiltered all the pairs with similarity lower than 0.7 as\nproposed by (Kouylekov et al, 2009). In this way\nwe obtained 13760 word pairs.\n"},{"#tail":"\n","@confidence":"0.999640888888889","#text":"\nTable 2 shows the accuracy results calculated over\nthe original RTE3 and RTE5 test sets, training our\nclassifier over the corresponding development sets.\nThe first two rows of the table show that pruned\nparaphrase tables always outperform the other lexi-\ncal resources used for comparison, with an accuracy\nincrease up to 3%. In particular, we observe that us-\ning 0.2 as a pruning threshold provides a good trade-\noff between coverage and precision, leading to our\nbest results on both datasets (63.50% for RTE3, and\n62.67% for RTE5). It?s worth noting that these re-\nsults, compared with the average scores reported by\nparticipants in the two editions of the RTE Challenge\n(AVG column), represent an accuracy improvement\nof more than 1%. Overall, these results confirm our\nclaim that increasing the coverage using context sen-\nsitive phrase pairs obtained from large parallel cor-\npora, results in better performance not only in CLTE,\n"},{"#tail":"\n","@confidence":"0.999207","#text":"\nbut also in the monolingual scenario.\nThe comparison with the results achieved on\nmonolingual data obtained by automatically trans-\nlating the Spanish hypotheses (RTE3-G row in Ta-\nble 2) leads to four main observations. First, we no-\ntice that dealing with MT-derived inputs, the optimal\npruning threshold changes from 0.2 to 0.1, leading\nto the highest accuracy of 63.50%. This suggests\nthat the noise introduced by incorrect translations\ncan be tackled by increasing the coverage of the\nparaphrase table. Second, in line with the findings\nof (Mehdad et al, 2010), the results obtained over\nthe MT-derived corpus are equal to those we achieve\nover the original RTE3 dataset (i.e. 63.50%). Third,\nthe accuracy obtained over the CLTE corpus using\ncombined phrase and paraphrase tables (62.88%, as\nreported in Table 1) is comparable to the best re-\nsult gained over the automatically translated dataset\n(63.50%). In all the other cases, the use of phrase\nand paraphrase tables on CLTE data outperforms\nthe results achieved on the same data after transla-\ntion. Finally, it?s worth remarking that applying our\nphrase matching method on the translated dataset\nwithout any additional source of knowledge would\nresult in an overall accuracy of 62.12%, which is\nlower than the result obtained using only phrase ta-\nbles on cross-lingual data (62.62%). This demon-\nstrates that phrase tables can successfully replace\nMT systems in the CLTE task.\nIn light of this, we suggest that extracting lexi-\ncal knowledge from parallel corpora is a preferable\nsolution to approach CLTE. One of the main rea-\nsons is that placing a black-box MT system at the\nfront-end of the entailment process reduces the pos-\nsibility to cope with wrong translations. Further-\nmore, the access to MT components is not easy (e.g.\nGoogle Translate limits the number and the size of\nqueries, while open source MT tools cover few lan-\nguage pairs). Moreover, the task of developing a\nfull-fledged MT system often requires the availabil-\nity of parallel corpora, and is much more complex\nthan extracting lexical knowledge from them.\n"},{"#tail":"\n","@confidence":"0.99875762","#text":"\nIn this paper we approached the cross-lingual Tex-\ntual Entailment task focusing on the role of lexi-\ncal knowledge extracted from bilingual parallel cor-\npora. One of the main difficulties in CLTE raises\nfrom the lack of adequate knowledge resources to\nbridge the lexical gap between texts and hypothe-\nses in different languages. Our approach builds on\nthe intuition that the vast amount of knowledge that\ncan be extracted from parallel data (in the form of\nphrase and paraphrase tables) offers a possible so-\nlution to the problem. To check the validity of our\nassumptions we carried out several experiments on\nan English-Spanish corpus derived from the RTE3\ndataset, using phrasal matches as a criterion to ap-\nproximate entailment. Our results show that phrase\nand paraphrase tables allow to: i) outperform the re-\nsults achieved with the few multilingual lexical re-\nsources available, and ii) reach performance levels\nabove the average scores obtained by participants in\nthe monolingual RTE3 challenge. These improve-\nments can be explained by the fact that the lexi-\ncal knowledge extracted from parallel data provides\ngood coverage both at the level of single words, and\nat the level of phrases.\nAs a further contribution, we explored the appli-\ncation of paraphrase tables extracted from parallel\ndata in the traditional monolingual scenario. Con-\ntrasting results with those obtained with the most\nwidely used resources in TE, we demonstrated the\neffectiveness of paraphrase tables as a mean to over-\ncome the bias towards single words featured by the\nexisting resources.\nOur future work will address both the extraction\nof lexical information from bilingual parallel cor-\npora, and its use for TE and CLTE. On one side,\nwe plan to explore alternative ways to build phrase\nand paraphrase tables. One possible direction is to\nconsider linguistically motivated approaches, such\nas the extraction of syntactic phrase tables as pro-\nposed by (Yamada and Knight, 2001). Another in-\nteresting direction is to investigate the potential of\nparaphrase patterns (i.e. patterns including part-\nof-speech slots), extracted from bilingual parallel\ncorpora with the method proposed by (Zhao et al,\n2009). On the other side we will investigate more\nsophisticated methods to exploit the acquired lexi-\ncal knowledge. As a first step, the probability scores\nassigned to phrasal entries will be considered to per-\nform weighted phrase matching as an improved cri-\nterion to approximate entailment.\n"},{"#tail":"\n","@confidence":"0.993028","#text":"\nThis work has been partially supported by the EC-\nfunded project CoSyne (FP7-ICT-4-24853).\n"}],"#text":"\n","sectionHeader":[{"#tail":"\n","@confidence":"0.982068","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.996425","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.71102","@genericHeader":"method","#text":"\n3 Using Parallel Corpora for CLTE\n"},{"#tail":"\n","@confidence":"0.991892","@genericHeader":"method","#text":"\n4 Experiments on CLTE\n"},{"#tail":"\n","@confidence":"0.992142","@genericHeader":"conclusions","#text":"\n6 Conclusion and Future Work\n"},{"#tail":"\n","@confidence":"0.981823","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.98461","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.999311","#text":"\nTable 1: Accuracy results on CLTE using different lexical\n"},{"#tail":"\n","@confidence":"0.996079","#text":"\nTable 2: Accuracy results on monolingual RTE using different lexical resources.\n"}],"page":[{"#tail":"\n","@confidence":"0.959119","#text":"\n1336\n"},{"#tail":"\n","@confidence":"0.9733","#text":"\n1337\n"},{"#tail":"\n","@confidence":"0.932813","#text":"\n1339\n"},{"#tail":"\n","@confidence":"0.951765","#text":"\n1340\n"},{"#tail":"\n","@confidence":"0.955537","#text":"\n1341\n"},{"#tail":"\n","@confidence":"0.930844","#text":"\n1342\n"},{"#tail":"\n","@confidence":"0.861208","#text":"\n1343\n"},{"#tail":"\n","@confidence":"0.948945","#text":"\n1345\n"}],"figureCaption":{"#tail":"\n","@confidence":"0.999159","#text":"\nFigure 1: Accuracy on CLTE by pruning the phrase table\n"},"table":[{"#tail":"\n","@confidence":"0.266483","#text":"\n- XDXF Dictionaries7: 22,486 entries.\n"},{"#tail":"\n","@confidence":"0.9921174","#text":"\nMWN DIC PHT PPHT Acc. ?\nx 55.00 0.00\nx 59.88 +4.88\nx 62.62 +7.62\nx x 62.88 +7.88\n"},{"#tail":"\n","@confidence":"0.99781975","#text":"\nDataset WN VO WIKI PPHT PPHT 0.1 PPHT 0.2 PPHT 0.3 AVG\nRTE3 61.88 62.00 61.75 62.88 63.38 63.50 63.00 62.37\nRTE5 62.17 61.67 60.00 61.33 62.50 62.67 62.33 61.41\nRTE3-G 62.62 61.5 60.5 62.88 63.50 62.00 61.5 -\n"}],"email":{"#tail":"\n","@confidence":"0.917973","#text":"\nfederico@fbk.eu\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.060828","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.9969085","#text":"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1336?1345, Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics"},"#text":"\n","author":[{"#tail":"\n","@confidence":"0.665896","#text":"Povo"},{"#tail":"\n","@confidence":"0.999705","#text":"Matteo Negri"},{"#tail":"\n","@confidence":"0.392177","#text":"Povo"},{"#tail":"\n","@confidence":"0.999688","#text":"Marcello Federico"},{"#tail":"\n","@confidence":"0.512626","#text":"Povo"}],"abstract":{"#tail":"\n","@confidence":"0.999801739130435","#text":"This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. We claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge."},"title":{"#tail":"\n","@confidence":"0.862573","#text":"Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment Yashar Mehdad FBK irst and Uni. of Trento"},"email":[{"#tail":"\n","@confidence":"0.991053","#text":"mehdad@fbk.eu"},{"#tail":"\n","@confidence":"0.431084","#text":"FBKirst"},{"#tail":"\n","@confidence":"0.982694","#text":"negri@fbk.eu"},{"#tail":"\n","@confidence":"0.617557","#text":"FBKirst"},{"#tail":"\n","@confidence":"0.997015","#text":"federico@fbk.eu"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. Proceedings of COLING-ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Baker, Fillmore, Lowe, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"elations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al, 2010; Dagan et al, 2009). These include, just to mention the most popular 1337 ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al, 2010; Kouylekov et al, 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and","@endWordPosition":"1274","@position":"8240","annotationId":"T1","@startWordPosition":"1271","@citStr":"Baker et al., 1998"}},"title":{"#tail":"\n","#text":"The Berkeley FrameNet project."},"booktitle":{"#tail":"\n","#text":"Proceedings of COLING-ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Collin F Baker"},{"#tail":"\n","#text":"Charles J Fillmore"},{"#tail":"\n","#text":"John B Lowe"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005)."},"#text":"\n","marker":{"#tail":"\n","#text":"Bannard, Callison-Burch, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ween texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores. Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in (Bentivogli et al, 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works (Bannard and Callison-Burch, 2005; Zhao et al, 2009; Kouylekov et al, 2009) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words. Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources. As regards the first issue, it?s worth noting that in the monolingual scenario simple ?bag of words? (or ?bag of ngrams?) approaches are per se sufficient to achieve r","@endWordPosition":"1466","@position":"9458","annotationId":"T2","@startWordPosition":"1463","@citStr":"Bannard and Callison-Burch, 2005"},{"#tail":"\n","#text":"ange from 76 to 48 million entries, with an average of 3.9 words per phrase. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al, 1991), multidocument summarization (McKeown et al, 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined. Moreover, in order to experiment with different paraphrase sets providing different degrees of coverage and precision, we pruned the main paraphrase table based on the probabi","@endWordPosition":"2455","@position":"15550","annotationId":"T3","@startWordPosition":"2452","@citStr":"Bannard and Callison-Burch, 2005"}]},"title":{"#tail":"\n","#text":"Paraphrasing with Bilingual Parallel Corpora."},"booktitle":{"#tail":"\n","#text":"Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Colin Bannard"},{"#tail":"\n","#text":"Chris Callison-Burch"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Roy Bar-haim , Jonathan Berant , Ido Dagan , Iddo Greental , Shachar Mirkin , Eyal Shnarch , and Idan Szpektor. 2008. Efficient semantic deduction and approximate matching over compact parse forests. Proceedings of the TAC 2008 Workshop on Textual Entailment."},"#text":"\n","marker":{"#tail":"\n","#text":"Greental, 2008"},"title":{"#tail":"\n","#text":"Efficient semantic deduction and approximate matching over compact parse forests."},"booktitle":{"#tail":"\n","#text":"Proceedings of the TAC 2008 Workshop on Textual Entailment."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Iddo Greental"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2010. The Sixth PASCAL Recognizing Textual Entailment Challenge."},"#text":"\n","marker":{"#tail":"\n","#text":"Bentivogli, 2010"},"title":{"#tail":"\n","#text":"Peter Clark, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Luisa Bentivogli"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Proceedings of the the Text Analysis Conference (TAC 2010)."},"#text":"\n","marker":{"#tail":"\n","#text":"2010"},"booktitle":{"#tail":"\n","#text":"Proceedings of the the Text Analysis Conference (TAC"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-04)."},"#text":"\n","marker":{"#tail":"\n","#text":"Chklovski, Pantel, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al, 2010; Dagan et al, 2009). These include, just to mention the most popular 1337 ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al, 2010; Kouylekov et al, 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the sem","@endWordPosition":"1269","@position":"8209","annotationId":"T4","@startWordPosition":"1266","@citStr":"Chklovski and Pantel, 2004"}},"title":{"#tail":"\n","#text":"Verbocean: Mining the web for fine-grained semantic verb relations."},"booktitle":{"#tail":"\n","#text":"Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-04)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Timothy Chklovski"},{"#tail":"\n","#text":"Patrick Pantel"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Ido Dagan and Oren Glickman. 2004. Probabilistic textual entailment: Generic applied modeling of language variability. Proceedings of the PASCAL Workshop of Learning Methods for Text Understanding and Mining."},"#text":"\n","marker":{"#tail":"\n","#text":"Dagan, Glickman, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al, 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognizers and normalizers) has to confront, at the cross-lingual level, ","@endWordPosition":"241","@position":"1603","annotationId":"T5","@startWordPosition":"238","@citStr":"Dagan and Glickman, 2004"}},"title":{"#tail":"\n","#text":"Probabilistic textual entailment: Generic applied modeling of language variability."},"booktitle":{"#tail":"\n","#text":"Proceedings of the PASCAL Workshop of Learning Methods for Text Understanding and Mining."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ido Dagan"},{"#tail":"\n","#text":"Oren Glickman"}]}},{"volume":{"#tail":"\n","#text":"04"},"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Journal of Natural Language Engineering , Volume 15, Special Issue 04, pp i-xvii."},"journal":{"#tail":"\n","#text":"Journal of Natural Language Engineering , Volume 15, Special Issue"},"#text":"\n","pages":{"#tail":"\n","#text":"pp i-xvii."},"marker":{"#tail":"\n","#text":"Dagan, Dolan, Magnini, Roth, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cating that words from the text and the hypothesis entail each other, thus being interchangeable. Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al, 2010; Dagan et al, 2009). These include, just to mention the most popular 1337 ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al, 2010; Kouylekov et al, 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and","@endWordPosition":"1249","@position":"8081","annotationId":"T6","@startWordPosition":"1246","@citStr":"Dagan et al, 2009"}},"title":{"#tail":"\n","#text":"Recognizing textual entailment: Rational, evaluation and approaches."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ido Dagan"},{"#tail":"\n","#text":"Bill Dolan"},{"#tail":"\n","#text":"Bernardo Magnini"},{"#tail":"\n","#text":"Dan Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Michael Denkowski and Alon Lavie. 2010. Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level. Proceedings of Human Language Technologies (HLT-NAACL 2010)."},"#text":"\n","marker":{"#tail":"\n","#text":"Denkowski, Lavie, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"acted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1http://www.statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al, 1991), multidocument summarization (McKeown et al, 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using th","@endWordPosition":"2423","@position":"15345","annotationId":"T7","@startWordPosition":"2420","@citStr":"Denkowski and Lavie, 2010"}},"title":{"#tail":"\n","#text":"Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level."},"booktitle":{"#tail":"\n","#text":"Proceedings of Human Language Technologies (HLT-NAACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michael Denkowski"},{"#tail":"\n","#text":"Alon Lavie"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Georgiana Dinu and Rui Wang. 2009. Inference Rules and their Application to Recognizing Textual Entailment. Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)."},"#text":"\n","marker":{"#tail":"\n","#text":"Dinu, Wang, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1http://www.statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al, 1991), multidocument summarization (McKeown et al, 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined. ","@endWordPosition":"2429","@position":"15375","annotationId":"T8","@startWordPosition":"2426","@citStr":"Dinu and Wang, 2009"}},"title":{"#tail":"\n","#text":"Inference Rules and their Application to Recognizing Textual Entailment."},"booktitle":{"#tail":"\n","#text":"Proceedings of the 12th Conference of the European Chapter of the ACL (EACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Georgiana Dinu"},{"#tail":"\n","#text":"Rui Wang"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Claudio Giuliano. 2007. jLSI a tool for latent semantic indexing. Software available at http://tcc.itc.it/research/textec/toolsresources/jLSI.html."},"#text":"\n","marker":{"#tail":"\n","#text":"Giuliano, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"the three most 12We pruned the paraphrase table (PPHT), with probabilities set to 0.1 (PPHT 0.1), 0.2 (PPHT 0.2), and 0.3 (PPHT 0.3) widely used English resources for Textual Entailment (Bentivogli et al, 2010), namely: WordNet (WN). WordNet 3.0 has been used to extract a set of 5396 pairs of words connected by the hyponymy and synonymy relations. VerbOcean (VO). VerbOcean has been used to extract 18232 pairs of verbs connected by the ?stronger-than? relation (e.g. ?kill? stronger-than ?injure?). Wikipedia (WIKI). We performed Latent Semantic Analysis (LSA) over Wikipedia using the jLSI tool (Giuliano, 2007) to measure the relatedness between words in the dataset. Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al, 2009). In this way we obtained 13760 word pairs. 5.3 Results and Discussion Table 2 shows the accuracy results calculated over the original RTE3 and RTE5 test sets, training our classifier over the corresponding development sets. The first two rows of the table show that pruned paraphrase tables always outperform the other lexical resources used for comparison, with an accuracy increase up to 3%. In particular, we observe that using 0.2 as a ","@endWordPosition":"4163","@position":"26483","annotationId":"T9","@startWordPosition":"4162","@citStr":"Giuliano, 2007"}},"title":{"#tail":"\n","#text":"jLSI a tool for latent semantic indexing. Software available at http://tcc.itc.it/research/textec/toolsresources/jLSI.html."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Claudio Giuliano"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Lidija Iordanskaja, Richard Kittredge, and Alain Polg re.. 1991. Lexical selection and paraphrase in a meaning text generation model. Natural Language Generation in Articial Intelligence and Computational Linguistics."},"journal":{"#tail":"\n","#text":"Natural Language Generation in Articial Intelligence and Computational Linguistics."},"#text":"\n","marker":{"#tail":"\n","#text":"Iordanskaja, Kittredge, re, 1991"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1http://www.statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al, 1991), multidocument summarization (McKeown et al, 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. In ","@endWordPosition":"2408","@position":"15238","annotationId":"T10","@startWordPosition":"2405","@citStr":"Iordanskaja et al, 1991"}},"title":{"#tail":"\n","#text":"Lexical selection and paraphrase in a meaning text generation model."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Lidija Iordanskaja"},{"#tail":"\n","#text":"Richard Kittredge"},{"#tail":"\n","#text":"Alain Polg re"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Thorsten Joachims. 1999. Making large-scale support vector machine learning practical."},"#text":"\n","marker":{"#tail":"\n","#text":"Joachims, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"t matches, the lexical matches between pairs of phrases which have already been identified as exact matches are not considered. Once matching for each n-gram level has been concluded, the number of matches (Mn) and the number of phrases in the hypothesis (Nn) are used to estimate the portion of phrases in H that are matched at each level (n). The phrasal matching score for each n-gram level is calculated as follows: Scoren = Mn Nn To combine the phrasal matching scores obtained at each n-gram level, and optimize their relative weights, we trained a Support Vector Machine classifier, SVMlight (Joachims, 1999), using each score as a feature. 4 Experiments on CLTE To address the first two questions outlined in Section 1, we experimented with the phrase matching method previously described, contrasting the effectiveness of lexical information extracted from parallel corpora with the knowledge provided by other resources used in the same way. 4.1 Dataset The dataset used for our experiments is an EnglishSpanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800)","@endWordPosition":"2902","@position":"18223","annotationId":"T11","@startWordPosition":"2901","@citStr":"Joachims, 1999"}},"title":{"#tail":"\n","#text":"Making large-scale support vector machine learning practical."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Thorsten Joachims"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Philipp Koehn, Franz Josef Och, and Daniel Marcu 2003. Statistical Phrase-Based Translation. Proceedings of HLT/NAACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Koehn, Och, Marcu, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"es. In particular, ?YES? and ?NO? judgements are assigned considering the proportion of words in the hypothesis that are found also in the text. This way to approximate entailment reflects the intuition that, as a directional relation between the text and the hypothesis, the full content of H has to be found in T. 3.1 Extracting Phrase and Paraphrase Tables Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al, 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101. We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses t","@endWordPosition":"2167","@position":"13763","annotationId":"T12","@startWordPosition":"2164","@citStr":"Koehn et al, 2003"}},"title":{"#tail":"\n","#text":"Statistical Phrase-Based Translation."},"booktitle":{"#tail":"\n","#text":"Proceedings of HLT/NAACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Franz Josef Och"},{"#tail":"\n","#text":"Daniel Marcu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. Proceedings of the Conference of the Association for Computational Linguistics (ACL)."},"#text":"\n","marker":{"#tail":"\n","#text":"Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007"},"location":{"#tail":"\n","#text":"Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101. We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al, 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1http://www.statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phr","@endWordPosition":"2268","@position":"14389","annotationId":"T13","@startWordPosition":"2265","@citStr":"Koehn et al, 2007"}},"title":{"#tail":"\n","#text":"Moses: Open Source Toolkit for Statistical Machine Translation."},"booktitle":{"#tail":"\n","#text":"Proceedings of the Conference of the Association for Computational Linguistics (ACL)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Hieu Hoang"},{"#tail":"\n","#text":"Alexandra Birch"},{"#tail":"\n","#text":"Chris Callison-Burch"},{"#tail":"\n","#text":"Marcello Federico"},{"#tail":"\n","#text":"Nicola Bertoldi"},{"#tail":"\n","#text":"Brooke Cowan"},{"#tail":"\n","#text":"Wade Shen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Milen Kouleykov and Bernardo Magnini. 2005. Tree edit distance for textual entailment. Proceedings of RALNP-2005, International Conference on Recent Advances in Natural Language Processing."},"#text":"\n","marker":{"#tail":"\n","#text":"Kouleykov, Magnini, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rst three questions, giving motivations for the use of bilingual parallel corpora in CLTE, and showing the results of our experiments. Section 5 addresses the last question, reporting on our experiments with paraphrase tables extracted from phrase tables on the monolingual RTE datasets. Section 6 concludes the paper, and outlines the directions of our future research. 2 Lexical resources for TE and CLTE All current approaches to monolingual TE, either syntactically oriented (Rus et al, 2005), or applying logical inference (Tatu and Moldovan, 2005), or adopting transformation-based techniques (Kouleykov and Magnini, 2005; Bar-Haim et al, 2008), incorporate different types of lexical knowledge to support textual inference. Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable. Hypernymy/hyponymy chains can provide en","@endWordPosition":"1084","@position":"7001","annotationId":"T14","@startWordPosition":"1081","@citStr":"Kouleykov and Magnini, 2005"}},"title":{"#tail":"\n","#text":"Tree edit distance for textual entailment."},"booktitle":{"#tail":"\n","#text":"Proceedings of RALNP-2005, International Conference on Recent Advances in Natural Language Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Milen Kouleykov"},{"#tail":"\n","#text":"Bernardo Magnini"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Milen Kouylekov, Yashar Mehdad, and Matteo Negri. 2010. Mining Wikipedia for Large-Scale Repositories of Context-Sensitive Entailment Rules. Proceedings of the Language Resources and Evaluation Conference (LREC 2010)."},"#text":"\n","marker":{"#tail":"\n","#text":"Kouylekov, Mehdad, Negri, 2010"},"title":{"#tail":"\n","#text":"Mining Wikipedia for Large-Scale Repositories of Context-Sensitive Entailment Rules."},"booktitle":{"#tail":"\n","#text":"Proceedings of the Language Resources and Evaluation Conference (LREC"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Milen Kouylekov"},{"#tail":"\n","#text":"Yashar Mehdad"},{"#tail":"\n","#text":"Matteo Negri"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Yashar Mehdad, Alessandro Moschitti and Fabio Massimo Zanzotto. 2010. Syntactic/semantic structures for textual entailment recognition. Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010)."},"#text":"\n","marker":{"#tail":"\n","#text":"Mehdad, Moschitti, Zanzotto, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al, 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognize","@endWordPosition":"231","@position":"1538","annotationId":"T15","@startWordPosition":"228","@citStr":"Mehdad et al, 2010"},{"#tail":"\n","#text":"he behaviour of the engine, and devise adhoc solutions to specific entailment problems. This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques. Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system?s behaviour. Along this direction, we 1336 start from the acquisition and use of lexical knowledge, which represents the basic building block of any TE system. Using the basic solution proposed by (Mehdad et al, 2010) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions: (1) What is the potential of the existing multilingual lexical resources to approach CLTE? To answer this question we experiment with lexical knowledge extracted from bilingual dictionaries, and from a multilingual lexical database. Such experiments show two main limitations of these resources, namely: i) their limited coverage, and ii) the difficulty to capture contextual information when only associations between single words (or at most named entities and mult","@endWordPosition":"589","@position":"3820","annotationId":"T16","@startWordPosition":"586","@citStr":"Mehdad et al, 2010"},{"#tail":"\n","#text":"g that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al, 2010; Dagan et al, 2009). These include, just to mention the most popular 1337 ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al, 2010; Kouylekov et al, 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses. Wikipedia is often use","@endWordPosition":"1280","@position":"8275","annotationId":"T17","@startWordPosition":"1277","@citStr":"Mehdad et al, 2010"},{"#tail":"\n","#text":"ction addresses the third and the fourth research questions outlined in Section 1. Building on the positive results achieved on the cross-lingual scenario, we investigate the possibility to exploit bilingual parallel corpora in the traditional monolingual scenario. Using the same approach discussed in Section 4, we compare the results achieved with English paraphrase tables with those obtained with other widely used monolingual knowledge resources over two RTE datasets. For the sake of completeness, we report in this section also the results obtained adopting the ?basic solution? proposed by (Mehdad et al, 2010). Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario. 5.1 Dataset We experiment with the original RTE3 and RTE5 datasets, annotated with token, lemma, and stem information using the TreeTagger and the Snowball stemmer. In addition to confront our method with the solution proposed by (Mehdad et al, 2010) we translated t","@endWordPosition":"3920","@position":"24984","annotationId":"T18","@startWordPosition":"3917","@citStr":"Mehdad et al, 2010"},{"#tail":"\n","#text":"rpora, results in better performance not only in CLTE, 1342 but also in the monolingual scenario. The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations. First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second, in line with the findings of (Mehdad et al, 2010), the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset (i.e. 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation. Finally, it?s worth remarking that applying our phrase matching method on the translated dataset without any ","@endWordPosition":"4445","@position":"28202","annotationId":"T19","@startWordPosition":"4442","@citStr":"Mehdad et al, 2010"}]},"title":{"#tail":"\n","#text":"Syntactic/semantic structures for textual entailment recognition."},"booktitle":{"#tail":"\n","#text":"Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yashar Mehdad"},{"#tail":"\n","#text":"Alessandro Moschitti"},{"#tail":"\n","#text":"Fabio Massimo Zanzotto"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery of Inference Rules from Text.. Proceedings of ACM Conference on Knowledge Discovery and Data Mining (KDD-01)."},"#text":"\n","marker":{"#tail":"\n","#text":"Lin, Pantel, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rchangeable. Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al, 2010; Dagan et al, 2009). These include, just to mention the most popular 1337 ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al, 2010; Kouylekov et al, 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of e","@endWordPosition":"1264","@position":"8169","annotationId":"T20","@startWordPosition":"1261","@citStr":"Lin and Pantel, 2001"}},"title":{"#tail":"\n","#text":"DIRT - Discovery of Inference Rules from Text.."},"booktitle":{"#tail":"\n","#text":"Proceedings of ACM Conference on Knowledge Discovery and Data Mining (KDD-01)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dekang Lin"},{"#tail":"\n","#text":"Patrick Pantel"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Kathleen R. McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and summarizing news on a daily basis with Columbias Newsblaster. Proceedings of the Human Language Technology Conference.."},"#text":"\n","marker":{"#tail":"\n","#text":"McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"fferent degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1http://www.statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al, 1991), multidocument summarization (McKeown et al, 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases fo","@endWordPosition":"2415","@position":"15289","annotationId":"T21","@startWordPosition":"2412","@citStr":"McKeown et al, 2002"}},"title":{"#tail":"\n","#text":"Tracking and summarizing news on a daily basis with Columbias Newsblaster."},"booktitle":{"#tail":"\n","#text":"Proceedings of the Human Language Technology Conference.."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kathleen R McKeown"},{"#tail":"\n","#text":"Regina Barzilay"},{"#tail":"\n","#text":"David Evans"},{"#tail":"\n","#text":"Vasileios Hatzivassiloglou"},{"#tail":"\n","#text":"Judith L Klavans"},{"#tail":"\n","#text":"Ani Nenkova"},{"#tail":"\n","#text":"Carl Sable"},{"#tail":"\n","#text":"Barry Schiffman"},{"#tail":"\n","#text":"Sergey Sigelman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Yashar Mehdad, Matteo Negri, and Marcello Federico. 2010. Towards Cross-Lingual Textual Entailment."},"#text":"\n","marker":{"#tail":"\n","#text":"Mehdad, Negri, Federico, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al, 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognize","@endWordPosition":"231","@position":"1538","annotationId":"T22","@startWordPosition":"228","@citStr":"Mehdad et al, 2010"},{"#tail":"\n","#text":"he behaviour of the engine, and devise adhoc solutions to specific entailment problems. This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques. Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system?s behaviour. Along this direction, we 1336 start from the acquisition and use of lexical knowledge, which represents the basic building block of any TE system. Using the basic solution proposed by (Mehdad et al, 2010) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions: (1) What is the potential of the existing multilingual lexical resources to approach CLTE? To answer this question we experiment with lexical knowledge extracted from bilingual dictionaries, and from a multilingual lexical database. Such experiments show two main limitations of these resources, namely: i) their limited coverage, and ii) the difficulty to capture contextual information when only associations between single words (or at most named entities and mult","@endWordPosition":"589","@position":"3820","annotationId":"T23","@startWordPosition":"586","@citStr":"Mehdad et al, 2010"},{"#tail":"\n","#text":"g that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al, 2010; Dagan et al, 2009). These include, just to mention the most popular 1337 ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al, 2010; Kouylekov et al, 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses. Wikipedia is often use","@endWordPosition":"1280","@position":"8275","annotationId":"T24","@startWordPosition":"1277","@citStr":"Mehdad et al, 2010"},{"#tail":"\n","#text":"ction addresses the third and the fourth research questions outlined in Section 1. Building on the positive results achieved on the cross-lingual scenario, we investigate the possibility to exploit bilingual parallel corpora in the traditional monolingual scenario. Using the same approach discussed in Section 4, we compare the results achieved with English paraphrase tables with those obtained with other widely used monolingual knowledge resources over two RTE datasets. For the sake of completeness, we report in this section also the results obtained adopting the ?basic solution? proposed by (Mehdad et al, 2010). Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario. 5.1 Dataset We experiment with the original RTE3 and RTE5 datasets, annotated with token, lemma, and stem information using the TreeTagger and the Snowball stemmer. In addition to confront our method with the solution proposed by (Mehdad et al, 2010) we translated t","@endWordPosition":"3920","@position":"24984","annotationId":"T25","@startWordPosition":"3917","@citStr":"Mehdad et al, 2010"},{"#tail":"\n","#text":"rpora, results in better performance not only in CLTE, 1342 but also in the monolingual scenario. The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations. First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second, in line with the findings of (Mehdad et al, 2010), the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset (i.e. 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation. Finally, it?s worth remarking that applying our phrase matching method on the translated dataset without any ","@endWordPosition":"4445","@position":"28202","annotationId":"T26","@startWordPosition":"4442","@citStr":"Mehdad et al, 2010"}]},"title":{"#tail":"\n","#text":"Towards Cross-Lingual Textual Entailment."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yashar Mehdad"},{"#tail":"\n","#text":"Matteo Negri"},{"#tail":"\n","#text":"Marcello Federico"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010)."},"#text":"\n","marker":{"#tail":"\n","#text":"2010"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Dan Moldovan and Adrian Novischi. 2002. Lexical chains for question answering. Proceedings of COLING."},"#text":"\n","marker":{"#tail":"\n","#text":"Moldovan, Novischi, 2002"},"title":{"#tail":"\n","#text":"Lexical chains for question answering."},"booktitle":{"#tail":"\n","#text":"Proceedings of COLING."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dan Moldovan"},{"#tail":"\n","#text":"Adrian Novischi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Matteo Negri and Yashar Mehdad. 2010. Creating a Bilingual Entailment Corpus through Translations with Mechanical Turk: $100 for a 10-day Rush. Proceedings of the NAACL 2010 Workshop on Creating Speech and Language Data With Amazons Mechanical Turk ."},"#text":"\n","marker":{"#tail":"\n","#text":"Negri, Mehdad, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" phrase matching method previously described, contrasting the effectiveness of lexical information extracted from parallel corpora with the knowledge provided by other resources used in the same way. 4.1 Dataset The dataset used for our experiments is an EnglishSpanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 (MTurk), adopting the methodology proposed by (Negri and Mehdad, 2010). The method relies on translation-validation cycles, defined as separate jobs routed to MTurk?s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untru","@endWordPosition":"3021","@position":"18983","annotationId":"T27","@startWordPosition":"3018","@citStr":"Negri and Mehdad, 2010"}},"title":{"#tail":"\n","#text":"Creating a Bilingual Entailment Corpus through Translations with Mechanical Turk: $100 for a 10-day Rush."},"booktitle":{"#tail":"\n","#text":"Proceedings of the NAACL 2010 Workshop on Creating Speech and Language Data With Amazons Mechanical Turk ."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matteo Negri"},{"#tail":"\n","#text":"Yashar Mehdad"}]}},{"volume":{"#tail":"\n","#text":"29"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):1951."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Och, Ney, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sociation probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al, 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101. We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al, 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities o","@endWordPosition":"2239","@position":"14216","annotationId":"T28","@startWordPosition":"2236","@citStr":"Och and Ney, 2003"}},"title":{"#tail":"\n","#text":"A systematic comparison of various statistical alignment models."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Franz Josef Och"},{"#tail":"\n","#text":"Hermann Ney"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Emanuele Pianta, Luisa Bentivogli, and Christian Girardi. 2002. MultiWordNet: Developing and Aligned Multilingual Database. Proceedings of the First International Conference on Global WordNet."},"#text":"\n","marker":{"#tail":"\n","#text":"Pianta, Bentivogli, Girardi, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"are per se sufficient to achieve results above baseline. In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lexical matches between texts and hypotheses in different languages. This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the EnglishWordNet (e.g. MultiWordNet (Pianta et al, 2002)) have been created for several languages, with different degrees of coverage. As an example, the 57,424 synsets of the Spanish section of MultiWordNet algned to English cover just around 50% of the WordNet?s synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the crosslingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable co","@endWordPosition":"1652","@position":"10643","annotationId":"T29","@startWordPosition":"1649","@citStr":"Pianta et al, 2002"}},"title":{"#tail":"\n","#text":"MultiWordNet: Developing and Aligned Multilingual Database."},"booktitle":{"#tail":"\n","#text":"Proceedings of the First International Conference on Global WordNet."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Emanuele Pianta"},{"#tail":"\n","#text":"Luisa Bentivogli"},{"#tail":"\n","#text":"Christian Girardi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Vasile Rus, Art Graesser, and Kirtan Desai 2005. Lexico-Syntactic Subsumption for Textual Entailment."},"#text":"\n","marker":{"#tail":"\n","#text":"Rus, 2005"},"title":{"#tail":"\n","#text":"Art Graesser, and Kirtan Desai"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Vasile Rus"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Proceedings of RANLP 2005."},"#text":"\n","marker":{"#tail":"\n","#text":"2005"},"booktitle":{"#tail":"\n","#text":"Proceedings of RANLP"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Helmut Schmid 2005. Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings of the International Conference on New Methods in Language Processing."},"#text":"\n","marker":{"#tail":"\n","#text":"Schmid, 2005"},"title":{"#tail":"\n","#text":"Probabilistic Part-of-Speech Tagging Using Decision Trees."},"booktitle":{"#tail":"\n","#text":"Proceedings of the International Conference on New Methods in Language Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Helmut Schmid"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Marta Tatu andDan Moldovan. 2005. A semantic approach to recognizing textual entailment. Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005)."},"#text":"\n","marker":{"#tail":"\n","#text":"Moldovan, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"f available knowledge sources. Sections 3 and 4 address the first three questions, giving motivations for the use of bilingual parallel corpora in CLTE, and showing the results of our experiments. Section 5 addresses the last question, reporting on our experiments with paraphrase tables extracted from phrase tables on the monolingual RTE datasets. Section 6 concludes the paper, and outlines the directions of our future research. 2 Lexical resources for TE and CLTE All current approaches to monolingual TE, either syntactically oriented (Rus et al, 2005), or applying logical inference (Tatu and Moldovan, 2005), or adopting transformation-based techniques (Kouleykov and Magnini, 2005; Bar-Haim et al, 2008), incorporate different types of lexical knowledge to support textual inference. Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each o","@endWordPosition":"1076","@position":"6927","annotationId":"T30","@startWordPosition":"1075","@citStr":"Moldovan, 2005"}},"title":{"#tail":"\n","#text":"A semantic approach to recognizing textual entailment."},"booktitle":{"#tail":"\n","#text":"Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Marta Tatu andDan Moldovan"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric. Proceedings of WMT09."},"#text":"\n","marker":{"#tail":"\n","#text":"Snover, Madnani, Dorr, Schwartz, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" in a number of NLP applications such as natural language generation (Iordanskaja et al, 1991), multidocument summarization (McKeown et al, 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined. Moreover, in order to experiment with different paraphrase sets providing different degrees of coverage and precision, we pruned the main paraphrase table based on the probabilities, associated to its entries, of 0.1, 0.2 and 0.3. The number of phrase pairs extracted varies from 6 million to about 80000, with an average of 3.2 words per phrase. 3.2 Phrasal Matching Method In order to m","@endWordPosition":"2491","@position":"15763","annotationId":"T31","@startWordPosition":"2488","@citStr":"Snover et al, 2009"}},"title":{"#tail":"\n","#text":"Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric."},"booktitle":{"#tail":"\n","#text":"Proceedings of WMT09."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matthew Snover"},{"#tail":"\n","#text":"Nitin Madnani"},{"#tail":"\n","#text":"Bonnie Dorr"},{"#tail":"\n","#text":"Richard Schwartz"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Rui Wang and Yi Zhang,. 2009. Recognizing Textual Relatedness with Predicate-Argument Structures."},"#text":"\n","marker":{"#tail":"\n","#text":"Wang, Zhang, 2009"},"title":{"#tail":"\n","#text":"Recognizing Textual Relatedness with Predicate-Argument Structures."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Rui Wang"},{"#tail":"\n","#text":"Yi Zhang"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2009)."},"#text":"\n","marker":{"#tail":"\n","#text":"2009"},"booktitle":{"#tail":"\n","#text":"Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Kenji Yamada and Kevin Knight 2001. A Syntax-Based Statistical Translation Model. Proceedings of the Conference of the Association for Computational Linguistics (ACL)."},"#text":"\n","marker":{"#tail":"\n","#text":"Yamada, Knight, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" scenario. Contrasting results with those obtained with the most widely used resources in TE, we demonstrated the effectiveness of paraphrase tables as a mean to overcome the bias towards single words featured by the existing resources. Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (Yamada and Knight, 2001). Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al, 2009). On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. 1343 Acknowledgments This work has been partially supported by the ECfunded project CoSyne (F","@endWordPosition":"5013","@position":"31685","annotationId":"T32","@startWordPosition":"5010","@citStr":"Yamada and Knight, 2001"}},"title":{"#tail":"\n","#text":"A Syntax-Based Statistical Translation Model."},"booktitle":{"#tail":"\n","#text":"Proceedings of the Conference of the Association for Computational Linguistics (ACL)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kenji Yamada"},{"#tail":"\n","#text":"Kevin Knight"}]}},{"volume":{"#tail":"\n","#text":"04"},"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2009. Extracting Paraphrase Patterns from Bilingual Parallel Corpora. Journal of Natural Language Engineering , Volume 15, Special Issue 04, pp 503-526."},"journal":{"#tail":"\n","#text":"Journal of Natural Language Engineering , Volume 15, Special Issue"},"#text":"\n","pages":{"#tail":"\n","#text":"503--526"},"marker":{"#tail":"\n","#text":"Zhao, Wang, Liu, Li, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ia is often used to extract probabilistic entailment rules based word similarity/relatedness scores. Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in (Bentivogli et al, 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works (Bannard and Callison-Burch, 2005; Zhao et al, 2009; Kouylekov et al, 2009) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words. Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources. As regards the first issue, it?s worth noting that in the monolingual scenario simple ?bag of words? (or ?bag of ngrams?) approaches are per se sufficient to achieve results above basel","@endWordPosition":"1470","@position":"9476","annotationId":"T33","@startWordPosition":"1467","@citStr":"Zhao et al, 2009"}},"title":{"#tail":"\n","#text":"Extracting Paraphrase Patterns from Bilingual Parallel Corpora."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Shiqi Zhao"},{"#tail":"\n","#text":"Haifeng Wang"},{"#tail":"\n","#text":"Ting Liu"},{"#tail":"\n","#text":"Sheng Li"}]}}]}}]}}
