y the arg max operation in Eq. 1, i.e. it explores the space of all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on
n in Eq. 1, i.e. it explores the space of all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on the correct infle
 Question Inversion you did say the eighteenth? Verb Treatment you_did say the eighteenth? Catalan Sentence has dit el divuit ? Spanish Sentence i, has dicho el dieciocho ? ing operation. This makes it impossible to translate the verb itself, because it is then unknown to the system. The same holds for combinations of pronouns and verbs that are unseen in training, e. g. the training corpus contains the bigram 'I went', but not the one 'she went'. In order to overcome this problem, we train our lexicon model using maximum entropy. 5.1 The Maximum Entropy Approach The maximum entropy approach (Berger et al., 1996) presents a powerful framework for the combination of several knowledge sources. This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy. The distribution is required to satisfy constraints, which represent facts known from the data. These constraints are expressed on the basis of feature functions hu,(s,t), where (s, t) is a pair of source and target word. The lexicon probability of a source word given the target word has the following functional form 1 t) Z(t) exp [Y‘ with the normalization factor Z(t) = Eexp [E 
m. The features we use in our model are • a lexical feature (for the entries of the transformed vocabulary): 128, (s, t) = (5(s, s') • 6(t, t') P(s 350 • the verb contained in a transformed lexicon entry (e.g. 'go' for 'you_go' or 'you_will_go): hs, ,v(s ,t) = S(s. s') • V erb(t, v) , where 1, if t contains the verb v V erb(t, v) = 0, otherwise This enables us to translate the verb alone even if it occurs in the training corpus only as a spliced entry. For an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance (Berger et al., 1996) or (Ratnaparkhi, 1997). 5.2 Training We performed the following training steps: • transform the English (= source language) part of the corpus as described in Sections 4.1 and 4.2 • train the statistical translation system using this modified source language corpus 1 • with the resulting alignment, train the lexicon model using maximum entropy with the features described in Section 5.1 This training can be performed using converging iterative training procedures like described by (Darroch and Ratcliff, 1972) or (Della Pietra et al., 1997) 2. The basic training procedures for the translation s
ollowing criteria are used: • WER (word error rate): The word error rate is based on the Levenshtein distance. It is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the reference string. Since some sentences in the develop and test set occur several times with different reference translations (which holds especially for short sentences like 'okay, good-bye'), we calculate the minimal distance to this set of references as proposed in (NieBen et al., 2000). • BLEU (bilingual evaluation understudy): (Papineni et al., 2002) have proposed a 3 The Brill tagger can be downloaded from http://www.research.microsoft.com/users/brill/ 351 Table 4: Statistics of the training, develop and test set of the English-Spanish-Catalan LC-STAR corpus (*number of words without punctuation marks) English Spanish Catalan Original Transformed Training Sentences 13 352 Words Words&quot; 123 454 114 099 118 534 118 137 101 738 92 383 96 997 96 503 Vocabulary Size 2 154 2 776 3 933 3 572 Singletons 790 (37%) 1 165 (42%) 1 844 (47%) 1 658 (47%) Develop Sentences 272 Words Unknown Words 2 267 2 096 2217 2211 21 22 34 34 Test Sentences 262 Word
tes. These numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy: there, we have an average of 4.20 for Catalan and 4.46 for Spanish. Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4. This shows that our method was successful in producing a more focused lexicon probability distribution. We performed translation experiments with an implementation of the IBM-4 translation model (Brown et al., 1993). A description of the system can be found in (Tillmann and Ney, 2002). Table 5 presents an assessment of translation quality for both the language pairs English—Catalan and English—Spanish. We see that there is a significant decrease in error rate for the translation into Catalan. This change is consistent across both error rates, the WER and 100—BLEU. For translations from English into Spanish, the improvement is less substantial. A reason for this might be that the Spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher: 1.57 for Spanish versus 1.53 
