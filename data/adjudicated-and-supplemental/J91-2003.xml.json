{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.40284","#text":"\nalso show metalevel rules for interpreting &quot;but.&quot;\n2. The Paragraph as a Discourse Unit\n"},{"#tail":"\n","@confidence":"0.99352","#text":"\n1. Reasoning takes place in a three-level structure consisting of an object\nlevel, a referential evel, and a metalevel.\n2. The object level is used to describe the current situation, and in our case\nis reserved for the formal representation f paragraph sentences. For the\nsake of simplicity, the object level will consist of a first order theory.\n3. The referential level, denoted by R, consists of theories representing\n"},{"#tail":"\n","@confidence":"0.930058","#text":"\nZadrozny and Jensen Semantics of Paragraphs\n? A deductive closure operator is a function Th : P(Sent) --* P(Sent)\n(a) T c Th(T), for any T,\n(b) Th(Th(T)) = Th(T),\n(c) Th(T) is finite, for finite T; additionally, we require it to be\nground, for ground T.\n? A theory T is consistent if there is no formula ~ such that both ~ and -~\nbelong to Th(T) (and inconsistent o herwise).\n? A model of a theory T is defined, as usual, as an interpretation defined on\na certain domain, which satisfies all formulas of T. The collection of all\n(finite) models of a theory T will be denoted by Mods(T).\n? The set of all subformulas of a collection of formulas F is denoted by\n"},{"#tail":"\n","@confidence":"0.639398","#text":"\n? If t, t r E G(T), t ~ t I, share a predicate, we say that there is a c-link\nbetween t and tC A c-path is defined as a chain of c-links; i.e. if\n"},{"#tail":"\n","@confidence":"0.94800125","#text":"\n1. For all &quot;sentences&quot; Si, (a) or (b) or (c) holds:\n(a) Direct reference to the topic:\nTp C Pred(Si)\n(b) Indirect reference to the topic:\nIf ? E Pred(Si) & (? -~ T?) E T, then Tp C Pred(T?)\n(c) Direct reference to a previous entence:\nIf ? E Pred(Si) & (~ ~ T?) E T then Pred(Si_l)MPred(? ~ TV~ ) # 9~\n2. Either (i) or (ii) is satisfied:\n(i) Existence of a topic sentence: Tp C Pred(Si), for some sentence Si;\n(ii) Existence of a topic sentence: a theory of Tp belongs to R, i.e. if 0 is\nthe conjunction of predicates of Tp then 0 ~ To E R, for\nsome To.\n"},{"#tail":"\n","@confidence":"0.881407","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nthat constants are introduced by NPs. We have then\n(i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d),\ninfection(i), death(b), year(1347).\n(ii) Formulae\n$1: \\[t ime: year(1347); event : enter(s,m) & ship(s) & port(m) &\n"},{"#tail":"\n","@confidence":"0.8210878","#text":"\nComputational Linguistics Volume 17, Number 2\n? Quantity. Say neither too much nor too little.\n? Quality. Try to make your contribution one that is true.\n? Relation. Be relevant.\n? Manner. Avoid obscurity and ambiguity; be brief and orderly.\n"}],"figure":[{"#tail":"\n","@confidence":"0.673709285714286","#text":"\n<1 <2 <5\n&quot;flag&quot; &quot;strike&quot; &quot;strike ~ flag&quot;\n(2)\n&quot;cloth&quot; &quot;hit/anger&quot; &quot;lower&quot; j (I)\n(4)\n&quot;tail&quot; (3)\n&quot;music&quot;\n"},{"#tail":"\n","@confidence":"0.903605083333333","#text":"\nComputational Linguistics Volume 17, Number 2\nf -1\nII place(m)! r\n' i h?rb?rCm)\nI\nPI sh ip (s ) - -~ port(m) j -- time :1:547 I\n0\ns m -~\nI name: Messin?l\ns ~ nome : Block Deoth l\n, , , d Z\nI \\] in fec t ion~ness (d)J\np l ill suddenly I\n/ /\nF'&quot; I I o event (e') I\nI I\nI I\nI I\nP3 .- a.q in-time (24+ t) I\nb t? I\n- -deoth(~ ~\\[ io infecti?n (i) I\nl?g?nizing (b)-~ ' o e,Lent (e)\nI\nIi person(p) diseose(d)l I\n"}],"author":{"#tail":"\n","@confidence":"0.543428","#text":"\nWlodek Zadrozny ?\n"},"equation":[{"#tail":"\n","@confidence":"0.904831","#text":"\nDefinition S: enter(xl~ x2) & ship(x1) & port(x2) & bring(x3~ x4) & disease(x4) & xl = s\n& x2 = m & x3 = s & x4 = d,\n"},{"#tail":"\n","@confidence":"0.98090025","#text":"\nenter(x, y) --* {come_in (x~ y); place(y)}\nenter(x, y) --* {join(x, y)&group(y);. . .}\n(el)\n(e2)\n"},{"#tail":"\n","@confidence":"0.872241","#text":"\nR = {(?, <?) : ? E Formulae}\nwhere---for each ? - - <? is a partially ordered (by a relation of\n"},{"#tail":"\n","@confidence":"0.936984705882353","#text":"\nenter(x, y) --. {come_in(x, y); place(y);...} (el)\n/* enter--to come into a place */\nenter(x, y) --* join(x, y)&group(y); typically : professionals(y)} (e2)\n/* enter--to join a group; typically of professionals */\nship(x) --* {large_boat(x); 3y carry(x, y)&(people(y) v goods(y));...} (shl)\n/* ship--a large boat for carrying people or goods on the sea */\nship(x) ---, {(large,aircraft(x) V space_vehicle(x));...}\nbring(x, y) --* {carry(x, y);...}\nbring(x, y) --. {cause(x, y);...}\ndisease(y) ---, {illness(y);...}\n/* disease\n(sh2)\n(bl)\n/* bring--to carry */\n(b2)\n/* bring--to cause */\n(de1)\n"},{"#tail":"\n","@confidence":"0.965106285714286","#text":"\ndisaster(y) ~ {. . . ; 3x cause(y, x)&harm(x) . . .}\nport(x) ~ {harbor(x);. . .}\n( . . . )\n(dr1)\n/* disaster---causes a harm */\n(pl)\n/* port--harbor */\n"},{"#tail":"\n","@confidence":"0.771794","#text":"\nI I(F) ~- 1--\\[ (e= 0c: (Ve G m)(31 < ne)~(e) = ~de -'~ Z~\\]}\ne~m\n"},{"#tail":"\n","@confidence":"0.873361142857143","#text":"\n/ / /S\nf in t~ .,..,,~1\n.e I c - - - - -s h I bit _.. i f l . - j &quot;~ J ~.....,,1~1\ne2 sh2, b2&quot; a a\nI I I\na a Q\nFigure 1\n"},{"#tail":"\n","@confidence":"0.94535","#text":"\nt = ~ --* T and t ~ = ~' --* T' belong to a c-path, then ~ ~ ~'.\n"},{"#tail":"\n","@confidence":"0.759757","#text":"\nPTc(T) = {T U T' : T' -- ?p and p is a C - maximal element of C(T)}\n"},{"#tail":"\n","@confidence":"0.721739","#text":"\nPT(T) = PT< (PTc(Th(T) )\nPT is well defined after we specify that PT of a set of theories is the set of the PTs\n(for both < and C):\nPT{~( {T~, T2,...} ) = PT(~(T1) U PT,\\](T2) U...\n"},{"#tail":"\n","@confidence":"0.6151865","#text":"\nbring(xo, d) & disease(d) & name(d, BlackDeath) & (Xo -- s V\nx0 = d V x0 -- m)\\]\n$2: t ime : past; event : rapidly: strike(yo) & (yo -- s V yo = m V\ny0 -- d)\\]\n$3: 3t, t'{\\[time : t; infection(i)\\] & \\[time: t' c (t, t + 24h);\nevent :come(b) & death(b) & agonizing(b)\\]}\n"},{"#tail":"\n","@confidence":"0.943599666666667","#text":"\nship(x) --, {large: boat(x); 3ycarry(x,y) & (people(y) v goods(y)) & agent(x);...} (shl)\n/* ship--a large boat for carrying people or goods on the sea */\nbring(x, y) --* {carry(x, y); ...} (bl)\nstrike(x, y) --* {hit(x, y); agent(x) & patient(y);...}\nstrike(x) --~ {hit(x); agent(x);...}\nstrike(x) --* {illness(x) & By suddenly:harm(x, y);...}\n/* bring--to carry */\n(sla)\n(slb)\n/* strike---to hit */\n(s2 _ex)\n/* strike---to harm suddenly; &quot;they were struck by illness */\n"},{"#tail":"\n","@confidence":"0.939858375","#text":"\ncome(x) --* {3t\\[time : t; event :arrive(x)\\]} (ct_l)\n/* to come--to arrive (...) in the course of time */\ninfection(x) --~ {3e, y, z\\[e = event :infect(y, z) & person(y) & disease(z))\\]\n& x -- result(e)} (i_1)\n/* infection--the r sult of being infected by a disease */\nagonizing(x) ~ {3y causes(x, y) & pain(y)} (a_l)\n/* agonizing---causing great pain */\nenter(x, y) - . {come_in(x, y); place(y)} (e_l)\n"},{"#tail":"\n","@confidence":"0.9808992","#text":"\ncheap(x) --, {~elegant(x); poor_quality(x);-~expensive} (cl)\nexpensive(x) ~ -~cheap( x ) (encl)\nyacht(x) ~ {ship(x) & small(x)} (yl)\nelegant(x) --, { ~cheap( x ) ~ . . . } (el)\nelegant(x) & yacht(x) --, {status-symbol(x)} (e_yl)\n"},{"#tail":"\n","@confidence":"0.959505875","#text":"\nelegant(yo ). Then\nTcheap -= {-~elegant(x); poor_quality(x) ; -~expensive(x) }\nTyacht = {ship(x) & small(x)}\nTelcgant = {expensive(x)}\nTyacht & elegant = {status_symbol(x)}\nTcheap/~d = {poor-quality(x) ; -~expensive(x)}\nTyacht/' = Tyacht\nTyacht&elegant / @ = Tyacht&elegant\n"},{"#tail":"\n","@confidence":"0.9976265","#text":"\nPT(~ but ~) = {T}~ where\nT = {yacht(yo)~ elegant(yo)~cheap(yo)~ship(yo) & small(yo)~\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.366131","#text":"\nKaren Jensew\n"},{"#tail":"\n","@confidence":"0.935365","#text":"\n2.1 Approaches to Paragraph Analysis\n"},{"#tail":"\n","@confidence":"0.997913","#text":"\n2.2 Our View of Paragraphs: An Informal Sketch\n"},{"#tail":"\n","@confidence":"0.991095","#text":"\n3.1 Finite Representations, Finite Theories\n"},{"#tail":"\n","@confidence":"0.667027","#text":"\nDefinitions\n"},{"#tail":"\n","@confidence":"0.998246","#text":"\n3.2 The Structure of Background Knowledge\n"},{"#tail":"\n","@confidence":"0.731558","#text":"\n3.3 How to Use Background Knowledge\n"},{"#tail":"\n","@confidence":"0.518474","#text":"\nDefinitions\n"},{"#tail":"\n","@confidence":"0.992221","#text":"\n3.4 Summary and Discussion\n"},{"#tail":"\n","@confidence":"0.999311","#text":"\n4.1 Comparison with Other Approaches\n"},{"#tail":"\n","@confidence":"0.976478","#text":"\n5.1 The Example Revisited: Preparation for Building a Model\n"},{"#tail":"\n","@confidence":"0.998915","#text":"\n5.2 p-Models\n"},{"#tail":"\n","@confidence":"0.99937","#text":"\n6.1 A Formalization of Gricean Maxims\n"},{"#tail":"\n","@confidence":"0.935471","#text":"\nMetarule Gla\n"},{"#tail":"\n","@confidence":"0.994442","#text":"\n6.2 Semantics of the Conjunction &quot;But&quot;\n"}],"subsubsectionHeader":[{"#tail":"\n","@confidence":"0.366949","#text":"\n3.3.2 Cartesian Products. Formalization of the interpretation produced in 3.3.1. is pre-\n"},{"#tail":"\n","@confidence":"0.539472","#text":"\n5.1.2 Referential Level. We use only standard dictionaries as a source of background\n"},{"#tail":"\n","@confidence":"0.828237","#text":"\n5.1.3 Provability and Anaphora Resolution. To formalize a part of the process for\n"},{"#tail":"\n","@confidence":"0.84869","#text":"\n6.2.1 The Semantic Function of &quot;But&quot; &quot;But&quot; introduces an element of surprise into\n"}],"footnote":{"#tail":"\n","@confidence":"0.922348","#text":"\n1 J. Burke, The Day the Universe Changed. 1986. Little, Brown & Co., Boston, Massachusetts, p. 55.\n"},"title":{"#tail":"\n","@confidence":"0.383772","#text":"\nSemantics of Paragraphs\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.999684981818182","#text":"\nAllen, J. (1987). Natural Language\nUnderstanding. Benjamin/Cummings.\nMenlo Park, California.\nBerwick, R.C. (1986). &quot;Learning from\nPositive-Only Examples: The Subset\nPrinciple and Three Case Studies.&quot; In\nMichalski, R.S. et al, eds. Machine\nLearning Vol. II. Morgan Kaufmann. Los\nAltos, California: 625-645.\nBlack, J.B., and Bower, G.H. (1979).\n&quot;Episodes as Chunks in Narrative\nMemory.&quot; Journal of Verbal Learning and\nVerbal Behavior 18: 309-318.\nBond, S.J., and Hayes, J.R. (1983). Cues\nPeople Use to Paragraph Text. manuscript.\nDept. of Psychology, Carnegie-Mellon\nUniversity.\nBrachman, R.J., Fikes, R.E., and Levesque,\nH.J. (1985). &quot;Krypton: A Functional\nApproach to Knowledge Representation.&quot;\nIn Brachman, R.J., and Levesque, H.J.,\neds. Readings in Knowledge Representation.\nMorgan Kaufmann. Los Altos, California:\n411-430.\nBraden-Harder, L. and Zadrozny, W. (1989).\nLexicons for Broad Coverage Semantics. IBM\nResearch Division Report, RC 15568.\nChafe, W.L. (1979). &quot;The Flow of Thought\nand the Flow of Language.&quot; In Givon, T.,\ned., Syntax and Semantics, Vol. 12.\nAcademic Press. New York, New York.\nCharniak, E. (1983). &quot;Passing Markers: A\nTheory of Contextual Influence in\nLanguage Comprehension.&quot; Cognitive\nScience, 7(3): 171-190.\nCharniak, E., and McDermott, D. (1985).\nIntroduction to Artificial Intelligence.\nAddison-Wesley. Reading, Massachusetts.\nCrothers, E.J. (1979). Paragraph Structure\nInference. Ablex Publishing Corp.,\nNorwood, New Jersey.\nEtherington, D.W., and Mercer, R.E. (1987).\n&quot;Domain Circumscription: A\nReevaluation.&quot; Computational Intelligence\n3: 94--99.\nFrisch, A. (1987). &quot;Inference without\nChaining.&quot; Proc. IJCAI-87. Milan, Italy:\n515-519.\nFowler, H.W. (1965). A Dictionary of Modern\nEnglish Usage. Oxford University Press.\nNew York, New York.\nGenesereth, M.R., and Nilsson, N.J. (1987).\nLogical Foundations of Artificial Intelligence.\nMorgan Kaufmann. Los Altos, California.\nGrice, H.P. (1978). &quot;Further Notes on Logic\n"},{"#tail":"\n","@confidence":"0.999813447154471","#text":"\nComputational Linguistics Volume 17, Number 2\nand Conversation.&quot; In Cole, P., ed. Syntax\nand Semantics 9:Pragmatics. Academic\nPress. New York, New York: 113-128.\nGrice, H.P. (1975). &quot;Logic and\nConversation.&quot; In Cole, P., and Morgan,\nJ.L., eds., Syntax and Semantics 3: Speech\nActs. Academic Press. New York New\nYork: 41-58.\nGroesser, A.C. (1981). Prose Comprehension\nBeyond the Word. Springer. New York,\nNew York.\nGrosz, B.J. (1978). DISCOURSE\nKNOWLEDGE--Section 4 of Walker, D.E.,\ned., Understanding Spoken Language.\nNorth-Holland. New York, New York:\n229-344.\nGrosz, B.J. (1977). &quot;The Representation a d\nUse of Focus in a System for\nUnderstanding Dialogs.&quot; Proc. IJCAI-77.\nW. Kaufmann. Los Altos, California:\n67-76.\nHaberlandt, K., Berian, C., and Sandson, J.\n(1980). &quot;The Episode Schema in Story\nProcessing.&quot; Journal of Verbal Learning and\nVerbal Behavior 19: 635--650.\nHalliday, M.A.K., and Hasan, R. (1976).\nCohesion in English. Longman Group Ltd.\nLondon.\nHaugeland, J. (1985). Artificial Intelligence:\nThe Very Idea. MIT Press. Cambridge,\nMassachusetts.\nHinds, J. (1979). &quot;Organizational Patterns in\nDiscourse.&quot; In Givon, T., ed., Syntax and\nSemantics, Vol. 12. Academic Press. New\nYork, New York.\nHintikka, J. (1985). The Game of Language.\nD. Reidel. Dordrecht.\nHirst, G. (1987). Semantic Interpretation a d\nthe Resolution of Ambiguity. Cambridge\nUniversity Press. Cambridge.\nHobbs, J.R. (1982). &quot;Towards an\nUnderstanding of Coherence in\nDiscourse.&quot; In Lehnert, W.G., and Ringle,\nM.H., eds. Strategies for Natural Language\nProcessing. Lawrence Erlbaum. Hillsdale,\nNew Jersey: 223-244.\nHobbs, J.R. (1979). &quot;Coherence and\nCoreference.&quot; Cognitive Science 3: 67-90.\nHobbs, J.R. (1978). &quot;Resolving Pronoun\nReferences.&quot; Lingua 44: 311-338.\nHobbs, J.R. (1977). 38 Examples of Elusive\nAntecedents from Published Texts.\nResearch Report 77-2. Department of\nComputer Science, City College, CUNY,\nNew York.\nHobbs, J.R. (1976). Pronoun Resolution.\nResearch Report 76-1. Dept. of Computer\nScience. City College, CUNY, New York.\nHobbs, J., Stickel, M., Martin, P., and\nEdwards, D. (1988). &quot;Interpretation as\nAbduction.&quot; In Proc. of 26th Annual\nMeeting of the Association for Computational\nLinguistics, ACL: 95-103.\nJackendoff, R. (1983). Semantics and\nCognition. The MIT Press. Cambridge,\nMassachusetts.\nJensen, K. (1988). &quot;Issues in Parsing.&quot; In\nA. Blaser, ed., Natural Language at the\nCompute, r. Springer-Verlag, Berlin,\nGermany: 65-83.\nJensen, K. (1986). Parsing Strategies in a\nBroad-Coverage Grammar of English.\nResearch Report RC 12147. IBM\nT.J. Watson Research Center, Yorktown\nHeights, New York.\nJensen, K., and Binot, J.-L. (1988).\n&quot;Disambiguating Prepositional Phrase\nAttachments by Using On-line Dictionary\nDefinitions.&quot; Computational Linguistics\n13.3-4.251-260 (special issue on the\nlexicon).\nJohnson-Laird, P.N. (1983). Mental Models.\nHarvard University Press. Cambridge,\nMassachusetts.\nKamp, H. (1981). &quot;A Theory of Truth and\nSemantic Representation.&quot; In Groenendijk,\nJ.A.G., et al eds., Formal Methods in the\nStudy of Language, I. Mathematisch\nCentrum, Amsterdam: 277-322.\nLabov, W. (1973). &quot;The Boundaries of Words\nand Their Meanings.&quot; In Fishman, J., New\nWays of Analyzing Variation in English.\nGeorgetown U. Press. Washington, D.C.:\n340-373.\nLakoff, G. (1987). Women, Fire and Dangerous\nThings. The University of Chicago Press.\nChicago, Illinois.\nLevesque, H.J. (1984). &quot;A Logic of Implicit\nand Explicit Beliefs.&quot; Proc. AAAI-84.\nAAAI: 198-202.\nLongacre, R.E. (1979). The Paragraph as a\nGrammatical Unit. In Givon, T., ed.,\nSyntax and Semantics, Vol. 12. Academic\nPress. New York, New York.\nLongman Dictionary of Contemporary English.\n(1978). Longman Group Ltd., London.\nLyons, J. (1969). Introduction to Theoretical\nLinguistics. Cambridge University Press.\nCambridge, England.\nMann, W.C., and Thompson, S.A. (1983).\nRelational Propositions in Discourse.\nInformation Sciences Institute Research\nReport 83-115.\nMoens, M., and Steedman, M. (1987).\n&quot;Temporal Ontology in Natural\nLanguage.&quot; Proc. 25th Annual Meeting of\nthe ACL. Stanford, California: 1-7.\nMontague, R. (1970). &quot;Universal Grammar.&quot;\nTheoria 36: 373-398.\nMycielski, J. (1981). &quot;Analysis without\nActual Infinity.&quot; Journal of Symbolic Logic,\n46(3): 625-633.\n"},{"#tail":"\n","@confidence":"0.999535043478261","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nPatel-Schneider, P.S. (1985). &quot;A Decidable\nFirst Order Logic for Knowledge\nRepresentation.&quot; Proc. IJCAI-85. AAAI:\n455-458.\nPerlis, D. (1985). &quot;Languages with Self\nReference I: Foundations.&quot; Artificial\nIntelligence 25: 301-322.\nPoole, D. (1988). &quot;A Logical Framework for\nDefault Reasoning.&quot; Artificial Intelligence\n36(1): 27-47.\nQuirk&quot; R., Greenbaum, S., Leech, G., and\nSvartvik, J. (1972). A Grammar of\nContemporary English. Longman Group\nLtd. London.\nReggia, J.A. (1985). &quot;Abductive Inference.&quot;\nIn Karma, K.N., ed., Expert Systems in\nGovernment Symposfum. IEEE: 484-489.\nReiter, R. (1987). &quot;A Theory of Diagnosis\nfrom First Principles.&quot; Artificial\nIntelligence, 32(1): 57-95.\nSidner, C. (1983). &quot;Focusing in the\nComprehension f Definite Anaphora.&quot; In\nBrady, M., and Berwick&quot; R., eds.,\nComputational Models of Discourse, MIT\nPress. Cambridge, Massachusetts:\n330-367.\nSells, P. (1985). Lectures on Contemporary\nSyntactic Theories. CSLI lecture notes;\nno. 3.\nShoenfield, J.R. (1987). Mathematical Logic.\nAddison-Wesley. New York&quot; New York.\nSinclair, J.M., ed. (1987). Looking Up. An\naccount of the COBUILD project. Collins\nELT, London.\nSmall, S.L., Cottrell, G.W., and Tanenhaus,\nM.K., eds. (1988). Lexical Ambiguity\nResolution. Morgan Kaufmann. San Mateo,\nCalifornia.\nTurner, M. (1987). Death is the Mother of\nBeauty. The University of Chicago Press.\nChicago, Illinois.\nvan Dijk, T.A., and Kintch, W. (1983).\nStrategies of Discourse Comprehension.\nAcademic Press. Orlando, Florida.\nWarriner, J.E. (1963). English Grammar and\nComposition. Harcourt, Brace & World,\nInc., New York New York.\nWebber, B. (1983). &quot;So What Can We Talk\nAbout Now?&quot; In Brad~ M., and Berwick,\nR., eds., Computational Models of Discourse.\nMIT Press. Cambridge, Massachusetts:\n147-154.\nWebber, B. (1987). &quot;The Interpretation f\nTense in Discourse.&quot; Proc. 25th Annual\nMeeting of the ACL, ACL: 147-154.\nWebster's Seventh New Collegiate Dictionary.\n(1963). Merriam-Webster, Inc. Springfield,\nMassachusetts.\nWoods, W. (1987). &quot;Don't Blame the Tool.&quot;\nComputational Intelligence 3(3): 228-237.\nZadrozny, W. (1987a). &quot;Intended Models,\nCircumscription and Commonsense\nReasoning.&quot; Proc. IJCAI-87: 909-916.\nZadrozny, W. (1987b). &quot;A Theory of Default\nReasoning.&quot; Proc. AAAI-87. Seattle,\nWashington: 385-390.\nZadrozny, W. (unpublished). The Logic of\nAbduction.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.964898833333333","#text":"\nIBM T. J. Watson Research Center\nWe present a computational theory of the paragraph. Within it we formally define coherence,\ngive semantics to the adversative conjunction &quot;but&quot; and to the Gricean maxim of quantity,\nand present some new methods for anaphora resolution.\nThe theory precisely characterizes the relationship between the content of the paragraph\nand background knowledge needed for its understanding. This is achieved by introducing a\nnew type of logical theory consisting of an object level, corresponding to the content of the\nparagraph, a referential level, which is a new logical level encoding background knowledge,\nand a metalevel containing constraints on models of discourse (e.g. a formal version of Gricean\nmaxims). We propose also specific mechanisms of interaction between these levels, resembling\nboth classical provability and abduction. Paragraphs are then represented by a class of structures\ncalled p-models.\n"},{"#tail":"\n","@confidence":"0.988501766233766","#text":"\nLogic and knowledge have been often discussed by linguists. Anaphora is another\nprominent subject in linguistic analyses. Not so frequently examined are different ypes\nof cohesion. And it is quite rare to find the word &quot;paragraph&quot; in articles or books\nabout natural language understanding, although paragraphs are grammatical units\nand units of discourse. But it is possible to speak formally about he role of background\nknowledge, cohesion, coherence and anaphora--all within one, flexible and natural,\nlogical system--if one examines the semantic role of the linguistic construct called a\nparagraph.\nParagraphs have been sometimes described, rather loosely, as &quot;units of thought.&quot;\nWe establish a correspondence b tween them and certain types of logical models,\nthereby making the characterization f paragraphs more precise. The correspondence\ngives us also an opportunity to identify and attack--with some success, we believe--\nthree interesting and important problems: (1) how to define formally coherence and\ntopic, (2) how to resolve anaphora, and (3) what is the formal meaning of linkages (con-\nnectives) such as but, however, and, certainly, usually, because, then, etc. These questions\nare central from our point of view because: (1) the &quot;unity&quot; of a paragraph stems from\nits coherence, while the &quot;aboutness&quot; of thought can be, at least to some extent, de-\nscribed as existence of a topic; (2) without determining reference of pronouns and\nphrases, the universes of the models are undefined; and (3) the linkages, which make\nsentences into paragraphs, have semantic roles that must be accounted for. We can ex-\nplain then the process of building a computational model of a paragraph (a p-model) as\nan interaction between its sentences, background knowledge to which these sentences\nrefer, and metatheoretical operators that indicate types of permitted models.\n? P.O. Box 704, Yorktown Heights, NY 10598\n(~ 1991 Association for Computational Linguistics\nComputational Linguistics Volume 17, Number 2\nAt this point the reader may ask: what is so special about paragraphs; does all\nthis mean that a chapter, a book or a letter do not have any formal counterparts? We\nbelieve they do. But we simply do not yet know how corresponding formal structures\nwould be created from models of paragraphs. ~Ib answer this question we may need\nmore advanced theories of categorization and learning than exist today. On the other\nhand, the paragraph is the right place to begin: it is the next grammatical unit after\nthe sentence; connectives providing cohesion operate here, not at the level of an entire\ndiscourse; and it is the smallest reasonable domain of anaphora resolution. Further-\nmore, we will argue, it is the smallest domain in which topic and coherence can be\ndefined.\nThe formalization of paragraph structure requires the introduction of a new type\nof logical theory and a corresponding class of models. As we know, the usual log-\nical structures consist of an object level theory T and provability relation F-; within\nthe context of the semantics of natural anguage, the object theory contains a logical\ntranslation of the surface form of sentences, and F- is the standard provability relation\n(logical consequence). In mathematical logic, this scheme is sometimes extended by\nadding a metalevel assumption, for instance postulating the standardness of natural\nnumbers. In artificial intelligence, a metarule typically, the closed world assumption\nof circumscription---can beused in dealing with theoretical questions, like the frame\nproblem. But a formal account of natural language understanding requires more. It\nrequires at least a description (a) of how background knowledge about objects and\nrelations that the sentences describe is used in the process of understanding, and (b) of\ngeneral constraints on linguistic communications, asexpressed for instance in Gricean\nmaxims. It is well known that without the former it is impossible to find references of\npronouns or attachments of prepositional phrases; background knowledge, as it turns\nout, is also indispensable in establishing coherence. We have then reasons for introduc-\ning a new logical level--a referential level R, which codes the background knowledge.\nAs for Gricean maxims, we show that they can be expressed formally and can be used\nin a computational model of communication. We include them in a metalevel M, which\ncontains global constraints on models of a text and definitions of meta-operators such\nas the conjunction but. We end up with three-level logical theories (M, T, R, ~-R + M),\nwhere a provability relation ~-~ + M, based on R and M, can be used, for example, to\nestablish the reference of pronouns.\nThis work is addressed primarily to our colleagues working on computational\nmodels of natural language; but it should be also of interest o linguists, logicians,\nand philosophers. It should be of interest o linguists because the notion that a para-\ngraph is equivalent o a model is something concrete to discuss; because p-models\nare as formal as formal languages (and therefore something satisfyingly theoretical\nto argue about); and because new directions for analysis are opened beyond the sen-\ntence. The work should be of interest o logicians because it introduces a new type of\nthree-level theory, and corresponding models. The theory of these structures, which\nare based on linguistic constructs, will differ from classical model theory--for instance,\nby the fact that names of predicates of an object theory matter, because they connect\nthe object heory with the referential level. This work should be of interest o philoso-\nphers for many of the same reasons: it makes more sense to talk about the meaning\nof a paragraph than about the meaning of a sentence. The following parallel can be\ndrawn: a sentence is meaningful only with respect o a model of a paragraph, exactly\nas the truth value of a formula can be computed only with respect o a given model.\nMoreover, it is possible in this framework to talk about meaning without mentioning\nthe idea of possible worlds. However, we do not identify meaning with truth condi-\ntions; in this paper, the meaning of a sentence is its role in the model of the paragraph\n"},{"#tail":"\n","@confidence":"0.9853933125","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nin which this sentence occurs. Our intuitive concept of meaning is similar to Lakoff's\n(1987) Idealized Cognitive Model (ICM). Needless to say, we believe in the possibil-\nity of formalizing ICMs, although in this paper we will not try to express, in logic,\nprototype ffects, metaphors, or formal links with vision.\nThe paper is presented in six sections. In Section 2, we discuss the grammatical\nfunction of the paragraph and we show, informally, how a formal model of a para-\ngraph might actually be built. In Section 3 we give the logical preliminaries to our\nanalysis. We discuss a three-part logical structure that includes a referential level, and\nwe introduce a model for plausible meaning. Section 4 discusses paragraph coherence,\nand Section 5 constructs a model of a paragraph, a p-model, based on the information\ncontained in the paragraph itself and background information contained in the ref-\nerential evel R. Section 5 further motivates the use of the referential level, showing\nhow it contributes to the resolution of anaphoric reference. In Section 6, we broaden\nour presentation of the metalevel, introducing some metalevel axioms, and sketching\nways by which they can be used to reduce ambiguity and construct new models. We\n"},{"#tail":"\n","@confidence":"0.999326741935484","#text":"\nRecent syntactic theory--that is, in the last 30 years--has been preoccupied with\nsentence-level analysis. Within discourse theory, however, some significant work has\nbeen done on the analysis of written paragraphs. We can identify four different linguis-\ntic approaches to paragraphs: prescriptivist, psycholinguist, extualist, and discourse-\noriented.\nThe prescriptivist approach is typified in standard English grammar textbooks,\nsuch as Warriner (1963). In these sources, a paragraph is notionally defined as some-\nthing like a series of sentences that develop one single topic, and rules are laid down\nfor the construction of an ideal (or at least an acceptable) paragraph. Although these\ndictates are fairly clear, the underlying notion of topic is not.\nAn example of psycholinguistically oriented research work can be found in Bond\nand Hayes (1983). These authors take the position that a paragraph is a psychologically\nreal unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found\nthree major formal devices that are used, by readers, to identify a paragraph: (1) the\nrepetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference;\nand (3) paragraph length, as determined by spatial and/or sentence-count i formation.\nOther psycholinguistic studies that confirm the validity of paragraph units can be\nfound in Black and Bower (1979) and Haberlandt et al (1980).\nThe textualist approach to paragraph analysis is exemplified by E. J. Crothers. His\nwork is taxonomic, in that he performs detailed escriptive analyses of paragraphs. He\nlists, classifies, and discusses various types of inference, by which he means, generally,\n&quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112)\nhave collected convincing evidence of the existence of language chunks--real struc-\ntures, not just orthographic conventions--that are smaller than a discourse, larger than\na sentence, generally composed of sentences, and recursive in nature (like sentences).\nThese chunks are sometimes called &quot;episodes,&quot; and sometimes &quot;paragraphs.&quot; Accord-\ning to Hinds (1979), paragraphs are made up of segments, which in turn are made up of\nsentences or clauses, which in turn are made up of phrases. Paragraphs therefore give\nhierarchical structure to sentences. Hinds discusses three major types of paragraphs,\nand their corresponding segment types. The three types are procedural (how-to), ex-\npository (essay), and narrative (in this case, spontaneous conversation). For each type,\n"},{"#tail":"\n","@confidence":"0.9608448","#text":"\nComputational Linguistics Volume 17, Number 2\nits segments are distinguished by bearing distinct relationships to the paragraph topic\n(which is central, but nowhere clearly defined). Segments themselves are composed\nof clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern\nand the remark-reply pattern.\n"},{"#tail":"\n","@confidence":"0.989512261904762","#text":"\nAlthough there are other discussions of the paragraph as a central element of discourse\n(e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all\nof them share a certain limitation in their formal techniques for analyzing paragraph\nstructure. Discourse linguists how little interest in making the structural descriptions\nprecise enough so that a computational grammar of text could adapt them and use\nthem. Our interest, however, lies precisely in that area.\nWe suggest that the paragraph is a grammatical nd logical unit. It is the small-\nest linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first\nreasonable domain of anaphora resolution, and of coherent thought about a central\ntopic.\nA paragraph can be thought of as a grammatical unit in the following sense: it\nis the discourse unit in which a functional (or a predicate-argument) structure can be\ndefinitely assigned to sentences/strings. For instance, Sells (1985, p. 8) says that the\nsentence &quot;Reagan thinks bananas,&quot; which is otherwise strange, is in fact acceptable if\nit occurs as an answer to the question &quot;What is Kissinger's favorite fruit?&quot; The pairing\nof these two sentences may be said to create a small paragraph. Our point is that an\nacceptable structure can be assigned to the utterance &quot;Reagan thinks bananas&quot; only\nwithin the paragraph inwhich this utterance occurs. We believe that, in general, no unit\nlarger than a paragraph is necessary to assign a functional structure to a sentence, and\nthat no smaller discourse fragment, such as two (or one) neighboring sentences, will\nbe sufficient for this task. That is, we can ask in the first sentence of a paragraph about\nKissinger's favorite fruit, elaborate the question and the circumstances in the next few\nsentences, and give the above answer at the end. We do not claim that a paragraph is\nnecessarily described by a set of grammar rules in some grammar formalism (although\nit may be); rather, it has the grammatical role of providing functional structures that\ncan be assigned to strings.\nThe logical structure of paragraphs will be analyzed in the next sections. At this\npoint we would like to present some intuitions that led to this analysis. But first we\nwant to identify our point of departure. In order to resolve anaphora nd to establish\nthe coherence or incoherence of a text, one must appeal to the necessary background\nknowledge. Hence, a formal analysis of paragraphs must include a formal description\nof background knowledge and its usage. Furthermore, this background knowledge\ncannot be treated as a collection of facts or formulas in some formal anguage, because\nthat would preclude dealing with contradictory word senses, or multiple meanings.\nSecondly, this background knowledge is not infinite and esoteric. In fact, to a large ex-\ntent it can be found in standard reference works such as dictionaries and encyclopedias.\nTo argue for these points, we can consider the following paragraph: 1\nIn the summer of 1347 a merchant ship returning from the Black Sea entered the\nSicilian port of Messina bringing with it the horrifying disease that came to be\nknown as the Black Death. It struck rapidly. Within twenty-four hours of\ninfection and the appearance ofthe first small black pustule came an agonizing\ndeath. The effect of the Black Death was appalling. In less than twenty ears half\n"},{"#tail":"\n","@confidence":"0.989901215686275","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nthe population of Europe had been killed, the countryside devastated, and a\nperiod of optimism and growing economic welfare had been brought o a\nsudden and catastrophic end.\nThe sentences that compose a paragraph must stick together; to put it more tech-\nnically, they must cohere. This means very often that they show cohesion in the sense\nof Halliday (1976)--semantic l nks between elements. Crucially, also, the sentences of\na paragraph must all be related to a topic.\nHowever, in the example paragraph, very few instances can be found here of the\nformal grammatical devices for paragraph cohesion. There are no connectives, and\nthere are only two anaphoric pronouns (both occurrences of &quot;it'0. In each case, there\nare multiple possible referents for the pronoun. The paragraph is coherent because it\nhas a topic: &quot;Black Death&quot;; all sentences mention it, explicitly or implicitly.\nNotice that resolving anaphora precedes the discovery of a topic. A few words\nabout his will illustrate the usage of background knowledge. By parsing with syntactic\ninformation alone, we show that resolution of the first &quot;it&quot; reference hinges on the\nproper attachment of the participial clause &quot;bringing within it... &quot;. If the &quot;bringing&quot;\nclause modifies &quot;Messina,&quot; then &quot;Messina&quot; is the subject of '`bringing&quot; and must be\nthe referent for &quot;it.&quot; If the clause modifies &quot;port,&quot; then &quot;port&quot; is the desired referent;\nif the clause is attached at the level of the main verb of the sentence, then &quot;ship&quot; is\nthe referent.\nBut syntactic relations do not suffice to resolve anaphora: Hobbs' (1976) algorithm\nfor resolving the reference of pronouns, depending only on the surface syntax of\nsentences in the text, when applied to &quot;it&quot; in the example paragraph, fails in both\ncases to identify the most likely referent NP.\nAdding selectional restrictions (semantic feature information, Hobbs 1977) does\nnot solve the problem, because isolated features offer only part of the background\nknowledge necessary for reference disambiguation. Later, Hobbs (1979, 1982) proposed\na knowledge base in which information about language and the world would be\nencoded, and he emphasized the need for using &quot;salience&quot; in choosing facts from this\nknowledge base.\nWe will investigate the possibility that the structure of this knowledge base can\nactually resemble the structure of, for example, natural language dictionaries. The\nprocess of finding referents could then be automated.\nDetermining that the most likely subject for &quot;bringing,&quot; in the first sentence, is the\nnoun &quot;ship&quot; is done in the following fashion. The first definition for &quot;bring&quot; in W7\n(Webster's Seventh Collegiate Dictionary) is &quot;to convey, lead, carry, or cause to come along\nwith one...&quot; The available possible subjects for &quot;bringing&quot; are &quot;Messina,&quot; port,&quot; and\n&quot;ship.&quot; &quot;Messina&quot; is listed in the Pronouncing Gazetteer of W7, which means that it\nis a place (and is so identified in the subtitle of the Gazetteer). So we can substitute\nthe word &quot;place&quot; for the word &quot;Messina.&quot; Then we check the given definitions for the\nwords &quot;place,&quot; port,&quot; and &quot;ship&quot; in both dictionaries. LDOCE (Longman Dictionary of\nContemporary English) proves particularly useful at this point. Definitions for &quot;place&quot;\nbegin: &quot;a particular part of space... &quot;. Definitions for &quot;port&quot; include: &quot;harbour... &quot;; &quot;an\nopening in the side of a ship... &quot;. But the first entry for &quot;ship&quot; in LDOCE reads &quot;a large\nboat for carrying people or goods... &quot;. This demonstrates a very quick connection with\nthe definition for the verb &quot;bring,&quot; since the word &quot;carry&quot; occurs in both definitions.\nIt requires much more time and effort to find a connection between &quot;bring&quot; and either\nof the other two candidate subject words &quot;place&quot; or &quot;port.&quot; Similar techniques can be\nused to assign &quot;disease&quot; as the most probable referent for the second &quot;it&quot; anaphor in\nour example paragraph.\n"},{"#tail":"\n","@confidence":"0.994129363636364","#text":"\nComputational Linguistics Volume 17, Number 2\nEqually significant in this instance is the realization that a dictionary points to\nsynonym and paraphrase r lations, and thereby verifies the cohesiveness of the pas-\nsage. Through the dictionary (LDOCE again), we establish shared-word relationships\nbetween and among the words &quot;disease,&quot; &quot;Black death,&quot; infection,&quot; &quot;death,&quot; &quot;killed,&quot;\nand &quot;end.&quot; Note that there is no other means, short of appealing to human under-\nstanding or to some hand-coded body of predicate assertions, for making these rela-\ntionships.\nThis demonstrates that information eeded to identify and resolve anaphoric ref-\nerences can be found, to an interesting extent, in standard ictionaries and thesauri.\n(Other reference works could be treated as additional sources of world knowledge.)\nThis type of consultation uses existing natural language texts as a referential evel for\nprocessing purposes. It is the lack of exactly this notion of referential level that has\nstood in the way of other linguists who have been interested in the paragraph as a\nunit. Crothers (1979, p. 112), for example, bemoans the fact that his &quot;theory lacks a\nworld knowledge component, a mental 'encyclopedia,' which could be invoked to gen-\nerate inferences... &quot; With respect to that independent source of knowledge, our main\ncontributions are two. First, we identify its possible structure (a collection of partially\nordered theories) and make formal the choice of a most plausible interpretation. In\nother words, we recognize it as a separate logical evel--the referential level. Second,\nwe suggest that natural language reference works, like dictionaries and thesauri, can\nquite often fill the role of the referential level.\n"},{"#tail":"\n","@confidence":"0.992558666666666","#text":"\nThe goal of this section is to introduce a formalism describing how background knowl-\nedge is used in understanding text. The term &quot;logic of reference&quot; denotes a formal\ndescription of this process of consulting various sources of information in order to\nproduce an interpretation of a text. The formalist will be presented in a number of\nsteps in which we will elaborate one simple example:\nExample 1\nEntering the port, a ship brought a disease.\nThis sentence can be translated into the logical formula (ignoring only the past\ntense of &quot;bring&quot; and the progressive of &quot;enter'9:\n"},{"#tail":"\n","@confidence":"0.989317888888889","#text":"\nwhere s, m, d, are constants.\nWe adopt he three-level semantics as a formal tool for the analysis of paragraphs.\nThis semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for\ndefault and commonsense r asoning. It should not come as a surprise that we can\nnow use this apparatus for text/discourse analysis; after all, many natural anguage\ninferences are based on defaults, and quite often they can be reduced to choosing most\nplausible interpretations of predicates. For instance, relating &quot;they&quot; to &quot;apples&quot; in the\nsentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a):\nWe bought the boys apples because they were so cheap\n"},{"#tail":"\n","@confidence":"0.986946","#text":"\nZadrozny and Jensen Semantics of Paragraphs\ncan be an example of such a most plausible choice.\nThe main ideas of the three-level semantics can be stated as follows:\n"},{"#tail":"\n","@confidence":"0.95151541025641","#text":"\nbackground knowledge, from which information relevant o the\nunderstanding of a given piece of text has to be extracted. It constrains\ninterpretations of the predicates of an object theory. Its structure and the\nextraction methods will be discussed below.\n4. Understanding has as its goal construction of an interpretation of the\ntext, i.e. building some kind of model. Since not all logically permissible\nmodels are linguistically appropriate, one needs a place, namely the\nmetalevel, to put constraints on types of models. Gricean maxims belong\nthere; Section 6 will be devoted to a presentation of the metalevel rules\ncorresponding to them.\nWe have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural\nlanguage programs, such as on-line grammars and dictionaries, can be used as referen-\ntial levels for commonsense r asoning--for example, to disambiguate PP attachment.\nThis means that information contained in grammars and dictionaries can be used to\nconstrain possible interpretations of the logical predicates of an object-level theory.\nThe referential structures we are going to use are collections of logical theories,\nbut the concept of reference is more general. Some of the intuitions we associate with\nthis notion have been very well expressed by Turner (1987, pp. 7-8):\n... Semantics i constrained by our models of ourselves and our worlds. We have\nmodels of up and down that are based by the way our bodies actually function.\nOnce the word &quot;up&quot; is given its meaning relative to our experience with gravity,\nit is not free to &quot;slip&quot; into its opposite. &quot;Up&quot; means up and not down . . . . We\nhave a model that men and women couple to produce offspring who are similar\nto their parents, and this model is grounded in genetics, and the semantics of\nkinship metaphor is grounded in this model. Mothers have a different role than\nfathers in this model, and thus there is a reason why &quot;Death is the father of\nbeauty&quot; fails poetically while &quot;Death is the mother of beauty&quot; succeeds ....\nIt is precisely this &quot;grounding&quot; of logical predicates in other conceptual structures\nthat we would like to capture. We investigate here only the &quot;grounding&quot; in logical the-\nories. However, it is possible to think about constraining linguistic or logical predicates\nby simulating physical experiences (cf. Woods 1987).\nWe assume here that a translation of the surface forms of sentences into a logical\nformalism is possible. Its details are not important for our aim of giving a semantic\ninterpretation of paragraphs; the main theses of our theory do not depend on a logical\nnotation. So we will use a very simple formalism, like the one above, resembling the\nstandard first order language. But, obviously, there are other possibilities--for instance,\nthe discourse representation structures (DRS's) of Kamp (1981), which have been used\nto translate a subset of English into logical formulas, to model text (identified with a list\nof sentences), to analyze a fragment of English, and to deal with anaphora. The logical\n"},{"#tail":"\n","@confidence":"0.986723888888889","#text":"\nComputational Linguistics Volume 17, Number 2\nnotation of Montague (1970) is more sophisticated, and may be considered another pos-\nsibility. Jackendoff's (1983) formalism is richer and resembles more closely an English\ngrammar. Jackendoff (1983, p. 14) writes &quot;it would be perverse not to take as a working\nassumption that language is a relatively efficient and accurate ncoding of the infor-\nmation it conveys.&quot; Therefore a formalism of the kind he advocates would probably be\nmost suitable for an implementation f our semantics. It will also be a model for our\nsimplified logical notation (cf. Section 5). We can envision asystem that uses data struc-\ntures produced by a computational grammar to obtain the logical form of sentences.\n"},{"#tail":"\n","@confidence":"0.999401485714286","#text":"\nUnless explicitly stated otherwise, we assume that formulas are expressed in a certain\n(formal) language L without equality; the extension L(=) of L is going to be used only\nin Section 5 for dealing with noun phrase references. This means that natural language\nexpressions such as &quot;A is B,&quot; &quot;A is the same as B,&quot; etc. are not directly represented by\nlogical equality; similarly, &quot;not&quot; is often not treated as logical negation; cf. Hintikka\n(1985).\nAll logical notions that we are going to consider, such as theory or model, will\nbe finitary. For example, a model would typically contain fewer than a hundred ele-\nments of different logical sorts. Therefore these notions, and all other constructs we\nare going to define (axioms, metarules, definitions etc.) are computational, lthough\nusually we will not provide explicit algorithms for computing them. The issues of\ncontrol are not so important for us at this point; we restrict ourselves to describing\nthe logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff\n(1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu-\ntational linguistics. As a logical postulate it is not very radical; it is possible within a\nfinitary framework to develop that part of mathematics that is used or has potential\napplications in natural science, such as mathematical analysis (cf. Mycielski 1981).\nOn the other hand, a possible obstacle to our strategy of using only finite objects is\nthe fact that the deductive closure of any set of formulas is not finite in standard logic,\nwhile, clearly, we will have to deduce new facts from formal representations of text\nand background knowledge. But there are several ways to avoid this obstruction. For\nexample, consider theories consisting of universal formulas without function symbols.\nLet Th(T) of such a theory T be defined as T plus ground clauses/sets of literals prov-\nable from T in standard logic. It is easily seen that it is a closure, i.e. Th(Th(T)) = Th(T);\nand obviously, it is finite, for finite T. It makes ense then to require that logical conse-\nquences of paragraph sentences have similar finite representations. However, in order\nnot to limit the expressive power of the formal language, we should proceed in a\nslightly different manner. The easiest way to achieve the above requirement is by pos-\ntulating that all universes of discourse are always finite, and therefore all quantifiers\nactually range over finite domains. In practice, we would use those two and other\ntricks: we could forbid more than three quantifier changes, because ven in mathe-\nmatics more than three are rare; we could restrict he size of universes of discourse to\nsome large number such as 1001; we could allow only a fixed finite nesting of func-\ntion symbols (or operators) in formulas; etc. The intention of this illustration was to\nconvince the reader that we now can introduce the following set of definitions.\n"},{"#tail":"\n","@confidence":"0.8486845","#text":"\n? A theory is a finite set of sentences Sent (formulas without free variables\nin some formal anguage).\n"},{"#tail":"\n","@confidence":"0.905356142857143","#text":"\nForm(F). ~ is a ground instance of a formula G if ~ contains no variables,\nand ~ = ~, for some substitution 0.\nThus, we do not require Th(T) to be closed under substitution i stances of tautologies.\nAlthough in this paper we take modus ponens as the main rule of inference, in general\none can consider deductive closures with respect o weaker, nonstandard logics, (cf.\nLevesque 1984; Frisch 1987; Patel-Schneider 1985). But we won't pursue this topic\nfurther here.\n"},{"#tail":"\n","@confidence":"0.996628666666667","#text":"\nBackground knowledge is not a simple list of meaning postulates--it has a structure\nand it may contain contradictions and ambiguities. These actualities have to be taken\ninto account in any realistic model of natural language understanding. For instance, the\nverb &quot;enter&quot; is polysemous. But, unless context specifies otherwise, &quot;to come in&quot; is a\nmore plausible meaning than &quot;to join a group.&quot; Assuming some logical representation\nof this knowledge, we can write that\n"},{"#tail":"\n","@confidence":"0.986505857142857","#text":"\nand e2 ~enter el.\nTwo things should be explained now about this notation:\nMeanings of predicates/words are represented on the right-hand sides of\nthe arrows as collections of formulas--i.e., theories. The main idea is that\nthese mini-theories of predicates appearing in a paragraph will jointly\nprovide enough constraints o exclude implausible interpretations. (One\ncan think of meanings as regions in space, and of constraints as sets of\nlinear inequalities approximating these regions). How this can be done,\nwe will show in a moment.\nThese theories are partially ordered; and their partial orders are written\nas <enter(x,y), or <enter, or <i, or simply <, depending on context. This is\nour way of making formal the asymmetry of plausibility of different\nmeanings of a predicate. Again, a way of exploiting it will be shown\nbelow.\n"},{"#tail":"\n","@confidence":"0.473255333333333","#text":"\nComputational Linguistics Volume 17, Number 2\nDefinition\nA referential level R is a structure\n"},{"#tail":"\n","@confidence":"0.990670363636364","#text":"\nplausibility) collection of implications ? ~ T?.\nThe term ? --* T stands for the theory {? --* ~- : ~- E T}, and\n~) ---* {?1, ?2, ' ' &quot;} abbreviates {? --* ?1, ? --* ?2, . . .}.\nIt is convenient to assume also that all formulas, except &quot;laws&quot;--which\nare supposed to be always true---have the least preferred empty\ninterpretation ~.\nWe suppose also that interpretations are additionally ranked according\nto the canonical partial ordering on subformulas. The ranking provides a\nnatural method of dealing with exceptions, as in the case of finding an\ninterpretation of ~ & p & fl with R containing (~ --* y), (c~ & fl ~ -~y),\nwhere -~y would be preferred to ywif both are consistent, and both\ndefaults are equally preferred. This means that preference is given to\nmore specific information. For instance, the sentence The officer went out\nand struck the flag will get the reading &quot;lowered the flag,&quot; if the\nappropriate theory of strike(x, y)&flag(y) is part of background\nknowledge; if not, it will be understood as &quot;hit the flag.&quot;\nThe referential level (R, <) may contain the theories listed below. Since we view\na dictionary as an (imperfect) embodiment of a referential level, we have derived the\nformulas in every theory T? from a dictionary definition of the term ?. We believe that\neven such a crude model can be useful in practice, but a refinement of this model will\nbe needed to have a sophisticated theory of a working natural anguage understanding\nsystem.\n"},{"#tail":"\n","@confidence":"0.913832","#text":"\nillness caused by an infection */\n"},{"#tail":"\n","@confidence":"0.593448","#text":"\nZadrozny and Jensen Semantics of Paragraphs\n"},{"#tail":"\n","@confidence":"0.999424852941176","#text":"\nNote: We leave undefined the semantics of adverbs uch as typically in (e2). This ad-\nverb appears in the formula s an operator; our purpose in choosing this representation\nis to call the reader's attention to the fact that for any real applications the theories\nwill have to be more complex and very likely written in a higher order language (cf.\nSection 4).\nThe theories, which we describe here only partially, restricting ourselves to their\nrelevant parts, represent the meanings of concepts. We assume as before that (el) is\nmore plausible than (e2), i.e. e2 <enter el; similarly, for (shl), (sh2) and <ship, etc. This\nparticular ordering of theories is based on the ordering of meanings of the correspond-\ning words in dictionaries (derived and less frequent meanings have lower priority).\nBut one can imagine deriving such orderings by other means, such as statistics.\nThe partial order <enter has the theories {el, e2, ~} as its domain; ~ is the least\npreferred empty interpretation corresponding to our lack of knowledge about the\npredicate; it is used when both (el) and (e2) are inconsistent with a current object\ntheory. The domain is ordered by the relation of preference ~<enter e2 <enter el. The\ntheory (el) will always be used in constructing theories and models of paragraphs\nin which the expression &quot;enter&quot; (in any grammatical form) appears, unless assuming\nit would lead to an inconsistency. In such a case, the meaning of &quot;to enter&quot; would\nchange to (e2), or some other theory belonging to R.\nWe would like to stress three points now: (1) the above implications are based\non the definitions that actually occur in dictionaries; (2) the ordering can actually be\nfound in some dictionaries--it is not our own arbitrary invention; (3) it is natural to\ntreat a dictionary definition as a theory, since it expresses &quot;the analysis of a set of\nfacts in their relation to one another,&quot; different definitions corresponding to possible\ndifferent analyses. (Encyclopedia articles are even more theory-like.) In this sense, the\nnotion of a referential level is a formalization of a real phenomenon.\nObviously, dictionaries or encyclopedias do not include all knowledge an agent\nmust have to function in the world, or a program should possess in order to under-\nstand any kind of discourse. Although the exact boundary between world knowledge\nand lexical knowledge is impossible to draw, we do know that lexicons usually contain\nvery little information about human behavior or temporal relations among objects of\nthe world. Despite all these differences, we may assume that world knowledge and\nlexical knowledge (its proper subset) have a similar formal structure. And in the ex-\namples that we present, it is the structure that matters.\n"},{"#tail":"\n","@confidence":"0.998144","#text":"\nThe next few pages will be devoted to an analysis of the interaction of background\nknowledge with a logical representation f a text. We will describe two modes of such\nan interaction; both seem to be present in our understanding of language. One exploits\ndifferences in plausibility of the meanings of words and phrases, in the absence of\ncontext (e.g., the difference between a central and a peripheral sense, or between a\nfrequent and a rare meaning). The other one takes advantage of connections between\nthose meanings. We do not claim that this is the only possible such analysis; rather,\n"},{"#tail":"\n","@confidence":"0.961609076923077","#text":"\nComputational Linguistics Volume 17, Number 2\nwe present a formal model which can perhaps be eventually disproved and replaced\nby a better one. As far as we know, this is the first such formal proposal.\n3.3.1 Dominance. In Figure I the theories of &quot;enter,&quot; ship,&quot; etc. and the partial orders\nare represented graphically; more plausible theories are positioned higher. A path\nthrough this graph chooses an interpretation of the sentence S. For instance, the path\nf int = {el, shl~ pl, bl, dl} and S say together that\nA large boat (ship) that carries people or goods came into the harbor and carried a\ndisease (illness).\nSince it is the &quot;highest&quot; path, fint is the most plausible (relative to R) interpretation of\nthe words that appear in the sentence. Because it is also consistent, it will be chosen\nas a best interpretation of S, (cf. Zadrozny 1987a, 1987b). Another theory, consisting\nof f~ = {el, sh2, pl, b2~ dl} and S, saying that\nA space vehicle came into the harbor and caused a disease~illness\nis less plausible according to that ordering. As it turns out, f~ is never constructed in\nthe process of building an interpretation of a paragraph containing the sentence S,\nunless assuming fint would lead to a contradiction, for instance within the higher level\ncontext of a science fiction story.\nThe collection of these most plausible consistent interpretations of a given theory\nT is denoted by PT< (T). Then fint belongs to PT< (Th({S})), but this is not true for f'.\nNote: One should remember that, in general, because all our orderings are partial,\nthere can be more than one most plausible interpretation of a sentence or a paragraph,\nand more that one &quot;next best&quot; interpretation. Moreover, to try to impose a total order\non all the paths (i.e. the cartesian product defined in Section 3.3.2) would be a mistake;\nit would mean that ambiguities, represented in our formalism by existence of more\nthan one (&quot;best'0 interpretation of a text, are outlawed.\n"},{"#tail":"\n","@confidence":"0.793276888888889","#text":"\nsented below. Any path through the graph of Figure 1 is an element .of the cartesian\nproduct I'I~Esubformulas(S) (~I, of the partial orderings.\nFigure 2 explains the geometric intuitions we associate with the product and the\nordering. The product itself is given by the following definition:\nDefinition\nLet F be a collection of formulas ~e, e G m, for some natural number m; and let, for\neach e, <e be a partial ordering on the collection theories\nof ~be. Define:\n{?e .... ,?e G,}\n"},{"#tail":"\n","@confidence":"0.999424","#text":"\nWe denote by < the partial order induced on II(F) by the orderings <e and the canoni-\ncal ordering of subformulas (a formula is &quot;greater&quot; than its subformulas). The geomet-\nrical meaning of this ordering can be expressed as &quot;higher paths are more important\nprovided they pass through the most specific nodes.&quot;\n"},{"#tail":"\n","@confidence":"0.581419","#text":"\nZadrozny and Jensen Semantics of Paragraphs\n"},{"#tail":"\n","@confidence":"0.818683333333333","#text":"\nThe partial ordering of theories of the referential level R and the ordering of interpretations.\nSince (shl) and (bl) dominate (respectively) (sh2) and (b2), the path f, nt represents a more\nplausible interpretation than ft.\n"},{"#tail":"\n","@confidence":"0.991891333333333","#text":"\nThe cartesian product I-I <i=-<1 x <2 x K3 can be depicted as a collection of all paths through\nthe graphs representing the partial orderings; a path chooses only one element from each\nordering--thus (1) and (2) are &quot;legal&quot; paths, while (4) is not. Also, more plausible theories\nappear higher: &quot;cloth&quot; > &quot;music&quot; > ~. More specific paths are preferred: Assuming that all\nhigher paths, like path (1), are excluded by inconsistency, path (2) is the most plausible\ninterpretation f c~&fl, and it is preferred to (3). (More explanations in the text).\nTo make Figure 2 more intuitive we assigned some meanings to the partial orders.\nThus, (1 represents some possible meanings of &quot;flag,&quot; shown with the help of the &quot;key\nwords;&quot; the meaning &quot;piece of cloth&quot; preferred to &quot;deer's tail.&quot; The word &quot;strike&quot; has\ndozens of meanings, and we can imagine that the meaning of the transitive verb being\nrepresented by (2, with &quot;hit in anger&quot; at the top, then &quot;hit, e.g. a ball&quot; and &quot;discover&quot;\nequally preferred, and then all other possible meanings. The trivial (3 representing\n"},{"#tail":"\n","@confidence":"0.980444765957447","#text":"\nComputational Linguistics Volume 17, Number 2\n&quot;strike a flag&quot; should remind us that we already know all that from Section 3.2. Notice\nthat path (2) does not give us the correct interpretation f &quot;strike the flag,&quot; which is\ncreated from &quot;cloth&quot;~-&quot;lower.&quot;\nEach element of the cartesian product I-\\[ <i represents a set of possible meanings.\nThese meanings can be combined in various ways, the simplest of which consists of\ntaking their union as we did in 3.3.1. But a paragraph isn't just a sum of its sentences,\nas a sentence isn't simply a concatenation f its phrases. The cohesion devices--such\nas &quot;but,&quot; &quot;unless,&quot; since&quot;--arrange s ntences together, and they also have semantic\nfunctions. This is reflected, for instance, in the way various pieces of background\nknowledge are pasted together. Fortunately, at this point we can abstract from this by\nintroducing an operator variable ? whose meaning will be, as a default, that of a set\ntheoretic union, U; but, as we describe it in Section 6.2, it can sometimes be changed\nto a more sophisticated join operator. There, when considering the semantics of &quot;but,&quot;\nwe'll see that referential level theories can be combined in slightly more complicated\nways. In other words, a partial theory corresponding to a paragraph cannot be just a\nsum of the theories of its sentencesIthe arrangement of those theories hould obey\nthe metalevel composition rules, which give the semantics of connectives. However,\nfrom a purely formal point of view, @ can be any function producing a theory from a\ncollection of theories.\nThe cartesian product represents all possible amalgamations of these elementary\ntheories. In other words, this product is the space of possible combinations of mean-\nings, some of which will be inconsistent with the object level theory T. We can imme-\ndiately exclude the inconsistent combinations, eliminating at least some nonsense:\nI:I(F) = {f E II(F) : ?f is consistent with T}\nIt remains now to fill in the details of the construction of PT<. We assume that a text\nP can be translated into a (ground) theory/5 (a set of logical sentences); T = Th(P) is\nthe set of logical consequences of P. We denote by F the set Form(Th(_fi))--the set of all\nsubformulas of Th(/5), about which we shall seek information at the referential level\nR. If F = {~bl(C'~),... ~b,(C'n)} (~/is a collection of constants that are arguments of ~bi),\nis this theory, we have to describe a method of augmenting it with the background\nknowledge. We can assume without loss of generality that each ~i(~i) in F has, in R, a\ncorresponding partial order <i of theories of ~i(xi). We now substitute the constants c'i\nfor the variables xi inside the theories of <i. With a slight abuse of notation, we will use\nthe same symbol <i for the new ordering. The product spaces II(F) and I:I(F) can then\nbe defined as before, with the new orderings in place of the ones with variables. Notice\nthat if only some of the variables of ~bi(~i) were bound by c'i, the same construction\nwould work. We have arrived then at a general method of linking object level formulas\nwith their theories from R.\nNow we can define PT< (T) of the theory T as the set of most likely consistent\ntheories of T given by (H(F), <), where F = Form(T):\nPT<(T) = {TUT' :T '= ?f and f is a maximal element of (I~I(F), <)}\nNotice that PT< (T) can contain more than one theory, meaning that T is ambiguous.\nThis is a consequence of the fact that the cartesian product is only partially ordered\nby <. The main reason for using ground instances ~i(Ci) in modifying the orderings\nis the need to deal with multiple occurrences of the same predicate, as in\nJohn went to the bank by the bank.\n"},{"#tail":"\n","@confidence":"0.970881533333333","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nThe above construction is also very close in spirit to Poole's (1988) method for default\nreasoning, where object theories are augmented by ground instances of defaults.\n3.3.3 Coherence Links. The reasoning that led to the intended interpretation fi t in our\ndiscussion of dominance was based on the partial ordering of the theories of R. We\nwant to exploit now another property of the theories of R--their coherence. Finding\nan interpretation for a natural anguage text or sentence typically involves an appeal\nto coherence. Consider\n$2: Entering the port, a ship brought adisaster.\nUsing the coherence link between (b2) and (dr1) (cf. Section 3.2)--the presence of\ncause(*, ,) in the theories of &quot;bring&quot; and &quot;disaster&quot;--we can find a partial coherent\ninterpretation T E PTc(Th({S2})) of $2. In this interpretation, theories explaining the\nmeanings of terms are chosen on the basis of shared terms. This makes (b2) (&quot;to bring&quot;\nmeans &quot;to cause&quot;) plausible and therefore it would be included in T. The formalization\nof all this is given below:\n"},{"#tail":"\n","@confidence":"0.989041666666667","#text":"\n? The set of all theories about the formulas of T is defined as:\nHere, we ignore the ordering, because we are interested only in\nconnections between concepts (represented by words).\n"},{"#tail":"\n","@confidence":"0.9825163","#text":"\nUnder this condition, for any predicate, only one of its theories will\nbelong to a c-path. A c-path therefore chooses only one meaning for each\nterm.\n? C(T) will denote the set of all c-paths in G(T) consistent with T, i.e. for\neach p E C(T), Op td T is consistent.\nThis construction is like the one we have encountered when defining\nI~I(T). The details should be filled out exactly as before; we leave this to\nthe reader.\n? We define PTc(T) of a theory T as the set of most coherent consistent\ntheories of T given by C(T):\n"},{"#tail":"\n","@confidence":"0.989375857142857","#text":"\nGoing back to $2, PTc(Th(S2)) contains also the interpretation based on\nthe coherence link between &quot;ship&quot; and &quot;bring,&quot; which involves &quot;carry.&quot;\nBased on the just-described coherence relations, we conclude that\nsentence $2 is ambiguous; it has two interpretations, based on the two\nsenses of &quot;bring.&quot; Resolution of ambiguities involves factors beyond the\nscope of this section--for instance, Gricean maxims and topic (Section 6),\nor various notions of context (cf. Small et al 1988). We will continue the\n"},{"#tail":"\n","@confidence":"0.727520666666667","#text":"\nComputational Linguistics Volume 17, Number 2\ntopic of the interaction of object level theories with background\nknowledge by showing how the two methods of using background\nknowledge can be combined.\n3.3.4 Partial Theories. A finer interpretation of an object level theory T--its partial\ntheory--is obtained by the iteration:\n"},{"#tail":"\n","@confidence":"0.9971556","#text":"\nNotice that coherence does not decide between (el) and (e2) given the above R, but\nthe iteration produces two theories of $2, both of which assert that the meaning of\n&quot;ship entered&quot; is &quot;ship came.&quot;\nA ship~boat came into the harbor/port and caused~brought a disaster.\nA ship~boat came into the harbor/port and carried/brought a disaster.\nPT({S1}) contains only one interpretation based on fint&quot;\nA ship~boat came into the harbor~port and carried~brought a disease.\nPartial theories will be the main syntactic onstructs in the subsequent sections. In\nparticular, the p-models will be defined as some special models or partial theories of\nparagraphs.\n"},{"#tail":"\n","@confidence":"0.99938635","#text":"\nWe have shown that finding an interpretation of a sentence depends on two graph-\ntheoretical properties--coherence and dominance. Coherence is a purely &quot;associative&quot;\nproperty; we are interested only in the existence of links between represented con-\ncepts/theories. Dominance uses the directionality of the partial orders.\nA partial theory PT(T) of an object theory T corresponding to a paragraph is\nobtained by joining most plausible theories or sentences, collocations, and words of\nthe paragraph. However, this simple picture must be slightly retouched to account\nfor semantic roles of inter- and intra-sentential connectives uch as &quot;but,&quot; and to\nassure consistency of the partial theory. These modifications have complicated the\ndefinitions a little bit. The above definitions capture the fact that even if, in principle,\nany consistent combination of the mini-theories about predicates can be extended to\nan interpretation, we are really interested only in the most plausible ones. The theory\nPT(T) is called &quot;partial&quot; because it does not contain all knowledge about predicates--\nless plausible properties are excluded from consideration, although they are accessible\nshould an inconsistency appear. Moreover, the partiality is related to the unutilized\npossibility of iterating the operator PT (cf. Section 4).\nHow can we now summarize what we have learned about he three logical levels?\nTo begin with, one should notice that they are syntactically distinct. If object level\ntheories are expressed by collections of first order formulas, metalevel definitions--\ne.g., to express as a default hat ? is a set theoretical union--require another language,\n"},{"#tail":"\n","@confidence":"0.9951774375","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nsuch as higher order logic or set theory, where one can define predicates dealing\nwith models, consistency, and provability. Even if all background knowledge were\ndescribed, as in our examples, by sets of first order theories, because of the preferences\nand inconsistencies of meanings, we could not treat R as a flat database of facts--such\na model simply would not be realistic. Rather, R must be treated as a separate logical\nlevel for these syntactic reasons, and because of its function--being a pool of possibly\nconflicting semantic onstraints.\nThe last point may be seen better if we look at some differences between our\nsystem and KRYPTON, which also distinguishes between an object heory and back-\nground knowledge (cf. Brachman et al 1985). KRYPTON's A-box, encoding the object\ntheory as a set of assertions, uses standard first order logic; the T-box contains informa-\ntion expressed in a frame-based language quivalent to a fragment of FOL. However,\nthe distinction between the two parts is purely functional--that is, characterized in\nterms of the system's behavior. From the logical point of view, the knowledge base\nis the union of the two boxes, i.e. a theory, and the entailment is standard. In our\nsystem, we also distinguish between the &quot;definitional&quot; and factual information, but\nthe &quot;definitional&quot; part contains collections of mutually excluding theories, not just\nof formulas describing a semantic network. Moreover, in addition to proposing this\nstructure of R, we have described the two mechanisms for exploiting it, &quot;coherence&quot;\nand &quot;dominance,&quot; which are not variants of the standard first order entailment, but\nabduction.\nThe idea of using preferences among theories is new, hence it was described in\nmore detail. &quot;Coherence,&quot; as outlined above, can be understood as a declarative (or\nstatic) version of marker passing (Hirst 1987; Charniak 1983), with one difference: the\nactivation spreads to theories that share a predicate, not through the IS-A hierarchy,\nand is limited to elementary facts about predicates appearing in the text.\nThe metalevel rules we are going to discuss in Section 6, and that deal with the\nGricean maxims and the meaning of &quot;but,&quot; can be easily expressed in the languages\nof set theory or higher order logic, but not everything expressible in those languages\nmakes sense in natural language. Hence, putting limitations on the expressive power\nof the language of the metalevel will remain as one of many open problems.\n"},{"#tail":"\n","@confidence":"0.999442882352941","#text":"\nWe are now in a position to use the notion of the referential level in a formal definition\nof coherence and topic. Having done that, we will turn our attention to the resolution\nof anaphora, linking it with the provability relation (abduction) t-R+M and a metarule\npostulating that a most plausible model of a paragraph is one in which anaphors have\nreferences. Since the example paragraph we analyze has only one connective (&quot;and&quot;),\nwe can postpone a discussion of connectives until Section 6.\nBuilding an interpretation f a paragraph does not mean finding all of its possible\nmeanings; the implausible ones should not be computed at all. This viewpoint has\nbeen reflected in the definition of a partial theory as a most plausible interpretation\nof a sequence of predicates. Now we want to restrict he notion of a partial theory by\nintroducing the formal notions of topic and coherence. We can then later (Section 5.2)\ndefine p-models--a category of models corresponding to paragraphs--as models of\ncoherent theories that satisfy all metalevel conditions.\nThe partial theories pick up from the referential level the most obvious or the\nmost important information about a formula. This immediate information may be\ninsufficient to decide the truth of certain predicates. It would seem therefore that\nthe iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b).\n"},{"#tail":"\n","@confidence":"0.936012666666667","#text":"\nComputational Linguistics Volume 17, Number 2\nHowever, there are at least three arguments against iterating PT. First of all, iteration\nwould increase the complexity of building a model of a paragraph; infinite iteration\nwould almost certainly make impossible such a construction i real time. Secondly, the\ncooperative principle of Grice (1975, 1978), under the assumption that referential levels\nof a writer and a reader are quite similar, implies that the writer should structure the\ntext in a way that makes the construction of his intended model easy for the reader;\nand this seems to imply that he should appeal only to the most direct knowledge of the\nreader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit\ninformation ecessary for understanding a piece of text is about 8:1; furthermore, our\nreading of the analysis of five paragraphs by Crothers (1979) strongly suggests that\nonly the most direct or obvious inferences are being made in the process of building a\nmodel or constructing a theory of a paragraph. Thus, for example, we can expect hat\nin the worst case only one or two steps of such an iteration would be needed to find\nanswers to wh-questions.\nLet P be a paragraph, let /3 = ($1,..., Sn) be its translation into a sequence of\nlogical formulas. The set of all predicates appearing in X will be denoted by Pred(X).\nDefinition\nLet T be a partial theory of a paragraph P. A sequence of predicates appearing in i6,\ndenoted by Tp, is called a topic of the paragraph P, if it is a longest sequence satisfying\nthe conditions (1) and (2) below:\n"},{"#tail":"\n","@confidence":"0.993911285714286","#text":"\nThe last two conditions ay that either the discussed concept (topic) already exists in\nthe background knowledge or it must be introduced in a sentence. For instance, we\ncan see that the sentence The effect of the Black Death was appalling can be assumed to\nbe a topic sentence.\nThe first three conditions make the requirements for a collection of sentences to\nhave a topic. Either every sentence talks about he topic (as, for instance, the first two\nsentences of the paragraph about the Black Death), or a sentence refers to the topic\n"},{"#tail":"\n","@confidence":"0.986499844444444","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nthrough background knowledge----the topic appears in a theory about an entity or a\nrelation of the sentence (in the case of Within twenty-four hours of infection . . . . &quot;infec-\ntion&quot; can be linked to &quot;disease'--cf. Sections 2 and 4.2), or else a sentence laborates\na fragment of the previous sentence (the theme The effect of... being developed in In\nless than... ).\nThe definition allows a paragraph to have more than one topic. For instance, a\nparagraph consisting of\nJohn thinks Mary is pretty. John thinks Mary is intelligent. John wants to marry\nher.\ncan be either about {John(\\]), Mary(m), think(j, m, pretty(m))}, or about John, Mary, and\nmarrying. (Notice that the condition 2 (i) forbids us merging the two topics into a\nlarger one). Thus paragraphs can be ambiguous about what constitutes their topics.\nThe point is that they should have one.\nIt is also clear that what constitutes a topic depends on the way the content of\nparagraph sentences i represented. In the last case, if &quot;pretty&quot; were translated into a\npredicate, and not into a modifier of m (i.e. an operator), &quot;John thinking about Mary&quot;\ncould not be a topic, for it wouldn't be the longest sequence of predicates atisfying\nthe conditions (1) and (2).\nWe'd like to put forward a hypothesis that this relationship between topics and\nrepresentations can actually be useful: Because the requirement that a well-formed\nparagraph should have a topic is a very natural one (and we can judge pretty well\nwhat can be a topic and what can't), we can obtain a new method for judging semantic\nrepresentations. Thus, if the naive first order representation containing pretty(m) as\none of the formulas gives a wrong answer as to what is the topic of the above, or\nanother, paragraph, we can reject it in favor of a (higher order) representation i\nwhich adjectives and adverbs are operators, not predicates, and which provides us\nwith an intuitively correct opic. Such a method can be used in addition to the standard\ncriteria for judging representations, such as elegance and ability to express emantic\ngeneralizations.\nDefinition\nA partial theory T E PT(P) of the paragraph P is coherent iff the paragraph P has a\ntopic.\nA random permutation of just any sentences about a disease wouldn't be coher-\nent. But it would be premature to jump to the conclusion that we need more than just\nexistence of a topic as a condition for coherence. Although it may be the case that it\nwill be necessary in the future to introduce notions like &quot;temporal coherence,&quot; &quot;deictic\ncoherence,&quot; or &quot;causal coherence,&quot; there is no need to start multiplying beings now.\nWe can surmise that the random permutations we talk about would produce an in-\nconsistent theory; hence, the temporal, causal, and other aspects would be dealt with\nby consistency. But of course at this point it is just a hypothesis.\nAn important aspect of the definition is that coherence has been defined as a prop-\nerty of representation--in our case, it is a property of a formal theory. The existence of\nthe topic, the direct or indirect allusion to it, and anaphora (which will be addressed\nbelow) take up the issue of formal criteria for a paragraph definition, which was raised\n"},{"#tail":"\n","@confidence":"0.986385090909091","#text":"\nComputational Linguistics Volume 17, Number 2\nby Bond and Hayes (1983) (cf. also Section 2.1). The question of paragraph length can\nprobably be attended to by limiting the size of p-models, perhaps after introducing\nsome kind of metric on logical data structures.\nStill, our definition of coherence may not be restrictive nough: two collections\nof sentences, one referring to &quot;black&quot; (about black pencils, black pullovers, and black\npoodles), the other one about &quot;death&quot; (war, cancer, etc.), connected by a sentence\nreferring to both of these, could be interpreted as one paragraph about he new, broader\ntopic &quot;black + death.&quot; This problem may be similar to the situation in which current\nformal grammars allow nonsensical but parsable collections of words (e.g., &quot;colorless\ngreen ideas... '9, while before the advent of Chomskyan formalisms, a sentence was\ndefined as the smallest meaningful collection of words; Fowler (1965, p. 546) gives 10\ndefinitions of a sentence.\nIt then seems worth differentiating between the creation of a new concept like\n&quot;black + death,&quot; with a meaning given by a paraphrase of the example collection of\nsentences, and the acceptance of the new concept--storing it in R. In our case the\nconcept &quot;black + death,&quot; which does not refer to any normal experiences, would be\ndiscarded as useless, although the collection of sentences would be recognized as a\nstrange, even if coherent, paragraph.\nWe can also hope for some fine-tuning of the notion of topic, which would prevent\nmany offensive xamples. This approach is taken in computational syntactic grammars\n(e.g. Jensen 1986); the number of unlikely parses is severely reduced whenever pos-\nsible, but no attempt is made to define only the so-called grammatical strings of a\nlanguage.\nFinally, as the paragraph is a natural domain in which word senses can be reliably\nassigned to words or sentences can be syntactically disambiguated, larger chunks of\ndiscourse may be needed for precise assignment of topics, which we view as another\ntype of disambiguation. Notice also that for coherence, as defined above, it does not\nmatter whether the topic is defined as a longest, a shortest, or--simply--a sequence of\npredicates satisfying the conditions (1) and (2); the existence of a sequence is equivalent\nwith the existence of a shortest and a longest sequence. The reason for choosing a\nlongest sequence as the topic is our belief that the topic should rather contain more\ninformation about a paragraph than less.\n"},{"#tail":"\n","@confidence":"0.998067875","#text":"\nAt this point it may be proper to comment on the relationship between our theory of\ncoherence and theories advocated by others. We are going to make such a comparison\nwith the theories proposed by J. Hobbs (1979, 1982) that represent a more computa-\ntionally oriented approach to coherence, and those of T.A. van Dijk and W. Kintch\n(1983), who are more interested in addressing psychological nd cognitive aspects of\ndiscourse coherence. The quoted works seem to be good representatives for each of\nthe directions; they also point to related literature.\nThe approach we advocate is compatible with the work of these researchers, we\nbelieve. There are, however, some interesting differences: first of all, we emphasize the\nrole of paragraphs; econd, we talk about formal principles regulating the organization\nand use of knowledge in language understanding; and third, we realize that natural\nlanguage text (such as an on-line dictionary) can, in many cases, provide the type of\ncommonsense background information that Hobbs (for example) advocated but didn't\nknow how to access. (There are also some other, minor, differences. For instance, our\nthree-level semantics does not appeal to possible worlds, as van Dijk and Kintch do;\nneither is it objectivist, as Hobbs' semantics eems to be.)\n"},{"#tail":"\n","@confidence":"0.997081","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nWe shall discuss only the first two points, since the third one has already been\nexplained.\nThe chief difference between our approach and the other two lies in identifying the\nparagraph as a domain of coherence. Hobbs, van Dijk, and Kintch distinguish between\n&quot;local&quot; coherence~a property of subsequent sentences--and &quot;global&quot; coherence---a\nproperty of discourse as a whole. Hobbs explains coherence in terms of an inventory\nof &quot;local,&quot; possibly computable, coherence relations, like &quot;elaboration,&quot; &quot;occasion,&quot;\netc. (Mann and Thompson 1983 give an even more detailed list of coherence relations\nthan Hobbs.) Van Dijk and Kintch do this too, but they also describe &quot;macrostructures&quot;\nrepresenting the global content of discourse, and they emphasize psychological and\ncognitive strategies used by people in establishing discourse coherence. Since we have\nlinked coherence to models of paragraphs, we can talk simply about &quot;coherence&quot;--\nwithout adjectives--as property of these models. To us the first &quot;local&quot; domain seems\nto be too small, and the second &quot;global&quot; one too large, for constructing meaningful\ncomputational models. To be sure, we believe relations between pairs of sentences are\nworth investigating, especially in dialogs. However, in written discourse, the smallest\ndomain of coherence is a paragraph, very much as the sentence is the basic domain\nof grammaticality (although one can also judge the correctness of phrases).\nTo see the advantage of assuming that coherence is a property of a fragment of a\ntext/discourse, and not a relation between subsequent sentences, let us consider for\ninstance the text\nJohn took a train from Paris to Istanbul. He likes spinach.\nAccording to Hobbs (1979, p. 67), these two sentences are incoherent. However, the\nsame fragment, augmented with the third sentence Mary told him yesterday that the\nFrench spinach crop failed and Turkey is the only country... (ibid.) suddenly (for Hobbs)\nbecomes coherent. It seems that any analysis of coherence in terms of the relation\nbetween subsequent sentences cannot explain this sudden change; after all, the first\ntwo sentences didn't change when the third one was added. On the other hand, this\nchange is easily explained when we treat the first two sentences as a paragraph:\nif the third sentence is not a part of the background knowledge, the paragraph is\nincoherent. And the paragraph obtained by adding the third sentence is coherent.\nMoreover, coherence here is clearly the result of the existence of the topic &quot;John likes\nspinach.&quot;\nWe derive coherence from formal principles regulating the organization and use\nof knowledge in language understanding. Although, like the authors discussed above,\nwe stress the importance of inferencing and background knowledge in determining\ncoherence, we also address the problem of knowledge organization; for us the cen-\ntral problem is how a model emerges from such an organization. Hobbs sets forth\nhypotheses about the interaction of background knowledge with sentences that are\nexamined at a given moment; van Dijk and Kintch provide a wealth of psychological\ninformation on that topic. But their analyses of how such knowledge could be used\nare quasi-formal. Our point of departure is different: we assume a certain simple struc-\nture of the referential level (partial orders) and a natural way of using the knowledge\ncontained there (&quot;coherence links&quot; + &quot;most plausible = first&quot;). Then we examine what\ncorresponds to &quot;topic&quot; and &quot;coherence&quot;---they become mathematical concepts. In this\nsense our work refines these concepts, changes the way of looking at them by linking\nthem to the notion of paragraph, and puts the findings of the other researchers into a\nnew context.\n"},{"#tail":"\n","@confidence":"0.92294625","#text":"\nComputational Linguistics Volume 17, Number 2\n5. Models of Paragraphs\nWe argue below that paragraphs can be mapped into models with small, finite uni-\nverses. We could have chosen another, more abstract semantics, with infinite models,\nbut in this and all cases below we have in mind computational reasons for this enter-\nprise. Thus, as in the case of Kamp's (1981) DRS, we shall construct a kind of Herbrand\nmodel of texts, with common and proper names translated into unary predicates, in-\ntransitive verbs into unary predicates, and transitive verbs into binary predicates. In\nbuilding the logical model M of a collection of formulas S corresponding to the sen-\ntences of a paragraph, we assume that the universe of M contains constants introduced\nby elements of S, usually by ones corresponding toNPs, and possibly by some formu-\nlas picked by the construction from the referential level. However, we are interested\nnot in the relationship between truth conditions and representations of a sentence,\nbut in a formalization of the way knowledge is used to produce a representation f a\nsection of text. Therefore we need not only a logical description of the truth conditions\nof sentences, as presented by Kamp, but also a formal analysis of how background\nknowledge and metalevel operations are used in the construction of models. This\nextension is important and nontrivial; we doubt that one .can deal effectively with\ncoherence, anaphora, presuppositions or the semantics of connectives without it. We\nhave begun presenting such an analysis in Section 3, and we continue now.\n"},{"#tail":"\n","@confidence":"0.796156846153846","#text":"\nWe return now to the example paragraph, to illustrate how the interaction between\nan object heory and a referential level produces a coherent interpretation f the text\n(i.e., a p-model) and resolves the anaphoric references. The method will be similar\nto, but more formal than, what was presented in Section 2. In order not to bore the\nreader with the same details all over again, we will use a shorter version of the same\ntext.\nExample 2\nPI: In 1347 a ship entered the port of Messina bringing with it the disease\nthat came to be known as the Black Death.\nP2: It struck rapidly.\nP3: Within twenty-four hours of infection came an agonizing death.\n5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we\nwill use a logical notation in which formulas may have temporal and event compo-\nnents. We assume that any formal interpretation f time will agree with the intuitive\none. So it is not necessary now to present a formal semantics here. The reader may\nconsult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to\nsee what a formal interpretation f events in time might look like. Since sentences can\nrefer to events described by other sentences, we may need also a quotation operator;\nPerlis (1985) describes how first order logic can be augmented with such an operator.\nExtending and revising Jackendoff's (1983) formalism seems to us a correct method\nto achieve the correspondence b tween syntax and semantics expressed in the gram-\nmatical constraint (&quot;that one should prefer a semantic theory that explains otherwise\narbitrary generalizations about the syntax and the lexicon&quot;---ibid.).\nHowever, as noted before, we will use a simplified version of such a logical no-\ntation; we will have only time, event, result, and property as primitives. After these\nremarks we can begin constructing the model of the example paragraph. We assume\n"},{"#tail":"\n","@confidence":"0.972317571428571","#text":"\nThe notation t ime : ~(t); event : fl should be understood as meaning\nthat the event described by the formula fl took place in (or during) the\ntime period described by the formula c~(t), t ranges over instants of time\n(not intervals).\nNote. We assume that &quot;strike&quot; is used intransitively. But our construction of the\np-models of the paragraph would look exactly the same for the transitive meaning,\nexcept that we would be expected to infer that people were harmed by the illness.\n"},{"#tail":"\n","@confidence":"0.471887","#text":"\ninformation. The content of this information is of secondary importance we want to\nstress the formal, logical side of the interaction between the referential level and the\nobject theory. Therefore we represent both in our simplified logical notation, and not\nin English. All formulas at the referential level below have been obtained by a direct\ntranslation of appropriate ntries in Webster's and Longman. The translation in this\ncase was manual, but could be automated.\n? Referent ia l leve l (a fragment):\n"},{"#tail":"\n","@confidence":"0.9767192","#text":"\n/* enter--to come into a place */\nWe have shown, in Section 3, the role of preferences in building the model of a para-\ngraph. Therefore, to make our exposition clearer, we assume that all the above theories\nare equally preferred. Still, some interesting things will happen before we arrive at our\nintended model.\n"},{"#tail":"\n","@confidence":"0.973910181818182","#text":"\nfinding antecedents of anaphors, we have to introduce a new logical notion--the re-\nlation of weak R + M-abduction. This relation would hold, for instance, between the\nobject heory of our example paragraph and a formula expressing the equality of two\nconstants, i and i', denoting (respectively) the &quot;infection&quot; in the sentence Within twenty-\nfour hours of infection . . . . and the &quot;infection&quot; of the theory (dl)--a disease is an illness\ncaused by an infection. This equality i = i' cannot be proven, but it may be reasonably\nassumed--we know that in this case the infection i' caused the illness, which, in turn,\ncaused the death.\nThe necessity of this kind of merging of arguments has been recognized before:\nCharniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979)\nrefers to such operations using the terms knitting or petty conversational implicature.\nNeither Hobbs nor Charniak and McDermott ried then to make this notion precise,\nbut the paper by Hobbs et al (1988) moves in that direction. The purpose of this\nsubsection is to formalize and explain how assumptions like that one above can be\nmade.\nDefinition\nA formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there\nexists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r\n&quot;weak&quot; because it is enough to find one partial theory proving a given formula.)\nAs an example, in the case of the three-sentence paragraph, we have a partial\ntheory T1 based on (slb) saying that &quot; 'it' hits rapidly,&quot; and T2 saying that &quot;an illness\n('it') harms rapidly&quot; (s2_ex). Thus both statements are weakly provable.\n"},{"#tail":"\n","@confidence":"0.984239020833333","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nSince we view the metalevel constraints M rather as rules for choosing models than\nas special inference rules, the definition of the R+M-abduction is model-theoretic, not\nproof-theoretic:\nDefinition\nA preferred model of a theory T is an element of Mods(T) that satisfies metalevel con-\nstraints contained in M. The set of all preferred models of T is denoted by PM(T).\nA formula 4 of L(=), the language with equality, is weakly R + M-abductible from\nan object heory T, denoted by T ~-a+M G iff there exists a partial theory T E PT(T) and\na preferred model M E PM(T) such that M ~ G i.e. 4 is true in at least one preferred\nmodel of the partial theory T.\nNote: The notions of strong provability and strong R + M-abduction can be in-\ntroduced by replacing &quot;there exists&quot; by &quot;all&quot; in the above definitions (cf. Zadrozny\n1987b). We will have, however, no need for &quot;strong&quot; notions in this paper. Also, in a\npractical system, &quot;satisfies&quot; should be probably replaced by &quot;violates fewest.&quot;\nObviously, it is better to have references of pronouns resolved than not. After\nall, we assume that texts make sense, and that authors know these references. That\napplies to references of noun phrases too. On the other hand, there must be some\nrestrictions on possible references; we would rather assume that &quot;spinach&quot; ~ &quot;train&quot;\n(i.e. V x,y)(spinach(x) & train(y) --, x # y)), or &quot;ship&quot; # &quot;disease.&quot; Two elementary\nconditions limiting the number of equalities are: an equality N1 = N2 may be assumed\nonly if either N1 and N2 are listed as synonyms (or paraphrases) or their equality is\nexplicitly asserted by the partial theory T. Of course there are other conditions, like\n&quot;typically, the determiner 'a' introduces a new entity, while 'the' refers to an already\nintroduced constant.&quot; (But notice that in our example paragraph &quot;infection&quot; appears\nwithout an article.) All these, and other, guidelines can be articulated in the form of\nmetarules.\nWe define another partial order, this time on models Mods(T) of a partial theory\nT of a paragraph: M1 >= M2, if M1, satisfies more R + M-abductible qualities than\nM2. The principle articulating preference for having the references resolved can now\nbe expressed as\nMetarule 1\nAssume that T E PT(P) is a partial theory of a paragraph P. Every preferred model\nM E PM(T) is a maximal element of the ordering >= of Mods(T).\nTo explain the meaning of the metarule, let us analyze the paragraph (P1, P2, P3)\nand the background knowledge needed for some kind of rudimentary understanding\nof that text. The rule (i_1) (infection is a result of being infected by a disease... ), dealing\nwith the infection i, introduces a disease dl; we also know about the existence of the\ndisease d in 1347. Now, notice that there may be many models satisfying the object\ntheory of the paragraph P augmented by the background knowledge. But we can find\ntwo among them: in one, call it M1, d and dl are identical; in the other one, M2, they\nare distinct. The rule says that only the first one has a chance to be a preferred model\nof the paragraph; it has more noun phrase references resolved than the other model,\nor--formally--it satisfies more R + M-abductible qualities, and therefore M1 >= M2.\nThis reasoning, as the reader surely has noticed, resembles the example about\ninfections from the beginning of this section. The difference between the cases lies in\nthe equality d = dl being the result of a formal choice of a model, while i = i ~ wasn't\nproved, just &quot;reasonably&quot; assumed.\n"},{"#tail":"\n","@confidence":"0.985666097560976","#text":"\nComputational Linguistics Volume 17, Number 2\nIn interpreting texts, knowledge of typical subjects and typical objects of verbs\nhelps in anaphora resolution (cf. Braden-Harder & Zadrozny 1990). Thus if we know\nthat A farmer grows vegetables, either having obtained this information directly from\na text, or from R, we can reasonably assume tlhat He also grows some cotton refers to\nthe farmer, and not to a policeman mentioned in the same paragraph. Of course, this\nshould be only a defeasible assumption, if nothing indicates otherwise. We now want\nto express this strategy as a metarule:\nMetarule 2\nLet us assume that it is known that P(a, b) & Q(a) & R(b), and it is not known that\nP(a', X), for any X. Then models in which P(a, c) & R'(c) holds are preferred to models\nin which P(a',c) & R'(c) is true.\nOne can think of this rule as a model-theoretic version of Ockham's razor or\nabduction; it says &quot;minimize the number of things that have the property P(,, ,),&quot;\nand it allows us to draw certain conclusions on the basis of partial information. We\nshall see it in action in Section 5.2.\nWe have no doubts that various other metarules will be necessary; clearly, our\ntwo metarules cannot constitute the whole theory of anaphora resolution. They are\nintended as an illustration of the power of abduction, which in this framework helps\ndetermine the universe of the model (that is the set of entities that appear in it). Other\nfactors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping\n(Webber 1983) must play a role, too. Determining the relative importance of those\nfactors, the above metarules, and syntactic lues, appears to be an interesting topic in\nitself.\nNote: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric\n(with the pronoun following the element hat it refers to), not cataphoric (the other\nway around). This means that the &quot;it&quot; that brought he disease in P1 will not be con-\nsidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly\nthe right one to start out with, since anaphora is always the more typical direction of\nreference in English prose (Halliday and Hasan 1976, p. 329).\nSince techniques developed elsewhere may prove useful, at least for comparison,\nit is worth mentioning at this point that the proposed metarules are distant cousins\nof &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as-\nsumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and\ntheir kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc-\ntive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987),\n&quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob-\nviously, trying to establish precise connections for the metarules or the provability\nand the R + M-abduction would go much beyond the scope of an argument for the\ncorrespondence of paragraphs and models. These connections are being examined\nelsewhere (Zadrozny forthcoming).\n"},{"#tail":"\n","@confidence":"0.999207166666667","#text":"\nThe construction of a model of a paragraph, a p-model, must be based on the in-\nformation contained in the paragraph itself (the object theory) and in the referential\nlevel while the metalevel restricts ways that the model can be constructed, or, in other\nwords, provides criteria for choosing a most plausible model(s), if a partial theory is\nambiguous. This role of the metarules will be clearly visible in finding references of\npronouns in a simple case requiring only a rule postulating that these references be\n"},{"#tail":"\n","@confidence":"0.976022361702128","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nsearched for, and in a more complex case (in Section 5) when they can be found only\nby an interplay of background knowledge and (a formalization of) Gricean maxims.\nDefinition\nM is a p-model of a paragraph P iff there exists a coherent partial theory T E PT(P)\nsuch that M E PM(T).\nHaving defined the notion of a p-model, we can mimic now, in logic, the rea-\nsoning presented in Section 2.2. Using background information and the translation of\nsentences, we build a p-model of the paragraph. This involves determining the ref-\nerences of the pronoun &quot;it,&quot; and deciding whether &quot;struck&quot; in the sentence It struck\nrapidly means &quot;hit&quot; (slb) or &quot;harmed&quot; (s2_ex). We have then two meanings of &quot;strike&quot;\nand a number of possibilities for the pronouns.\nWe begin by constructing the two classes of preferred models given by (slb) and\n(s2_ex), respectively. It is easily seen that, in the models of the first class, based on\n{$2, slb}, (that is {rapidly:strike(yo) and strike(x) --~ hit(x)...}), together with all other\navailable information, do not let us R+M-abduct anything about y0, i.e., the referent for\nthe subject pronoun &quot;it&quot; in P2 (it struck rapidly). On the other hand, from {$2, s2_ex, dl}\nwe R + M-abduct hat y0 = d, i.e. the disease struck rapidly. That is the case because\ns2_ex implies that the agent hat &quot;struck rapidly&quot; is actually an illness. From rapidly :\nstrike(yo), strike(x) -* illness(x) & .... disease(y) --, illness(y) & .... and disease(d) we can\ninfer illness(yo) and illness(d); by the Metarule (1) we conclude that Y0 -- d. In other\nwords, the referent for the subject &quot;it&quot; is &quot;disease.&quot; Thus the Metarule (1) immediately\neliminates all the models from the first class given by (slb), in which &quot;struck&quot; means\n&quot;hit.&quot;\nNotice that we cannot prove in classical logic that the ship has brought the disease.\nBut we are allowed to assume it by the above formal rule as the most plausible state\nof affairs, or--in other words--we prove it in our three-level logic.\nWe are left then with models of the three sentences ($1, $2, $3) that contain {$2,\ns2_ex, dl}; they all satisfy y0 = d. We now use {Sl,shl,bl}(enter(s,m) & ship(s) &\nbring(xo~ d) & ...; ship(s) --~ (3y)carry(s,y) & ...; Vz\\[bring(xo, z) --* carry(xo, z)\\]). From\nthese facts we can conclude by Metarule (1) that x0 = s: a &quot;ship&quot; is an agent hat carries\ngoods; to &quot;bring&quot; means to &quot;carry&quot;; and the disease has been brought by something--\nwe obtain carry(xo, d) and carry(s, y); and then by Metarule (2), carry(s, d). That is, the\nreferent for the pronoun &quot;it&quot; in P1 (... bringing with it the disease... ) should be &quot;ship.&quot;\nObserve that we do not assert about the disease that it is a kind of goods or people;\nthe line of reasoning oes as follows: since ships are known to carry people or goods,\nand ports are not known to carry anything, we may assume that the ship carried the\ndisease along with its standard cargo.\nHaving resolved all pronoun references, with no ambiguity left, we conclude that\nthe class PM(P) consists of only one model, based on the the partial theory\n{$1, $2, $3, shl, bl, e_l, s2_ex, dl, de1, il, al, ctl}.\nThe model describes a situation in which the ship came into the port/harbor; the\nship brought he disease; the disease was caused by an infection; the disease harmed\nrapidly, causing a painful death; and so on.\nThe topic Tp of (P1, P2, P3) is the disease(x). The first sentence talks about it; the\nsecond one refers to it using the pronoun &quot;it,&quot; and the third one extends our knowl-\nedge about the topic, since &quot;disease' is linked to &quot;infection&quot; through dl. Furthermore,\n"},{"#tail":"\n","@confidence":"0.9299561","#text":"\nBut notice that our definition of topic licences also other analyses, for example, one\nin which all the predicates of the first sentence constitute the topic of the paragraph, $2\nelaborates $1 (in the sense of condition I (c) of the definition of topic), and $3 elaborates\n$2. Based on the larger context, we prefer the first analysis; however, a computational\ncriterion for such a preference remains as an open problem.\n6. On the Ro le of the Meta leve l\nWe have already seen examples of the application of metalevel rules. In the analysis\nof the paragraph, we applied one such rule expressing our commonsense knowledge\nabout the usage of pronouns. In this section we discuss two other sources of met-\nalevel axioms: Gricean cooperative principles, which reduce the number of possible\n"},{"#tail":"\n","@confidence":"0.997175133333333","#text":"\nZadrozny and Jensen Semantics of Paragraphs\ninterpretations of a text or an utterance; and connectives and modalities--such as\n&quot;but, .... unless,&quot; or &quot;maybe&quot;---which refer to the process of constructing the models\nor partial theories, and to some operations on them (see Figure 4).\nWe can see then two applications of metarules: in constructing models of a text\nfrom representations of sentences, and in reducing, or constraining, the ambiguity\nof the obtained structure. We begin by showing how to formalize the latter. In the\nnext subsection (6.1), assuming the Gricean maxims to be constraints on language\ncommunication, either spoken or written, we use their formal versions in building\npartial theories. A specific instance of the rule of &quot;quantity&quot; turns out to be applicable\nto anaphora resolution. That example will end our discussion of anaphora in this\narticle.\nThe last topic we intend to tackle is the semantic role of conjunctions. In subsection\n6.2 we present ametalevel axiom dealing with the semantic role of the adversative con-\njunction &quot;but;&quot; then we talk about some of its consequences for constructing models\nof text. This will complete our investigation of the most important issues concerning\nparagraph structure: coherence (how one can determine that a paragraph expresses\na &quot;thought'0, anaphora (how one can compute &quot;links&quot; between entities that a para-\ngraph talks about), and cohesion (what makes a paragraph more than just a sum of\nsentences). Of course, we will not have final answers to any of these problems, but\nwe do believe that the/a direction of search for computational models of text will be\nvisible at that point.\nWe assume aflat structure of the metalevel, envisioning it as a collection of (closed)\nformulas written in the language of set theory or higher order logic. In either of the two\ntheories it is possible to define the notions of a model, satisfiability, provability, etc. for\nany first order language (cf. e.g. Shoenfield 1967); therefore the metalevel formulas can\nsay how partial theories hould be constructed (specifying for instance the meaning of\n0) and what kinds of models are admissible. The metarules thus form a logical theory\nin a special anguage, such as the language of ZF-set theory. However, for the sake of\nreadability, we express all of them in English.\n"},{"#tail":"\n","@confidence":"0.993666722222222","#text":"\nA Gricean Cooperative Principle applies to text, too. For instance, in normal writing\npeople do not express common knowledge about typical functions of objects. In fact,\nas the reader may check for himself, there is nothing in Gricean maxims that does not\napply to written language. That the maxims play a semantic role is hardly surprising.\nBut that they can be axiomatized and used in building formal models of texts is new.\nWe present in the next couple of paragraphs our formalization of the first maxim, and\nsketch axiomatizations of the others. Then we will apply the formal rule in an example.\nGricean maxims, after formalization, belong to the rnetalevel. This can be seen from\nour formalization of the rule &quot;don't say too much.&quot; To this end we define redundancy\nof a partial theory T of a paragraph as the situation in which some sentences can be\nlogically derived from other sentences and from the theory T in a direct manner:\n(3S E/5)(3cr E R)\\[a E T& e : ~b --~ ~b &/St- ~ & {~} U (/5- {S}) F- S\\]\nThe meaning of this formula can be explained as follows: a paragraph P has been\ntranslated into its formal version/5 and is to be examined for redundancy. Its partial\ntheory PT(/5) has also been computed. The test will turn positive if, for some sentence\nS, we can find a rule/theorem a = ~ --* ~ in PT(P) such that the sentence S is implied\n(in a classical ogic) by the other sentences and ~. For example, if the paragraph\nabout Black Death were to contain also the sentence The ship carried people or goods, or\n"},{"#tail":"\n","@confidence":"0.923395333333333","#text":"\nThe Gricean maxims\nboth, which (in its logical form) belongs to R, it would be redundant: ~ = (shl), there.\nSimilarly, the definition takes care of the redundancy resulting from a simple repetition.\n"},{"#tail":"\n","@confidence":"0.989747777777778","#text":"\n(nonredundancy) If T1, T2 C PT(P) and T1 is less redundant han T2, then the theory\nT1 is preferred to T2. (Where &quot;less redundant&quot; means that the number of redundant\nsentences in T1 is smaller than in T2)\nThe relevant half of the Maxim of Quantity has been expressed by Gla. How\nwould we express the other maxims? The &quot;too little&quot; part of the first maxim might\nbe represented as a preference for unambiguous partial theories. The second maxim\nhas been assumed all the t ime--when constructing partial theories or models, the\nsentences of a paragraph are assumed to be true. The Maxim of Manner seems to us\nto be more relevant for critiquing the style of a written passage or for natural anguage\ngeneration; in the case of text generation, it can be construed as a requirement that\nthe produced text be coherent and cohesive.\nWe do not claim that Gla is the best or unique way of expressing the rule &quot;assume\nthat the writer did not say too much.&quot; Rather, we stress the possibility that one can\naxiomatize and productively use such a rule. We shall see this in the next example:\ntwo sentences, regarded as a fragment of paragraph, are a variation on a theme by\nHobbs (1979).\nExample 3\nThe captain is worried because the third officer can open his safe. He knows the combination.\nThe above metarule postulating &quot;nonredundancy&quot; implies that &quot;he&quot; = &quot;the third\nofficer, .... his&quot; = &quot;the captain's&quot; are the referents of the pronouns. This is because the\nformula\nsafe(x) --, (owns(y~ x) & cmbntn(z~ x) --, knows(y~ z) & can_open(y~ x)) E Tsafe,\nbelongs to R, since it is common knowledge about safes that they have owners, and\nalso combinations that are known to the owners. Therefore &quot;his&quot; = &quot;the third officer's&quot;\nwould produce a redundant formula, corresponding to the sentence The third officer\ncan open the third officer's safe. By the same token, The captain knows the combination\nwould be redundant too.\n"},{"#tail":"\n","@confidence":"0.944717552631579","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nWe now explain the details of this reasoning. One first proves that &quot;his&quot; = &quot;the\ncaptain's.&quot; Indeed, if &quot;his&quot; = &quot;the third officer's,&quot; then our example sentence would\nmean\n? The captain is worried because the third officer can open the third officer's safe;\nin logic:\ncaptain(x) & worry(x,s) & sentence(s)\n& s = 'the third officer can open the third officer's afe.&quot;\nWe assume also, based on common knowledge about worrying, that worry(x', s') -~\nS. That is, one worries about things that might possibly be or become true (S denotes\nthe logical formula corresponding to the sentence s, cf. Section 3); but one doesn't\nworry about things that are accepted as (almost) always true (such as the law of\ngravity), so that worry(x', 's') ~ -~(S E Tf), where f ranges over subformulas of S.\nIn our case, S immediately follows from Tsafe and X, where X = safe(sf) & third_offi-\ncer(o) & owns(o, sf)--the fact that &quot;the third officer can open the third officer's afe&quot; is\na consequence of general knowledge about the ownership of safes. And therefore the\ninterpretation with &quot;his&quot; = &quot;the captain's&quot; is preferred as less redundant by the rule\nGla. This theory contains representations of the two sentences, the theory of safes, a\ntheory of worrying, and the equality &quot;his&quot; = &quot;captain's.&quot;\nIt remains to prove that &quot;he&quot; = &quot;the third officer.&quot; Otherwise we have\nP1. The captain is worried because the third officer can open the captain's afe.\nP2. ? The captain knows the combination.\nClearly, the last sentence is true but redundant--the theory of &quot;safe&quot; and P1 entail P2:\n{P1} U Tsafe t- P2\nWe are left with the combination\nQ1. The captain is worried because the third officer can open the captain's afe.\nQ2. ? The third officer knows the combination.\nIn this case, Q2 does not follow from {Q1} u Tsafe and therefore Q1, Q2 is preferred to\nP1, P2 (by Gla). We obtain then\nThe captain is worried because the third officer can open the captain's afe.\nThe third officer knows the combination\nas the most plausible interpretation f our example sentences.\nNote: The reader must have noticed that we did not bother to distinguish the\nsentences P1, P2, Q1 and Q2 from their logical forms. Representing &quot;because&quot; and\n&quot;know&quot; adequately should be considered a separate topic; representing the rest (in\nthe first order convention of this paper) is trivial.\n6.1.1 Was the Use of a Gricean Maxim Necessary? Can one deal effectively with the\nproblem of reference without axiomatized Gricean maxims, for instance by using only\n"},{"#tail":"\n","@confidence":"0.994227","#text":"\nComputational Linguistics Volume 17, Number 2\n&quot;petty conversational implicature&quot; (Hobbs 1979), or the metarules of Section 5.2? It\nseems to us that the answer is no.\nAs a case in point, consider the process of finding the antecedent of the anaphor\n&quot;he&quot; in the sentences\nJohn can open Bill's safe. He knows the combination.\nHobbs (1979, 1982) proves &quot;he&quot; = &quot;John&quot; by assuming the relation of &quot;elaboration&quot;\nbetween the sentences. (Elaboration is a relation between two segments of a text. It\nintuitively means &quot;expressing the same thought from a different perspective,&quot; but has\nbeen defined formally as the existence of a proposition implied by both segments--\nhere the proposition is &quot;John can open the safe&quot;.) However, if we change the pair to\nthe triple\nBill has a safe under the painting of his yacht. John can open Bill's safe. He knows\nthe combination\nthe relation of elaboration holds between the segment consisting of the first two sen-\ntences of the triple and each of the two possible readings: John knows the combination\nand Bill knows the combination. In this case, elaboration cannot choose the correct ref-\nerent, but the rule Gla can and does. Clearly, an elaboration should not degenerate\ninto redundancy; the Gricean maxims are to keep it fresh.\nAs we have observed, correct interpretations cannot be chosen by an interaction of\nan object level theory and a referential level alone, because coherence, plausibility and\nconsistency are too weak to weed out wrong partial theories. Metarules are necessary.\nTrue, the captain knew the combination, but it was consistent that &quot;his&quot; might have\nreferred to &quot;the third officer's.&quot;\n"},{"#tail":"\n","@confidence":"0.992175","#text":"\nAny analysis of natural anguage text, to be useful for a computational system, will\nhave to deal with coherence, anaphora, and connectives. We have examined so far\nthe first two concepts; we shall present now our view of connectives to complete the\nargument about paragraphs being counterparts of models. We present ametalevel rule\nthat governs the behavior of the conjunction &quot;but;&quot; we formalize the manner in which\n&quot;but&quot; carries out the contradiction. Then we derive from it two rules that prevent\ninfelicitous uses of &quot;but.&quot;\nConnectives are function words--like conjunctions and some adverbs--that re\nresponsible simultaneously for maintaining cohesiveness within the text and for sig-\nnaling the nature of the relationships that hold between and among various text units.\n&quot;And,&quot; &quot;or,&quot; and &quot;but&quot; are the three main coordinating connectives in English. How-\never, &quot;but&quot; does not behave quite like the other two--semantically, &quot;but&quot; signals a\ncontradiction, and in this role it seems to have three subfunctions:\n.\n.\nOpposition (called &quot;adversative&quot; or &quot;contrary-to-expectation&quot; by\nHalliday and Hasan 1976; cf. also Quirk et al 1972, p. 672).\nThe ship arrived but the passengers could not get off.\nThe yacht is cheap but elegant.\nComparison. In this function, the first conjunct is not so directly\ncontradicted by the second. A contradiction exists, but we may have to\n"},{"#tail":"\n","@confidence":"0.994021166666667","#text":"\nZadrozny and Jensen Semantics of Paragraphs\ngo through additional levels of implication to find it. Consider the\nsentence:\n.\nThat basketball player is short, but he's very quick.\nAffirmation. This use of &quot;but&quot; always follows a negative clause, and\nactually augments the meaning of the preceding clause by adding\nsupporting information:\nThe disease not only killed thousands of people, but also ended a period\nof economic welfare.\nIn this section we consider only the first, or adversative, function of the coordinating\nconjunction &quot;but.&quot;\n"},{"#tail":"\n","@confidence":"0.984967576923077","#text":"\ndiscourse. Because it expresses some kind of contradiction, &quot;but&quot; has no role in the\npropositional calculus equivalent to the roles filled by &quot;and&quot; and &quot;or.&quot; Although there\nare logical formation rules using the conjunction operator (&quot;and&quot;) and the disjunction\noperator (&quot;or&quot;), there is no &quot;but&quot; operator. What, then, is the semantic role of &quot;but&quot;?\nWe believe that its function should be described at the metalevel as one of many rules\nguiding the construction of partial theories. This is expressed below.\nMetarule (BUT)\nThe formulas ? but k~, ~' but ~1 . . . . of a (formal representation f) paragraph P are\nto be interpreted as follows:\nIn the construction of any T E PT(P) instead of taking @f to be the union U\nof rr --* T~, take the union of ~ --* T?/{k~, ~', . . .}.\nThe symbol cr ---* T~,/{qt, ?d',...} denotes a maximal consistent with {or, ~, k~',...} sub-\ntheory of r~ --* T?, and in general T/T t will be a maximal consistent with T' subtheory\nof T.\n&quot;But&quot; is then an order to delete from background information everything contra-\ndicting ? , but to use what remains. Notice that &quot;and&quot; does not have this meaning; a\nmodel for ? and ? will not contain any part of a theory that contradicts either of the\nclauses ? or ?d.\nTypically this rule will be used to override defaults, to say that the expected\nconsequences of the first conjunct hold except for the fact expressed by the second\nconjunct; for instance: We were coming to see you, but it rained (so we didn't). The rule\nBUT is supposed to capture the &quot;contrary-to-expectation&quot; function of &quot;but.&quot;\nWe present now a simple example of building a model of a one-sentence paragraph\ncontaining &quot;but.&quot; We will use this example to explain how the rule BUT can be used.\nUsing background information presented below, we will construct a partial model for\nthis one-sentence paragraph.\n"},{"#tail":"\n","@confidence":"0.998795285714286","#text":"\nNote: Compare (yl) with (cl); in (yl) smallness is a property of a ship; this would\nbe more precisely expressed as yacht(x) --* \\[ship(x); property: small(x)\\]. This trick al-\nlows us not to conclude that &quot;a big ant is big,&quot; or &quot;a small elephant is small.&quot;\nWe ignore the problem of multiple meanings (theories) of predicates, and assume\nthe trivial ordering in which all formulas are equally preferred. (But note that (e_yl)\nis still preferred to (el) as a more specific theory of &quot;elegant;&quot; cf. Section 3.)\nConstruction of the model: In our case ? ___ yacht(yo) & cheap(yo) and ? =__\n"},{"#tail":"\n","@confidence":"0.9994385","#text":"\nWe can now use the Metarule (BUT) and construct the partial theories of the sentence.\nIn this case, there is only one:\n"},{"#tail":"\n","@confidence":"0.947752","#text":"\nstatus-symbol (Yo )~ poor_quality(yo ) }\nIn other words, the yacht in question is a poor quality small and elegant ship serving\nas an inexpensive status symbol.\nThe partial model of the theory T is quite trivial: it consists of one entity repre-\nsenting the yacht and of a bunch of its attributes. However, the size of the model is\nnot important here; what counts is the method of derivation of the partial theory.\n6.2.2 Confirming the Analysis. The Metarule (BUT) is supposed to capture the &quot;con-\ntrary-to-expectation&quot; function of &quot;but.&quot; The next two rules follow from our formaliza-\n"},{"#tail":"\n","@confidence":"0.999158625","#text":"\nZadrozny and Jensen Semantics of Paragraphs\ntion; their validity indirectly supports the plausibility of our analysis of &quot;but.&quot;\nBUT_C1 : ff but -~ is incorrect, if ? --* ? is a &quot;law.&quot;\ne.g. Henry was murdered but not killed.\nOur referential level is a collection of partially ordered theories; we have expressed\nthe fact that a theory of some ? is a &quot;law&quot; is by deleting the empty interpretation f ?\nfrom the partial order. If we accept he definition of a concept as given by necessary\nand sufficient conditions, the theories would all appear as laws. If we subscribe to a\nmore realistic view where definitions are given by a collection of central/prototypical\nand peripheral conditions, only the peripheral ones can be contradicted by &quot;but.&quot; In\neither formalization we get BUT_C1 as a consequence: Since &quot;laws&quot; cannot be deleted,\nBUT can't be applied, and hence its use in those kinds of sentences would be incorrect.\nW. Labov (1973) discussed sentences of the form\n,This is a chair but you can sit on it.\nThe sentence is incorrect, since the function &quot;one can sit on it&quot; belongs to the core of\nthe concept &quot;chair&quot;; so--contrary to the role of &quot;but&quot;--the sentence does not contain\nany surprising new elements. Using the Metarule (BUT) and the cooperative principle\nof Grice, we get\nBUT_C2: ? but ? is incorrect, if ? -* k~ is a &quot;law.&quot;\nThe Metarule (BUT) gives the semantics of &quot;but;&quot; the rules BUT_C1 and BUT_C2 follow\nfrom it (after formalization i a sufficiently strong rnetalanguage such as type theory\nor set theory). We can link all of them to procedures for constructing and evaluating\nmodels of text. Are they sufficient? Certainly not. We have not dealt with the other\nusages of '%ut;&quot; neither have we shown how to deal with the apparent asymmetry\nof conclusions: cheap but elegant seems to imply &quot;worth buying,&quot; but elegant but cheap\ndoesn't; we have ignored possible prototypical effects in our semantics. However, we\ndo believe that other ules, dealing with &quot;but&quot; or with other connectives, can be conve-\nniently expressed in our framework. (The main idea is that one should talk explicitly\nand formally about relations between text and background knowledge, and that this\nknowledge is more than just a list of facts--it has structure, and it is ambiguous.)\nFurthermore, the semantics of &quot;but&quot; as described above is computationally tractable.\nWe also believe that one could not present a similarly natural account of the se-\nmantics of &quot;but&quot; in traditional logics, because classical logics withstand contradictions\nwith great difficulty. Contradiction, however, is precisely what &quot;but&quot; expresses. No-\ntice that certain types of coordinating conjunctions often have their counterparts in\nclassical logic: copulative (and, or, neither-nor, besides, sometimes etc.), disjunctive (like\neither-or), illative (hence, for that reason). Others, such as explanatory (namely or viz.)\nor causal (for) conjunctions can probably be expressed somehow, for better or worse,\nwithin a classical framework. Thus the class of adversative conjunctions (but, still, and\nyet, nevertheless) is in that sense unique.\n"},{"#tail":"\n","@confidence":"0.998576","#text":"\nWe hope that the reader has found this paper coherent, and its topic--the correspon-\ndence between paragraphs and models--interesting. Our strategy was to divide the\n"},{"#tail":"\n","@confidence":"0.997542142857143","#text":"\nComputational Linguistics Volume 17, Number 2\nsubject into three subtopics: a theory of anaphora, which corresponds to the logical\ntheory of equality in p-models; a theory of background knowledge, expressed as the\nlogical theory of reference in the three-level semantics; and principles of communication\nencoded in metarules. These principles include Gricean maxims and the semantics of\ncohesion, and specify a model theory for the three-level semantics. The framework re-\nsulting from putting these theories together is computational, empirical, and verifiable\n(even if incomplete); furthermore, it has strong links to already existing natural an-\nguage processing systems. In particular, the new logical level--the referential level--is\nexemplified by on-line dictionaries and other reference works, from which we extract\nbackground information about defaults and plausibility rankings.\nWe also hope that the reader would be inclined to share our belief that natural\nlanguage text can be properly and usefully analyzed by means of a three-level seman-\ntics that includes an object level, a metalevel, and a referential level. We believe that\nthe coherence of an essay, a paper, or a book can be described by an extension of our\ntheory. The work of van Dijk and Kintch (1983) on &quot;macrostructures&quot; could probably\nform the basis for such an expansion. Similarly, much of the abovementioned work\nby Hobbs, Webber, Grosz, and Sidner can be incorporated into this framework.\nOur main intention was to demonstrate hat formal notions of background knowl-\nedge can be used to\n? define coherence, make it semantically distinct from mere consistency,\nand link it formally with the notion of a topic;\n? define a class of p-models--logical models of paragraphs;\n? provide a semantics for &quot;but&quot; (which exemplifies our understanding of\ngrammatical cohesion);\n? express the Gricean maxims formally, and use them in a computational\nmodel of communication (which seems to contradict Allen 1988, p. 464).\nMoreover, we tried to convince the reader that paragraph is an important linguistic\nunit, not only because of its pragmatic importance xemplified by coherence and links\nto background knowledge, but also because of its role in assignment of syntactic\nstructures (viz. ellipsis) and in semantics (viz. its possible role in evaluating semantic\nrepresentations).\nA great many issues have been omitted from our analysis. Thus, although we are\naware that anaphora resolution and consistency depend on previously processed text,\nthe problem of connecting a paragraph to such text has been conveniently ignored.\nNotice that this doesn't make our thesis about paragraphs being units of semantic\nprocessing any weaker, we have not claimed that paragraphs are independent. The\nquestions of how to translate from natural anguage to a logical notation needs a lot\nof attention; we have merely assumed that this can be done. Continuing this list, we\nhave accepted a very classical theory of meaning, given by Tarski: the truth is what\nis satisfied in a model. This theory should be refined, for instance by formalizing\nLakoff's (1987) concept of radial categories, and proposing mechanisms for exploiting\nit. By the same token, the concept of reference has to be broadened to include iconic\n(e.g., visual and tactile) information. And certainly it would be .nice to have a more\ndetailed theory describing the role of the metalevel. In particular, we can imagine\nthat the simple structure of a collection of set theoretic formulas can be replaced by\nsomething more interesting. We leave this as another open problem.\nWe have shown that it is possible to develop a formal system with an explicit\nrelationship between background knowledge and text, showing mechanisms that take\n"},{"#tail":"\n","@confidence":"0.99745847826087","#text":"\nZadrozny and Jensen Semantics of Paragraphs\nadvantage ofpreference, coherence, and contradiction (the reality of these phenomena\nhas never been disputed, but their semantic functions had not been investigated). We\nshould also mention that we have also begun some work on actually checking the\nempirical validity of this model (cf. e.g. Braden-Harder and Zadrozny 1990), using on-\nline dictionaries (LDOCE and Webster's) as the referential level. We know, of course,\nthat existing dictionaries are very imperfect, but (1) they can be accessed and used\nby computer programs; (2) they are getting better, as they are very systematically\ncreated with help of computers (see Sinclair 1987 for an account of how COBUILD\nwas constructed); (3) obviously, we can imagine useful new ones, like a Dictionary of\nPragmatics; (4) we believe that we can explore the new inference mechanisms even in\nsuch unrefined environments.\nAlthough the scheme we have proposed certainly needs further efinement, we\nbelieve that it is correct in two of its most important aspects: first, in the separation\nof current paragraph analysis (the object theory) from background information (the\nreferential level); and second, in asserting that the function of a paragraph is to allow\nthe building of a model of the situation described in the paragraph. This model can be\nstored, maybe modified, and subsequently used as a reference for processing following\nparagraphs.\nFinally, the paper can be viewed as an argument that the meaning of a sentence\ncannot be defined outside of a context, just as the truth value of a formula cannot be\ncomputed in a vacuum. A paragraph is the smallest example of such a context--it is\n&quot;a unit of thought.&quot;\n"},{"#tail":"\n","@confidence":"0.999165142857143","#text":"\nWe gratefully acknowledge the help\nprovided by our anonymous but diligent\nreferees, by Graeme Hirst and Susan\nMcRoy, and by our many colleagues at IBM\nResearch, all for whom helped us to avoid\nworst consequences of Murphy's Law\nduring the writing of this paper.\n"}],"#text":"\n","sectionHeader":[{"#tail":"\n","@confidence":"0.684992","@genericHeader":"abstract","#text":"\n1. Introduction\n"},{"#tail":"\n","@confidence":"0.919919","@genericHeader":"categories and subject descriptors","#text":"\n3. The Logic of Reference\n"},{"#tail":"\n","@confidence":"0.630283","@genericHeader":"general terms","#text":"\n4. Coherence of Paragraphs\n"},{"#tail":"\n","@confidence":"0.951906","@genericHeader":"conclusions","#text":"\n7. Conclusions\n"},{"#tail":"\n","@confidence":"0.966524","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.987236","@genericHeader":"references","#text":"\nReferences\n"}],"page":[{"#tail":"\n","@confidence":"0.99736","#text":"\n172\n"},{"#tail":"\n","@confidence":"0.998331","#text":"\n173\n"},{"#tail":"\n","@confidence":"0.999821","#text":"\n174\n"},{"#tail":"\n","@confidence":"0.998443","#text":"\n175\n"},{"#tail":"\n","@confidence":"0.999004","#text":"\n176\n"},{"#tail":"\n","@confidence":"0.99804","#text":"\n177\n"},{"#tail":"\n","@confidence":"0.990702","#text":"\n178\n"},{"#tail":"\n","@confidence":"0.998143","#text":"\n179\n"},{"#tail":"\n","@confidence":"0.982299","#text":"\n180\n"},{"#tail":"\n","@confidence":"0.995742","#text":"\n181\n"},{"#tail":"\n","@confidence":"0.994829","#text":"\n182\n"},{"#tail":"\n","@confidence":"0.997734","#text":"\n183\n"},{"#tail":"\n","@confidence":"0.99844","#text":"\n184\n"},{"#tail":"\n","@confidence":"0.997168","#text":"\n185\n"},{"#tail":"\n","@confidence":"0.995954","#text":"\n186\n"},{"#tail":"\n","@confidence":"0.992471","#text":"\n187\n"},{"#tail":"\n","@confidence":"0.991545","#text":"\n188\n"},{"#tail":"\n","@confidence":"0.994379","#text":"\n189\n"},{"#tail":"\n","@confidence":"0.978322","#text":"\n190\n"},{"#tail":"\n","@confidence":"0.994662","#text":"\n191\n"},{"#tail":"\n","@confidence":"0.997995","#text":"\n192\n"},{"#tail":"\n","@confidence":"0.969686","#text":"\n193\n"},{"#tail":"\n","@confidence":"0.998334","#text":"\n194\n"},{"#tail":"\n","@confidence":"0.996749","#text":"\n195\n"},{"#tail":"\n","@confidence":"0.997432","#text":"\n196\n"},{"#tail":"\n","@confidence":"0.977828","#text":"\n197\n"},{"#tail":"\n","@confidence":"0.994471","#text":"\n198\n"},{"#tail":"\n","@confidence":"0.997532","#text":"\n199\n"},{"#tail":"\n","@confidence":"0.86589","#text":"\n200\n"},{"#tail":"\n","@confidence":"0.980842","#text":"\n201\n"},{"#tail":"\n","@confidence":"0.951518","#text":"\n202\n"},{"#tail":"\n","@confidence":"0.982589","#text":"\n203\n"},{"#tail":"\n","@confidence":"0.9762","#text":"\n204\n"},{"#tail":"\n","@confidence":"0.954988","#text":"\n205\n"},{"#tail":"\n","@confidence":"0.971138","#text":"\n206\n"},{"#tail":"\n","@confidence":"0.895029","#text":"\n207\n"},{"#tail":"\n","@confidence":"0.893136","#text":"\n208\n"},{"#tail":"\n","@confidence":"0.997655","#text":"\n209\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.643331","#text":"\nFigure 2\n"},{"#tail":"\n","@confidence":"0.8767212","#text":"\nFigure 3\nThe p-model for the example paragraph\n&quot;disease&quot; is the only noun phrase mentioned or referred to in all three sentences; the\nsentence $1 is the topic sentence. The p-model of the paragraph is represented by\nFigure 3.\n"},{"#tail":"\n","@confidence":"0.776193","#text":"\nFigure 4\n"}],"table":[{"#tail":"\n","@confidence":"0.5243922","#text":"\nComputational Linguistics Volume 17, Number 2\ndisease(y) ---* {illness(y) & 3z (infection(z) & causes(z,y));...} (dl)\n/* disease illness caused by an infection */\ndeath(x) --~ {3t, y\\[x =' \\[t ime:t; event :die(y) & (ereature.(y) v plant(y))\\]'\\]} (de_l)\n/* death--an event in which a creature or a plant dies */\n"},{"#tail":"\n","@confidence":"0.6790215","#text":"\nComputational Linguistics Volume 17, Number 2\nExample 4\nThis yacht is cheap, but it is elegant.\nReferential level (a fragment)\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.681225","#tail":"\n","@no":"0","#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.982721","#text":"IBM T. J. Watson Research Center"},"author":{"#tail":"\n","@confidence":"0.999758","#text":"Karen Jensew"},"abstract":{"#tail":"\n","@confidence":"0.996068181818182","#text":"We present a computational theory of the paragraph. Within it we formally define coherence, give semantics to the adversative conjunction &quot;but&quot; and to the Gricean maxim of quantity, and present some new methods for anaphora resolution. The theory precisely characterizes the relationship between the content of the paragraph and background knowledge needed for its understanding. This is achieved by introducing a new type of logical theory consisting of an object level, corresponding to the content of the paragraph, a referential level, which is a new logical level encoding background knowledge, and a metalevel containing constraints on models of discourse (e.g. a formal version of Gricean maxims). We propose also specific mechanisms of interaction between these levels, resembling both classical provability and abduction. Paragraphs are then represented by a class of structures called p-models."},"title":{"#tail":"\n","@confidence":"0.8605505","#text":"Semantics of Paragraphs Wlodek Zadrozny ?"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Allen, J. (1987). Natural Language Understanding. Benjamin/Cummings. Menlo Park, California."},"#text":"\n","marker":{"#tail":"\n","#text":"Allen, 1987"},"location":{"#tail":"\n","#text":"Benjamin/Cummings. Menlo Park, California."},"title":{"#tail":"\n","#text":"Natural Language Understanding."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Allen"}}},{"date":{"#tail":"\n","#text":"1986"},"editor":{"#tail":"\n","#text":"In Michalski, R.S. et al, eds."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"liday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential level while the metalevel restricts ways that the model can be constructed, or, in other words, provides criteria for choosing a most","@endWordPosition":"13793","@position":"83691","annotationId":"T1","@startWordPosition":"13792","@citStr":"Berwick (1986)"}},"title":{"#tail":"\n","#text":"Learning from Positive-Only Examples: The Subset Principle and Three Case Studies.&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Berwick, R.C. (1986). &quot;Learning from Positive-Only Examples: The Subset Principle and Three Case Studies.&quot; In Michalski, R.S. et al, eds. Machine Learning Vol. II. Morgan Kaufmann. Los Altos, California: 625-645."},"#text":"\n","pages":{"#tail":"\n","#text":"625--645"},"marker":{"#tail":"\n","#text":"Berwick, 1986"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann."},"location":{"#tail":"\n","#text":"Los Altos, California:"},"booktitle":{"#tail":"\n","#text":"Machine Learning Vol. II."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R C Berwick"}}},{"volume":{"#tail":"\n","#text":"18"},"#tail":"\n","date":{"#tail":"\n","#text":"1979"},"rawString":{"#tail":"\n","#text":"Black, J.B., and Bower, G.H. (1979). &quot;Episodes as Chunks in Narrative Memory.&quot; Journal of Verbal Learning and Verbal Behavior 18: 309-318."},"journal":{"#tail":"\n","#text":"Journal of Verbal Learning and Verbal Behavior"},"#text":"\n","pages":{"#tail":"\n","#text":"309--318"},"marker":{"#tail":"\n","#text":"Black, Bower, 1979"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"mple of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count i formation. Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed escriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and r","@endWordPosition":"1574","@position":"10027","annotationId":"T2","@startWordPosition":"1571","@citStr":"Black and Bower (1979)"}},"title":{"#tail":"\n","#text":"Episodes as Chunks in Narrative Memory.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J B Black"},{"#tail":"\n","#text":"G H Bower"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"manuscript."},"date":{"#tail":"\n","#text":"1983"},"institution":{"#tail":"\n","#text":"Dept. of Psychology, Carnegie-Mellon University."},"rawString":{"#tail":"\n","#text":"Bond, S.J., and Hayes, J.R. (1983). Cues People Use to Paragraph Text. manuscript. Dept. of Psychology, Carnegie-Mellon University."},"#text":"\n","marker":{"#tail":"\n","#text":"Bond, Hayes, 1983"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ur different linguis- tic approaches to paragraphs: prescriptivist, psycholinguist, extualist, and discourse- oriented. The prescriptivist approach is typified in standard English grammar textbooks, such as Warriner (1963). In these sources, a paragraph is notionally defined as some- thing like a series of sentences that develop one single topic, and rules are laid down for the construction of an ideal (or at least an acceptable) paragraph. Although these dictates are fairly clear, the underlying notion of topic is not. An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count i formation. Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al (1980). The textualist approach to paragraph ","@endWordPosition":"1492","@position":"9494","annotationId":"T3","@startWordPosition":"1489","@citStr":"Bond and Hayes (1983)"},{"#tail":"\n","#text":"ions we talk about would produce an in- consistent theory; hence, the temporal, causal, and other aspects would be dealt with by consistency. But of course at this point it is just a hypothesis. An important aspect of the definition is that coherence has been defined as a prop- erty of representation--in our case, it is a property of a formal theory. The existence of the topic, the direct or indirect allusion to it, and anaphora (which will be addressed below) take up the issue of formal criteria for a paragraph definition, which was raised 189 Computational Linguistics Volume 17, Number 2 by Bond and Hayes (1983) (cf. also Section 2.1). The question of paragraph length can probably be attended to by limiting the size of p-models, perhaps after introducing some kind of metric on logical data structures. Still, our definition of coherence may not be restrictive nough: two collections of sentences, one referring to &quot;black&quot; (about black pencils, black pullovers, and black poodles), the other one about &quot;death&quot; (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about he new, broader topic &quot;black + death.&quot; This problem may be similar to the situatio","@endWordPosition":"10120","@position":"61459","annotationId":"T4","@startWordPosition":"10117","@citStr":"Bond and Hayes (1983)"}]},"title":{"#tail":"\n","#text":"Cues People Use to Paragraph Text."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S J Bond"},{"#tail":"\n","#text":"J R Hayes"}]}},{"date":{"#tail":"\n","#text":"1985"},"editor":{"#tail":"\n","#text":"In Brachman, R.J., and Levesque, H.J., eds."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" if all background knowledge were described, as in our examples, by sets of first order theories, because of the preferences and inconsistencies of meanings, we could not treat R as a flat database of facts--such a model simply would not be realistic. Rather, R must be treated as a separate logical level for these syntactic reasons, and because of its function--being a pool of possibly conflicting semantic onstraints. The last point may be seen better if we look at some differences between our system and KRYPTON, which also distinguishes between an object heory and back- ground knowledge (cf. Brachman et al 1985). KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains informa- tion expressed in a frame-based language quivalent to a fragment of FOL. However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior. From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard. In our system, we also distinguish between the &quot;definitional&quot; and factual information, but the &quot;definitional&quot; part contains collections of m","@endWordPosition":"8570","@position":"52328","annotationId":"T5","@startWordPosition":"8567","@citStr":"Brachman et al 1985"}},"title":{"#tail":"\n","#text":"Krypton: A Functional Approach to Knowledge Representation.&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Brachman, R.J., Fikes, R.E., and Levesque, H.J. (1985). &quot;Krypton: A Functional Approach to Knowledge Representation.&quot; In Brachman, R.J., and Levesque, H.J., eds. Readings in Knowledge Representation. Morgan Kaufmann. Los Altos, California: 411-430."},"#text":"\n","pages":{"#tail":"\n","#text":"411--430"},"marker":{"#tail":"\n","#text":"Brachman, Fikes, Levesque, 1985"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann."},"location":{"#tail":"\n","#text":"Los Altos, California:"},"booktitle":{"#tail":"\n","#text":"Readings in Knowledge Representation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R J Brachman"},{"#tail":"\n","#text":"R E Fikes"},{"#tail":"\n","#text":"H J Levesque"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Braden-Harder, L. and Zadrozny, W. (1989). Lexicons for Broad Coverage Semantics. IBM Research Division Report, RC 15568."},"journal":{"#tail":"\n","#text":"IBM Research Division Report, RC"},"#text":"\n","pages":{"#tail":"\n","#text":"15568"},"marker":{"#tail":"\n","#text":"Braden-Harder, Zadrozny, 1989"},"title":{"#tail":"\n","#text":"Lexicons for Broad Coverage Semantics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L Braden-Harder"},{"#tail":"\n","#text":"W Zadrozny"}]}},{"date":{"#tail":"\n","#text":"1979"},"editor":{"#tail":"\n","#text":"In Givon, T., ed.,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists how little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and o","@endWordPosition":"1809","@position":"11663","annotationId":"T6","@startWordPosition":"1808","@citStr":"Chafe 1979"}},"title":{"#tail":"\n","#text":"The Flow of Thought and the Flow of Language.&quot;"},"volume":{"#tail":"\n","#text":"12"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Chafe, W.L. (1979). &quot;The Flow of Thought and the Flow of Language.&quot; In Givon, T., ed., Syntax and Semantics, Vol. 12. Academic Press. New York, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Chafe, 1979"},"publisher":{"#tail":"\n","#text":"Academic Press."},"location":{"#tail":"\n","#text":"New York, New York."},"booktitle":{"#tail":"\n","#text":"Syntax and Semantics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W L Chafe"}}},{"date":{"#tail":"\n","#text":"1983"},"issue":{"#tail":"\n","#text":"3"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"efinitional&quot; and factual information, but the &quot;definitional&quot; part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, &quot;coherence&quot; and &quot;dominance,&quot; which are not variants of the standard first order entailment, but abduction. The idea of using preferences among theories is new, hence it was described in more detail. &quot;Coherence,&quot; as outlined above, can be understood as a declarative (or static) version of marker passing (Hirst 1987; Charniak 1983), with one difference: the activation spreads to theories that share a predicate, not through the IS-A hierarchy, and is limited to elementary facts about predicates appearing in the text. The metalevel rules we are going to discuss in Section 6, and that deal with the Gricean maxims and the meaning of &quot;but,&quot; can be easily expressed in the languages of set theory or higher order logic, but not everything expressible in those languages makes sense in natural language. Hence, putting limitations on the expressive power of the language of the metalevel will remain as one of many open problems. 4.","@endWordPosition":"8743","@position":"53451","annotationId":"T7","@startWordPosition":"8742","@citStr":"Charniak 1983"}},"title":{"#tail":"\n","#text":"Passing Markers: A Theory of Contextual Influence in Language Comprehension.&quot;"},"volume":{"#tail":"\n","#text":"7"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Charniak, E. (1983). &quot;Passing Markers: A Theory of Contextual Influence in Language Comprehension.&quot; Cognitive Science, 7(3): 171-190."},"journal":{"#tail":"\n","#text":"Cognitive Science,"},"#text":"\n","pages":{"#tail":"\n","#text":"171--190"},"marker":{"#tail":"\n","#text":"Charniak, 1983"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"rawString":{"#tail":"\n","#text":"Charniak, E., and McDermott, D. (1985). Introduction to Artificial Intelligence. Addison-Wesley. Reading, Massachusetts."},"#text":"\n","marker":{"#tail":"\n","#text":"Charniak, McDermott, 1985"},"publisher":{"#tail":"\n","#text":"Addison-Wesley."},"location":{"#tail":"\n","#text":"Reading, Massachusetts."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"on. This relation would hold, for instance, between the object heory of our example paragraph and a formula expressing the equality of two constants, i and i', denoting (respectively) the &quot;infection&quot; in the sentence Within twenty- four hours of infection . . . . and the &quot;infection&quot; of the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because","@endWordPosition":"12555","@position":"76467","annotationId":"T8","@startWordPosition":"12552","@citStr":"Charniak and McDermott (1985)"}},"title":{"#tail":"\n","#text":"Introduction to Artificial Intelligence."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E Charniak"},{"#tail":"\n","#text":"D McDermott"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1979"},"rawString":{"#tail":"\n","#text":"Crothers, E.J. (1979). Paragraph Structure Inference. Ablex Publishing Corp., Norwood, New Jersey."},"#text":"\n","marker":{"#tail":"\n","#text":"Crothers, 1979"},"publisher":{"#tail":"\n","#text":"Ablex Publishing Corp.,"},"location":{"#tail":"\n","#text":"Norwood, New Jersey."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"jectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count i formation. Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed escriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called &quot;episodes,&quot; and sometimes &quot;paragraphs.&quot; Accord- ing to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragr","@endWordPosition":"1627","@position":"10398","annotationId":"T9","@startWordPosition":"1626","@citStr":"Crothers (1979"},{"#tail":"\n","#text":" to some hand-coded body of predicate assertions, for making these rela- tionships. This demonstrates that information eeded to identify and resolve anaphoric ref- erences can be found, to an interesting extent, in standard ictionaries and thesauri. (Other reference works could be treated as additional sources of world knowledge.) This type of consultation uses existing natural language texts as a referential evel for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. Crothers (1979, p. 112), for example, bemoans the fact that his &quot;theory lacks a world knowledge component, a mental 'encyclopedia,' which could be invoked to gen- erate inferences... &quot; With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation. In other words, we recognize it as a separate logical evel--the referential level. Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill","@endWordPosition":"3155","@position":"20046","annotationId":"T10","@startWordPosition":"3154","@citStr":"Crothers (1979"},{"#tail":"\n","#text":"real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information ecessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect hat in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions. Let P be a paragraph, let /3 = ($1,..., Sn) be its translation into a sequence of logical formulas. The set of all predicates appearing in X will be denoted by Pred(X). Definition Let T be a partial theory of a paragraph P. A sequence of predicates appearing in i6, denoted by Tp, is called a to","@endWordPosition":"9218","@position":"56361","annotationId":"T11","@startWordPosition":"9217","@citStr":"Crothers (1979)"}]},"title":{"#tail":"\n","#text":"Paragraph Structure Inference."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E J Crothers"}}},{"volume":{"#tail":"\n","#text":"3"},"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Etherington, D.W., and Mercer, R.E. (1987). &quot;Domain Circumscription: A Reevaluation.&quot; Computational Intelligence 3: 94--99."},"journal":{"#tail":"\n","#text":"Computational Intelligence"},"#text":"\n","pages":{"#tail":"\n","#text":"94--99"},"marker":{"#tail":"\n","#text":"Etherington, Mercer, 1987"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a parag","@endWordPosition":"13753","@position":"83429","annotationId":"T12","@startWordPosition":"13750","@citStr":"Etherington and Mercer 1987"}},"title":{"#tail":"\n","#text":"Domain Circumscription: A Reevaluation.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D W Etherington"},{"#tail":"\n","#text":"R E Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Frisch, A. (1987). &quot;Inference without Chaining.&quot; Proc. IJCAI-87. Milan, Italy: 515-519."},"#text":"\n","pages":{"#tail":"\n","#text":"515--519"},"marker":{"#tail":"\n","#text":"Frisch, 1987"},"location":{"#tail":"\n","#text":"Milan, Italy:"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"efined on a certain domain, which satisfies all formulas of T. The collection of all (finite) models of a theory T will be denoted by Mods(T). ? The set of all subformulas of a collection of formulas F is denoted by Form(F). ~ is a ground instance of a formula G if ~ contains no variables, and ~ = ~, for some substitution 0. Thus, we do not require Th(T) to be closed under substitution i stances of tautologies. Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect o weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985). But we won't pursue this topic further here. 3.2 The Structure of Background Knowledge Background knowledge is not a simple list of meaning postulates--it has a structure and it may contain contradictions and ambiguities. These actualities have to be taken into account in any realistic model of natural language understanding. For instance, the verb &quot;enter&quot; is polysemous. But, unless context specifies otherwise, &quot;to come in&quot; is a more plausible meaning than &quot;to join a group.&quot; Assuming some logical representation of this knowledge, we can write that enter(x, y) --* {come","@endWordPosition":"4903","@position":"30579","annotationId":"T13","@startWordPosition":"4902","@citStr":"Frisch 1987"}},"title":{"#tail":"\n","#text":"Inference without Chaining.&quot;"},"booktitle":{"#tail":"\n","#text":"Proc. IJCAI-87."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Frisch"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1965"},"rawString":{"#tail":"\n","#text":"Fowler, H.W. (1965). A Dictionary of Modern English Usage. Oxford University Press. New York, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Fowler, 1965"},"publisher":{"#tail":"\n","#text":"Oxford University Press."},"location":{"#tail":"\n","#text":"New York, New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rictive nough: two collections of sentences, one referring to &quot;black&quot; (about black pencils, black pullovers, and black poodles), the other one about &quot;death&quot; (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about he new, broader topic &quot;black + death.&quot; This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words (e.g., &quot;colorless green ideas... '9, while before the advent of Chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words; Fowler (1965, p. 546) gives 10 definitions of a sentence. It then seems worth differentiating between the creation of a new concept like &quot;black + death,&quot; with a meaning given by a paraphrase of the example collection of sentences, and the acceptance of the new concept--storing it in R. In our case the concept &quot;black + death,&quot; which does not refer to any normal experiences, would be discarded as useless, although the collection of sentences would be recognized as a strange, even if coherent, paragraph. We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive xamples.","@endWordPosition":"10253","@position":"62315","annotationId":"T14","@startWordPosition":"10252","@citStr":"Fowler (1965"}},"title":{"#tail":"\n","#text":"A Dictionary of Modern English Usage."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"H W Fowler"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Genesereth, M.R., and Nilsson, N.J. (1987). Logical Foundations of Artificial Intelligence. Morgan Kaufmann. Los Altos, California."},"#text":"\n","marker":{"#tail":"\n","#text":"Genesereth, Nilsson, 1987"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann."},"location":{"#tail":"\n","#text":"Los Altos, California."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"th the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are b","@endWordPosition":"13741","@position":"83330","annotationId":"T15","@startWordPosition":"13738","@citStr":"Genesereth and Nilsson 1987"}},"booktitle":{"#tail":"\n","#text":"Logical Foundations of Artificial Intelligence."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M R Genesereth"},{"#tail":"\n","#text":"N J Nilsson"}]}},{"date":{"#tail":"\n","#text":"1978"},"editor":{"#tail":"\n","#text":"In Cole, P., ed."},"title":{"#tail":"\n","#text":"and Conversation.&quot;"},"volume":{"#tail":"\n","#text":"2"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Grice, H.P. (1978). &quot;Further Notes on Logic Computational Linguistics Volume 17, Number 2 and Conversation.&quot; In Cole, P., ed. Syntax and Semantics 9:Pragmatics. Academic Press. New York, New York: 113-128."},"#text":"\n","pages":{"#tail":"\n","#text":"113--128"},"marker":{"#tail":"\n","#text":"Grice, 1978"},"publisher":{"#tail":"\n","#text":"Academic Press."},"location":{"#tail":"\n","#text":"New York, New York:"},"booktitle":{"#tail":"\n","#text":"Further Notes on Logic Computational Linguistics Volume 17, Number"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"H P Grice"}}},{"date":{"#tail":"\n","#text":"1975"},"editor":{"#tail":"\n","#text":"In Cole, P., and Morgan, J.L., eds.,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"t obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information ecessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct o","@endWordPosition":"9123","@position":"55807","annotationId":"T16","@startWordPosition":"9122","@citStr":"Grice (1975"}},"title":{"#tail":"\n","#text":"Logic and Conversation.&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Grice, H.P. (1975). &quot;Logic and Conversation.&quot; In Cole, P., and Morgan, J.L., eds., Syntax and Semantics 3: Speech Acts. Academic Press. New York New York: 41-58."},"#text":"\n","pages":{"#tail":"\n","#text":"41--58"},"marker":{"#tail":"\n","#text":"Grice, 1975"},"publisher":{"#tail":"\n","#text":"Academic Press."},"location":{"#tail":"\n","#text":"New York New York:"},"booktitle":{"#tail":"\n","#text":"Syntax and Semantics 3: Speech Acts."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"H P Grice"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1981"},"rawString":{"#tail":"\n","#text":"Groesser, A.C. (1981). Prose Comprehension Beyond the Word. Springer. New York, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Groesser, 1981"},"publisher":{"#tail":"\n","#text":"Springer."},"location":{"#tail":"\n","#text":"New York, New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"t iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information ecessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect hat in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions. Let P be a paragraph, let /3 = ($1,..., Sn) be its translation into a sequence of logical formulas. The set o","@endWordPosition":"9188","@position":"56175","annotationId":"T17","@startWordPosition":"9187","@citStr":"Groesser (1981)"}},"booktitle":{"#tail":"\n","#text":"Prose Comprehension Beyond the Word."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A C Groesser"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1978"},"editor":{"#tail":"\n","#text":"Walker, D.E., ed., Understanding Spoken Language."},"rawString":{"#tail":"\n","#text":"Grosz, B.J. (1978). DISCOURSE KNOWLEDGE--Section 4 of Walker, D.E., ed., Understanding Spoken Language. North-Holland. New York, New York: 229-344."},"#text":"\n","pages":{"#tail":"\n","#text":"229--344"},"marker":{"#tail":"\n","#text":"Grosz, 1978"},"publisher":{"#tail":"\n","#text":"North-Holland."},"location":{"#tail":"\n","#text":"New York, New York:"},"title":{"#tail":"\n","#text":"DISCOURSE KNOWLEDGE--Section 4 of"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"B J Grosz"}}},{"date":{"#tail":"\n","#text":"1977"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"f Ockham's razor or abduction; it says &quot;minimize the number of things that have the property P(,, ,),&quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic lues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric (with the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, s","@endWordPosition":"13586","@position":"82392","annotationId":"T18","@startWordPosition":"13585","@citStr":"Grosz 1977"}},"title":{"#tail":"\n","#text":"The Representation a d Use of Focus in a System for Understanding Dialogs.&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Grosz, B.J. (1977). &quot;The Representation a d Use of Focus in a System for Understanding Dialogs.&quot; Proc. IJCAI-77. W. Kaufmann. Los Altos, California: 67-76."},"#text":"\n","pages":{"#tail":"\n","#text":"67--76"},"marker":{"#tail":"\n","#text":"Grosz, 1977"},"publisher":{"#tail":"\n","#text":"W. Kaufmann."},"location":{"#tail":"\n","#text":"Los Altos, California:"},"booktitle":{"#tail":"\n","#text":"Proc. IJCAI-77."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"B J Grosz"}}},{"volume":{"#tail":"\n","#text":"19"},"#tail":"\n","date":{"#tail":"\n","#text":"1980"},"rawString":{"#tail":"\n","#text":"Haberlandt, K., Berian, C., and Sandson, J. (1980). &quot;The Episode Schema in Story Processing.&quot; Journal of Verbal Learning and Verbal Behavior 19: 635--650."},"journal":{"#tail":"\n","#text":"Journal of Verbal Learning and Verbal Behavior"},"#text":"\n","pages":{"#tail":"\n","#text":"635--650"},"marker":{"#tail":"\n","#text":"Haberlandt, Berian, Sandson, 1980"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"y oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count i formation. Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed escriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sen","@endWordPosition":"1579","@position":"10055","annotationId":"T19","@startWordPosition":"1576","@citStr":"Haberlandt et al (1980)"},{"#tail":"\n","#text":" and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists how little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central topic. A paragraph can be tho","@endWordPosition":"1819","@position":"11727","annotationId":"T20","@startWordPosition":"1816","@citStr":"Haberlandt et al 1980"}]},"title":{"#tail":"\n","#text":"The Episode Schema in Story Processing.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Haberlandt"},{"#tail":"\n","#text":"C Berian"},{"#tail":"\n","#text":"J Sandson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1976"},"rawString":{"#tail":"\n","#text":"Halliday, M.A.K., and Hasan, R. (1976). Cohesion in English. Longman Group Ltd. London."},"#text":"\n","marker":{"#tail":"\n","#text":"Halliday, Hasan, 1976"},"publisher":{"#tail":"\n","#text":"Longman Group Ltd."},"location":{"#tail":"\n","#text":"London."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ocedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists how little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about ","@endWordPosition":"1813","@position":"11688","annotationId":"T21","@startWordPosition":"1810","@citStr":"Halliday and Hasan 1976"},{"#tail":"\n","#text":"termining the relative importance of those factors, the above metarules, and syntactic lues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric (with the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But,","@endWordPosition":"13707","@position":"83097","annotationId":"T22","@startWordPosition":"13704","@citStr":"Halliday and Hasan 1976"},{"#tail":"\n","#text":"ules that prevent infelicitous uses of &quot;but.&quot; Connectives are function words--like conjunctions and some adverbs--that re responsible simultaneously for maintaining cohesiveness within the text and for sig- naling the nature of the relationships that hold between and among various text units. &quot;And,&quot; &quot;or,&quot; and &quot;but&quot; are the three main coordinating connectives in English. How- ever, &quot;but&quot; does not behave quite like the other two--semantically, &quot;but&quot; signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition (called &quot;adversative&quot; or &quot;contrary-to-expectation&quot; by Halliday and Hasan 1976; cf. also Quirk et al 1972, p. 672). The ship arrived but the passengers could not get off. The yacht is cheap but elegant. Comparison. In this function, the first conjunct is not so directly contradicted by the second. A contradiction exists, but we may have to 202 Zadrozny and Jensen Semantics of Paragraphs go through additional levels of implication to find it. Consider the sentence: . That basketball player is short, but he's very quick. Affirmation. This use of &quot;but&quot; always follows a negative clause, and actually augments the meaning of the preceding clause by adding supporting informati","@endWordPosition":"16782","@position":"101142","annotationId":"T23","@startWordPosition":"16779","@citStr":"Halliday and Hasan 1976"}]},"title":{"#tail":"\n","#text":"Cohesion in English."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M A K Halliday"},{"#tail":"\n","#text":"R Hasan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"rawString":{"#tail":"\n","#text":"Haugeland, J. (1985). Artificial Intelligence: The Very Idea. MIT Press. Cambridge, Massachusetts."},"#text":"\n","marker":{"#tail":"\n","#text":"Haugeland, 1985"},"publisher":{"#tail":"\n","#text":"MIT Press."},"location":{"#tail":"\n","#text":"Cambridge, Massachusetts."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"& disease(x4) & xl = s & x2 = m & x3 = s & x4 = d, where s, m, d, are constants. We adopt he three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense r asoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural anguage inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating &quot;they&quot; to &quot;apples&quot; in the sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a): We bought the boys apples because they were so cheap 176 Zadrozny and Jensen Semantics of Paragraphs can be an example of such a most plausible choice. The main ideas of the three-level semantics can be stated as follows: 1. Reasoning takes place in a three-level structure consisting of an object level, a referential evel, and a metalevel. 2. The object level is used to describe the current situation, and in our case is reserved for the formal representation f paragraph sentences. For the sake of simplicity, the object level will consist of a first order theory. 3. Th","@endWordPosition":"3472","@position":"21965","annotationId":"T24","@startWordPosition":"3471","@citStr":"Haugeland 1985"}},"title":{"#tail":"\n","#text":"Artificial Intelligence: The Very Idea."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Haugeland"}}},{"date":{"#tail":"\n","#text":"1979"},"editor":{"#tail":"\n","#text":"In Givon, T., ed.,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nomic, in that he performs detailed escriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &quot;the linguistic-logical notions of consequent and presupposition&quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called &quot;episodes,&quot; and sometimes &quot;paragraphs.&quot; Accord- ing to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). S","@endWordPosition":"1676","@position":"10765","annotationId":"T25","@startWordPosition":"1675","@citStr":"Hinds (1979)"}},"title":{"#tail":"\n","#text":"Organizational Patterns in Discourse.&quot;"},"volume":{"#tail":"\n","#text":"12"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Hinds, J. (1979). &quot;Organizational Patterns in Discourse.&quot; In Givon, T., ed., Syntax and Semantics, Vol. 12. Academic Press. New York, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Hinds, 1979"},"publisher":{"#tail":"\n","#text":"Academic Press."},"location":{"#tail":"\n","#text":"New York, New York."},"booktitle":{"#tail":"\n","#text":"Syntax and Semantics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Hinds"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"rawString":{"#tail":"\n","#text":"Hintikka, J. (1985). The Game of Language. D. Reidel. Dordrecht."},"#text":"\n","marker":{"#tail":"\n","#text":"Hintikka, 1985"},"location":{"#tail":"\n","#text":"D. Reidel. Dordrecht."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"an envision asystem that uses data struc- tures produced by a computational grammar to obtain the logical form of sentences. 3.1 Finite Representations, Finite Theories Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references. This means that natural language expressions such as &quot;A is B,&quot; &quot;A is the same as B,&quot; etc. are not directly represented by logical equality; similarly, &quot;not&quot; is often not treated as logical negation; cf. Hintikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred ele- ments of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1","@endWordPosition":"4276","@position":"26900","annotationId":"T26","@startWordPosition":"4275","@citStr":"Hintikka (1985)"}},"title":{"#tail":"\n","#text":"The Game of Language."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Hintikka"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Hirst, G. (1987). Semantic Interpretation a d the Resolution of Ambiguity. Cambridge University Press. Cambridge."},"#text":"\n","marker":{"#tail":"\n","#text":"Hirst, 1987"},"publisher":{"#tail":"\n","#text":"Cambridge University Press. Cambridge."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tween the &quot;definitional&quot; and factual information, but the &quot;definitional&quot; part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, &quot;coherence&quot; and &quot;dominance,&quot; which are not variants of the standard first order entailment, but abduction. The idea of using preferences among theories is new, hence it was described in more detail. &quot;Coherence,&quot; as outlined above, can be understood as a declarative (or static) version of marker passing (Hirst 1987; Charniak 1983), with one difference: the activation spreads to theories that share a predicate, not through the IS-A hierarchy, and is limited to elementary facts about predicates appearing in the text. The metalevel rules we are going to discuss in Section 6, and that deal with the Gricean maxims and the meaning of &quot;but,&quot; can be easily expressed in the languages of set theory or higher order logic, but not everything expressible in those languages makes sense in natural language. Hence, putting limitations on the expressive power of the language of the metalevel will remain as one of many o","@endWordPosition":"8741","@position":"53435","annotationId":"T27","@startWordPosition":"8740","@citStr":"Hirst 1987"}},"title":{"#tail":"\n","#text":"Semantic Interpretation a d the Resolution of Ambiguity."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"G Hirst"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1982"},"editor":{"#tail":"\n","#text":"In Lehnert, W.G., and Ringle, M.H., eds."},"rawString":{"#tail":"\n","#text":"Hobbs, J.R. (1982). &quot;Towards an Understanding of Coherence in Discourse.&quot; In Lehnert, W.G., and Ringle, M.H., eds. Strategies for Natural Language Processing. Lawrence Erlbaum. Hillsdale, New Jersey: 223-244."},"#text":"\n","pages":{"#tail":"\n","#text":"223--244"},"marker":{"#tail":"\n","#text":"Hobbs, 1982"},"location":{"#tail":"\n","#text":"Hillsdale, New Jersey:"},"title":{"#tail":"\n","#text":"Towards an Understanding of Coherence in Discourse.&quot;"},"booktitle":{"#tail":"\n","#text":"Strategies for Natural Language Processing. Lawrence Erlbaum."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J R Hobbs"}}},{"volume":{"#tail":"\n","#text":"3"},"#tail":"\n","date":{"#tail":"\n","#text":"1979"},"rawString":{"#tail":"\n","#text":"Hobbs, J.R. (1979). &quot;Coherence and Coreference.&quot; Cognitive Science 3: 67-90."},"journal":{"#tail":"\n","#text":"Cognitive Science"},"#text":"\n","pages":{"#tail":"\n","#text":"67--90"},"marker":{"#tail":"\n","#text":"Hobbs, 1979"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ttached at the level of the main verb of the sentence, then &quot;ship&quot; is the referent. But syntactic relations do not suffice to resolve anaphora: Hobbs' (1976) algorithm for resolving the reference of pronouns, depending only on the surface syntax of sentences in the text, when applied to &quot;it&quot; in the example paragraph, fails in both cases to identify the most likely referent NP. Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation. Later, Hobbs (1979, 1982) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using &quot;salience&quot; in choosing facts from this knowledge base. We will investigate the possibility that the structure of this knowledge base can actually resemble the structure of, for example, natural language dictionaries. The process of finding referents could then be automated. Determining that the most likely subject for &quot;bringing,&quot; in the first sentence, is the noun &quot;ship&quot; is done in the following fashion. The first definition for &quot;bring&quot; in W7 (Webster's Sev","@endWordPosition":"2693","@position":"17120","annotationId":"T28","@startWordPosition":"2692","@citStr":"Hobbs (1979"},{"#tail":"\n","#text":" defined as a longest, a shortest, or--simply--a sequence of predicates satisfying the conditions (1) and (2); the existence of a sequence is equivalent with the existence of a shortest and a longest sequence. The reason for choosing a longest sequence as the topic is our belief that the topic should rather contain more information about a paragraph than less. 4.1 Comparison with Other Approaches At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others. We are going to make such a comparison with the theories proposed by J. Hobbs (1979, 1982) that represent a more computa- tionally oriented approach to coherence, and those of T.A. van Dijk and W. Kintch (1983), who are more interested in addressing psychological nd cognitive aspects of discourse coherence. The quoted works seem to be good representatives for each of the directions; they also point to related literature. The approach we advocate is compatible with the work of these researchers, we believe. There are, however, some interesting differences: first of all, we emphasize the role of paragraphs; econd, we talk about formal principles regulating the organization and","@endWordPosition":"10553","@position":"64127","annotationId":"T29","@startWordPosition":"10552","@citStr":"Hobbs (1979"},{"#tail":"\n","#text":"meaningful computational models. To be sure, we believe relations between pairs of sentences are worth investigating, especially in dialogs. However, in written discourse, the smallest domain of coherence is a paragraph, very much as the sentence is the basic domain of grammaticality (although one can also judge the correctness of phrases). To see the advantage of assuming that coherence is a property of a fragment of a text/discourse, and not a relation between subsequent sentences, let us consider for instance the text John took a train from Paris to Istanbul. He likes spinach. According to Hobbs (1979, p. 67), these two sentences are incoherent. However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.) suddenly (for Hobbs) becomes coherent. It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added. On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of","@endWordPosition":"10987","@position":"66956","annotationId":"T30","@startWordPosition":"10986","@citStr":"Hobbs (1979"},{"#tail":"\n","#text":"ed to be true. The Maxim of Manner seems to us to be more relevant for critiquing the style of a written passage or for natural anguage generation; in the case of text generation, it can be construed as a requirement that the produced text be coherent and cohesive. We do not claim that Gla is the best or unique way of expressing the rule &quot;assume that the writer did not say too much.&quot; Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example: two sentences, regarded as a fragment of paragraph, are a variation on a theme by Hobbs (1979). Example 3 The captain is worried because the third officer can open his safe. He knows the combination. The above metarule postulating &quot;nonredundancy&quot; implies that &quot;he&quot; = &quot;the third officer, .... his&quot; = &quot;the captain's&quot; are the referents of the pronouns. This is because the formula safe(x) --, (owns(y~ x) & cmbntn(z~ x) --, knows(y~ z) & can_open(y~ x)) E Tsafe, belongs to R, since it is common knowledge about safes that they have owners, and also combinations that are known to the owners. Therefore &quot;his&quot; = &quot;the third officer's&quot; would produce a redundant formula, corresponding to the sentence","@endWordPosition":"15782","@position":"95078","annotationId":"T31","@startWordPosition":"15781","@citStr":"Hobbs (1979)"},{"#tail":"\n","#text":"st plausible interpretation f our example sentences. Note: The reader must have noticed that we did not bother to distinguish the sentences P1, P2, Q1 and Q2 from their logical forms. Representing &quot;because&quot; and &quot;know&quot; adequately should be considered a separate topic; representing the rest (in the first order convention of this paper) is trivial. 6.1.1 Was the Use of a Gricean Maxim Necessary? Can one deal effectively with the problem of reference without axiomatized Gricean maxims, for instance by using only 201 Computational Linguistics Volume 17, Number 2 &quot;petty conversational implicature&quot; (Hobbs 1979), or the metarules of Section 5.2? It seems to us that the answer is no. As a case in point, consider the process of finding the antecedent of the anaphor &quot;he&quot; in the sentences John can open Bill's safe. He knows the combination. Hobbs (1979, 1982) proves &quot;he&quot; = &quot;John&quot; by assuming the relation of &quot;elaboration&quot; between the sentences. (Elaboration is a relation between two segments of a text. It intuitively means &quot;expressing the same thought from a different perspective,&quot; but has been defined formally as the existence of a proposition implied by both segments-- here the proposition is &quot;John can ","@endWordPosition":"16351","@position":"98452","annotationId":"T32","@startWordPosition":"16350","@citStr":"Hobbs 1979"}]},"title":{"#tail":"\n","#text":"Coherence and Coreference.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J R Hobbs"}}},{"volume":{"#tail":"\n","#text":"44"},"#tail":"\n","date":{"#tail":"\n","#text":"1978"},"rawString":{"#tail":"\n","#text":"Hobbs, J.R. (1978). &quot;Resolving Pronoun References.&quot; Lingua 44: 311-338."},"journal":{"#tail":"\n","#text":"Lingua"},"#text":"\n","pages":{"#tail":"\n","#text":"311--338"},"marker":{"#tail":"\n","#text":"Hobbs, 1978"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"f our example paragraph and a formula expressing the equality of two constants, i and i', denoting (respectively) the &quot;infection&quot; in the sentence Within twenty- four hours of infection . . . . and the &quot;infection&quot; of the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because it is enough to find one partial theory proving a g","@endWordPosition":"12561","@position":"76519","annotationId":"T33","@startWordPosition":"12560","@citStr":"Hobbs (1978"}},"title":{"#tail":"\n","#text":"Resolving Pronoun References.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J R Hobbs"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Research Report 77-2."},"date":{"#tail":"\n","#text":"1977"},"institution":{"#tail":"\n","#text":"Department of Computer Science,"},"rawString":{"#tail":"\n","#text":"Hobbs, J.R. (1977). 38 Examples of Elusive Antecedents from Published Texts. Research Report 77-2. Department of Computer Science, City College, CUNY, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Hobbs, 1977"},"location":{"#tail":"\n","#text":"City College, CUNY, New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sina&quot; is the subject of '`bringing&quot; and must be the referent for &quot;it.&quot; If the clause modifies &quot;port,&quot; then &quot;port&quot; is the desired referent; if the clause is attached at the level of the main verb of the sentence, then &quot;ship&quot; is the referent. But syntactic relations do not suffice to resolve anaphora: Hobbs' (1976) algorithm for resolving the reference of pronouns, depending only on the surface syntax of sentences in the text, when applied to &quot;it&quot; in the example paragraph, fails in both cases to identify the most likely referent NP. Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation. Later, Hobbs (1979, 1982) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using &quot;salience&quot; in choosing facts from this knowledge base. We will investigate the possibility that the structure of this knowledge base can actually resemble the structure of, for example, natural language dictionaries. The process of finding referents could then be automated. Determining that the most lik","@endWordPosition":"2671","@position":"16963","annotationId":"T34","@startWordPosition":"2670","@citStr":"Hobbs 1977"}},"title":{"#tail":"\n","#text":"38 Examples of Elusive Antecedents from Published Texts."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J R Hobbs"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Research Report 76-1."},"date":{"#tail":"\n","#text":"1976"},"institution":{"#tail":"\n","#text":"Dept. of Computer Science."},"rawString":{"#tail":"\n","#text":"Hobbs, J.R. (1976). Pronoun Resolution. Research Report 76-1. Dept. of Computer Science. City College, CUNY, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Hobbs, 1976"},"location":{"#tail":"\n","#text":"City College, CUNY, New York."},"title":{"#tail":"\n","#text":"Pronoun Resolution."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J R Hobbs"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1988"},"rawString":{"#tail":"\n","#text":"Hobbs, J., Stickel, M., Martin, P., and Edwards, D. (1988). &quot;Interpretation as Abduction.&quot; In Proc. of 26th Annual Meeting of the Association for Computational Linguistics, ACL: 95-103."},"#text":"\n","pages":{"#tail":"\n","#text":"95--103"},"marker":{"#tail":"\n","#text":"Hobbs, Stickel, Martin, Edwards, 1988"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"fection&quot; of the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because it is enough to find one partial theory proving a given formula.) As an example, in the case of the three-sentence paragraph, we have a partial theory T1 based on (slb) saying that &quot; 'it' hits rapidly,&quot; and T2 saying that &quot;an illness ('it') harms rapidly&quot; (s2_ex","@endWordPosition":"12595","@position":"76730","annotationId":"T35","@startWordPosition":"12592","@citStr":"Hobbs et al (1988)"}},"title":{"#tail":"\n","#text":"Interpretation as Abduction.&quot;"},"booktitle":{"#tail":"\n","#text":"In Proc. of 26th Annual Meeting of the Association for Computational Linguistics, ACL:"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Hobbs"},{"#tail":"\n","#text":"M Stickel"},{"#tail":"\n","#text":"P Martin"},{"#tail":"\n","#text":"D Edwards"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1983"},"rawString":{"#tail":"\n","#text":"Jackendoff, R. (1983). Semantics and Cognition. The MIT Press. Cambridge, Massachusetts."},"#text":"\n","marker":{"#tail":"\n","#text":"Jackendoff, 1983"},"publisher":{"#tail":"\n","#text":"The MIT Press."},"location":{"#tail":"\n","#text":"Cambridge, Massachusetts."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ing the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another pos- sibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate ncoding of the infor- mation it conveys.&quot; Therefore a formalism of the kind he advocates would probably be most suitable for an implementation f our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision asystem that uses data struc- tures produced by a computational grammar to obtain the logical form of sentences. 3.1 Finite Representations, Finite Theories Unless explicitly stated otherwise, we assume that formulas are","@endWordPosition":"4113","@position":"25917","annotationId":"T36","@startWordPosition":"4112","@citStr":"Jackendoff (1983"},{"#tail":"\n","#text":"ntikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred ele- ments of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu- tational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations o","@endWordPosition":"4371","@position":"27504","annotationId":"T37","@startWordPosition":"4370","@citStr":"Jackendoff (1983)"}]},"title":{"#tail":"\n","#text":"Semantics and Cognition."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R Jackendoff"}}},{"date":{"#tail":"\n","#text":"1988"},"editor":{"#tail":"\n","#text":"In A. Blaser, ed.,"},"title":{"#tail":"\n","#text":"Issues in Parsing.&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Jensen, K. (1988). &quot;Issues in Parsing.&quot; In A. Blaser, ed., Natural Language at the Compute, r. Springer-Verlag, Berlin, Germany: 65-83."},"#text":"\n","pages":{"#tail":"\n","#text":"65--83"},"marker":{"#tail":"\n","#text":"Jensen, 1988"},"publisher":{"#tail":"\n","#text":"Springer-Verlag,"},"location":{"#tail":"\n","#text":"Berlin, Germany:"},"booktitle":{"#tail":"\n","#text":"Natural Language at the Compute, r."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"K Jensen"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1986"},"rawString":{"#tail":"\n","#text":"Jensen, K. (1986). Parsing Strategies in a Broad-Coverage Grammar of English. Research Report RC 12147. IBM T.J. Watson Research Center, Yorktown Heights, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Jensen, 1986"},"location":{"#tail":"\n","#text":"Yorktown Heights, New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"differentiating between the creation of a new concept like &quot;black + death,&quot; with a meaning given by a paraphrase of the example collection of sentences, and the acceptance of the new concept--storing it in R. In our case the concept &quot;black + death,&quot; which does not refer to any normal experiences, would be discarded as useless, although the collection of sentences would be recognized as a strange, even if coherent, paragraph. We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive xamples. This approach is taken in computational syntactic grammars (e.g. Jensen 1986); the number of unlikely parses is severely reduced whenever pos- sible, but no attempt is made to define only the so-called grammatical strings of a language. Finally, as the paragraph is a natural domain in which word senses can be reliably assigned to words or sentences can be syntactically disambiguated, larger chunks of discourse may be needed for precise assignment of topics, which we view as another type of disambiguation. Notice also that for coherence, as defined above, it does not matter whether the topic is defined as a longest, a shortest, or--simply--a sequence of predicates satis","@endWordPosition":"10365","@position":"62993","annotationId":"T38","@startWordPosition":"10364","@citStr":"Jensen 1986"}},"title":{"#tail":"\n","#text":"Parsing Strategies in a Broad-Coverage Grammar of English. Research Report RC 12147. IBM T.J. Watson Research Center,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"K Jensen"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1988"},"rawString":{"#tail":"\n","#text":"Jensen, K., and Binot, J.-L. (1988). &quot;Disambiguating Prepositional Phrase Attachments by Using On-line Dictionary Definitions.&quot; Computational Linguistics 13.3-4.251-260 (special issue on the lexicon)."},"#text":"\n","marker":{"#tail":"\n","#text":"Jensen, Binot, 1988"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nding of a given piece of text has to be extracted. It constrains interpretations of the predicates of an object theory. Its structure and the extraction methods will be discussed below. 4. Understanding has as its goal construction of an interpretation of the text, i.e. building some kind of model. Since not all logically permissible models are linguistically appropriate, one needs a place, namely the metalevel, to put constraints on types of models. Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them. We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referen- tial levels for commonsense r asoning--for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner ","@endWordPosition":"3692","@position":"23323","annotationId":"T39","@startWordPosition":"3689","@citStr":"Jensen and Binot 1988"}},"title":{"#tail":"\n","#text":"Disambiguating Prepositional Phrase Attachments by Using On-line Dictionary Definitions.&quot; Computational Linguistics 13.3-4.251-260 (special issue on the lexicon)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Jensen"},{"#tail":"\n","#text":"J-L Binot"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1983"},"rawString":{"#tail":"\n","#text":"Johnson-Laird, P.N. (1983). Mental Models. Harvard University Press. Cambridge, Massachusetts."},"#text":"\n","marker":{"#tail":"\n","#text":"Johnson-Laird, 1983"},"publisher":{"#tail":"\n","#text":"Harvard University Press."},"location":{"#tail":"\n","#text":"Cambridge, Massachusetts."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"gical negation; cf. Hintikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred ele- ments of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu- tational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from forma","@endWordPosition":"4369","@position":"27485","annotationId":"T40","@startWordPosition":"4368","@citStr":"Johnson-Laird (1983)"}},"title":{"#tail":"\n","#text":"Mental Models."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"P N Johnson-Laird"}}},{"date":{"#tail":"\n","#text":"1981"},"editor":{"#tail":"\n","#text":"In Groenendijk, J.A.G., et al eds.,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"e to think about constraining linguistic or logical predicates by simulating physical experiences (cf. Woods 1987). We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another pos- sibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate ncoding of the info","@endWordPosition":"4043","@position":"25463","annotationId":"T41","@startWordPosition":"4042","@citStr":"Kamp (1981)"},{"#tail":"\n","#text":"logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred ele- ments of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu- tational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations of text and ba","@endWordPosition":"4373","@position":"27517","annotationId":"T42","@startWordPosition":"4372","@citStr":"Kamp (1981)"}]},"title":{"#tail":"\n","#text":"A Theory of Truth and Semantic Representation.&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Kamp, H. (1981). &quot;A Theory of Truth and Semantic Representation.&quot; In Groenendijk, J.A.G., et al eds., Formal Methods in the Study of Language, I. Mathematisch Centrum, Amsterdam: 277-322."},"#text":"\n","pages":{"#tail":"\n","#text":"277--322"},"marker":{"#tail":"\n","#text":"Kamp, 1981"},"location":{"#tail":"\n","#text":"Amsterdam:"},"booktitle":{"#tail":"\n","#text":"Formal Methods in the Study of Language, I. Mathematisch Centrum,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"H Kamp"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1973"},"rawString":{"#tail":"\n","#text":"Labov, W. (1973). &quot;The Boundaries of Words and Their Meanings.&quot; In Fishman, J., New Ways of Analyzing Variation in English. Georgetown U. Press. Washington, D.C.: 340-373."},"journal":{"#tail":"\n","#text":"In Fishman, J., New Ways of Analyzing Variation in English. Georgetown"},"#text":"\n","pages":{"#tail":"\n","#text":"340--373"},"marker":{"#tail":"\n","#text":"Labov, 1973"},"location":{"#tail":"\n","#text":"Washington, D.C.:"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"hat a theory of some ? is a &quot;law&quot; is by deleting the empty interpretation f ? from the partial order. If we accept he definition of a concept as given by necessary and sufficient conditions, the theories would all appear as laws. If we subscribe to a more realistic view where definitions are given by a collection of central/prototypical and peripheral conditions, only the peripheral ones can be contradicted by &quot;but.&quot; In either formalization we get BUT_C1 as a consequence: Since &quot;laws&quot; cannot be deleted, BUT can't be applied, and hence its use in those kinds of sentences would be incorrect. W. Labov (1973) discussed sentences of the form ,This is a chair but you can sit on it. The sentence is incorrect, since the function &quot;one can sit on it&quot; belongs to the core of the concept &quot;chair&quot;; so--contrary to the role of &quot;but&quot;--the sentence does not contain any surprising new elements. Using the Metarule (BUT) and the cooperative principle of Grice, we get BUT_C2: ? but ? is incorrect, if ? -* k~ is a &quot;law.&quot; The Metarule (BUT) gives the semantics of &quot;but;&quot; the rules BUT_C1 and BUT_C2 follow from it (after formalization i a sufficiently strong rnetalanguage such as type theory or set theory). We can link","@endWordPosition":"17738","@position":"106812","annotationId":"T43","@startWordPosition":"17737","@citStr":"Labov (1973)"}},"title":{"#tail":"\n","#text":"The Boundaries of Words and Their Meanings.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W Labov"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Lakoff, G. (1987). Women, Fire and Dangerous Things. The University of Chicago Press. Chicago, Illinois."},"#text":"\n","marker":{"#tail":"\n","#text":"Lakoff, 1987"},"location":{"#tail":"\n","#text":"Chicago, Illinois."},"title":{"#tail":"\n","#text":"Women, Fire and Dangerous Things. The University of Chicago Press."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"G Lakoff"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1984"},"rawString":{"#tail":"\n","#text":"Levesque, H.J. (1984). &quot;A Logic of Implicit and Explicit Beliefs.&quot; Proc. AAAI-84. AAAI: 198-202."},"#text":"\n","pages":{"#tail":"\n","#text":"198--202"},"marker":{"#tail":"\n","#text":"Levesque, 1984"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nterpretation defined on a certain domain, which satisfies all formulas of T. The collection of all (finite) models of a theory T will be denoted by Mods(T). ? The set of all subformulas of a collection of formulas F is denoted by Form(F). ~ is a ground instance of a formula G if ~ contains no variables, and ~ = ~, for some substitution 0. Thus, we do not require Th(T) to be closed under substitution i stances of tautologies. Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect o weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985). But we won't pursue this topic further here. 3.2 The Structure of Background Knowledge Background knowledge is not a simple list of meaning postulates--it has a structure and it may contain contradictions and ambiguities. These actualities have to be taken into account in any realistic model of natural language understanding. For instance, the verb &quot;enter&quot; is polysemous. But, unless context specifies otherwise, &quot;to come in&quot; is a more plausible meaning than &quot;to join a group.&quot; Assuming some logical representation of this knowledge, we can write that enter(x,","@endWordPosition":"4901","@position":"30566","annotationId":"T44","@startWordPosition":"4900","@citStr":"Levesque 1984"}},"title":{"#tail":"\n","#text":"A Logic of Implicit and Explicit Beliefs.&quot;"},"booktitle":{"#tail":"\n","#text":"Proc. AAAI-84. AAAI:"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"H J Levesque"}}},{"date":{"#tail":"\n","#text":"1979"},"editor":{"#tail":"\n","#text":"In Givon, T., ed.,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sitory (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &quot;switching&quot; patterns, uch as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists how little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central topic","@endWordPosition":"1815","@position":"11703","annotationId":"T45","@startWordPosition":"1814","@citStr":"Longacre 1979"}},"title":{"#tail":"\n","#text":"The Paragraph as a Grammatical Unit."},"volume":{"#tail":"\n","#text":"12"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Longacre, R.E. (1979). The Paragraph as a Grammatical Unit. In Givon, T., ed., Syntax and Semantics, Vol. 12. Academic Press. New York, New York. Longman Dictionary of Contemporary English."},"journal":{"#tail":"\n","#text":"Longman Dictionary of Contemporary English."},"#text":"\n","marker":{"#tail":"\n","#text":"Longacre, 1979"},"publisher":{"#tail":"\n","#text":"Academic Press."},"location":{"#tail":"\n","#text":"New York, New York."},"booktitle":{"#tail":"\n","#text":"Syntax and Semantics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R E Longacre"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1978"},"rawString":{"#tail":"\n","#text":"(1978). Longman Group Ltd., London."},"#text":"\n","marker":{"#tail":"\n","#text":"1978"},"publisher":{"#tail":"\n","#text":"Longman Group Ltd.,"},"location":{"#tail":"\n","#text":"London."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ous or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information ecessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvio","@endWordPosition":"9124","@position":"55814","annotationId":"T46","@startWordPosition":"9123","@citStr":"(1975, 1978)"},{"#tail":"\n","#text":"example paragraph and a formula expressing the equality of two constants, i and i', denoting (respectively) the &quot;infection&quot; in the sentence Within twenty- four hours of infection . . . . and the &quot;infection&quot; of the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because it is enough to find one partial theory proving a given fo","@endWordPosition":"12562","@position":"76526","annotationId":"T47","@startWordPosition":"12561","@citStr":"(1978, 1979)"}]},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"1969"},"rawString":{"#tail":"\n","#text":"Lyons, J. (1969). Introduction to Theoretical Linguistics. Cambridge University Press. Cambridge, England."},"#text":"\n","marker":{"#tail":"\n","#text":"Lyons, 1969"},"publisher":{"#tail":"\n","#text":"Cambridge University Press."},"location":{"#tail":"\n","#text":"Cambridge, England."},"title":{"#tail":"\n","#text":"Introduction to Theoretical Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Lyons"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1983"},"rawString":{"#tail":"\n","#text":"Mann, W.C., and Thompson, S.A. (1983). Relational Propositions in Discourse. Information Sciences Institute Research Report 83-115."},"#text":"\n","pages":{"#tail":"\n","#text":"83--115"},"marker":{"#tail":"\n","#text":"Mann, Thompson, 1983"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":", as Hobbs' semantics eems to be.) 190 Zadrozny and Jensen Semantics of Paragraphs We shall discuss only the first two points, since the third one has already been explained. The chief difference between our approach and the other two lies in identifying the paragraph as a domain of coherence. Hobbs, van Dijk, and Kintch distinguish between &quot;local&quot; coherence~a property of subsequent sentences--and &quot;global&quot; coherence---a property of discourse as a whole. Hobbs explains coherence in terms of an inventory of &quot;local,&quot; possibly computable, coherence relations, like &quot;elaboration,&quot; &quot;occasion,&quot; etc. (Mann and Thompson 1983 give an even more detailed list of coherence relations than Hobbs.) Van Dijk and Kintch do this too, but they also describe &quot;macrostructures&quot; representing the global content of discourse, and they emphasize psychological and cognitive strategies used by people in establishing discourse coherence. Since we have linked coherence to models of paragraphs, we can talk simply about &quot;coherence&quot;-- without adjectives--as property of these models. To us the first &quot;local&quot; domain seems to be too small, and the second &quot;global&quot; one too large, for constructing meaningful computational models. To be sure, we","@endWordPosition":"10806","@position":"65792","annotationId":"T48","@startWordPosition":"10803","@citStr":"Mann and Thompson 1983"}},"booktitle":{"#tail":"\n","#text":"Relational Propositions in Discourse. Information Sciences Institute Research Report"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W C Mann"},{"#tail":"\n","#text":"S A Thompson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Moens, M., and Steedman, M. (1987). &quot;Temporal Ontology in Natural Language.&quot; Proc. 25th Annual Meeting of the ACL. Stanford, California: 1-7."},"#text":"\n","pages":{"#tail":"\n","#text":"1--7"},"marker":{"#tail":"\n","#text":"Moens, Steedman, 1987"},"location":{"#tail":"\n","#text":"Stanford, California:"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"t. Example 2 PI: In 1347 a ship entered the port of Messina bringing with it the disease that came to be known as the Black Death. P2: It struck rapidly. P3: Within twenty-four hours of infection came an agonizing death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents. We assume that any formal interpretation f time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation f events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence b tween syntax and semantics expressed in the gram- matical constraint (&quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&quot;---ibid.). Howe","@endWordPosition":"11754","@position":"71656","annotationId":"T49","@startWordPosition":"11751","@citStr":"Moens and Steedman 1987"}},"title":{"#tail":"\n","#text":"Temporal Ontology in Natural Language.&quot;"},"booktitle":{"#tail":"\n","#text":"Proc. 25th Annual Meeting of the ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Moens"},{"#tail":"\n","#text":"M Steedman"}]}},{"volume":{"#tail":"\n","#text":"36"},"#tail":"\n","date":{"#tail":"\n","#text":"1970"},"rawString":{"#tail":"\n","#text":"Montague, R. (1970). &quot;Universal Grammar.&quot; Theoria 36: 373-398."},"journal":{"#tail":"\n","#text":"Theoria"},"#text":"\n","pages":{"#tail":"\n","#text":"373--398"},"marker":{"#tail":"\n","#text":"Montague, 1970"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"mantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another pos- sibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate ncoding of the infor- mation it conveys.&quot; Therefore a formalism of the kind he advocates would probably be most suitable for an implementation f our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision asystem that uses data struc- tures produced by ","@endWordPosition":"4089","@position":"25745","annotationId":"T50","@startWordPosition":"4088","@citStr":"Montague (1970)"}},"title":{"#tail":"\n","#text":"Universal Grammar.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R Montague"}}},{"date":{"#tail":"\n","#text":"1981"},"issue":{"#tail":"\n","#text":"3"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" lthough usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu- tational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations of text and background knowledge. But there are several ways to avoid this obstruction. For example, consider theories consisting of universal formulas without function symbols. Let Th(T) of such a theory T be defined as T plus ground clauses/sets of literals prov- able from T in standard logic. It is easily seen that it is a closure, i.e. ","@endWordPosition":"4424","@position":"27845","annotationId":"T51","@startWordPosition":"4423","@citStr":"Mycielski 1981"}},"title":{"#tail":"\n","#text":"Analysis without Actual Infinity.&quot;"},"volume":{"#tail":"\n","#text":"46"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Mycielski, J. (1981). &quot;Analysis without Actual Infinity.&quot; Journal of Symbolic Logic, 46(3): 625-633."},"journal":{"#tail":"\n","#text":"Journal of Symbolic Logic,"},"#text":"\n","pages":{"#tail":"\n","#text":"625--633"},"marker":{"#tail":"\n","#text":"Mycielski, 1981"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Mycielski"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"rawString":{"#tail":"\n","#text":"Zadrozny and Jensen Semantics of Paragraphs Patel-Schneider, P.S. (1985). &quot;A Decidable First Order Logic for Knowledge Representation.&quot; Proc. IJCAI-85. AAAI: 455-458."},"#text":"\n","pages":{"#tail":"\n","#text":"455--458"},"marker":{"#tail":"\n","#text":"Zadrozny, Jensen, 1985"},"location":{"#tail":"\n","#text":"P.S."},"title":{"#tail":"\n","#text":"Semantics of Paragraphs Patel-Schneider,"},"booktitle":{"#tail":"\n","#text":"Proc. IJCAI-85. AAAI:"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Zadrozny"},{"#tail":"\n","#text":"Jensen"}]}},{"volume":{"#tail":"\n","#text":"25"},"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"rawString":{"#tail":"\n","#text":"Perlis, D. (1985). &quot;Languages with Self Reference I: Foundations.&quot; Artificial Intelligence 25: 301-322."},"journal":{"#tail":"\n","#text":"Artificial Intelligence"},"#text":"\n","pages":{"#tail":"\n","#text":"301--322"},"marker":{"#tail":"\n","#text":"Perlis, 1985"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents. We assume that any formal interpretation f time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation f events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence b tween syntax and semantics expressed in the gram- matical constraint (&quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&quot;---ibid.). However, as noted before, we will use a simplified version of such a logical no- tation; we will have only time, event, result, and property as primitives. After these remarks we can begin constructing the m","@endWordPosition":"11788","@position":"71859","annotationId":"T52","@startWordPosition":"11787","@citStr":"Perlis (1985)"}},"title":{"#tail":"\n","#text":"Languages with Self Reference I: Foundations.&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Perlis"}}},{"date":{"#tail":"\n","#text":"1988"},"issue":{"#tail":"\n","#text":"1"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rection of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential level while the metalevel restricts ways that the model can be constructed, or, in other ","@endWordPosition":"13786","@position":"83647","annotationId":"T53","@startWordPosition":"13785","@citStr":"Poole (1988)"}},"title":{"#tail":"\n","#text":"A Logical Framework for Default Reasoning.&quot;"},"volume":{"#tail":"\n","#text":"36"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Poole, D. (1988). &quot;A Logical Framework for Default Reasoning.&quot; Artificial Intelligence 36(1): 27-47."},"journal":{"#tail":"\n","#text":"Artificial Intelligence"},"#text":"\n","pages":{"#tail":"\n","#text":"27--47"},"marker":{"#tail":"\n","#text":"Poole, 1988"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Poole"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1972"},"rawString":{"#tail":"\n","#text":"Quirk&quot; R., Greenbaum, S., Leech, G., and Svartvik, J. (1972). A Grammar of Contemporary English. Longman Group Ltd. London."},"#text":"\n","marker":{"#tail":"\n","#text":"Quirk, Greenbaum, Leech, Svartvik, 1972"},"publisher":{"#tail":"\n","#text":"Longman Group Ltd."},"location":{"#tail":"\n","#text":"London."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s of &quot;but.&quot; Connectives are function words--like conjunctions and some adverbs--that re responsible simultaneously for maintaining cohesiveness within the text and for sig- naling the nature of the relationships that hold between and among various text units. &quot;And,&quot; &quot;or,&quot; and &quot;but&quot; are the three main coordinating connectives in English. How- ever, &quot;but&quot; does not behave quite like the other two--semantically, &quot;but&quot; signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition (called &quot;adversative&quot; or &quot;contrary-to-expectation&quot; by Halliday and Hasan 1976; cf. also Quirk et al 1972, p. 672). The ship arrived but the passengers could not get off. The yacht is cheap but elegant. Comparison. In this function, the first conjunct is not so directly contradicted by the second. A contradiction exists, but we may have to 202 Zadrozny and Jensen Semantics of Paragraphs go through additional levels of implication to find it. Consider the sentence: . That basketball player is short, but he's very quick. Affirmation. This use of &quot;but&quot; always follows a negative clause, and actually augments the meaning of the preceding clause by adding supporting information: The disease not only ki","@endWordPosition":"16788","@position":"101169","annotationId":"T54","@startWordPosition":"16785","@citStr":"Quirk et al 1972"}},"title":{"#tail":"\n","#text":"A Grammar of Contemporary English."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Quirk"},{"#tail":"\n","#text":"S Greenbaum"},{"#tail":"\n","#text":"G Leech"},{"#tail":"\n","#text":"J Svartvik"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"editor":{"#tail":"\n","#text":"In Karma, K.N., ed.,"},"rawString":{"#tail":"\n","#text":"Reggia, J.A. (1985). &quot;Abductive Inference.&quot; In Karma, K.N., ed., Expert Systems in Government Symposfum. IEEE: 484-489."},"#text":"\n","pages":{"#tail":"\n","#text":"484--489"},"marker":{"#tail":"\n","#text":"Reggia, 1985"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential","@endWordPosition":"13774","@position":"83557","annotationId":"T55","@startWordPosition":"13773","@citStr":"Reggia (1985)"}},"title":{"#tail":"\n","#text":"Abductive Inference.&quot;"},"booktitle":{"#tail":"\n","#text":"Expert Systems in Government Symposfum. IEEE:"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J A Reggia"}}},{"date":{"#tail":"\n","#text":"1987"},"issue":{"#tail":"\n","#text":"1"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"phora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential level while the metalevel restricts ways that the model","@endWordPosition":"13782","@position":"83613","annotationId":"T56","@startWordPosition":"13781","@citStr":"Reiter (1987)"}},"title":{"#tail":"\n","#text":"A Theory of Diagnosis from First Principles.&quot;"},"volume":{"#tail":"\n","#text":"32"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Reiter, R. (1987). &quot;A Theory of Diagnosis from First Principles.&quot; Artificial Intelligence, 32(1): 57-95."},"journal":{"#tail":"\n","#text":"Artificial Intelligence,"},"#text":"\n","pages":{"#tail":"\n","#text":"57--95"},"marker":{"#tail":"\n","#text":"Reiter, 1987"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R Reiter"}}},{"date":{"#tail":"\n","#text":"1983"},"editor":{"#tail":"\n","#text":"In Brady, M., and Berwick&quot; R., eds.,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"r abduction; it says &quot;minimize the number of things that have the property P(,, ,),&quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic lues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric (with the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is alw","@endWordPosition":"13589","@position":"82412","annotationId":"T57","@startWordPosition":"13588","@citStr":"Sidner 1983"}},"title":{"#tail":"\n","#text":"Focusing in the Comprehension f Definite Anaphora.&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Sidner, C. (1983). &quot;Focusing in the Comprehension f Definite Anaphora.&quot; In Brady, M., and Berwick&quot; R., eds., Computational Models of Discourse, MIT Press. Cambridge, Massachusetts: 330-367."},"#text":"\n","pages":{"#tail":"\n","#text":"330--367"},"marker":{"#tail":"\n","#text":"Sidner, 1983"},"publisher":{"#tail":"\n","#text":"MIT Press."},"location":{"#tail":"\n","#text":"Cambridge, Massachusetts:"},"booktitle":{"#tail":"\n","#text":"Computational Models of Discourse,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"C Sidner"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"rawString":{"#tail":"\n","#text":"Sells, P. (1985). Lectures on Contemporary Syntactic Theories. CSLI lecture notes; no. 3."},"#text":"\n","pages":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Sells, 1985"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"hat a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical nd logical unit. It is the small- est linguistic representation f what, in logic, is called a &quot;model,&quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central topic. A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance, Sells (1985, p. 8) says that the sentence &quot;Reagan thinks bananas,&quot; which is otherwise strange, is in fact acceptable if it occurs as an answer to the question &quot;What is Kissinger's favorite fruit?&quot; The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance &quot;Reagan thinks bananas&quot; only within the paragraph inwhich this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one)","@endWordPosition":"1948","@position":"12538","annotationId":"T58","@startWordPosition":"1947","@citStr":"Sells (1985"}},"title":{"#tail":"\n","#text":"Lectures on Contemporary Syntactic Theories. CSLI lecture notes;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"P Sells"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Shoenfield, J.R. (1987). Mathematical Logic. Addison-Wesley. New York&quot; New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Shoenfield, 1987"},"publisher":{"#tail":"\n","#text":"Addison-Wesley."},"location":{"#tail":"\n","#text":"New York&quot; New York."},"booktitle":{"#tail":"\n","#text":"Mathematical Logic."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J R Shoenfield"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Sinclair, J.M., ed. (1987). Looking Up. An account of the COBUILD project. Collins ELT, London."},"#text":"\n","marker":{"#tail":"\n","#text":"Sinclair, ed, 1987"},"location":{"#tail":"\n","#text":"London."},"booktitle":{"#tail":"\n","#text":"Looking Up. An account of the COBUILD project. Collins ELT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J M Sinclair"},{"#tail":"\n","#text":"ed"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1988"},"editor":{"#tail":"\n","#text":"Small, S.L., Cottrell, G.W., and Tanenhaus, M.K., eds."},"rawString":{"#tail":"\n","#text":"Small, S.L., Cottrell, G.W., and Tanenhaus, M.K., eds. (1988). Lexical Ambiguity Resolution. Morgan Kaufmann. San Mateo, California."},"#text":"\n","marker":{"#tail":"\n","#text":"1988"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann."},"location":{"#tail":"\n","#text":"San Mateo, California."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"stent theories of T given by (H(F), <), where F = Form(T): PT<(T) = {TUT' :T '= ?f and f is a maximal element of (I~I(F), <)} Notice that PT< (T) can contain more than one theory, meaning that T is ambiguous. This is a consequence of the fact that the cartesian product is only partially ordered by <. The main reason for using ground instances ~i(Ci) in modifying the orderings is the need to deal with multiple occurrences of the same predicate, as in John went to the bank by the bank. 184 Zadrozny and Jensen Semantics of Paragraphs The above construction is also very close in spirit to Poole's (1988) method for default reasoning, where object theories are augmented by ground instances of defaults. 3.3.3 Coherence Links. The reasoning that led to the intended interpretation fi t in our discussion of dominance was based on the partial ordering of the theories of R. We want to exploit now another property of the theories of R--their coherence. Finding an interpretation for a natural anguage text or sentence typically involves an appeal to coherence. Consider $2: Entering the port, a ship brought adisaster. Using the coherence link between (b2) and (dr1) (cf. Section 3.2)--the presence of cau","@endWordPosition":"7562","@position":"46215","annotationId":"T59","@startWordPosition":"7562","@citStr":"(1988)"},{"#tail":"\n","#text":"the theory (dl)--a disease is an illness caused by an infection. This equality i = i' cannot be proven, but it may be reasonably assumed--we know that in this case the infection i' caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott ried then to make this notion precise, but the paper by Hobbs et al (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff there exists a partial theory T E PT(T) such that T F ~b, i.e. T proves ~ in logic. (We call F-r &quot;weak&quot; because it is enough to find one partial theory proving a given formula.) As an example, in the case of the three-sentence paragraph, we have a partial theory T1 based on (slb) saying that &quot; 'it' hits rapidly,&quot; and T2 saying that &quot;an illness ('it') harms rapidly&quot; (s2_ex","@endWordPosition":"12595","@position":"76730","annotationId":"T60","@startWordPosition":"12595","@citStr":"(1988)"},{"#tail":"\n","#text":"n of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure as- sumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abduc- tive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the in- formation contained in the paragraph itself (the object theory) and in the referential level while the metalevel restricts ways that the model can be constructed, or, in other ","@endWordPosition":"13786","@position":"83647","annotationId":"T61","@startWordPosition":"13786","@citStr":"(1988)"}]},"title":{"#tail":"\n","#text":"Lexical Ambiguity Resolution."},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Turner, M. (1987). Death is the Mother of Beauty. The University of Chicago Press. Chicago, Illinois."},"#text":"\n","marker":{"#tail":"\n","#text":"Turner, 1987"},"publisher":{"#tail":"\n","#text":"The University of Chicago Press."},"location":{"#tail":"\n","#text":"Chicago, Illinois."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referen- tial levels for commonsense r asoning--for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (1987, pp. 7-8): ... Semantics i constrained by our models of ourselves and our worlds. We have models of up and down that are based by the way our bodies actually function. Once the word &quot;up&quot; is given its meaning relative to our experience with gravity, it is not free to &quot;slip&quot; into its opposite. &quot;Up&quot; means up and not down . . . . We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model. Mothers have a different role than fathers in this model, and thus th","@endWordPosition":"3783","@position":"23928","annotationId":"T62","@startWordPosition":"3782","@citStr":"Turner (1987"}},"title":{"#tail":"\n","#text":"Death is the Mother of Beauty."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Turner"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1983"},"rawString":{"#tail":"\n","#text":"van Dijk, T.A., and Kintch, W. (1983). Strategies of Discourse Comprehension. Academic Press. Orlando, Florida."},"#text":"\n","marker":{"#tail":"\n","#text":"van Dijk, Kintch, 1983"},"publisher":{"#tail":"\n","#text":"Academic Press."},"location":{"#tail":"\n","#text":"Orlando, Florida."},"booktitle":{"#tail":"\n","#text":"Strategies of Discourse Comprehension."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"T A van Dijk"},{"#tail":"\n","#text":"W Kintch"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1963"},"rawString":{"#tail":"\n","#text":"Warriner, J.E. (1963). English Grammar and Composition. Harcourt, Brace & World, Inc., New York New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Warriner, 1963"},"publisher":{"#tail":"\n","#text":"Harcourt, Brace & World, Inc.,"},"location":{"#tail":"\n","#text":"New York New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"guity and construct new models. We also show metalevel rules for interpreting &quot;but.&quot; 2. The Paragraph as a Discourse Unit 2.1 Approaches to Paragraph Analysis Recent syntactic theory--that is, in the last 30 years--has been preoccupied with sentence-level analysis. Within discourse theory, however, some significant work has been done on the analysis of written paragraphs. We can identify four different linguis- tic approaches to paragraphs: prescriptivist, psycholinguist, extualist, and discourse- oriented. The prescriptivist approach is typified in standard English grammar textbooks, such as Warriner (1963). In these sources, a paragraph is notionally defined as some- thing like a series of sentences that develop one single topic, and rules are laid down for the construction of an ideal (or at least an acceptable) paragraph. Although these dictates are fairly clear, the underlying notion of topic is not. An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, b","@endWordPosition":"1426","@position":"9095","annotationId":"T63","@startWordPosition":"1425","@citStr":"Warriner (1963)"}},"title":{"#tail":"\n","#text":"English Grammar and Composition."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J E Warriner"}}},{"date":{"#tail":"\n","#text":"1983"},"editor":{"#tail":"\n","#text":"In Brad~ M., and Berwick, R., eds.,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"umber of things that have the property P(,, ,),&quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic lues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &quot;it&quot; is anaphoric (with the pronoun following the element hat it refers to), not cataphoric (the other way around). This means that the &quot;it&quot; that brought he disease in P1 will not be con- sidered to refer to the infection &quot;i&quot; or the death &quot;d&quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of re","@endWordPosition":"13594","@position":"82448","annotationId":"T64","@startWordPosition":"13593","@citStr":"Webber 1983"}},"title":{"#tail":"\n","#text":"So What Can We Talk About Now?&quot;"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Webber, B. (1983). &quot;So What Can We Talk About Now?&quot; In Brad~ M., and Berwick, R., eds., Computational Models of Discourse. MIT Press. Cambridge, Massachusetts: 147-154."},"#text":"\n","pages":{"#tail":"\n","#text":"147--154"},"marker":{"#tail":"\n","#text":"Webber, 1983"},"publisher":{"#tail":"\n","#text":"MIT Press."},"location":{"#tail":"\n","#text":"Cambridge, Massachusetts:"},"booktitle":{"#tail":"\n","#text":"Computational Models of Discourse."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"B Webber"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Webber, B. (1987). &quot;The Interpretation f Tense in Discourse.&quot; Proc. 25th Annual Meeting of the ACL, ACL: 147-154. Webster's Seventh New Collegiate Dictionary."},"#text":"\n","marker":{"#tail":"\n","#text":"Webber, 1987"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"a ship entered the port of Messina bringing with it the disease that came to be known as the Black Death. P2: It struck rapidly. P3: Within twenty-four hours of infection came an agonizing death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents. We assume that any formal interpretation f time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation f events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence b tween syntax and semantics expressed in the gram- matical constraint (&quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&quot;---ibid.). However, as noted ","@endWordPosition":"11756","@position":"71670","annotationId":"T65","@startWordPosition":"11755","@citStr":"Webber 1987"}},"title":{"#tail":"\n","#text":"The Interpretation f Tense in Discourse.&quot;"},"booktitle":{"#tail":"\n","#text":"Proc. 25th Annual Meeting of the ACL, ACL: 147-154. Webster's Seventh New Collegiate Dictionary."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"B Webber"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1963"},"rawString":{"#tail":"\n","#text":"(1963). Merriam-Webster, Inc. Springfield, Massachusetts."},"#text":"\n","marker":{"#tail":"\n","#text":"1963"},"publisher":{"#tail":"\n","#text":"Merriam-Webster, Inc. Springfield,"},"location":{"#tail":"\n","#text":"Massachusetts."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" construct new models. We also show metalevel rules for interpreting &quot;but.&quot; 2. The Paragraph as a Discourse Unit 2.1 Approaches to Paragraph Analysis Recent syntactic theory--that is, in the last 30 years--has been preoccupied with sentence-level analysis. Within discourse theory, however, some significant work has been done on the analysis of written paragraphs. We can identify four different linguis- tic approaches to paragraphs: prescriptivist, psycholinguist, extualist, and discourse- oriented. The prescriptivist approach is typified in standard English grammar textbooks, such as Warriner (1963). In these sources, a paragraph is notionally defined as some- thing like a series of sentences that develop one single topic, and rules are laid down for the construction of an ideal (or at least an acceptable) paragraph. Although these dictates are fairly clear, the underlying notion of topic is not. An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, b","@endWordPosition":"1426","@position":"9095","annotationId":"T66","@startWordPosition":"1426","@citStr":"(1963)"}},"@valid":"false"},{"date":{"#tail":"\n","#text":"1987"},"issue":{"#tail":"\n","#text":"3"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nd this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model. Mothers have a different role than fathers in this model, and thus there is a reason why &quot;Death is the father of beauty&quot; fails poetically while &quot;Death is the mother of beauty&quot; succeeds .... It is precisely this &quot;grounding&quot; of logical predicates in other conceptual structures that we would like to capture. We investigate here only the &quot;grounding&quot; in logical the- ories. However, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences (cf. Woods 1987). We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identifie","@endWordPosition":"3964","@position":"24966","annotationId":"T67","@startWordPosition":"3963","@citStr":"Woods 1987"}},"title":{"#tail":"\n","#text":"Don't Blame the Tool.&quot;"},"volume":{"#tail":"\n","#text":"3"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Woods, W. (1987). &quot;Don't Blame the Tool.&quot; Computational Intelligence 3(3): 228-237."},"journal":{"#tail":"\n","#text":"Computational Intelligence"},"#text":"\n","pages":{"#tail":"\n","#text":"228--237"},"marker":{"#tail":"\n","#text":"Woods, 1987"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W Woods"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Zadrozny, W. (1987a). &quot;Intended Models, Circumscription and Commonsense Reasoning.&quot; Proc. IJCAI-87: 909-916."},"#text":"\n","pages":{"#tail":"\n","#text":"909--916"},"marker":{"#tail":"\n","#text":"Zadrozny, 1987"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"on in order to produce an interpretation of a text. The formalist will be presented in a number of steps in which we will elaborate one simple example: Example 1 Entering the port, a ship brought a disease. This sentence can be translated into the logical formula (ignoring only the past tense of &quot;bring&quot; and the progressive of &quot;enter'9: Definition S: enter(xl~ x2) & ship(x1) & port(x2) & bring(x3~ x4) & disease(x4) & xl = s & x2 = m & x3 = s & x4 = d, where s, m, d, are constants. We adopt he three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense r asoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural anguage inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating &quot;they&quot; to &quot;apples&quot; in the sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a): We bought the boys apples because they were so cheap 176 Zadrozny and Jensen Semantics of Paragraphs can be an example of such a most plausible choice. The main ideas of","@endWordPosition":"3408","@position":"21560","annotationId":"T68","@startWordPosition":"3407","@citStr":"Zadrozny 1987"},{"#tail":"\n","#text":"of text has to be extracted. It constrains interpretations of the predicates of an object theory. Its structure and the extraction methods will be discussed below. 4. Understanding has as its goal construction of an interpretation of the text, i.e. building some kind of model. Since not all logically permissible models are linguistically appropriate, one needs a place, namely the metalevel, to put constraints on types of models. Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them. We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referen- tial levels for commonsense r asoning--for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (1987, pp. 7-8)","@endWordPosition":"3694","@position":"23338","annotationId":"T69","@startWordPosition":"3693","@citStr":"Zadrozny 1987"},{"#tail":"\n","#text":"theories of &quot;enter,&quot; ship,&quot; etc. and the partial orders are represented graphically; more plausible theories are positioned higher. A path through this graph chooses an interpretation of the sentence S. For instance, the path f int = {el, shl~ pl, bl, dl} and S say together that A large boat (ship) that carries people or goods came into the harbor and carried a disease (illness). Since it is the &quot;highest&quot; path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence. Because it is also consistent, it will be chosen as a best interpretation of S, (cf. Zadrozny 1987a, 1987b). Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering. As it turns out, f~ is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story. The collection of these most plausible consistent interpretations of a given theory T is denoted by PT< (T). Then fint belongs to PT< (Th({S})), but","@endWordPosition":"6269","@position":"38796","annotationId":"T70","@startWordPosition":"6268","@citStr":"Zadrozny 1987"},{"#tail":"\n","#text":"dicates. Now we want to restrict he notion of a partial theory by introducing the formal notions of topic and coherence. We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions. The partial theories pick up from the referential level the most obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imp","@endWordPosition":"9071","@position":"55456","annotationId":"T71","@startWordPosition":"9070","@citStr":"Zadrozny 1987"},{"#tail":"\n","#text":"ed model of a theory T is an element of Mods(T) that satisfies metalevel con- straints contained in M. The set of all preferred models of T is denoted by PM(T). A formula 4 of L(=), the language with equality, is weakly R + M-abductible from an object heory T, denoted by T ~-a+M G iff there exists a partial theory T E PT(T) and a preferred model M E PM(T) such that M ~ G i.e. 4 is true in at least one preferred model of the partial theory T. Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing &quot;there exists&quot; by &quot;all&quot; in the above definitions (cf. Zadrozny 1987b). We will have, however, no need for &quot;strong&quot; notions in this paper. Also, in a practical system, &quot;satisfies&quot; should be probably replaced by &quot;violates fewest.&quot; Obviously, it is better to have references of pronouns resolved than not. After all, we assume that texts make sense, and that authors know these references. That applies to references of noun phrases too. On the other hand, there must be some restrictions on possible references; we would rather assume that &quot;spinach&quot; ~ &quot;train&quot; (i.e. V x,y)(spinach(x) & train(y) --, x # y)), or &quot;ship&quot; # &quot;disease.&quot; Two elementary conditions limiting the","@endWordPosition":"12868","@position":"78244","annotationId":"T72","@startWordPosition":"12867","@citStr":"Zadrozny 1987"}]},"title":{"#tail":"\n","#text":"Intended Models, Circumscription and Commonsense Reasoning.&quot;"},"booktitle":{"#tail":"\n","#text":"Proc. IJCAI-87:"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W Zadrozny"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Zadrozny, W. (1987b). &quot;A Theory of Default Reasoning.&quot; Proc. AAAI-87. Seattle, Washington: 385-390. Zadrozny, W. (unpublished). The Logic of Abduction."},"#text":"\n","pages":{"#tail":"\n","#text":"385--390"},"marker":{"#tail":"\n","#text":"Zadrozny, 1987"},"location":{"#tail":"\n","#text":"Seattle, Washington:"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"on in order to produce an interpretation of a text. The formalist will be presented in a number of steps in which we will elaborate one simple example: Example 1 Entering the port, a ship brought a disease. This sentence can be translated into the logical formula (ignoring only the past tense of &quot;bring&quot; and the progressive of &quot;enter'9: Definition S: enter(xl~ x2) & ship(x1) & port(x2) & bring(x3~ x4) & disease(x4) & xl = s & x2 = m & x3 = s & x4 = d, where s, m, d, are constants. We adopt he three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense r asoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural anguage inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating &quot;they&quot; to &quot;apples&quot; in the sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a): We bought the boys apples because they were so cheap 176 Zadrozny and Jensen Semantics of Paragraphs can be an example of such a most plausible choice. The main ideas of","@endWordPosition":"3408","@position":"21560","annotationId":"T73","@startWordPosition":"3407","@citStr":"Zadrozny 1987"},{"#tail":"\n","#text":"of text has to be extracted. It constrains interpretations of the predicates of an object theory. Its structure and the extraction methods will be discussed below. 4. Understanding has as its goal construction of an interpretation of the text, i.e. building some kind of model. Since not all logically permissible models are linguistically appropriate, one needs a place, namely the metalevel, to put constraints on types of models. Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them. We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referen- tial levels for commonsense r asoning--for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (1987, pp. 7-8)","@endWordPosition":"3694","@position":"23338","annotationId":"T74","@startWordPosition":"3693","@citStr":"Zadrozny 1987"},{"#tail":"\n","#text":"theories of &quot;enter,&quot; ship,&quot; etc. and the partial orders are represented graphically; more plausible theories are positioned higher. A path through this graph chooses an interpretation of the sentence S. For instance, the path f int = {el, shl~ pl, bl, dl} and S say together that A large boat (ship) that carries people or goods came into the harbor and carried a disease (illness). Since it is the &quot;highest&quot; path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence. Because it is also consistent, it will be chosen as a best interpretation of S, (cf. Zadrozny 1987a, 1987b). Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering. As it turns out, f~ is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story. The collection of these most plausible consistent interpretations of a given theory T is denoted by PT< (T). Then fint belongs to PT< (Th({S})), but","@endWordPosition":"6269","@position":"38796","annotationId":"T75","@startWordPosition":"6268","@citStr":"Zadrozny 1987"},{"#tail":"\n","#text":"dicates. Now we want to restrict he notion of a partial theory by introducing the formal notions of topic and coherence. We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions. The partial theories pick up from the referential level the most obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction i real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imp","@endWordPosition":"9071","@position":"55456","annotationId":"T76","@startWordPosition":"9070","@citStr":"Zadrozny 1987"},{"#tail":"\n","#text":"ed model of a theory T is an element of Mods(T) that satisfies metalevel con- straints contained in M. The set of all preferred models of T is denoted by PM(T). A formula 4 of L(=), the language with equality, is weakly R + M-abductible from an object heory T, denoted by T ~-a+M G iff there exists a partial theory T E PT(T) and a preferred model M E PM(T) such that M ~ G i.e. 4 is true in at least one preferred model of the partial theory T. Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing &quot;there exists&quot; by &quot;all&quot; in the above definitions (cf. Zadrozny 1987b). We will have, however, no need for &quot;strong&quot; notions in this paper. Also, in a practical system, &quot;satisfies&quot; should be probably replaced by &quot;violates fewest.&quot; Obviously, it is better to have references of pronouns resolved than not. After all, we assume that texts make sense, and that authors know these references. That applies to references of noun phrases too. On the other hand, there must be some restrictions on possible references; we would rather assume that &quot;spinach&quot; ~ &quot;train&quot; (i.e. V x,y)(spinach(x) & train(y) --, x # y)), or &quot;ship&quot; # &quot;disease.&quot; Two elementary conditions limiting the","@endWordPosition":"12868","@position":"78244","annotationId":"T77","@startWordPosition":"12867","@citStr":"Zadrozny 1987"}]},"title":{"#tail":"\n","#text":"A Theory of Default Reasoning.&quot;"},"booktitle":{"#tail":"\n","#text":"Proc. AAAI-87."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W Zadrozny"}}}]}}]}}
