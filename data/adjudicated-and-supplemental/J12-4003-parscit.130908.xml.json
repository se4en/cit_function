{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Gerber, Matthew and Joyce Chai. 2010. Beyond NomBank: A study of implicit arguments for nominal predicates. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583\u20131592, Uppsala."},"#text":"\n","pages":{"#tail":"\n","#text":"1583--1592"},"marker":{"#tail":"\n","#text":"Gerber, Chai, 2010"},"location":{"#tail":"\n","#text":"Uppsala."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":", and this section will identify the points most relevant to implicit argumentation. 2.1 Discourse Comprehension in Cognitive Science The traditional view of sentence-level semantics has been that meaning is compositional. That is, one can derive the meaning of a sentence by carefully composing the meanings of its constituent parts (Heim and Kratzer 1998). There are counterexamples to a compositional theory of semantics (e.g., idioms), but those are more the exception than the rule. Things change, however, when one starts to group sentences together 3 This article builds on our previous work (Gerber and Chai 2010). 757 Computational Linguistics Volume 38, Number 4 to form coherent textual discourses. Consider the following examples, borrowed from Sanford (1981, page 5): (4) Jill came bouncing down the stairs. (5) Harry rushed off to get the doctor. Examples (4) and (5) describe three events: bounce, rush, and get. These events are intricately related. One cannot simply create a conjunction of the propositions bounce, rush, and get and expect to arrive at the author\u2019s intended meaning, which presumably involves Jill\u2019s becoming injured by her fall and Harry\u2019s actions to help her. The mutual dependence of","@endWordPosition":"1288","@position":"8362","annotationId":"T1","@startWordPosition":"1285","@citStr":"Gerber and Chai 2010"},{"#tail":"\n","#text":"e fillers that are present in the discourse. 5. Evaluation 5.1 Data All evaluations in this study were performed using a randomized cross-validation configuration. The 1,247 predicate instances were annotated document by document. In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances. Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold. This evaluation set-up is an improvement versus the one we previously reported (Gerber and Chai 2010), in which fixed partitions were used for training, development, and testing. During training, the system was provided with annotated predicate instances. The system identified missing argument positions and generated a set of candidates for each such position. A candidate three-tuple (p, iargn, c') was given a positive label if the candidate implicit argument c (the primary filler) was annotated as filling the missing argument position; otherwise, the candidate three-tuple was given a negative label. During testing, the system was presented with each predicate instance and was required to ide","@endWordPosition":"10940","@position":"69120","annotationId":"T2","@startWordPosition":"10937","@citStr":"Gerber and Chai 2010"},{"#tail":"\n","#text":"otation would benefit the ten predicates considered. In order to estimate the potential benefits, we measured the effect of training set size on system performance. We retrained the discriminative model for each evaluation fold using incrementally larger subsets of the complete training set for the fold. Figure 2 shows the results, which indicate minimal gains beyond 80% of the training set. Based on these results, we feel that future work should emphasize feature and model development over training data expansion, as gains appear to trail off significantly. 6.2 Feature Assessment Previously (Gerber and Chai 2010), we assessed the importance of various implicit argument feature groups by conducting feature ablation tests. In each test, the discriminative model was retrained and reevaluated without a particular group of features. We summarize the findings of this study in this section. 6.2.1 Semantic Roles are Essential. We observed statistically significant losses when excluding features that relate the semantic roles of elements in c' to the semantic role of the missing argument position. For example, Feature 1 appears as the top-ranked feature in eight out of ten fold evaluations (see Appendix Table ","@endWordPosition":"12646","@position":"80222","annotationId":"T3","@startWordPosition":"12643","@citStr":"Gerber and Chai 2010"},{"#tail":"\n","#text":"i in Examples (52) and (53). In these sentences, Olivetti participates in the marked exporting and shipping events. Second, the model identified a tendency for exporters and shippers to also be sellers (e.g., Features 1, 4, and 23 made large contributions to the prediction). Using this knowledge, the system extracted information that could not be extracted by the baseline heuristic or a traditional SRL system. 787 Computational Linguistics Volume 38, Number 4 6.6 Comparison with Previous Results In a previous study, we reported initial results for the task of implicit argument identification (Gerber and Chai 2010). This article presents two major advancements versus our prior work. First, this article presents a more rigorous evaluation set-up, which was not used in our previous study. Our previous study used fixed partitions of training, development, and testing data. As a result, feature and model parameter selections overfit the development data; we observed a 23-point difference in F1 between the development (65%) and testing (42%) partitions. The small size of the testing set also led to small sample sizes and large p-values during significance testing. The crossvalidated approach reported in this","@endWordPosition":"14424","@position":"91512","annotationId":"T4","@startWordPosition":"14421","@citStr":"Gerber and Chai 2010"}]},"title":{"#tail":"\n","#text":"Beyond NomBank: A study of implicit arguments for nominal predicates."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matthew Gerber"},{"#tail":"\n","#text":"Joyce Chai"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Gerber, Matthew and Joyce Chai. 2010. Beyond NomBank: A study of implicit arguments for nominal predicates. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583\u20131592, Uppsala."},"#text":"\n","pages":{"#tail":"\n","#text":"1583--1592"},"marker":{"#tail":"\n","#text":"Gerber, Chai, 2010"},"location":{"#tail":"\n","#text":"Uppsala."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":", and this section will identify the points most relevant to implicit argumentation. 2.1 Discourse Comprehension in Cognitive Science The traditional view of sentence-level semantics has been that meaning is compositional. That is, one can derive the meaning of a sentence by carefully composing the meanings of its constituent parts (Heim and Kratzer 1998). There are counterexamples to a compositional theory of semantics (e.g., idioms), but those are more the exception than the rule. Things change, however, when one starts to group sentences together 3 This article builds on our previous work (Gerber and Chai 2010). 757 Computational Linguistics Volume 38, Number 4 to form coherent textual discourses. Consider the following examples, borrowed from Sanford (1981, page 5): (4) Jill came bouncing down the stairs. (5) Harry rushed off to get the doctor. Examples (4) and (5) describe three events: bounce, rush, and get. These events are intricately related. One cannot simply create a conjunction of the propositions bounce, rush, and get and expect to arrive at the author\u2019s intended meaning, which presumably involves Jill\u2019s becoming injured by her fall and Harry\u2019s actions to help her. The mutual dependence of","@endWordPosition":"1288","@position":"8362","annotationId":"T5","@startWordPosition":"1285","@citStr":"Gerber and Chai 2010"},{"#tail":"\n","#text":"e fillers that are present in the discourse. 5. Evaluation 5.1 Data All evaluations in this study were performed using a randomized cross-validation configuration. The 1,247 predicate instances were annotated document by document. In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances. Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold. This evaluation set-up is an improvement versus the one we previously reported (Gerber and Chai 2010), in which fixed partitions were used for training, development, and testing. During training, the system was provided with annotated predicate instances. The system identified missing argument positions and generated a set of candidates for each such position. A candidate three-tuple (p, iargn, c') was given a positive label if the candidate implicit argument c (the primary filler) was annotated as filling the missing argument position; otherwise, the candidate three-tuple was given a negative label. During testing, the system was presented with each predicate instance and was required to ide","@endWordPosition":"10940","@position":"69120","annotationId":"T6","@startWordPosition":"10937","@citStr":"Gerber and Chai 2010"},{"#tail":"\n","#text":"otation would benefit the ten predicates considered. In order to estimate the potential benefits, we measured the effect of training set size on system performance. We retrained the discriminative model for each evaluation fold using incrementally larger subsets of the complete training set for the fold. Figure 2 shows the results, which indicate minimal gains beyond 80% of the training set. Based on these results, we feel that future work should emphasize feature and model development over training data expansion, as gains appear to trail off significantly. 6.2 Feature Assessment Previously (Gerber and Chai 2010), we assessed the importance of various implicit argument feature groups by conducting feature ablation tests. In each test, the discriminative model was retrained and reevaluated without a particular group of features. We summarize the findings of this study in this section. 6.2.1 Semantic Roles are Essential. We observed statistically significant losses when excluding features that relate the semantic roles of elements in c' to the semantic role of the missing argument position. For example, Feature 1 appears as the top-ranked feature in eight out of ten fold evaluations (see Appendix Table ","@endWordPosition":"12646","@position":"80222","annotationId":"T7","@startWordPosition":"12643","@citStr":"Gerber and Chai 2010"},{"#tail":"\n","#text":"i in Examples (52) and (53). In these sentences, Olivetti participates in the marked exporting and shipping events. Second, the model identified a tendency for exporters and shippers to also be sellers (e.g., Features 1, 4, and 23 made large contributions to the prediction). Using this knowledge, the system extracted information that could not be extracted by the baseline heuristic or a traditional SRL system. 787 Computational Linguistics Volume 38, Number 4 6.6 Comparison with Previous Results In a previous study, we reported initial results for the task of implicit argument identification (Gerber and Chai 2010). This article presents two major advancements versus our prior work. First, this article presents a more rigorous evaluation set-up, which was not used in our previous study. Our previous study used fixed partitions of training, development, and testing data. As a result, feature and model parameter selections overfit the development data; we observed a 23-point difference in F1 between the development (65%) and testing (42%) partitions. The small size of the testing set also led to small sample sizes and large p-values during significance testing. The crossvalidated approach reported in this","@endWordPosition":"14424","@position":"91512","annotationId":"T8","@startWordPosition":"14421","@citStr":"Gerber and Chai 2010"}]},"title":{"#tail":"\n","#text":"Beyond NomBank: A study of implicit arguments for nominal predicates."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matthew Gerber"},{"#tail":"\n","#text":"Joyce Chai"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Liu, Chang and Hwee Ng. 2007. Learning predictive structures for semantic role labeling of nombank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208\u2013215, Prague."},"#text":"\n","pages":{"#tail":"\n","#text":"208--215"},"marker":{"#tail":"\n","#text":"Liu, Ng, 2007"},"location":{"#tail":"\n","#text":"Prague."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" find that implicit arguments add 71% to the argument structures that are present in NomBank. Using the corpus, we train a discriminative model that is able to identify implicit arguments with an F1 score of 50%, significantly outperforming an informed baseline model. This article describes our investigation, explores a wide variety offeatures important for the task, and discusses future directions for work on implicit argument identification. 1. Introduction Recent work has shown that semantic role labeling (SRL) can be applied to nominal predicates in much the same way as verbal predicates (Liu and Ng 2007; Johansson and Nugues 2008; Gerber, Chai, and Meyers 2009). In general, the nominal SRL problem is formulated as follows: Given a predicate that is annotated in NomBank as bearing arguments, identify these arguments within the clause or sentence that contains the predicate. As shown in our previous work (Gerber, Chai, and Meyers 2009), this problem definition ignores the important fact that many nominal predicates do not bear arguments in the local context. Such predicates need to be addressed in order for nominal SRL to be used by downstream applications such as automatic question answering,","@endWordPosition":"170","@position":"1153","annotationId":"T9","@startWordPosition":"167","@citStr":"Liu and Ng 2007"}},"title":{"#tail":"\n","#text":"Learning predictive structures for semantic role labeling of nombank."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Chang Liu"},{"#tail":"\n","#text":"Hwee Ng"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Ruppenhofer, Josef, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2009. Semeval-2010 task 10: Linking events and their participants in discourse. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 106\u2013111, Boulder, CO."},"#text":"\n","pages":{"#tail":"\n","#text":"106--111"},"marker":{"#tail":"\n","#text":"Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2009"},"location":{"#tail":"\n","#text":"Boulder, CO."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" In our previous work, we demonstrated the importance of filtering out nominal predicates that take no local arguments (Gerber, Chai, and Meyers 2009). This approach leads to appreciable gains for certain nominals. The approach does not attempt to actually recover implicit arguments, however. 4 Identification of the implicit patient in Example (17) (the ball) should be sensitive to the phenomenon of sense anaphora. If Example (16) was changed to \u201ca ball,\u201d then we would have no implicit patient in Example (17). 762 Gerber and Chai SRL of Implicit Arguments for Nominal Predicates Most recently, Ruppenhofer et al. (2009) proposed SemEval Task 10, \u201cLinking Events and Their Participants in Discourse,\u201d which evaluated implicit argument identification systems over a common test set. The task organizers annotated implicit arguments across entire passages, resulting in data that cover many distinct predicates, each associated with a small number of annotated instances. As described by Ruppenhofer et al. (2010), three submissions were made to the competition, with two of the submissions attempting the implicit argument identification part of the task. Chen et al. (2010) extended a standard SRL system by widening the","@endWordPosition":"3895","@position":"25043","annotationId":"T10","@startWordPosition":"3892","@citStr":"Ruppenhofer et al. (2009)"}},"title":{"#tail":"\n","#text":"Semeval-2010 task 10: Linking events and their participants in discourse."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Josef Ruppenhofer"},{"#tail":"\n","#text":"Caroline Sporleder"},{"#tail":"\n","#text":"Roser Morante"},{"#tail":"\n","#text":"Collin Baker"},{"#tail":"\n","#text":"Martha Palmer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Ruppenhofer, Josef, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. Semeval-2010 task 10: Linking events and their participants in discourse. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 45\u201350, Uppsala."},"#text":"\n","pages":{"#tail":"\n","#text":"45--50"},"marker":{"#tail":"\n","#text":"Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010"},"location":{"#tail":"\n","#text":"Uppsala."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"enomenon of sense anaphora. If Example (16) was changed to \u201ca ball,\u201d then we would have no implicit patient in Example (17). 762 Gerber and Chai SRL of Implicit Arguments for Nominal Predicates Most recently, Ruppenhofer et al. (2009) proposed SemEval Task 10, \u201cLinking Events and Their Participants in Discourse,\u201d which evaluated implicit argument identification systems over a common test set. The task organizers annotated implicit arguments across entire passages, resulting in data that cover many distinct predicates, each associated with a small number of annotated instances. As described by Ruppenhofer et al. (2010), three submissions were made to the competition, with two of the submissions attempting the implicit argument identification part of the task. Chen et al. (2010) extended a standard SRL system by widening the candidate window to include constituents from other sentences. A small number of features based on the FrameNet frame definitions were extracted for these candidates, and prediction was performed using a log-linear model. Tonelli and Delmonte (2010) also extended a standard SRL system. Both of these systems achieved an implicit argument F1 score of less than 0.02. The organizers and part","@endWordPosition":"3952","@position":"25434","annotationId":"T11","@startWordPosition":"3949","@citStr":"Ruppenhofer et al. (2010)"},{"#tail":"\n","#text":"roup was small, and the predicates were carefully chosen to maximize the observed frequency of implicit argumentation. We annotated a large number of implicit arguments for this group of predicates with the goal of training models that generalize well to the testing data. In the following section, we describe the implicit argument annotation process and resulting data set. 3. Implicit Argument Annotation and Analysis As shown in the previous section, the existence of implicit arguments has been recognized for quite some time. This type of information, however, was not formally annotated until Ruppenhofer et al. (2010) conducted their SemEval task on implicit argument identification. There are two reasons why we chose to create an independent data set for implicit arguments. The first reason is the aforementioned sparsity of the SemEval data set. The second reason is that the SemEval data set is not built on top of the Penn TreeBank, which is the gold-standard syntactic base for all work in this article. Working on top of the Penn TreeBank makes the annotations immediately compatible with PropBank, NomBank, and a host of other resources that also build on the TreeBank. 3.1 Data Annotation 3.1.1 Predicate Se","@endWordPosition":"4213","@position":"27113","annotationId":"T12","@startWordPosition":"4210","@citStr":"Ruppenhofer et al. (2010)"},{"#tail":"\n","#text":"e gold-standard PropBank and NomBank labels were used. Note, however, that many features were derived from the output of an automatic SRL process that occasionally produced errors (e.g., Feature 13, which used PMI scores between automatically identified arguments). These errors were present in both the training and evaluation stages. We also assumed the existence of gold-standard syntactic structure when possible. This was done in order to focus our investigation on the semantic nature of implicit arguments. 5.2 Scoring Metrics We evaluated system performance using the methodology proposed by Ruppenhofer et al. (2010). For each missing argument position of a predicate instance, the system was required to either (1) identify a single constituent that fills the missing argument position or (2) make no prediction and leave the missing argument position unfilled. To give partial credit for inexact argument bounds, we scored predictions using the Dice coefficient, which is defined as follows: 2 ∗ |Predicted � True| Dice(Predicted, True) = (10) |Predicted |+ |True| 780 Gerber and Chai SRL of Implicit Arguments for Nominal Predicates Predicted contains the tokens that the model believes fill the implicit argument","@endWordPosition":"11392","@position":"72192","annotationId":"T13","@startWordPosition":"11389","@citStr":"Ruppenhofer et al. (2010)"},{"#tail":"\n","#text":"sion ∗ Recall = 0.72 1 Precision + Recall We calculated the F1 score for the entire testing fold by aggregating the counts used in the above precision and recall calculations. Similarly, we aggregated the counts across all folds to arrive at a single F1 score for the evaluated system. We used a bootstrap resampling technique similar to those developed by Efron and Tibshirani (1993) to test the significance of the performance difference between various systems. Given a test pool comprising M missing argument positions iargn along with 11 Our evaluation methodology differs slightly from that of Ruppenhofer et al. (2010) in that we use the Dice metric to compute precision and recall, whereas Ruppenhofer et al. reported the Dice metric separately from exact-match precision and recall. = 0.6 781 Computational Linguistics Volume 38, Number 4 the predictions by systems A and B for each iargn, we calculated the exact p-value of the performance difference as follows: 1. Create r random resamples from M with replacement. 2. For each resample Ri, compute the system performance difference dRi = ARi − BRi and store dri in D. 3. Find the largest symmetric interval [min, max] around the mean of D that does not include ze","@endWordPosition":"11839","@position":"75004","annotationId":"T14","@startWordPosition":"11836","@citStr":"Ruppenhofer et al. (2010)"}]},"title":{"#tail":"\n","#text":"Semeval-2010 task 10: Linking events and their participants in discourse."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 5th International Workshop on Semantic Evaluation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Josef Ruppenhofer"},{"#tail":"\n","#text":"Caroline Sporleder"},{"#tail":"\n","#text":"Roser Morante"},{"#tail":"\n","#text":"Collin Baker"},{"#tail":"\n","#text":"Martha Palmer"}]}}]}}}}
