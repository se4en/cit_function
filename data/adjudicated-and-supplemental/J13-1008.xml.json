{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.918596666666667","#text":"\n? Nuance Communications, 505 First Ave. S, Suite 700, Seattle, WA 98104. E-mail: yuvalmarton@gmail.com.\n?? Center for Computational Learning, Columbia University. E-mail: habash@ccls.columbia.edu.\n? Center for Computational Learning, Columbia University. E-mail: rambow@ccls.columbia.edu.\n"},{"#tail":"\n","@confidence":"0.906047142857143","#text":"\n1. evaluating all our parsing models in both gold and non-gold conditions (where\nbefore this was true for only select models in Sections 4?5),\n2. using a newer version of our Arabic functional morphology resource (Section 5),\n3. evaluating several of our most notable parsing models with an additional parser\n(Section 6),\n4. exploring two additional training methods, as already mentioned above (Sec-\ntion 7), and\n"},{"#tail":"\n","@confidence":"0.871639888888889","#text":"\n one uses functional feature values (such as those used here for the first time in\nArabic NLP),\n one uses yet another representation level to account for the otherwise non-identity\nagreement patterns of irrational plurals,\n one handles the loss of overt number agreement in constructions such as VS\n(where the verb precedes its subject), and\n one adequately represents the otherwise ?inverse? number agreement (a phe-\nnomenon common to other Semitic languages, such as Hebrew, too).\n4. Basic Parsing Experiments\n"},{"#tail":"\n","@confidence":"0.879324625","#text":"\nComputational Linguistics Volume 39, Number 1\nupper bound for using these features). We started by looking at individual features\n(including POS tag sets) and their prediction accuracy. We then explored various feature\ncombinations in a hill-climbing fashion. We examined these issues in the following\norder:\n1. the contribution of POS tag sets to the parsing quality, as a function of the amount\nof information encoded in the tag set, using (a) gold input, and (b) machine-\npredicted POS tags;\n2. the contribution of numerous inflectional features in a controlled fashion, using (c)\ngold input and (d) machine-predicted input; (e) the prediction accuracy of each\ninflectional feature;\n3. the contribution of the lexical features in a similar fashion, again using (f) gold input\nand (g) predicted input; (h) the prediction accuracy of each lexical feature;\n4. (i) certain feature combinations and (j) the embedding of the best combination in the\nPOS tag set; and\n5. (k) further feature engineering of select useful features.\n"},{"#tail":"\n","@confidence":"0.507241666666667","#text":"\n11 Nivre (2008) reports that non-projective and pseudo-projective algorithms outperform the ?eager?\nprojective algorithm in MaltParser, but our training data did not contain any non-projective\ndependencies. The Nivre ?standard? algorithm is also reported there to do better on Arabic, but in a\npreliminary experimentation, it did slightly worse than the ?eager? one, perhaps due to the high\npercentage of right branching (left headed structures) in our Arabic training set?an observation already\nnoted in Nivre (2008).\n12 The terms feature and attribute are overloaded in the literature. We use them in the linguistic sense, unless\nspecifically noted otherwise, e.g., MaltParser feature(s).\n13 It is slightly different from the default configuration.\n"},{"#tail":"\n","@confidence":"0.394491","#text":"\n5. Parsing Experiments with Functional Features\n"},{"#tail":"\n","@confidence":"0.880964285714286","#text":"\ncomparing three variations:\n Training on gold feature values (as has been the case so far)\n Training on predicted feature values (as in Goldberg and Elhadad 2010)\n Training on the novel combination of gold and predicted features (denoted below\nas g+p)\n20 Recall that DET2 was only defined for MaltParser, and not for the Easy-First Parser.\n21 Although conceived independently, this hypothesis resembles self-training (McClosky, Charniak, and\n"},{"#tail":"\n","@confidence":"0.9334908","#text":"\n1. Using morphological features with the MaltParser and training on gold tags\n(Table 15).\n2. Using morphological features with the MaltParser and training on a combination\nof gold and predicted tags (Table 16).\n3. Using morphological features with the Easy-First parser and training on a combi-\n"}],"figure":[{"#tail":"\n","@confidence":"0.950981208333333","#text":"\nthe Arabic phrase \n\n\n\n /\n\n\n\n\n\n\n\n\n \n\n\n\nbAb AlsyAr Aljdyd/Aljdyd (?door the-car\nthe-newmasc.sg/fem.sg [lit.]),\n2 using syntactic agreement: if the-new is masculine (\n\n\n\n),\n"},{"#tail":"\n","@confidence":"0.6793195","#text":"\nBuckwalter 2007): (alphabetically) Abt?jHxd?rzs?SDTD???fqklmnhwy and the additional symbols: ? , ?\n\n,\nA? \n\n, A?\n\n, w? \n, y? \n,  , ? , a , u , i \n\n, ? , ? , u? , ?? \n\n.\n"},{"#tail":"\n","@confidence":"0.62315375","#text":"\n\n\n&quot; k-t-b (?writing related?) and\nthe pattern 1A2i3.3\n"},{"#tail":"\n","@confidence":"0.393093","#text":"\n4 PATB-tokenized words; see Section 2.5.\n5 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity.\n6 Note that the functional and form-based feature values for verbs always coincide.\n"},{"#tail":"\n","@confidence":"0.682951384615385","#text":"\n\n\n,\n\n\nAl?kyAt (?smart?), of the feminine plural (and rational) \n\n\n$% HafiydAt (?granddaugh-\nters?) is feminine plural; but the adjective, &\n\n( -\n AlHkwmy (?the-governmental?),\n"},{"#tail":"\n","@confidence":"0.985603519230769","#text":"\nVRB\n./0\n\n t?ml\n?work?\nMOD\nPRT\n1\n\n\n+ fy\n?in?\nOBJ\nNOM\n) 2\n3\n AlmdArs\n?the-schools?\nMOD\nNOM\n\n&\n\n( -\n AlHkwmy\n?the-governmental?\nSBJ\nNOM\n\n\n\n\n$% HfydAt\n?granddaughters?\nMOD\nNOM\n\n\n\n\n,\n\n Al?kyAt\n?smart?\nIDF\nNOM\n4\n\n\n\n! AlkAtb\n?the-writer?\n"},{"#tail":"\n","@confidence":"0.978141394736842","#text":"\nnormalized\nword-form\n99.3 29,737 collapse certain spelling variations into a single representation, e.g., 1\n\n5\n\n A?ly\n(?automatic?) and 15\n\nA?l? (?to?) are collapsed into 1\n\n5 Aly\nnon-normalized\nword-form\n98.9 29,980 ?raw? input (except for PATB segmentation), e.g., the uncollapsed forms\nabove\nLEMMA\n(diacritized)\n96.7 16,837 abstraction over inflected forms, e.g., the lemma of 4\n\n\n\n\n\n!\n\n( makAtib (?offices?)\nis 4\n\n\n\n-\n\n( maktab (?office?)\nLMM 98.3 15,305 undiacritized lemma (lemma with vowels and other diacritics removed),\ne.g., 4\n\n\n"},{"#tail":"\n","@confidence":"0.8720897","#text":"\n The node labeled 6\n\n\n\n ?yAm (?days?) should be the subject, not the object, of the\nmain verb 7( mrt (?passed?). Morphologically, 6\n\n\n\n ?yAm is masculine plural,\n"},{"#tail":"\n","@confidence":"0.994099554621849","#text":"\nComputational Linguistics Volume 39, Number 1\nBaseline\nV\n\n7( mrt\n?passed?\nMOD\nP\n19\n: ?l?\n?upon?\nOBJ\nN\n\n\n$\n\n\n\n%\n\nA?xtfA?\n?disappearance?\nIDF\nN\n.\n\n(\n\n7  Alzmyl\n?the colleague?\n<<<IDF>>>\nN\n)\n\n82\n3\n Almhnds\n?the engineer?\nMOD\nPN\n. . .\n<<<OBJ>>>\nN\n6\n\n\n\n ?yAm\n?days?\nBest\nV\n\n7( mrt\n?passed?\nMOD\nP\n19\n: ?l?\n?upon?\nOBJ\nN\n\n\n$\n\n\n\n%\n\nA?xtfA?\n?disappearance?\nIDF\nN\n.\n\n(\n\n7  Alzmyl\n?the colleague?\nMOD\nN\n)\n\n82\n3\n Almhnds\n?the engineer?\nMOD\nPN\n. . .\nSBJ\nN\n6\n\n\n\n ?yAm\n?days?\nFigure 2\nError analysis example. . . . )823  .\n\n(\n\n7  \n\n$\n\n\n\n%\n\n19\n: 6\n\n\n\n\n\n"}],"author":[{"#tail":"\n","@confidence":"0.790901","#text":"\nYuval Marton?\n"},{"#tail":"\n","@confidence":"0.888936","#text":"\nNizar Habash??\n"},{"#tail":"\n","@confidence":"0.952293","#text":"\nOwen Rambow?\n"}],"equation":[{"#tail":"\n","@confidence":"0.866814","#text":"\n\n\n\n"},{"#tail":"\n","@confidence":"0.8554828","#text":"\n\n\n\n!\n\n"},{"#tail":"\n","@confidence":"0.6336685","#text":"\nword form  \n\n\n\n"},{"#tail":"\n","@confidence":"0.928655666666667","#text":"\n\n\n$%/ \n\n\n$% Hafiyd+a/Hafiyd+At (?grand-\n"},{"#tail":"\n","@confidence":"0.9129306","#text":"\nlar adjective * \n\n ?zraq+? (?blue?) is \n+  zarqA?+? not &+ \n\n"},{"#tail":"\n","@confidence":"0.920075481481482","#text":"\nCATiB Annotation example. &\n\n( -\n ) 2\n3\n 1\n\n\n+\n\n\n\n\n,\n\n 4\n\n\n\n!\n\n\n\n\n$% ./0\n\n t?ml HfydAt AlkAtb\n"},{"#tail":"\n","@confidence":"0.899358","#text":"\n\n\n &quot; k t b (?writing-related?)\n"},{"#tail":"\n","@confidence":"0.36104","#text":"\nCORE12+DET+LMM+PERSON+FN*NG.20\n"},{"#tail":"\n","@confidence":"0.818361","#text":"\n\n(\n\n7  Alzmyl (?the colleague?),\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.918947","#text":"\n2.1 Morphemes\n"},{"#tail":"\n","@confidence":"0.84862","#text":"\n2.2 Lexeme, Lexical Features, and Inflectional Features\n"},{"#tail":"\n","@confidence":"0.6976","#text":"\n2.3 Form-Based vs. Functional Features\n"},{"#tail":"\n","@confidence":"0.580326","#text":"\n2.4 Morpho-Syntactic Interactions\n"},{"#tail":"\n","@confidence":"0.924335","#text":"\n2.6 Core POS Tag Sets\n"},{"#tail":"\n","@confidence":"0.878253","#text":"\n2.7 Extended POS Tag Sets\n"},{"#tail":"\n","@confidence":"0.998946","#text":"\n4.1 Data Sets and Parser\n"},{"#tail":"\n","@confidence":"0.972743","#text":"\n4.2 The Effect of POS Tag Richness on Parsing Quality\n"},{"#tail":"\n","@confidence":"0.990581","#text":"\n4.3 Inflectional Features and Their Contribution to Parsing Quality\n"},{"#tail":"\n","@confidence":"0.889896","#text":"\n4.4 Lexical Features and Their Contribution to Parsing Quality\n"},{"#tail":"\n","@confidence":"0.655578","#text":"\n4.5 Inflectional and Lexical Feature Combination and Its Contribution to\nParsing Quality\n"},{"#tail":"\n","@confidence":"0.995281","#text":"\n4.6 Additional Feature Engineering\n"},{"#tail":"\n","@confidence":"0.961868","#text":"\n5.1 Functional Feature Representation for Broken Plurals (using ElixirFM)\n"},{"#tail":"\n","@confidence":"0.998664","#text":"\n5.2 Functional Gender and Number Features, and the Rationality Feature\n"},{"#tail":"\n","@confidence":"0.91176","#text":"\n8.1 Validating Results on an Unseen Test Set\n"},{"#tail":"\n","@confidence":"0.993821","#text":"\n8.2 Best Results on Length-Filtered Input\n"},{"#tail":"\n","@confidence":"0.96677","#text":"\n8.3 Error Analysis\n"}],"footnote":[{"#tail":"\n","@confidence":"0.8727342","#text":"\ndetermine the upper bound for their contribution to parsing quality. Similar to previous\n1 Other morphological features, such as MOOD or ASPECT, do not interact with syntax at all. Note also that\nwe do not commit to a specific linguistic theory with these terms; hence, other theoretical terms such as\nthe Minimalist feature checking may be used here just as well.\n2 All Arabic transliterations are presented in the HSB transliteration scheme (Habash, Soudi, and\n"},{"#tail":"\n","@confidence":"0.709751","#text":"\n3 The digits in the pattern correspond to the positions where root radicals are inserted.\n"},{"#tail":"\n","@confidence":"0.567538666666667","#text":"\n7 We ignore the rare ?false idafa? construction (Habash 2010, p. 102).\n8 The 44 tags in CORE44 are based on the tokenized version of Arabic words. There are 34 untokenized core\ntags as used in MADA+TOKAN (Habash, Rambow, and Roth 2012).\n"},{"#tail":"\n","@confidence":"0.763261","#text":"\n16 http://sourceforge.net/projects/elixir-fm.\n17 We also applied the manipulations described in Section A.3 to FNNUM, giving us the variants\nFNNUMDGT and FNNUMDGTBIN, which we tested similarly.\n18 In this article, we use a newer version of the corpus by Alkuhlani and Habash (2011) than the one we\nused in Marton, Habash, and Rambow (2011).\n19 The paper by Alkuhlani and Habash (2012) presents additional, more sophisticated models that we do\nnot use in this article.\n"}],"construct":[{"#tail":"\n","@confidence":"0.4471422","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nTable 1\nPenn Arabic Treebank part 3 v3.1 data split.\nsplit # tokens # sentences sentence length\n(avg. # tokens)\n"},{"#tail":"\n","@confidence":"0.551839714285714","#text":"\nComputational Linguistics Volume 39, Number 1\nTable 2\nParsing performance with each POS tag set, on gold and predicted input. LAS = labeled\nattachment accuracy (dependency + relation). UAS = unlabeled attachment accuracy\n(dependency only). LS = relation label prediction accuracy. LAS diff = difference between labeled\nattachment accuracy on gold and predicted input. POS acc = POS tag prediction accuracy.\ntag set gold predicted gold-pred. POS tag setLAS UAS LS LAS UAS LS LAS diff. acc. size\n"}],"title":{"#tail":"\n","@confidence":"0.8136055","#text":"\nDependency Parsing of Modern Standard\nArabic with Lexical and Inflectional Features\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.987852689655172","#text":"\nAlkuhlani, Sarah and Nizar Habash. 2011.\nA corpus for modeling morpho-syntactic\nagreement in Arabic: Gender, number\nand rationality. In Proceedings of the 49th\nAnnual Meeting of the Association for\nComputational Linguistics (ACL),\npages 357?362, Portland, OR.\nAlkuhlani, Sarah and Nizar Habash. 2012.\nIdentifying broken plurals, irregular\ngender, and rationality in Arabic text. In\nProceedings of the 13th Conference of the\nEuropean Chapter of the Association for\nComputational Linguistics, pages 675?685,\nAvignon.\nBuchholz, Sabine and Erwin Marsi. 2006.\nCoNLL-X shared task on multilingual\ndependency parsing. In Proceedings of\nComputational Natural Language Learning\n(CoNLL), pages 149?164, New York, NY.\nBuckwalter, Timothy A. 2004. Buckwalter\nArabic Morphological Analyzer\nVersion 2.0. Linguistic Data Consortium,\nUniversity of Pennsylvania, 2002.\nLDC Catalog No.: LDC2004L02,\nISBN 1-58563-324-0.\nCollins, Michael, Jan Hajic, Lance Ramshaw,\nand Christoph Tillmann. 1999. A statistical\nparser for Czech. In Proceedings of the 37th\nAnnual Meeting of the Association for\nComputational Linguistics (ACL),\npages 505?512, College Park, MD.\nCowan, Brooke and Michael Collins.\n2005. Morphology and reranking for\nthe statistical parsing of Spanish.\nIn Proceedings of Human Language\nTechnology (HLT) and the Conference on\nEmpirical Methods in Natural Language\nProcessing (EMNLP), pages 795?802,\nMorristown, NJ.\nDada, Ali. 2007. Implementation of\nArabic numerals and their syntax in\nGF. In Proceedings of the Workshop on\nComputational Approaches to Semitic\nLanguages, pages 9?16, Prague.\nDiab, Mona. 2007. Towards an optimal\nPOS tag set for modern standard Arabic\nprocessing. In Proceedings of Recent\nAdvances in Natural Language Processing\n(RANLP), pages 91?96, Borovets.\nDiab, Mona and Yassine Benajiba.\n(in preparation). From raw text to base\nphrase chunks: The new generation of\nAMIRA Tools for the processing of\nModern Standard Arabic.\nDiab, Mona, Kadri Hacioglu, and Daniel\nJurafsky. 2004. Automatic tagging of\nArabic text: From raw text to base phrase\n23 Available for downloading at http://www1.ccls.columbia.edu/ CATiB/parser.\n"},{"#tail":"\n","@confidence":"0.999699521367521","#text":"\nComputational Linguistics Volume 39, Number 1\nchunks. In Proceedings of the 4th Meeting of\nthe North American Chapter of the Association\nfor Computational Linguistics (NAACL) -\nHuman Language Technology (HLT),\npages 149?152, Boston, MA.\nEryigit, G?lsen, Joakim Nivre, and Kemal\nOflazer. 2008. Dependency parsing of\nTurkish. Computational Linguistics,\n34(3):357?389.\nGoldberg, Yoav and Michael Elhadad. 2010.\nAn efficient algorithm for easy-first\nnon-directional dependency parsing.\nIn Proceedings of Human Language\nTechnology (HLT): The North American\nChapter of the Association for Computational\nLinguistics (NAACL), pages 742?750,\nLos Angeles, CA.\nGreen, Spence and Christopher D. Manning.\n2010. Better Arabic parsing: Baselines,\nevaluations, and analysis. In Proceedings of\nthe 23rd International Conference on\nComputational Linguistics (COLING),\npages 394?402, Beijing.\nHabash, Nizar. 2010. Introduction to Arabic\nNatural Language Processing. Morgan &\nClaypool Publishers.\nHabash, Nizar, Reem Faraj, and Ryan Roth.\n2009. Syntactic Annotation in the\nColumbia Arabic Treebank. In Proceedings\nof MEDAR International Conference on\nArabic Language Resources and Tools,\npages 125?135, Cairo.\nHabash, Nizar, Ryan Gabbard, Owen\nRambow, Seth Kulick, and Mitch Marcus.\n2007. Determining case in Arabic:\nLearning complex linguistic behavior\nrequires complex linguistic features. In\nProceedings of the 2007 Joint Conference on\nEmpirical Methods in Natural Language\nProcessing and Computational Natural\nLanguage Learning (EMNLP-CoNLL),\npages 1,084?1,092, Prague.\nHabash, Nizar and Owen Rambow. 2005.\nArabic tokenization, part-of-speech\ntagging and morphological\ndisambiguation in one fell swoop.\nIn Proceedings of the 43rd Annual Meeting of\nthe Association for Computational Linguistics\n(ACL), pages 573?580, Ann Arbor, MI.\nHabash, Nizar, Owen Rambow, and Ryan\nRoth. 2012. MADA+TOKAN Manual.\nTechnical Report CCLS-12-01, Columbia\nUniversity, New York, NY.\nHabash, Nizar and Ryan Roth. 2009. CATiB:\nThe Columbia Arabic treebank. In\nProceedings of the ACL-IJCNLP 2009\nConference Short Papers, pages 221?224,\nSuntec.\nHabash, Nizar, Abdelhadi Soudi, and\nTim Buckwalter. 2007. On Arabic\ntransliteration. In A. van den Bosch and\nA. Soudi, editors, Arabic Computational\nMorphology: Knowledge-based and Empirical\nMethods, pages 15?22. Springer, Berlin.\nHajic?, Jan and Barbora Vidov?-Hladk?. 1998.\nTagging inflective languages: Prediction\nof morphological categories for a rich,\nstructured tagset. In Proceedings of the\nInternational Conference on Computational\nLinguistics (COLING) - the Association for\nComputational Linguistics (ACL),\npages 483?490, Stroudsburg, PA.\nHohensee, Matt and Emily M. Bender.\n2012. Getting more from morphology\nin multilingual dependency parsing.\nIn Proceedings of the 2012 Conference of the\nNorth American Chapter of the Association\nfor Computational Linguistics: Human\nLanguage Technologies, pages 315?326,\nMontr?al.\nK?bler, Sandra, Ryan McDonald, and Joakim\nNivre. 2009. Dependency Parsing. Synthesis\nLectures on Human Language\nTechnologies. Morgan and Claypool\nPublishers.\nKulick, Seth, Ryan Gabbard, and Mitch\nMarcus. 2006. Parsing the Arabic Treebank:\nAnalysis and improvements. In Proceedings\nof the Treebanks and Linguistic Theories\nConference, pages 31?42, Prague.\nMaamouri, Mohamed, Ann Bies, Timothy A.\nBuckwalter, and Wigdan Mekki. 2004.\nThe Penn Arabic Treebank: Building a\nlarge-scale annotated Arabic corpus.\nIn Proceedings of the NEMLAR Conference on\nArabic Language Resources and Tools,\npages 102?109, Cairo.\nMarton, Yuval, Nizar Habash, and Owen\nRambow. 2010. Improving Arabic\ndependency parsing with inflectional and\nlexical morphological features. In\nProceedings of Workshop on Statistical\nParsing of Morphologically Rich Languages\n(SPMRL) at the 11th Meeting of the North\nAmerican Chapter of the Association for\nComputational Linguistics (NAACL) -\nHuman Language Technology (HLT),\npages 13?21, Los Angeles, CA.\nMarton, Yuval, Nizar Habash, and Owen\nRambow. 2011. Improving Arabic\ndependency parsing with lexical and\ninflectional surface and functional\nfeatures. In Proceedings of the 49th Annual\nMeeting of the Association for Computational\nLinguistics (ACL), pages 1,586?1,596,\nPortland, OR.\n"},{"#tail":"\n","@confidence":"0.999801653333333","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nMcClosky, David, Eugene Charniak, and\nMark Johnson. 2006. Effective self-training\nfor parsing. In Proceedings of the North\nAmerican Chapter of the Association for\nComputational Linguistics (NAACL) -\nHuman Language Technology (HLT),\npages 152?159, Brooklyn, New York.\nNilsson, Jens and Joakim Nivre. 2008.\nMaltEval: An evaluation and visualization\ntool for dependency parsing. In Proceedings\nof the sixth Conference on Language Resources\nand Evaluation (LREC), pages 161?166,\nMarrakech.\nNivre, Joakim. 2003. An efficient algorithm\nfor projective dependency parsing.\nIn Proceedings of the 8th International\nConference on Parsing Technologies\n(IWPT), pages 149?160, Nancy.\nNivre, Joakim. 2008. Algorithms for\ndeterministic incremental dependency\nparsing. Computational Linguistics,\n34(4):513?553.\nNivre, Joakim. 2009. Parsing Indian\nlanguages with MaltParser. In Proceedings\nof the ICON09 NLP Tools Contest:\nIndian Language Dependency Parsing,\npages 12?18, Hyderabad, India.\nNivre, Joakim, Igor M. Boguslavsky,\nand Leonid K. Iomdin. 2008.\nParsing the SynTagRus Treebank of\nRussian. In Proceedings of the 22nd\nInternational Conference on Computational\nLinguistics (COLING), pages 641?648,\nManchester.\nNivre, Joakim, Johan Hall, Jens Nilsson,\nAtanas Chanev, Gulsen Eryigit, Sandra\nKubler, Svetoslav Marinov, and Erwin\nMarsi. 2007. MaltParser: A language-\nindependent system for data-driven\ndependency parsing. Natural Language\nEngineering, 13(2):95?135.\nPetrov, Slav, Dipanjan Das, and Ryan\nMcDonald. 2012. A universal\npart-of-speech tagset. In Proceedings of the\nConference on Language Resources and\nEvaluation (LREC), pages 2,089?2,096.\nRambow, Owen, Bonnie Dorr, David Farwell,\nRebecca Green, Nizar Habash, Stephen\nHelmreich, Eduard Hovy, Lori Levin,\nKeith J. Miller, Teruko Mitamura,\nFlorence Reeder, and Siddharthan\nAdvaith. 2006. Parallel syntactic\nannotation of multiple languages.\nIn Proceedings of the Fifth Conference on\nLanguage Resources and Evaluation (LREC),\npages 559?564, Genoa.\nSmr?, Otakar. 2007. Functional Arabic\nMorphology. Formal System and\nImplementation. Ph.D. thesis, Charles\nUniversity, Prague.\nTsarfaty, Reut and Khalil Sima?an. 2007.\nThree-dimensional parametrization for\nparsing morphologically rich languages.\nIn Proceedings of the 10th International\nConference on Parsing Technologies (IWPT),\npages 156?167, Morristown, NJ.\nZitouni, Imed, Jeffrey S. Sorensen, and Ruhi\nSarikaya. 2006. Maximum entropy based\nrestoration of Arabic diacritics. In\nProceedings of the 21st International\nConference on Computational Linguistics\n(COLING) and the 44th Annual Meeting of\nthe Association for Computational Linguistics\n(ACL), pages 577?584, Sydney.\n"},{"#tail":"\n","@confidence":"0.6683755","#text":"\nComputational Linguistics Volume 39, Number 1\nA. Appendix: Additional Feature Engineering\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.998368277777778","#text":"\nWe explore the contribution of lexical and inflectional morphology features to dependency\nparsing of Arabic, a morphologically rich language with complex agreement patterns. Using con-\ntrolled experiments, we contrast the contribution of different part-of-speech (POS) tag sets and\nmorphological features in two input conditions: machine-predicted condition (in which POS tags\nand morphological feature values are automatically assigned), and gold condition (in which their\ntrue values are known). We find that more informative (fine-grained) tag sets are useful in the\ngold condition, but may be detrimental in the predicted condition, where they are outperformed\nby simpler but more accurately predicted tag sets. We identify a set of features (definiteness,\nperson, number, gender, and undiacritized lemma) that improve parsing quality in the predicted\ncondition, whereas other features are more useful in gold. We are the first to show that functional\nfeatures for gender and number (e.g., ?broken plurals?), and optionally the related rationality\n(?humanness?) feature, are more helpful for parsing than form-based gender and number. We\nfinally show that parsing quality in the predicted condition can dramatically improve by training\nin a combined gold+predicted condition. We experimented with two transition-based parsers,\nMaltParser and Easy-First Parser. Our findings are robust across parsers, models, and input\nconditions. This suggests that the contribution of the linguistic knowledge in the tag sets and\nfeatures we identified goes beyond particular experimental settings, and may be informative for\nother parsers and morphologically rich languages.\n"},{"#tail":"\n","@confidence":"0.984737","#text":"\nFor Arabic?as for other morphologically rich languages?the role of morphology is\noften expected to be essential in syntactic modeling, and the role of word order is less\nimportant than in morphologically poorer languages such as English. Morphology\n"},{"#tail":"\n","@confidence":"0.953437733333333","#text":"\nSubmission received: October 1, 2011; revised submission received: June 16, 2012; accepted for publication:\nAugust 3, 2012.\n? 2013 Association for Computational Linguistics\nComputational Linguistics Volume 39, Number 1\ninteracts with syntax in two ways: agreement and assignment. In agreement, there is\ncoordination between the morphological features of two words in a sentence based\non their syntactic configuration (e.g., subject?verb or noun?adjective agreement in\nGENDER and/or NUMBER). In assignment, specific morphological feature values are\nassigned in certain syntactic configurations (e.g., CASE assignment for the subject or\ndirect object of a verb).1\nParsing model design aims to come up with features that best help parsers learn\nthe syntax and choose among different parses. The choice of optimal linguistic features\ndepends on three factors: relevance, redundancy, and accuracy. A feature has relevance\nif it is useful in making an attachment (or labeling) decision. A particular feature may\nor may not be relevant to parsing. For example, the GENDER feature may help parse\n"},{"#tail":"\n","@confidence":"0.9685715","#text":"\nit should attach to the masculine door, resulting in the meaning ?the car?s new door?;\nif the-new is feminine ( \n"},{"#tail":"\n","@confidence":"0.958567961538462","#text":"\n), it should attach to the feminine the-car, resulting in ?the\ndoor of the new car.? Conversely, the ASPECT feature does not constrain any syntactic\ndecision. Even if relevant, a feature may not necessarily contribute to optimal perfor-\nmance because it may be redundant with other features that surpass it in relevance. For\nexample, as we will see, the DET and STATE features alone both help parsing because\nthey help identify the idafa construction, but they are redundant with each other and the\nDET feature is more helpful because it also helps with adjectival modification of nouns.\nFinally, the accuracy of automatically predicting the feature values (ratio of correct\npredictions out of all predictions) of course affects the value of a feature on unseen text.\nEven if relevant and non-redundent, a feature may be hard to predict with sufficient\naccuracy by current technology, in which case it will be of little or no help for parsing,\neven if helpful when its gold values are provided. As we will see, the CASE feature is\nvery relevant and not redundant, but it cannot be predicted with high accuracy and\noverall it is not useful.\nDifferent languages vary with respect to which features may be most helpful given\nvarious tradeoffs among these three factors. In the past, it has been shown that if we\ncan recognize the relevant morphological features in assignment configurations well\nenough, then they contribute to parsing accuracy. For example, modeling CASE in Czech\nimproves Czech parsing (Collins et al 1999): CASE is relevant, not redundant, and can\nbe predicted with sufficient accuracy. It has been more difficult showing that agreement\nmorphology helps parsing, however, with negative results for dependency parsing in\nseveral languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin\n2008; Nivre 2009).\nIn this article we investigate morphological features for dependency parsing of\nModern Standard Arabic (MSA). For MSA, the space of possible morphological features\nis fairly large. We determine which morphological features help and why. We further\n"},{"#tail":"\n","@confidence":"0.997152380952381","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nresults, assignment features, specifically CASE, are very helpful in MSA, though only\nunder gold conditions: Because CASE is rarely explicit in the typically undiacritized\nwritten MSA, it has a dismal accuracy rate, which makes it useless when used in a\nmachine-predicted (real, non-gold) condition. In contrast with previous results, we\nshow agreement features are quite helpful in both gold and predicted conditions. This\nis likely a result of MSA having a rich agreement system, covering both verb?subject\nand noun?adjective relations. The result holds for both the MaltParser (Nivre 2008) and\nthe Easy-First Parser (Goldberg and Elhadad 2010).\nAdditionally, almost all work to date in MSA morphological analysis and part-of-\nspeech (POS) tagging has concentrated on the morphemic form of the words. Often,\nhowever, the functional morphology (which is relevant to agreement, and relates to\nthe meaning of the word) is at odds with the ?surface? (form-based) morphology; a\nwell-known example of this are the ?broken? (irregular) plurals of nominals. We show\nthat by modeling the functional morphology rather than the form-based morphology,\nwe obtain a further increase in parsing performance (again, both when using gold and\nwhen using predicted POS and morphological features). To our knowledge, this work\nis the first to use functional morphology features in MSA processing.\nAs a further contribution of this article, we show that for parsing with pre-\ndicted POS and morphological features, training on a combination of gold and pre-\ndicted POS and morphological feature values outperforms the alternative training\nscenarios.\nThe article is structured as follows. We first present relevant Arabic linguistic facts,\ntheir representation in the annotated corpus we use, and variations of abstraction\nthereof in several POS tag sets (Section 2). We follow with a survey of related work\n(Section 3), and describe our basic experiments in Section 4. We first explore the con-\ntribution of various POS tag sets, (form-based) morphological features, and promising\ncombinations thereof, to Arabic dependency parsing quality?in straightforward fea-\nture engineering design and combination heuristics. We also explore more sophisticated\nfeature engineering for the determiner (DET) feature. In Section 5, we proceed to an\nextended exploration of functional features. This includes using functional NUMBER\nand GENDER feature values, instead of form-based values; using the non-form-based\nrationality (RAT) feature; and combinations thereof. We additionally consider the appli-\ncability of our results to a different parser (Section 6) and consider combining gold and\npredicted data for training (Section 7). Section 8 presents a result validation on unseen\ntest data, as well as an analysis of parsing error types under different conditions. We\nconclude and provide a download link to our model in Section 9. Last, we include an\nappendix with further explorations of PERSON feature engineering, ?binning? of Arabic\nnumber constructions according to their complex syntactic patterns, and embedding\nuseful morphological features in the POS tag set. Much of Sections 2?5 was presented\nin two previous publications (Marton, Habash, and Rambow 2010, 2011). This article\nextends that previous work by:\n"},{"#tail":"\n","@confidence":"0.90792675","#text":"\nComputational Linguistics Volume 39, Number 1\n5. providing an extended discussion and comparison of several notable and best\nperforming models, including analyses of their performance per dependency tag\n(Section 8).\n"},{"#tail":"\n","@confidence":"0.9988448","#text":"\nIn this section, we present the linguistic concepts relevant to our discussion of Arabic\nparsing, and the data we use for our experiments. We start with the central concept of\nthe morpheme followed by the more abstract concepts of the lexeme and lexical and\ninflectional features. Throughout this section, we use the term feature in its linguistic\nsense, as opposed to its machine learning sense that we use in Section 4. Discussions of\nthe challenges of form-based (morpheme-based) versus functional features on the one\nhand, and morpho-syntactic interactions on the other hand, follow. Finally, we present\nthe annotated corpus we use, and the various POS tag sets, that are extracted from this\ncorpus (in varying degrees of abstraction and lexicalization), and which we use in the\nrest of the article.\n"},{"#tail":"\n","@confidence":"0.998116166666667","#text":"\nWords can be described in terms of their morphemes (atomic units bearing mean-\ning); in Arabic, in addition to concatenative prefixes and suffixes, there are templatic\n(non-contiguous) morphemes called root and pattern. The root is typically a triplet\nof consonants (a.k.a. radicals). The pattern is a template made of vowels, sometimes\nadditional consonants, and place-holders for the root radicals. The root conveys some\nbase meaning, which patterns may modify in various ways. A combination of a root\nand a pattern is called a stem. More on root and pattern can be found in Section 2.2.\nArabic also includes a set of clitics that are tokenized in all Arabic treebanks, with the\nexception of the Arabic definite article,  Al+ (?the?), which is kept attached to the stem.\nWe consider the definite article a prefix, and its presence affects the value of the DET\nfeature in models containing it (see Section 4.3). An example of morphological analysis\nto the level of morphemes is the word  \n"},{"#tail":"\n","@confidence":"0.864564333333333","#text":"\nyu+kAtib+uwn (?they correspond?); it\nhas one prefix and one suffix (which at a deeper level may be viewed together as one\ncircumfix), in addition to a stem composed of the root \n"},{"#tail":"\n","@confidence":"0.949610888888889","#text":"\nArabic words can also be described in terms of lexemes and inflectional features. We\ndefine the lexeme as the set of word forms that only vary inflectionally among each\nother. A lemma is one of these word forms, used for representing the lexeme word set.\nFor example, Arabic verb lemmas are third-person masculine singular perfective. We\nexplore using both a diacritized LEMMA feature, and an undiacritized lemma (hereafter\nLMM). Just as the lemma abstracts over inflectional morphology, the root abstracts\nover both inflectional and derivational morphology and thus provides a deeper level\nof lexical abstraction, indicating the ?core? meaning of the word. The pattern is a\ngenerally complementary abstraction, sometimes indicating semantic notions such as\n"},{"#tail":"\n","@confidence":"0.992541857142857","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\ncausation and reflexiveness, among other things. We use the pattern of the lemma, not\nof the word form. We group the ROOT, PATTERN, LEMMA, and LMM in our discussion\nas lexical features (see Section 4.4). Nominal lexemes can be further classified into two\ngroups: denoting rational (i.e., human) entities, or irrational (i.e., non-human) entities.\nThe rationality (or RAT) feature interacts with syntactic agreement and other inflectional\nfeatures (discussed next); as such, we group it with those features in this article.\nThe inflectional features define the space of variations of the word forms associated\nwith a lexeme. Words4 vary along nine dimensions: GENDER, NUMBER, and PERSON (for\nnominals and verbs); ASPECT, VOICE, and MOOD (for verbs); and CASE, STATE (construct\nstate, idafa), and the attached definite article proclitic DET (for nominals). Inflectional\nfeatures abstract away from the specifics of morpheme forms. Some inflectional features\naffect more than one morpheme in the same word. For example, changing the value of\nthe ASPECT feature in the earlier example from imperfective to perfective yields the\n"},{"#tail":"\n","@confidence":"0.8527025","#text":"\n# kAtab+uwA (?they corresponded?), which differs in terms of prefix,\nsuffix, and pattern.\n"},{"#tail":"\n","@confidence":"0.974819727272727","#text":"\nSome inflectional features, specifically gender and number, are expressed using dif-\nferent morphemes in different words (even within the same POS). There are four\nsound gender-number suffixes in Arabic:5 +? (null morpheme) for masculine singular,\n\n+ + for feminine singular, + +wn for masculine plural, and + +At for feminine\nplural. Form-based GENDER and NUMBER feature values are set only according to these\nfour morphemes (and a few others, ignored for simplicity). There are exceptions and\nalternative ways to express GENDER and NUMBER, however, and functional feature\nvalues take them into account: Depending on the lexeme, plurality can be expressed\nusing sound plural suffixes or using a pattern change together with singular suffixes.\nA sound plural example is the word pair \n"},{"#tail":"\n","@confidence":"0.990197428571429","#text":"\ndaughter/granddaughters.) On the other hand, the plural of the inflectionally and\nmorphemically feminine singular word &' ( madras+a (?school?) is the word ) (\nmadAris+? (?schools?), which is feminine and plural inflectionally, but has a masculine\nsingular suffix. This irregular inflection, known as broken plural, is similar to the English\nmouse/mice, but is much more common in Arabic (over 50% of plurals in our training\ndata). A similar inconsistency appears in feminine nominals that are not inflected\nusing sound gender suffixes, for example, the feminine form of the masculine singu-\n"},{"#tail":"\n","@confidence":"0.9468218","#text":"\n* *?zraq+a. To address this\ninconsistency in the correspondence between inflectional features and morphemes, and\ninspired by Smr? (2007), we distinguish between two types of inflectional features: form-\nbased (a.k.a. surface, or illusory) features and functional features.6\nMost available Arabic NLP tools and resources model morphology using form-\nbased (?surface?) inflectional features, and do not mark rationality; this includes the\nPenn Arabic Treebank (PATB) (Maamouri et al 2004), the Buckwalter morphological\nanalyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis\nand Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash,\nRambow, and Roth 2012). The Elixir-FM analyzer (Smr? 2007) readily provides the\n"},{"#tail":"\n","@confidence":"0.9789338","#text":"\nComputational Linguistics Volume 39, Number 1\nfunctional inflectional number feature, but not full functional gender (only for adjectives\nand verbs but not for nouns), nor rationality. In this article, we use an in-house system\nwhich provides functional gender, number, and rationality features (Alkuhlani and\nHabash 2012). See Section 5.2 for more details.\n"},{"#tail":"\n","@confidence":"0.997502375","#text":"\nInflectional features and rationality interact with syntax in two ways. In agreement\nrelations, two words in a specific syntactic configuration have coordinated values for\nspecific sets of features. MSA has standard (i.e., matching value) agreement for subject?\nverb pairs on PERSON, GENDER, and NUMBER, and for noun?adjective pairs on NUMBER,\nGENDER, CASE, and DET. There are, however, three very common cases of exceptional\nagreement: Verbs preceding subjects are always singular, adjectives of irrational plural\nnouns are always feminine singular, and verbs whose subjects are irrational plural\nare also always feminine singular. See the example in Figure 1: the adjective, \n"},{"#tail":"\n","@confidence":"0.992537","#text":"\nof the feminine plural (and irrational) ) ( madAris (?schools?) is feminine singular.\nThis exceptional agreement is orthogonal to the form-function inconsistency discussed\nearlier. In other words, having a sound or broken plural has no bearing on whether the\nnoun is rational or not?and hence whether an adjectival modifier should agree with it\nby being feminine-singular or -plural. Note also that all agreement rules, including the\nexceptional agreement rules, refer to functional number and gender, not to form-based\nnumber and gender.\n"},{"#tail":"\n","@confidence":"0.7424995","#text":"\nAl?kyAt fy AlmdArs AlHkwmy (?The writer?s smart granddaughters work for public schools?).\nThe words in the tree are presented in the Arabic reading direction (from right to left).\n"},{"#tail":"\n","@confidence":"0.979751657142857","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nMSA exhibits assignment relations in CASE and STATE marking. Different types\nof dependents have different CASE, for example, verbal subjects are always marked\nNOMINATIVE (for a discussion of case in MSA, see Habash et al [2007]). STATE is a\nmarker on nouns; when a noun heads an idafa construction, its STATE is (?construct?).\nCASE and STATE are rarely explicitly manifested in undiacritized MSA. The DET feature\nplays an important role in distinguishing between N-N construct (idafa), in which only\nthe last noun bears the definite article,7 and N-A (noun-adjectival modifier), in which\nboth elements generally exhibit agreement in definiteness (and agreement in other\nfeatures, too). Although only N-N may be followed by additional N elements in Idafa\nrelation, both constructions may be followed by one or more adjectival modifiers.\nLexical features do not constrain syntactic structure as inflectional features do.\nInstead, bilexical dependencies are used to model semantic relations that often are the\nonly way to disambiguate among different possible syntactic structures.\n2.5 Corpus, CATiB Format, and the CATIB6 POS Tag Set\nWe use the Columbia Arabic Treebank (CATiB) (Habash and Roth 2009). Specifically,\nwe use the portion converted from Part 3 of the PATB to the CATiB format, which en-\nriches the CATiB dependency trees with full PATB morphological information. CATiB?s\ndependency representation is based on traditional Arabic grammar and emphasizes\nsyntactic case relations. It has a reduced POS tag set consisting of six tags only (hence-\nforth CATIB6). The tags are: NOM (non-proper nominals including nouns, pronouns,\nadjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS\n(passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX\n(punctuation). CATiB uses a standard set of eight dependency relations: SBJ and OBJ\nfor subject and (direct or indirect) object, respectively (whether they appear pre- or post-\nverbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and\nother less common relations that we will not discuss here. For other PATB-based POS\ntag sets, see Sections 2.6 and 2.7.\nThe CATiB Treebank uses the word segmentation of the PATB. It splits off several\ncategories of orthographic clitics, but not the definite article + Al+ (?the?). In all of\nthe experiments reported in this article, we use the gold segmentation. Tokenization in-\nvolves further decisions on the segmented token forms, such as spelling normalization,\nwhich we only briefly touch on here (in Section 4.1). An example CATiB dependency\ntree is shown in Figure 1. For the corpus statistics, see Table 1. For more information on\nCATiB, see Habash and Roth (2009) and Habash, Faraj, and Roth (2009).\n"},{"#tail":"\n","@confidence":"0.995014","#text":"\nLinguistically, words have associated POS tags, e.g., ?verb? or ?noun,? which further\nabstract over morphologically and syntactically similar lexemes. Traditional Arabic\ngrammars often describe a very general three-way distinction into verbs, nominals,\nand particles. In comparison, the tag set of the Buckwalter Morphological Analyzer\n(Buckwalter 2004) used in the PATB has a core POS set of 44 tags (CORE44) before mor-\nphological extension.8 Cross-linguistically, a core set containing around 12 tags is often\n"},{"#tail":"\n","@confidence":"0.986276625","#text":"\nComputational Linguistics Volume 39, Number 1\nassumed as a ?universal tag set? (Rambow et al 2006; Petrov, Das, and McDonald 2012).\nWe have adapted the list from Rambow et al (2006) for Arabic, and call it here CORE12. It\ncontains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun\n(PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle\n(PRT), abbreviation (AB), and punctuation (PNX). The CATIB6 tag set can be viewed as\na further reduction, with the exception that CATIB6 contains a passive voice tag (a mor-\nphological feature); this tag constitutes only 0.5% of the tags in the training, however.\n"},{"#tail":"\n","@confidence":"0.921243","#text":"\nThe notion of ?POS tag set? in natural language processing usually does not refer to\na core set. Instead, the Penn English Treebank (PTB) uses a set of 46 tags, including\nnot only the core POS, but also the complete set of morphological features (this tag set\nis still fairly small since English is morphologically impoverished). In PATB-tokenized\nMSA, the corresponding type of tag set (core POS extended with a complete description\nof morphology) would contain upwards of 2,000 tags, many of which are extremely\nrare (in our training corpus of about 300,000 words, we encounter only 430 POS tags\nwith complete morphology). Therefore, researchers have proposed tag sets for MSA\nwhose size is similar to that of the English PTB tag set, as this has proven to be a\nuseful size computationally. These tag sets are hybrids in the sense that they are neither\nsimply the core POS, nor the complete morphologically enriched tag set, but instead\nthey selectively enrich the core POS tag set with only certain morphological features.\nA more detailed discussion of the various available Arabic tag sets can be found in\nHabash (2010).\nThe following are the various tag sets we use in this article: (a) the core POS tag\nsets CORE44 and the newly introduced CORE12; (b) CATiB Treebank tag set (CATIB6)\n(Habash and Roth 2009) and its newly introduced extension of CATIBEX created using\nsimple regular expressions on word form, indicating particular morphemes such as the\nprefix  Al+ or the suffix  +wn; this tag set is the best-performing tag set for Arabic\non predicted values as reported in Section 4; (c) the PATB full tag set with complete\nmorphological tag (BW) (Buckwalter 2004); and two extensions of the PATB reduced\ntag set (PENN POS, a.k.a. RTS, size 24 [Diab, Hacioglu, and Jurafsky 2004]), both\noutperforming it: (d) Kulick, Gabbard, and Marcus (2006)?s tag set (KULICK), size 43,\none of whose most important extensions is the marking of the definite article clitic, and\n(e) Diab and Benajiba?s (in preparation) EXTENDED RTS tag set (ERTS), which marks\ngender, number, and definiteness, size 134.\n"},{"#tail":"\n","@confidence":"0.986514545454546","#text":"\nMuch work has been done on the use of morphological features for parsing of morpho-\nlogically rich languages. Collins et al (1999) report that an optimal tag set for parsing\nCzech consists of a basic POS tag plus a CASE feature (when applicable). This tag set\n(size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set\n(size ?3000+). They also report that the use of gender, number, and person features\ndid not yield any improvements. The results for Czech are the opposite of our results\nfor Arabic, as we will see. This may be due to CASE tagging having a lower error\nrate in Czech (5.0%) (Hajic? and Vidov?-Hladk? 1998) compared with Arabic (?14.0%,\nsee Table 3). Similarly, Cowan and Collins (2005) report that the use of a subset of\nSpanish morphological features (number for adjectives, determiners, nouns, pronouns,\nand verbs; and mode for verbs) outperforms other combinations. Our approach is\n"},{"#tail":"\n","@confidence":"0.987171677419355","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\ncomparable to their work in terms of its systematic exploration of the space of mor-\nphological features. We also find that the number feature helps for Arabic. Looking\nat Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima?an (2007) report\nthat extending POS and phrase structure tags with definiteness information helps\nunlexicalized PCFG parsing.\nAs for work on Arabic (MSA), results have been reported on the PATB (Kulick,\nGabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Depen-\ndency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash\nand Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation\nconsistency, and introduced an enhanced split-state constituency grammar, including\nlabels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports\nexperiments on Arabic parsing using his MaltParser (Nivre et al 2007), trained on the\nPADT. His results are not directly comparable to ours because of the different treebank\nrepresentations, even though all the experiments reported here were performed using\nthe MaltParser.\nOur results agree with previous work on Arabic and Hebrew in that marking the\ndefinite article is helpful for parsing. We go beyond previous work, however, and\nexplore additional lexical and inflectional features. Previous work with MaltParser in\nRussian, Turkish, and Hindi showed gains with CASE but not with agreement features\n(Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009).\nOur work is the first to show gains using agreement in MaltParser and in Arabic\ndependency parsing, and the first to use functional features for this task. Furthermore,\nwe demonstrate that our results carry over successfully to another parser, the Easy-First\nParser (Goldberg and Elhadad 2010) (Section 6).\nHohensee and Bender (2012) have conducted a study on dependency parsing for\n21 languages using features that encode whether the values for certain attributes are\nequal or not for a node and its governor. These features are potentially powerful,\nbecause they generalize to the very notion of agreement, away from the specific values\nof the attributes on which agreement occurs.9 We expect this kind of feature to yield\nlower gains for Arabic, unless:\n"},{"#tail":"\n","@confidence":"0.9555934","#text":"\nWe examined a large space of settings. In all our experiments, we contrasted the results\nobtained using machine-predicted input with the results obtained using gold input (the\n9 We do not relate to specific results in their study because it has been brought to our attention that\nHohensee and Bender (2012) are in the process of rechecking their code for errors, and rerunning their\nexperiments (personal communication).\n"},{"#tail":"\n","@confidence":"0.978010909090909","#text":"\nIn Section 5 we explore using functional (instead of form-based) feature values. In\nSection 6 we repeat key experiments with another parser, illustrating the robustness\nof our findings across these frameworks. In Section 7 we explore alternative training\nmethods, and their impact on key models.\nAll results are reported mainly in terms of labeled attachment accuracy score (the\nparent word and the type of dependency relation to it, abbreviated as LAS), which is also\nused for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment\naccuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS)\nare also given. For statistical significance, we use McNemar?s test on non-gold LAS, as\nimplemented by Nilsson and Nivre (2008). We denote p < 0.05 and p < 0.01 with + and\n++, respectively.\n"},{"#tail":"\n","@confidence":"0.981989928571429","#text":"\nFor all the experiments reported in this article, we used the training portion of PATB\nPart 3 v3.1 (Maamouri et al 2004), converted to the CATiB Treebank format, as men-\ntioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen,\nand Sarikaya (2006); and we further split the devtest into two equal parts: a devel-\nopment (dev) set and a blind test set. For all experiments, unless specified otherwise,\nwe used the dev set.10 We kept the test unseen (?blind?) during training and model\ndevelopment. Statistics about this split (after conversion to the CATiB dependency\nformat) are given in Table 1.\nFor all experiments reported in this section we used the syntactic dependency\nparser MaltParser v1.3 (Nivre 2003, 2008; K?bler, McDonald, and Nivre 2009), a\ntransition-based parser with an input buffer and a stack, which uses SVM classifiers\n10 We use the term ?dev set? to denote a non-blind test set, used for model development (feature selection\nand feature engineering). We do not perform further weight optimization (which, if done, is done on a\nseparate ?tuning set?).\n"},{"#tail":"\n","@confidence":"0.98754675","#text":"\nto predict the next state in the parse derivation. All experiments were done using the\nNivre ?eager? algorithm.11\nThere are five default attributes in the MaltParser terminology for each token in the\ntext: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word\nID), and deprel (the dependency relation between the current word and its parent).\nThere are default MaltParser features (in the machine learning sense),12 which are the\nvalues of functions over these attributes, serving as input to the MaltParser internal\nclassifiers. The most commonly used feature functions are the top of the input buffer\n(next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following\nitems on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser\nfeatures are defined as POS tag at stk[0], word-form at buf[0], and so on. K?bler,\nMcDonald, and Nivre (2009) describe a ?typical? MaltParser model configuration of\nattributes and features.13 Starting with it, in a series of initial controlled experiments,\nwe settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for\nPOS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] +\nstk[0]. We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]),\nldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively,\ndependents of the specified argument). This new MaltParser configuration resulted in\ngains of 0.3?1.1% in labeled attachment accuracy (depending on the POS tag set) over\nthe default MaltParser configuration. We also experimented with using normalized\nword-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is\ncommon in parsing and statistical machine translation literature, but it resulted in a\nsmall decrease in performance, so we settled on using non-normalized word-forms. All\nexperiments reported here were conducted using this new configuration. To recap, it has\nthe following MaltParser attributes (machine learning features): 4 word-form attributes,\n7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre\n?eager? algorithm), totaling 16 attributes and two more for every new feature described\nin Section 4.3 and on (e.g., CASE).\n"},{"#tail":"\n","@confidence":"0.968154266666667","#text":"\nIn this section, we compare the effect on parsing quality of a number of POS tag sets\nvarying in their richness, in both gold and predicted settings.\nGold POS tag values. We turn first to the contribution of POS information to parsing\nquality, as a function of the amount of information encoded in the POS tag set (i.e., the\nrelevance of a tag set). A first rough estimation for the amount of information is the actual\ntag set size, as it appears in the training data. For this purpose we compared the POS\ntag sets discussed in sections 2.6 and 2.7. In optimal conditions (using gold POS tags),\nthe richest tag set (BW) is indeed the best performer (84.0%), and the poorest (CATIB6) is\nthe worst (81.0%). Mid-size tag sets are in the high (82%), with the notable exception of\nKULICK, which does better than ERTS, in spite of having one fourth the tag set size; more-\nover, it is the best performer in unlabeled attachment accuracy (86.0%), in spite of being\nless than tenth the size of BW. Our extended mid-size tag set, CATIBEX, was a mid-level\nperformer as expected. Columns 2?4 in Table 2 show results with gold input, and the\nrightmost column shows the number of tag types actually occurring in the training data.\nPredicted POS tag values. So far we discussed optimal (gold) conditions. But in prac-\ntice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as\ninput, as opposed to gold (human-annotated) tags.14 The more informative the tag set,\nthe less accurate the tag prediction might be, so the effect on overall parsing quality\nis unclear. Put differently, we are interested in the tradeoff between relevance and accu-\nracy. Therefore, we repeated the experiments with POS tags predicted by the MADA\ntoolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012)15 (see Table 2,\n14 Some parsers predict POS tags internally, instead of receiving them as input, but this is not the case in this\narticle.\n15 We use MADA v3.1 in all of our experiments. We note that MADA v3.1 was tuned on the same\ndevelopment set that we use for making our parsing model choices; ideally, we would have chosen a\ndifferent development set for our work on parsing, but we thought it would be best to use MADA as a\nblack box component (for past and future comparability), and did not have sufficient data to carve out\nfrom a second development set (while retaining a test set). We do not take this as a major concern for our\nresults. In fact, although MADA was tuned to maximize its core POS accuracy (the untokenized version\nof CORE44), CORE44 did not yield best parsing quality on MADA-predicted input (see Table 2).\n"},{"#tail":"\n","@confidence":"0.996947076923077","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\ncolumns 5?7). It turned out that BW, the best gold performer but with lowest POS pre-\ndiction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer\nwith predicted tags. The simplest tag set, CATIB6, and its extension, CATIBEX, benefited\nfrom the highest POS prediction accuracy (97.7%), and their performance suffered the\nleast. CATIBEX was the best performer with predicted POS tags. Performance drop and\nPOS prediction accuracy are given in columns 8 and 9.\nThese results suggest that POS tag set accuracy is as important to parsing quality,\nif not more important, than its relevance. In other words, when designing a parsing\nmodel, one might want to consider that in the tradeoff, mediocre accuracy may be worse\nthan mediocre relevance. Later we see a similar trend for other features as well (e.g.,\nCASE in Section 4.3). In Section 7 we also present a training method that largely mitigates\n(but doesn?t resolve) this issue of mediocre accuracy of relevant features.\n"},{"#tail":"\n","@confidence":"0.997907757575758","#text":"\nExperimenting with inflectional features is especially important in Arabic parsing,\nas it is morphologically rich. In order to explore the contribution of inflectional and\nlexical information in a controlled manner, we focused on the best performing core\n(?morphology-free?) POS tag set, CORE12, as baseline; using three different set-ups,\nwe added nine inflectional features (with either gold values, or with values predicted\nby MADA): DET (presence of determiner), PERSON, ASPECT, VOICE, MOOD, GENDER,\nNUMBER, STATE, and CASE. For a brief reminder and examples for each feature, see the\nrightmost column in Table 3, or for more details refer back to Section 2.\nIn set-up All, we augmented the baseline model with all nine features (as nine\nadditional MaltParser attributes); in set-up Sep, we augmented the baseline model with\neach of these features, one at a time, separately; and in set-up Greedy, we combined\nthem in a greedy heuristic (since the entire feature space is too vast to exhaust): starting\nwith the most gainful feature from Sep, adding the next most gainful feature, keeping\nit if it helped, or discarding it otherwise, and repeating this heuristics through the least\ngainful feature. See Table 4.\nGold feature values. We applied the three setups (All, Sep, and Greedy) with gold POS\ntags and gold morphological tags, to examine the contribution of the morphological\nfeatures in optimal conditions. The top left section of Table 4 shows that applying all\ninflectional features together yields gains over the baseline. Examining the contribution\nof each feature separately (second top left Sep section), we see that CASE, followed by\nSTATE and DET, were the top contributors. Performance of CASE is the notable difference\nfrom the predicted conditions (see following discussion). No single feature outper-\nformed the All set-up in gold. Surprisingly, only CASE and STATE helped in the Greedy\nset-up (85.4%, our highest result in gold), although one might expect feature DET to have\nhelped, too (since it is highly relevant: It participates in agreement, and interacts with\nthe idafa construction). This shows that there is redundancy in the information provided\nby DET on the one hand and CASE and STATE on the other, presumably because both\nsets of feature help identify the same construction, idafa.\nPredicted feature values. We re-applied the three set-ups with predicted feature values\n(right-hand side half of Table 4). Set-up All hurts performance on the machine-predicted\ninput. This can be explained if one examines the prediction accuracy of each feature (top\nhalf, third section of Table 3). Features which are not predicted with very high accuracy,\nsuch as CASE (86.3%), can dominate the negative contribution, even though they are\n"},{"#tail":"\n","@confidence":"0.8007244","#text":"\nComputational Linguistics Volume 39, Number 1\nTable 3\nPrediction accuracy, value set sizes, descriptions, and value examples of features used in this\nwork. Accuracy was measured over the development set. * = The set includes a ?N/A? value(s).\nfeature acc. set size comments and examples\n"},{"#tail":"\n","@confidence":"0.473332","#text":"\n-( mktb for the example above.\nROOT 98.4 9,646 further abstraction over inflection and patterns; typically a consonant\ntriplet, a.k.a. radicals, e.g., \n"},{"#tail":"\n","@confidence":"0.9038226","#text":"\nPATTERN 97.0 338 sequence of vowels and consonants with placeholders for the root radicals,\ne.g., ma12a3 (?location-related?); typically a derivational modification to the\nbasic meaning of the root, such as a location or instrument, but inflectional\nvariations such as aspect, voice, number and gender also exist; we use the\npattern of the lemma, not the inflected form, which may differ in cases such\n"},{"#tail":"\n","@confidence":"0.993459285714286","#text":"\ntop contributors, that is, highly relevant, in optimal (gold) conditions (see previous\nparagraph). The determiner feature (DET), followed by the STATE feature, were top\nindividual contributors in set-up Sep. Adding the features that participate in agreement,\nnamely, DET and the PNG features (PERSON, NUMBER, GENDER), in the Greedy set-up,\nyielded a 1.4% gain over the CORE12 baseline. These results suggest that for a successful\nfeature combination, one should take into account not only the relevance of the features,\nbut also their accuracy.\n"},{"#tail":"\n","@confidence":"0.7589791","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nTable 4\nCORE12 POS tag set with morphological inflectional features. Left half: Using gold POS tag and\nfeature values. In it: Top part (All ): Adding all nine inflectional features to CORE12. Second part\n(Sep): Adding each feature separately to CORE12. Third part (Greedy): Greedily adding next\nbest feature from Sep, and keeping it if improving score. Right half: Same as left half, but with\npredicted POS tag and feature values. Statistical significance tested only on predicted (non-gold)\ninput, against the CORE12 baseline.\ngold POS and feature values predicted POS and feature values\nSet-up CORE12+. . . LAS UAS LS CORE12+. . . LAS UAS LS\n"},{"#tail":"\n","@confidence":"0.998992777777778","#text":"\nNext, we experimented with adding the lexical features, which involve semantic ab-\nstraction to some degree: the diacritized LEMMA, the undiacritized lemma (LMM), the\nROOT, and the PATTERN (which is the pattern of the LEMMA). A notable advantage\nof lexical abstraction is that it reduces data sparseness, and explicitly ties together\nsemantically related words. We experimented with the same set-ups as above: All, Sep,\nand Greedy.\nGold feature values. The left-hand side half of Table 5 shows that adding all four\nfeatures yielded gains similar to adding a lemma feature separately. With gold tags,\nhowever, no proper subset of the lexical features beats the set of all lexical features.\nPredicted feature values. The right-hand side of Table 5 shows that adding all four\nfeatures yielded a minor gain in set-up All. LMM was the best single contributor, closely\nfollowed by ROOT in Sep. CORE12+LMM+ROOT (with or without LEMMA) was the best\ngreedy combination in set-up Greedy, and also provides the best performance of all\nexperiments with lexical features only. Due to the high redundancy of LEMMA and LMM\n(only 0.01% absolute gain when adding LEMMA in the Greedy set-up, which appears\nlarger only due to rounding in the table), we do not consider LEMMA in feature combina-\ntions from this point on. Note, however, that LEMMA?and all the lexical features?are\npredicted with high accuracy (top half, second section of Table 3).\n"},{"#tail":"\n","@confidence":"0.99732","#text":"\nWe now combine morphological and lexical features. Following the same greedy\nheuristic as in the previous sections, we augmented the best inflection-based model\nCORE12+DET+PNG with lexical features, and found that the undiacritized lemma (LMM)\nimproved performance on predicted input (80.2%) (see Table 6). Adding more lexical\nfeatures does not help, however, suggesting that some of the information in the lexical\nfeatures is redundant with the information in the morphological features. See the Ap-\npendix, Section A.1, for our attempt to extend the tag set by embedding the best feature\ncombination in it.\n"},{"#tail":"\n","@confidence":"0.98092275","#text":"\nSo far we have experimented with morphological feature values as extracted from\nthe PATB (gold) or predicted by MADA; we also used the same MaltParser feature\nconfiguration for all added features (i.e., stk[0] + buf[0]). It is likely, however, that from\na machine-learning perspective, representing similar categories with the same tag, or\nTable 6\nModels with inflectional and lexical morphological features together (predicted value-guided\nheuristic). Statistical significance tested only on predicted input, against the CORE12 baseline.\ntag set gold predictedLAS UAS LS LAS UAS LS\n"},{"#tail":"\n","@confidence":"0.940876235294118","#text":"\ntaking into account further-away tokens in the sentence, may be useful for learning.\nTherefore, we next experimented with modifying some inflectional features that proved\nmost useful in predicted input.\nAs DET may help disambiguate N-N / N-A constructions (and N-N-N, N-A-A, . . . ,\nsee Section 2), we attempted modeling the DET values of previous and next elements\n(as MaltParser?s stk[1] + buf[1], in addition to the modeled stk[0] + buf[0]). This vari-\nant, denoted DET2, indeed helps: When added to the CORE12 baseline model, DET2\nimproves non-gold parsing quality by more than 0.3%, compared to DET, as shown in\nTable 7. This variant yields a small improvement also when used in combination with\nthe PNG and LMM features, as shown in the second part of Table 7?but only in gold.\nThese results suggest an intricate interaction between the extended relevance of the\ndeterminer feature, and its redundancy with the PNG features (and note that all fea-\ntures involved are predicted with high accuracy). A possible explanation might be that\nform-based feature representation is inherently inadequate here, and therefore its high\naccuracy may not be very indicative. We explore non-form-based (functional) feature\nrepresentation in Section 5. For more on our feature engineering, see the Appendix,\nSection A.2.\n"},{"#tail":"\n","@confidence":"0.99898575","#text":"\nSection 4 explored the contribution of various POS tag sets, (form-based) morphological\nfeatures, and promising combinations thereof, to Arabic dependency parsing quality?\nin straightforward feature engineering design and combination heuristics. This section\nexplores more sophisticated feature engineering: using functional NUMBER and GENDER\nfeature values, instead of form-based values; using the non-form-based rationality (RAT)\nfeature; and combinations thereof. For additional experiments regarding alternative\nrepresentation for digit tokens, and the ?binning? Arabic number constructions accord-\ning to their complex syntactic patterns, see the Appendix, Section A.3.\n"},{"#tail":"\n","@confidence":"0.98289225","#text":"\nThe NUMBER feature we have thus far extracted from PATB with MADA only reflects\nform-based (as opposed to functional) values, namely, broken plurals are marked as\nsingular. This might have a negative effect for learning generalizations over the complex\nagreement patterns in MSA, beyond memorization of word pairs seen together in\n"},{"#tail":"\n","@confidence":"0.986707105263158","#text":"\nComputational Linguistics Volume 39, Number 1\ntraining. To address this issue, one can use the Arabic morphological tool ElixirFM16\n(Smr? 2007). For each given word form, it outputs a list of possible analyses, each\ncontaining a lemma and a functional NUMBER (and other features). We replaced the\nsurface NUMBER value for all nominals marked as singular in our data with ElixirFM?s\nfunctional value, using the MADA-predicted lemma to disambiguate multiple ElixirFM\nanalyses. These experiments are denoted with FNNUM. In training, of the lemma types\nsent to ElixirFM for analysis, about 20% received no analysis (OOV). A manual observa-\ntion of a small sample revealed that at least half of those were proper names (and hence\ntheir NUMBER value would have stayed singular). Almost 9% of the ElixirFM-analyzed\ntypes (over 7% of the tokens) changed their NUMBER value. In the dev set, the OOV\nrate was less than 9%, and almost 11% of the ElixirFM-analyzed types changed their\nNUMBER value. This amounts to 4.4% of all tokens.\nWe used ElixirFM to determine the values for FNNUM, the functional number\nfeature. We used this feature in our best model so far, CORE12+DET+PNG+LMM, instead\nof the form-based NUMBER feature.17 The ElixirFM-based models yielded small gains\nof up to 0.1% over this best model on predicted input. We then modified the ElixirFM-\nbased best model to use the enhanced DET2 feature. This variation yielded a similarly\nsmall gain, altogether less than 0.2% from its ElixirFM-free counterparts.\n"},{"#tail":"\n","@confidence":"0.995949409090909","#text":"\nThe ElixirFM lexical resource used previously provided functional NUMBER feature\nvalues but no functional GENDER values, nor RAT (rationality, or humanness) values.\nTo address this issue, we use a version of the PATB3 training and dev sets manually\nannotated with functional gender, number, and rationality (Alkuhlani and Habash\n2011).18 This is the first resource providing all three features (ElixirFm only provides\nfunctional number, and to some extent functional gender). We conducted experiments\nwith gold features to assess the potential of these features, and with predicted fea-\ntures, obtained from training a simple maximum likelihood estimation classifier on this\nresource (Alkuhlani and Habash 2012).19 The first part of Table 8 shows that the RAT\n(rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains\nin machine-predicted input). The next two parts show the advantages of functional\ngender and number (denoted with a FN* prefix) over their surface-based counterparts.\nThe fourth part of the table shows the combination of these functional features with\nthe other features that participated in the best combination so far (LMM, the extended\nDET2, and PERSON); without RAT, this combination is at least as useful as its form-based\ncounterpart, in both gold and predicted input; adding RAT to this combination yields\n0.4% (absolute) gain in gold, offering further support to the relevance of the rationality\nfeature, but a slight decrease in predicted input, presumably due to insufficient accuracy\nagain. The last part of the table revalidates the gains achieved with the best controlled\nfeature combination, using CATIBEX?the best performing tag set with predicted in-\nput. Note, however, that the 1% (absolute) advantage of CATIBEX (without additional\nfeatures) over the morphology-free CORE12 on machine-predicted input (Table 2) has\n"},{"#tail":"\n","@confidence":"0.633468428571429","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nTable 8\nModels with functional features: GENDER, NUMBER, rationality (RAT). FN* = functional\nfeature(s) based on Alkuhlani and Habash (2011); GN = GENDER+NUMBER; GNR = GENDER+\nNUMBER+RAT. Statistical significance tested only for CORE12+. . . models on predicted input,\nagainst the CORE12 baseline.\nmodel (POS tag set and features) gold predictedLAS UAS LS LAS UAS LS\n"},{"#tail":"\n","@confidence":"0.928832884615385","#text":"\nshrunk with these functional feature combinations to 0.3%. We take it as further support\nto the relevance of our functional morphology features, and their partial redundancy\nwith the form-based morphological information embedded in the CATIBEX POS tags.\n6. Evaluation of Results with Easy-First Parser\nIn this section, we validate the contribution of key tag sets and morphological features?\nand combinations thereof?using a different parser: the Easy-First Parser (Goldberg and\nElhadad 2010). As in Section 4, all models are evaluated on both gold and non-gold\n(machine-predicted) feature values.\nThe Easy-First Parser is a shift-reduce parser (as is MaltParser). Unlike MaltParser,\nhowever, it does not attempt to attach arcs ?eagerly? as early as possible (as in previous\nsections), or at the latest possible stage (an option we abandoned early on in preliminary\nexperiments). Instead, the Easy-First Parser keeps a stack of partially built treelets, and\nattaches them to one another in order of confidence (from high confidence, ?easy?\nattachment, to low, as estimated by the classifier). Labeling the relation arcs is done\nin a second pass, with a separate training step, after all attachments have been decided\n(the code for which was added after the publication of Goldberg and Elhadad (2010),\nwhich only included an unlabeled attachment version).\nSetting machine-learning features for Easy-First Parser is not as simple and elegant\nas for MaltParser, but it gives the feature designer greater flexibility. For example, the\nPOS tag can be dynamically split (or not) according to the token?s word-form and/or\nthe already-built attachment treelets, whereas in MaltParser, one can meld several\nfeatures into a single complex feature only if applied unconditionally to all tokens.\nThe Easy-First Parser?s first version comes with the code for the features used in its\nfirst publication. These include POS tag splitting and feature melding for prepositional\nattachment chains (e.g., parent-preposition-child). For greater control of the contribu-\ntion of the various POS tag and morphological features in the experiments, and for\n"},{"#tail":"\n","@confidence":"0.987972071428571","#text":"\nComputational Linguistics Volume 39, Number 1\na better ?apples-to-apples? comparison with MaltParser (as used here), we disabled\nthese features, and instead used features (and selected feature melding) that were as\nequivalent to MaltParser as possible.\nTable 9 shows results with Easy-First Parser. Results with Easy-First Parser are\nconsistently higher than the corresponding results with MaltParser, with similar trends\nfor the various features? contribution: Functional GENDER and NUMBER features con-\ntribute more than their form-based counterparts, in both gold and predicted conditions;\nrationality (RAT) as a single feature on top of the POS tag set helps in gold (and with\nEasy-First Parser, also in predicted conditions)?but when used in combination with\nPERSON, LMM, functional GENDER, and NUMBER, it actually slightly lowers parsing\nscores in predicted conditions (but with Easy-First Parser, it helps in gold conditions);\nDET is the most useful single feature in predicted conditions (from those we tried here);\nand the best performing model in predicted conditions is the same as with MaltParser:\n"},{"#tail":"\n","@confidence":"0.986296842105263","#text":"\nAs before, we see that the patterns of gain achieved with the ?morphology-free?\nCORE12 hold also for CATIBEX, the best performing tag set on predicted input. Inter-\nestingly, with this parser, the greater 1.6% (absolute) advantage of CATIBEX (without\nadditional features) over the morphology-free CORE12 on machine-predicted input\n(compare with only 1% in MaltParser in Table 2) has shrunk completely with these\nfunctional feature combinations. This suggests that Easy-First Parser is more resilient\nto accuracy errors (presumably due to its design to make less ambiguous decisions\nearlier), and hence can take better advantage of the relevant information encoded in\nour functional morphology features.\n7. Combined Gold and Predicted Features for Training\nSo far, we have only evaluated models trained on gold POS tag set and morphological\nfeature values. Some researchers, however, including Goldberg and Elhadad (2010),\ntrain on predicted feature values instead. It makes sense that training on predicted\nfeatures yields better scores for evaluation on predicted features, since the training\nbetter resembles the test. But we argue that it also makes sense that training on a\ncombination of gold and predicted features (one copy of each) might do even better,\nbecause good predictions of feature values are reinforced (since they repeat the gold\npatterns), whereas noisy predicted feature values are still represented in training (in\npatterns that do not repeat the gold).21 To test our hypothesis, we start this section by\n"},{"#tail":"\n","@confidence":"0.9465368","#text":"\nJohnson 2006), where the parser is re-trained on its own predicted parsing output, together with the\noriginal labeled training data. Note, however, that we re-train on gold and predicted feature values (e.g.,\nPOS tag, GENDER, or NUMBER), but we always use gold training data for HEAD and DEPREL. In both cases\nthe parsers seem to benefit from training data (features) that better resemble the test data, while retaining\nbias toward the gold and correctly predicted data.\n"},{"#tail":"\n","@confidence":"0.496733714285714","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nTable 9\nSelect models trained using the Easy-First Parser. Statistical significance tested only for\nCORE12. . . models on predicted input: significance of the Easy-First Parser CORE12 baseline\nmodel against its MaltParser counterpart; and significance of all other CORE12+. . . models\nagainst the Easy-First Parser CORE12 baseline model.\nmodel (POS tag set and features) gold predictedLAS UAS LS LAS UAS LS\n"},{"#tail":"\n","@confidence":"0.97186565","#text":"\nvalues yields better scores when evaluated on gold, too (although later we see this is\nnot always the case). More interestingly, when evaluated on predicted feature values,\ntraining on predicted feature values yields better parsing scores than when training on\ngold, and training on g+p yields best scores, in support of our hypothesis. Therefore,\nin the rest of the table (and in the rest of the experiments), we apply the g+p training\nvariant to the best models so far, both in MaltParser and Easy-First Parser. The next part\nin Table 10 shows that this trend is consistent also with the best feature combinations so\nfar. Interestingly, the RAT feature contributes to improvement only in the g+p condition,\npresumably because of its low prediction accuracy.\nIn Table 11, we repeated most of these experiments with other tag sets: CATIBEX\nand BW (best performers on predicted and gold input, respectively). We can see in\nthis table that the same trends hold for these POS tag sets as well. Interestingly, the\n?morphology-free? CORE12 (in Table 10) outperforms CATIBEX here (Table 11), mak-\ning CORE12+DET2+LMM+PERSON+FN*NGR our best MaltParser model on predicted\nfeature values. Similarly, the Easy-First Parser model CORE12+DET+LMM+PERSON+\nFN*NG outperforms its CATIBEX counterpart (CATIBEX+DET+LMM+PERSON+FN*NG),\nresulting in our best model on the dev set in machine-predicted condition (82.7%).22\nThe richest POS tag set, BW, which is also the worst predicted tag set and worst\nperformer on predicted input, had the most dramatic gains from using g+p: more than\n22 See Section 9 for download information.\n"},{"#tail":"\n","@confidence":"0.792674888888889","#text":"\nComputational Linguistics Volume 39, Number 1\nTable 10\nAlternatives to training on gold-only feature values. Top: Select MaltParser CORE12+. . . models\nre-trained on predicted or gold + predicted feature values. Bottom: Similar models to the top\nhalf, with the Easy-First Parser. Statistical significance tested only for CORE12+. . . models on\npredicted input: significance of the MaltParser models from the MaltParser CORE12 baseline\nmodel, and significance of the Easy-First Parser models from the Easy-First Parser CORE12\nbaseline.\nmodel (POS tag set and features) gold predictedLAS UAS LS LAS UAS LS\n"},{"#tail":"\n","@confidence":"0.8754445","#text":"\nleading models.\nThe results in Tables 10 and 11 suggest that our g+p training method is superior to\nthe alternatives (independently of parser choice) due to making the parser more resilient\nto lower accuracy in the input. It also suggests that g+p training enables the parser to\nbetter exploit relevant data when represented in ?cleaner? separate features, as opposed\nto when the POS tags are split into ambiguous form-based cases as in CATIBEX. Future\nexperimentation is needed in order to test this latter conjecture.\n8. Result Validation and Discussion\n"},{"#tail":"\n","@confidence":"0.9678595","#text":"\nOnce experiments on the development set were done, we ran the best performing form-\nbased non-gold-based models from Section 4 on a previously unseen test set. This set\n"},{"#tail":"\n","@confidence":"0.470250857142857","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nTable 11\nAlternatives to training on gold-only feature values for CATIBEX and BW tag sets. Top: Select\nMaltParser models re-trained on predicted or gold + predicted feature values. Bottom: Similar\nmodels to the top half, with the Easy-First Parser. (Statistical significance was tested only for\nCORE12+. . . models ? none here).\nmodel (POS tag set and features) gold predictedLAS UAS LS LAS UAS LS\n"},{"#tail":"\n","@confidence":"0.957604333333333","#text":"\nComputational Linguistics Volume 39, Number 1\nis the test split of part 3 of the PATB (hereafter PATB3-TEST; see Table 1 for details).\nTable 12 shows that the same trends held on this set too, with even greater relative gains,\nup to almost 2% absolute gains.\nWe then also revalidated the contribution of the best performing models from\nSections 5?7 on PATB3-TEST. Here, too, the same trends held. Results are shown in\n"},{"#tail":"\n","@confidence":"0.99962075","#text":"\nFor better comparison with work of others, we adopt the suggestion made by Green\nand Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.\nWe report these filtered results in Table 14. Filtered results are consistently higher (as\nexpected). Results are about 0.9% absolute higher on the development set, and about\n0.6% higher on the test set. The contribution of the RAT feature across sets is negligible\n(or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but\nabout 0.15% gain on the test set. For clarity and conciseness, we only show the best\nmodel (with RAT) in Table 14.\n"},{"#tail":"\n","@confidence":"0.883323166666667","#text":"\nWe perform two types of error analyses. First, we analyze the attachment accuracy\nby attachment relation type on PATB3-DEV. Our hypothesis is that the syntactic re-\nlations which are involved in agreement or assignment configurations will show an\nimprovement when the relevant morphological features are used, but other syntactic\nTable 13\nResults on PATB3-TEST for models that performed best on PATB3-DEV ? predicted input. Using\nMaltParser, unless indicated otherwise. g+p = trained on combination of gold and predicted\ninput (instead of gold-only). Statistical significance tested only for CORE12+. . . models: For\nMaltParser CORE12+. . . models against the MaltParser CORE12 baseline model output, and for\nEasy-First Parser CORE12+. . . models against the Easy-First Parser CORE12 baseline model\noutput.\nPOS tag set LAS UAS LS\n"},{"#tail":"\n","@confidence":"0.962665428571429","#text":"\nrelations will not. Second, we analyze the grammaticality of the obtained parse trees\nwith respect to agreement and assignment phenomena. Here, our hypothesis is that\nwhen using morphological features, the grammaticality of the obtained parse trees will\nincrease.\nAttachment accuracy by relation type. Our first hypothesis is illustrated in Figure 2.\nOn the left, we see the parse provided by our baseline system (MaltParser using only\nCORE12), which has two errors:\n"},{"#tail":"\n","@confidence":"0.9132172","#text":"\nand 7( mrt (?passed?) is feminine singular, obeying the agreement pattern un-\nder which a non-rational subject following the verb always triggers a feminine\nsingular verbal form.\n The node labeled )823  Almhnds (?the engineer?) should not be in an idafa (gen-\nitive construction) dependency with its governor .\n"},{"#tail":"\n","@confidence":"0.9562759","#text":"\nbut in a modifier relation (a sort of apposition, in this case). This must be the case\nbecause a noun that is the head of an idafa construction cannot have a definite\ndeterminer, as is the case here.\nBoth errors could be corrected (to the correct form as in our best model, on the right-\nhand side of Figure 2) if functional morphological features were available to the parser,\nincluding the rationality feature, and if the parser could learn the agreement rule for\nnon-rational subjects, as well as the requirement that the head of an idafa construction\ncannot have a definite article.\nOur first hypothesis is generally borne out. We discuss three conditions in more\ndetail:\n"},{"#tail":"\n","@confidence":"0.898385","#text":"\nnation of gold and predicted tags (Table 17).\nIn all cases, for controlled investigation, we compare the error reduction resulting\nfrom adding morphological features to a ?morphology-free? baseline, which in all\ncases we take to be the MaltParser trained on the gold CORE12, and evaluated on\n"},{"#tail":"\n","@confidence":"0.95572","#text":"\n7( mrt ?yAm ?l? A?xtfA? Alzmyl Almhnds\n. . . (?Several days have passed since the disappearance of the colleague the engineer . . . ?), as\nparsed by the baseline system using only CORE12 (left) and as using the best performing model\n(right). Bad predictions are marked with <<< . . . >>>. The words in the tree are presented in\nthe Arabic reading direction (from right to left).\nmachine-predicted input (except for Table 17, where the Easy-First Parser is trained and\nevaluated instead).\nWe start out by investigating the behavior of MaltParser, using all gold tags for\ntraining. The accuracy by relation type is shown in Table 15. Using just CORE12, we\nsee that some attachments (subject, modifications) are harder than others (objects,\nidafa). We see that by adding LMM, all attachment types improve a little bit; this is\nas expected, because this feature provides a slight lexical abstraction. We then add\nfeatures designed to improve idafa and those relations subject to agreement, subject,\nand nominal modification (DET2, PERSON, NUMBER, GENDER). We see that, as expected,\nsubject, nominal modification, and idafa reduce error by substantial margins (error\nreduction over CORE12 is greater than 10%; in the case of idafa it is 21.8%), and all other\nrelations (including object and prepositional attachment) improve to a lesser degree\n(error reduction of 7.1% or less). We assume that the non-agreement relations (object\n"},{"#tail":"\n","@confidence":"0.713662666666667","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nTable 15\nTraining the MaltParser on gold tags, accuracy by gold attachment type (selected): subject,\nobject, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by a\npreposition, idafa, and overall results (repeated).\nmodel (POS and morphological features) SBJ OBJ MOD-N MOD-Prep IDF total\n"},{"#tail":"\n","@confidence":"0.981996321428571","#text":"\nand prepositional attachment) improve because of the overall improvement in the\nparse due to the improvements in the other relations.\nWhen we move to the functional features, using functional number and gender,\nwe see a further reduction in the agreement-related attachments, namely, subject and\nnominal modification (error reductions over baseline of 13.7% and 12.9%, respectively).\nIdafa decreases slightly (because this relation is not affected by the functional features),\nwhereas object stays the same. Surprisingly, prepositional attachment also improves,\nwith an error reduction of 8.1%. Again, we can only explain this by proposing that the\nimprovement in nominal modification attachment has the indirect effect of ruling out\nsome bad prepositional attachments as well.\nWe then add the rationality feature (last line of Table 15). We now see that all\nrelations affected by agreement or assignment perform worse than without the ratio-\nnality feature. In contrast, all other relations improve. The decrease in performance can\nbe explained by the fact that the rationality (RAT) feature is not predicted with high\naccuracy; because it interacts directly with agreement, and because we are training on\ngold annotation, the models trained do not correspond to the seen data. We expect\nrationality to contribute when we look at training that includes predicted features.\n(We have no explanation for the improvement in the other relations.)\nWe now turn to training the MaltParser on a combination of gold and predicted\nPOS and morphological feature values (g+p; Section 7). The accuracy by relation is\nshown in Table 16. The table repeats (in the first row) the results for the MaltParser\ntrained only using gold CORE12 features. First, we see that using the same single feature,\nbut training on gold and predicted tags, we obtain an across-the-board improvement,\nTable 16\nTraining the MaltParser on gold and predicted tags, accuracy by gold attachment type (selected):\nsubject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun)\nby a preposition, idafa, and overall results (repeated).\nmodel (POS and morphological features) SBJ OBJ MOD-N MOD-Prep IDF total\n"},{"#tail":"\n","@confidence":"0.85727425","#text":"\nTraining the Easy-First Parser on gold and predicted tags, accuracy by gold attachment type\n(selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or\na noun) by a preposition, idafa, and overall results (repeated).\nmodel (POS and morphological features) SBJ OBJ MOD-N MOD-Prep IDF total\n"},{"#tail":"\n","@confidence":"0.997686657894737","#text":"\nwith error reductions between 3.6% and 9.3%, with no apparent patterns. (Prepositional\nmodifications even show a slight decline in attachment accuracy). This row (using only\nCORE12 and training on gold and predicted) now becomes our baseline for subsequent\ndiscussion of error reduction. If we then add the form-based features, we again find that\nthe error rate decrease for subject, nominal modification, and idafa (the relations affected\nby agreement and assignment) is greater than that for the other relations; with this train-\ning corpus, however, the separation is not as stark, with subject decreasing its error rate\nby 9.6% and prepositional modification by 8.7%. Notably, idafa shows the greatest error\nrate reduction we have seen so far: 30.2%. When we turn to functional features, we again\nsee a further increase in performance across the board. And, as expected, the penalty for\nusing the rationality feature disappears because we have trained on predicted features\nas well. In fact the improvement due to rationality specifically benefits the relations\naffected by agreement and assignment, with subject reducing error by 13.4% now,\nnominal modification by 14.7%, and idafa by 34.0%. The tree on the right in Figure 2\nis the parse tree returned by this model, and both the subject and the idafa relation are\ncorrectly analyzed. Note that the increase in the accuracy of idafa is probably not related\nto the interaction of syntax and morphology in assignment, because assignment in the\nidafa construction is not affected by rationality. Instead, we suspect that the parser can\nexploit the very different profile of the rationality feature in the dependent node of the\nidafa and modification constructions. Looking just at nominals, we see in the gold corpus\nthat 62% of the dependents in a modification relation have no inherent rationality (this\nis the case notably for adjectives), whereas this number for idafa is only 18%. In contrast,\nthe dependent of an idafa is irrational 66% of the time, whereas for modification that\nnumber is only 16%.\nFinally, we turn to the use of the Easy-First Parser (Section 6). The accuracy by\nrelation is shown in Table 17. When we switch from MaltParser to Easy-First Parser,\nwe get an overall error reduction of 4.2%, which is reflected fairly evenly among the\nrelations, with two outliers: subjects improve by 9.0%, whereas idafa increases its error\nrate by 5.5%! We do not have an immediate analysis for this behavior, because idafa\nis usually considered an ?easy? relation (no word can intervene between the linked\nwords), as reflected in the high accuracy numbers for this relation. Furthermore, when\nwe inspect the unlabeled accuracy scores (not shown here), we see that the unlabeled\nattachment score for idafa also decreases. Thus, we must reject a plausible hypothesis,\nnamely, that the parser gets the relations right but the labeler (which in the Easy-First\nParser is a separate, second-pass module) gets the labels wrong. When we train the\nEasy-First Parser on gold and predicted, we see a similar improvement pattern over just\ntraining on gold as we did with the MaltParser; one exception is that the idafa relation\nimproves greatly again. Finally, we add the functional morphological features (training\n"},{"#tail":"\n","@confidence":"0.974545604651163","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\non gold and predicted). Again, the pattern we observe (by comparing error reduction\nagainst using Easy-First Parser trained only using CORE12 on gold and predicted) are\nvery similar to the pattern we observed with the MaltParser in the same conditions.\nOne difference stands out, however: whereas the MaltParser can exploit the rationality\nfeature when trained on gold and predicted, the Easy-First Parser cannot. Object and\nprepositional modification perform identically with or without rationality, but subject\nand idafa perform worse; only nominal modification performs better (with overall per-\nformance decreasing). If we inspect the unlabeled attachment scores for subjects, we do\ndetect an increase in accuracy (from 85.0% to 85.4%); perhaps the parser can exploit the\nrationality feature, but the labeler cannot.\nGrammaticality of parse trees. We now turn to our second type of error analysis, the\nevaluation of the grammaticality of the parse trees in terms of gender and number\nagreement patterns. We use the agreement checker code developed by Alkuhlani and\nHabash (2011) and evaluate our baseline (MaltParser using only CORE12), best perform-\ning model (Easy-First Parser using CORE12 + DET+LMM+PERSON+FN*NGR g+p), and\nthe gold reference. The agreement checker verifies, for all verb?nominal subject relations\nand noun?adjective relations found in the tree, whether the agreement conditions are\nmet or not. The accuracy number reflects the percentage of such relations found which\nmeet the agreement criteria. Note that we use the syntax given by the tree, not the\ngold syntax. For all three trees, however, we used gold morphological features for\nthis evaluation even when those features were not used in the parsing task. This is\nbecause we want to see to what extent the predicted morphological features help find\nthe correct syntactic relations, not whether the predicted trees are intrinsically coherent\ngiven possibly false predicted morphology. The results can be found in Table 18. We note\nthat the grammaticality of the gold corpus is not 100%; this is approximately equally\ndue to errors in the checking script and to annotation errors in the gold standard.\nWe take the given grammaticality of the gold corpus as a topline for this analysis.\nNominal modification has a smaller error band between baseline and gold compared\nwith subject?verb agreement. We assume this is because subject?verb agreement is more\ncomplex (it depends on their relative order), and because nominal modification can\nhave multiple structural targets, only one of which is correct, although all, however,\nare plausible from the point of view of agreement. The error reduction relative to the\ngold topline is 62% and 76% for nominal agreement and verb agreement, respectively.\nThus, we see that our second hypothesis?that the use of morphological features will\nreduce grammaticality errors in the resulting parse trees with respect to agreement\nphenomena?is borne out.\nIn summary, we see that not only do morphological (and functional morpholog-\nical features in particular) improve parsing, but they improve parsing in the way\nTable 18\nAnalysis of grammaticality of agreement relations between verb and subject and between a noun\nand a nominal modifier (correct agreement in percent).\nmodel (POS and morphological features) noun-modifier subject-verb\n"},{"#tail":"\n","@confidence":"0.97258075","#text":"\nComputational Linguistics Volume 39, Number 1\nthat we expect: (a) those relations affected by agreement and assignment contribute\nmore than those that are not, and (b) agreement errors in the resulting parse trees are\nreduced.\n"},{"#tail":"\n","@confidence":"0.987375826086957","#text":"\nWe explored the contribution of different morphological features (both inflectional\nand lexical) to dependency parsing of Arabic. Starting with form-based morphological\nfeatures, we find that definiteness (DET), PERSON, NUMBER, GENDER, and undiacritized\nlemma (LMM) are most helpful for Arabic dependency parsing on predicted (non-\ngold) input. We further find that functional gender, number, and rationality features\n(FN*GENDER, FN*NUMBER, RAT) improve over form-based-only morphological fea-\ntures, as expected when considering the complex agreement rules of Arabic. To our\nknowledge, this is the first result in Arabic NLP using functional morphological fea-\ntures, and showing an improvement over form-based features.\nThis article presented a large number of results. We summarize them next.\n1. We observe a tradeoff among the three factors (relevance, redundancy, and ac-\ncuracy) of morphological features in parsing quality. The best performing tag set\n(BW) under the gold condition (i.e., it is very relevant) is worst under the machine-\npredicted condition, because of its dismal prediction accuracy rate. The tag set\nwith highest prediction accuracy (CATIB6) does not necessarily yield the best\nresults in dependency parsing accuracy, because it is not very relevant. A simple\nextension of CATIB6, however, that improves its relevance (CATIBEX) but retains\nsufficient accuracy improves the overall parsing quality.\n2. Lexical features do help parsing, and the most helpful in predicted condition is\nthe undiacritized lemma (LMM) feature. Although LMM is more ambiguous than\nthe diacritized LEMMA feature, it has half the error rate of LEMMA which makes it\na more reliable (accurate) feature. When using LMM, LEMMA is highly redundant\n(and vice versa).\n"},{"#tail":"\n","@confidence":"0.986628625","#text":"\nin predicted condition. This is a result of their high relevance and their high\nprediction accuracy. In contrast, CASE and STATE are the best performers in the\ngold condition (i.e., highly relevant) but not in the predicted condition (where\nCASE is actually the worst feature). The rationality (RAT) feature is more helpful\nin the gold condition, which suggests it is relevant; its associated parsing results\nin predicted condition are not as good, however. Presumably, this is because of its\nlower prediction accuracy.\n4. When evaluating in the machine-predicted input condition, training on data with\ngold and predicted morphological features (g+p training) consistently improves\nresults over training on gold. This novel technique most likely addresses the\nnegative effect of feature prediction error by introducing the common errors to\nthe parsing model in training. A side effect of it is that using correct predictions\nby the parser is reinforced, because constructions with correctly predicted values\nappear twice as often in g+p training.\n5. All of these results carry over successfully to another parser (Easy-First Parser),\nsuggesting the insights are not specific to MaltParser.\n"},{"#tail":"\n","@confidence":"0.952919066666667","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\n6. Our best model was trained with the Easy-First Parser, containing the follow-\ning features: CORE12+DET+LMM+PERSON+FN*NGR, with g+p feature values\nfor training. We make this model available, together with the source code.23\nAlthough we only experimented with Arabic dependency parsing, we believe that\nthe evaluation framework we presented and many of our conclusions will carry over to\nother languages (particularly, Semitic and morphology-rich languages) and syntactic\nrepresentations (e.g., phrase structure). Some of our conclusions are more language\nindependent (e.g., those involving the use of predicted training conditions).\nIn future work, we intend to improve the prediction of functional morphological\nfeatures?especially RAT?in order to improve dependency parsing accuracy in pre-\ndicted condition. We also intend to investigate how these features can be integrated into\nother parsing frameworks; we expect them to help independently of the framework. The\nability to represent the relevant morphological information in a manner that is useful to\nattachment decisions is, of course, crucial to improving parsing quality.\n"},{"#tail":"\n","@confidence":"0.9961966","#text":"\nThis work was supported by the DARPA GALE program, contract HR0011-08-C-0110. Y. Marton\nperformed most of the work on this paper while he was at the Center for Computational Learning\nSystems at Columbia University and at the IBM Watson Research Center. We thank Joakim Nivre\nfor his useful remarks, Ryan Roth for his help with MADA, and Sarah Alkuhlani for her help with\nfunctional features. We also thank three anonymous reviewers for thoughtful comments.\n"},{"#tail":"\n","@confidence":"0.99584725","#text":"\nThe following sections describe additional experiments, with negative or small gains,\npresented here for completeness.\nA.1 Embedding Morphological Features Within the POS Tags\nAfter discovering our best form-based feature combination, we explored whether mor-\nphological data should be added to an Arabic parsing model as stand-alone machine\nlearning features, or whether they should be used to enhance and extend a POS tag set.\nWe created a new POS tag set, CORE12EX, size 81 (and 96.0% prediction accuracy), by\nextending the CORE12 tag set with the features that most improved the CORE12 baseline:\nDET and the PNG-features. But CORE12EX did worse than its non-extended (but feature-\nenhanced) counterpart, CORE12+DET+PNG. Another variant, CORE12EX+DET+PNG,\nwhich used both the extended tag set and the additional DET and PNG-features, did\nnot improve over CORE12+DET+PNG either.\n"},{"#tail":"\n","@confidence":"0.995698833333333","#text":"\nAfter extending the determiner feature (DET2), the next gainful feature that we could\nalter was PERSON. We changed the values of proper names from ?N/A? to ?3?\n(third-person). But this change resulted in a slight decrease in performance, so it was\nabandoned.\nA.3 Digit Tokens and Number Binning\nDigit tokens (e.g., 4, as opposed to four) are marked singular by default. They don?t\nshow surface agreement with a noun, even though the corresponding number-word\ntoken would. Therefore we replaced the digit tokens? NUMBER value with ?N,? and\ndenoted these experiments with NUMDGT.24\nWe further observe that MSA displays complex agreement patterns with num-\nbers (Dada 2007). Therefore, we alternatively experimented with binning the digit\ntokens? NUMBER value accordingly:\n the number 0 and numbers ending with 00\n the number 1 and numbers ending with 01\n the number 2 and numbers ending with 02\n the numbers 3?10 and those ending with 03?10\n the numbers, and numbers ending with, 11?99\n all other number tokens (e.g., 0.35 or 7/16)\nWe denoted these experiments with NUMDGTBIN. Almost 1.5% of the tokens are\ndigit tokens in the training set, and 1.2% in the dev set.\nNumber binning did not have a consistent contribution in either gold or predicted\nvalue conditions (results not shown), so it was abandoned as well.\n24 We didn?t mark the number-words because in our training data there were fewer than 30 lemmas of\nfewer than 2,000 such tokens, and hence presumably their agreement patterns can be more easily learned.\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.32239","#text":"\nNuance Communications\n"},{"#tail":"\n","@confidence":"0.875665","#text":"\nCenter for Computational Learning\nSystems, Columbia University\n"},{"#tail":"\n","@confidence":"0.896631","#text":"\nCenter for Computational Learning\nSystems, Columbia University\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.986035","@genericHeader":"abstract","#text":"\n1. Introduction\n"},{"#tail":"\n","@confidence":"0.721699","@genericHeader":"categories and subject descriptors","#text":"\n2. Experimental Data and Relevant Linguistic Concepts\n"},{"#tail":"\n","@confidence":"0.718423","@genericHeader":"related work","#text":"\n3. Related Work\n"},{"#tail":"\n","@confidence":"0.957016","@genericHeader":"method","#text":"\n9. Conclusions and Future Work\n"},{"#tail":"\n","@confidence":"0.597951","@genericHeader":"method","#text":"\n3. GENDER and NUMBER and their functional variants are the most useful for parsing\n"},{"#tail":"\n","@confidence":"0.966924","@genericHeader":"method","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.976966","@genericHeader":"method","#text":"\nReferences\n"},{"#tail":"\n","@confidence":"0.948004","@genericHeader":"method","#text":"\nA.2 Extended PERSON Feature\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.778144","#text":"\nTable 13.\n"},{"#tail":"\n","@confidence":"0.4216276","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nTable 14\nResults for best performing model on PATB3-DEV and PATB3-TEST for sentences up to 70 tokens\nlong (predicted input).\nmodel evaluated on LAS UAS LS\n"},{"#tail":"\n","@confidence":"0.676009","#text":"\nTable 17\n"}],"page":[{"#tail":"\n","@confidence":"0.997447","#text":"\n162\n"},{"#tail":"\n","@confidence":"0.999416","#text":"\n163\n"},{"#tail":"\n","@confidence":"0.999724","#text":"\n164\n"},{"#tail":"\n","@confidence":"0.998917","#text":"\n165\n"},{"#tail":"\n","@confidence":"0.99898","#text":"\n166\n"},{"#tail":"\n","@confidence":"0.998951","#text":"\n167\n"},{"#tail":"\n","@confidence":"0.995944","#text":"\n168\n"},{"#tail":"\n","@confidence":"0.997877","#text":"\n169\n"},{"#tail":"\n","@confidence":"0.99471","#text":"\n170\n"},{"#tail":"\n","@confidence":"0.955293","#text":"\n171\n"},{"#tail":"\n","@confidence":"0.996779","#text":"\n172\n"},{"#tail":"\n","@confidence":"0.998041","#text":"\n173\n"},{"#tail":"\n","@confidence":"0.998604","#text":"\n174\n"},{"#tail":"\n","@confidence":"0.995176","#text":"\n175\n"},{"#tail":"\n","@confidence":"0.628237","#text":"\n176\n"},{"#tail":"\n","@confidence":"0.992706","#text":"\n177\n"},{"#tail":"\n","@confidence":"0.997451","#text":"\n178\n"},{"#tail":"\n","@confidence":"0.996562","#text":"\n179\n"},{"#tail":"\n","@confidence":"0.991897","#text":"\n180\n"},{"#tail":"\n","@confidence":"0.926131","#text":"\n181\n"},{"#tail":"\n","@confidence":"0.998127","#text":"\n182\n"},{"#tail":"\n","@confidence":"0.990265","#text":"\n183\n"},{"#tail":"\n","@confidence":"0.985784","#text":"\n184\n"},{"#tail":"\n","@confidence":"0.980628","#text":"\n185\n"},{"#tail":"\n","@confidence":"0.998613","#text":"\n186\n"},{"#tail":"\n","@confidence":"0.983863","#text":"\n188\n"},{"#tail":"\n","@confidence":"0.983153","#text":"\n189\n"},{"#tail":"\n","@confidence":"0.96717","#text":"\n190\n"},{"#tail":"\n","@confidence":"0.964508","#text":"\n191\n"},{"#tail":"\n","@confidence":"0.96744","#text":"\n192\n"},{"#tail":"\n","@confidence":"0.987802","#text":"\n193\n"},{"#tail":"\n","@confidence":"0.999674","#text":"\n194\n"}],"figureCaption":{"#tail":"\n","@confidence":"0.587005","#text":"\nFigure 1\n"},"table":[{"#tail":"\n","@confidence":"0.9709985","#text":"\ntraining 341,094 11,476 29.7\ndev 31,208 1,043 29.9\nunseen test 29,944 1,007 29.7\nTOTAL 402,246 13,526 29.7\n"},{"#tail":"\n","@confidence":"0.998598571428572","#text":"\nCATIB6 81.0 83.7 92.6 78.3 82.0 90.6 ?2.7 97.7 6\nCATIBEX 82.5 85.0 93.4 79.7 83.3 91.4 ?2.8 97.7 44\nCORE12 82.9 85.4 93.5 78.7 82.5 90.6 ?4.2 96.3 12\nCORE44 82.7 85.2 93.3 78.4 82.2 90.4 ?4.3 96.1 40\nERTS 83.0 85.2 93.8 78.9 82.6 91.0 ?4.0 95.5 134\nKULICK 83.6 86.0 94.0 79.4 83.2 91.1 ?4.2 95.7 32\nBW 84.0 85.8 94.8 72.6 77.9 86.5 ?11.4 81.8 430\n"},{"#tail":"\n","@confidence":"0.935963740740741","#text":"\nas broken plurals\nDET 99.6 3* presence of the determiner morpheme  Al\nPERSON 99.1 4* first, second, or third person (or N/A)\nASPECT 99.1 5* perfective, imperfective and imperative for verbs (or N/A)\nVOICE 98.9 4* active or passive voice for verbs (or N/A)\nMOOD 98.6 5* indicative, subjunctive, jussive for verbs (or N/A)\nGENDER 99.3 3* (form-based) masculine or feminine (or N/A)\nNUMBER 99.5 4* (form-based) singular, dual, or plural (or N/A)\nSTATE 95.6 4* construct (head of idafa), definite, or indefinite (or N/A)\nCASE 86.3 5* nominative, accusative or genitive (or N/A)\nNUMDGT 99.5 7* a NUMBER feature with digit token representation; see Section A.3\nNUMDGTBIN 99.5 12* a NUMBER feature with number ?binning? according to syntactic agreement\npatterns; see Section A.3\nFNNUM 99.2 6* a functional NUMBER feature, using ElixirFM; see Section 5.1\nFNNUMDGT 99.2 7* a functional NUMBER feature with digit token representation, using\nElixirFM; see Sections 5.1 and A.3\nFNNUMDGTBIN 99.2 12* a functional NUMBER feature with number ?binning? according to syntactic\nagreement patterns, using ElixirFM; see Sections 5.1 and A.3\nFN*GENDER 98.6 6* a functional GENDER feature, using our in-house resource; see Section 5.2\nFN*NUM 99.0 7* a functional NUMBER feature, using our in-house resource; see Section 5.2\nFN*NUMDGTBIN 99.0 13* a functional NUMBER feature with number ?binning? according to syntactic\nagreement patterns, using our in-house resource; see Sections 5.2 and A.3\nRAT 95.6 5* rationality (humanness) feature; rational, irrational, ambiguous, unknown\nor N/A; using our in-house resource; see Section 5.2\nPNG ? ? abbrev. for PERSON, NUMBER, and GENDER (a.k.a. ?-features); similarly for\nPG\nFN*NGR ? ? abbrev. for functional NUMBER, GENDER, and RAT; similarly for FN*NG\n"},{"#tail":"\n","@confidence":"0.99252876923077","#text":"\nA\nll (baseline repeated) 82.9 85.4 93.5 (baseline repeated) 78.7 82.5 90.6\n+ all 9 infl. features 85.2 86.6 95.3 + all 9 infl. features 77.9 82.1 90.0\nSe\np\n+CASE 84.6 86.3 95.0 +DET 79.8++ 83.2 91.5\n+STATE 84.2 86.4 94.4 +STATE 79.4++ 82.9 91.2\n+DET 84.0 86.2 94.2 +GENDER 78.8 82.4 90.8\n+NUMBER 83.1 85.5 93.6 +PERSON 78.7 82.5 90.7\n+PERSON 83.1 85.4 93.7 +NUMBER 78.7 82.4 90.6\n+VOICE 83.1 85.4 93.6 +VOICE 78.6 82.4 90.6\n+MOOD 83.1 85.5 93.5 +ASPECT 78.6 82.4 90.5\n+ASPECT 83.0 85.4 93.5 +MOOD 78.5 82.4 90.5\n+GENDER 83.0 85.2 93.6 +CASE 75.8 80.2 88.5\nG\nre\ned\ny\n+CASE+STATE 85.4 86.9 95.5 +DET+STATE 79.4++ 82.8 91.2\n+CASE+STATE+DET 85.2 86.7 95.4 +DET+GENDER 79.9++ 83.2 91.7\n+CASE+STATE+NUMBER 85.4 86.9 95.5 +DET+GENDER+PERSON 79.9++ 83.2 91.7\n+CASE+STATE+PERSON 85.3 86.8 95.4 +DET+PNG 80.1++ 83.3 91.8\n+CASE+STATE+VOICE 85.3 86.8 95.4 +DET+PNG+VOICE 80.0++ 83.2 91.7\n+CASE+STATE+MOOD 85.2 86.7 95.4 +DET+PNG+ASPECT 80.0++ 83.2 91.8\n+CASE+STATE+ASPECT 85.2 86.8 95.4 +DET+PNG+MOOD 80.0++ 83.2 91.8\n+CASE+STATE+GENDER 85.3 86.8 95.4 ?\n"},{"#tail":"\n","@confidence":"0.818212363636364","#text":"\nComputational Linguistics Volume 39, Number 1\nTable 5\nModels with lexical morpho-semantic features. Top: Adding all lexical features together on top\nof the CORE12 baseline. Center: Adding each feature separately. Bottom: Greedily adding best\nfeatures from previous part, on predicted input. Statistical significance tested only on predicted\n(non-gold) input, against the CORE12 baseline.\nset-up CORE12+. . . gold predictedLAS UAS LS LAS UAS LS\nA\nll CORE12 (baseline repeated) 82.9 85.4 93.5 78.7 82.5 90.6\n+ all lexical features 83.4 85.5 93.9 78.9 82.5 90.8\nSe\np\n+LMM (lemma without diacritics) 83.3 85.5 93.8 79.0+ 82.5 90.8\n+ROOT 83.2 85.5 93.7 78.9+ 82.6 90.7\n+LEMMA 83.4 85.5 93.8 78.8 82.4 90.7\n+PATTERN 83.1 85.5 93.6 78.6 82.4 90.6\nG\nre\ned\ny +LMM+ROOT 83.3 85.5 93.9 79.0++ 82.6 90.9\n+LMM+ROOT+LEMMA 83.3 85.4 93.8 79.1++ 82.6 90.9\n+LMM+ROOT+PATTERN 83.4 85.5 93.9 78.9 82.6 90.8\n"},{"#tail":"\n","@confidence":"0.99799325","#text":"\nCORE12+DET+PNG (rep.) 84.2 86.2 94.5 80.1++ 83.3 91.8\nCORE12+DET+PNG+LMM 84.4 86.4 94.6 80.2++ 83.3 91.9\nCORE12+DET+PNG+LMM +ROOT 84.3 86.3 94.6 80.1++ 83.3 91.8\nCORE12+DET+PNG+LMM +PATTERN 84.4 86.3 94.6 80.0++ 83.2 91.8\n"},{"#tail":"\n","@confidence":"0.932086222222222","#text":"\nMarton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features\nTable 7\nModels with re-engineered DET and PERSON inflectional features. Statistical significance tested\nonly on predicted input, against the CORE12 baseline.\nmodel (POS tag set and infl. feature) gold predictedLAS UAS LS LAS UAS LS\nCORE12+DET (repeated) 84.0 86.2 94.2 79.8++ 83.2 91.5\nCORE12+DET2 84.1 86.4 94.3 80.1++ 83.5 91.7\nCORE12+DET+PNG+LMM (repeated) 84.4 86.4 94.6 80.2++ 83.3 91.9\nCORE12+DET2+PNG+LMM 84.6 86.5 94.7 80.2++ 83.4 91.9\n"},{"#tail":"\n","@confidence":"0.999065272727273","#text":"\nCORE12 (repeated) 82.9 85.4 93.5 78.7 82.5 90.6\n+FN*RATIONAL 83.7 85.8 94.0 78.7 82.5 90.7\n+GENDER (repeated) 83.0 85.2 93.6 78.8 82.4 90.8\n+FN*GENDER 83.3 85.5 93.7 78.9+ 82.6 90.9\n+NUMBER (repeated) 83.1 85.5 93.6 78.7 82.4 90.6\n+FN*NUMBER 83.3 85.6 93.7 78.9+ 82.5 90.7\n+DET2+LMM+PNG (repeated) 84.6 86.5 94.7 80.2++ 83.4 91.9\n+DET2+LMM+PERSON +FN*NGR 85.0 86.7 94.9 80.3++ 83.7 91.6\n+DET2+LMM+PERSON +FN*NG 84.6 86.5 94.7 80.4++ 83.5 91.9\nCATIBEX+DET2+LMM+PERSON+FN*NGR 84.1 85.9 94.4 80.7 84.0 91.9\nCATIBEX+DET2+LMM+PERSON+FN*NG 83.5 85.4 94.1 80.7 83.7 92.2\n"},{"#tail":"\n","@confidence":"0.975477375","#text":"\nCORE12 (MaltParser baseline, repeated) 82.9 85.4 93.5 78.7 82.5 90.6\nCORE12 (Easy-First Parser) 83.5 86.0 93.9 79.6++ 83.5 91.3\nCORE12+NUMBER 83.3 85.7 94.0 79.5 83.4 91.3\nCORE12+FN*NUMBER 83.5 85.9 94.0 79.8 83.6 91.4\nCORE12+GENDER 83.5 86.0 94.0 79.5 83.5 91.3\nCORE12+FN*GENDER 83.6 86.1 94.0 79.7 83.6 91.3\nCORE12+RAT 84.2 86.4 94.4 79.6 83.6 91.3\nCORE12+DET 84.3 86.7 94.5 80.6++ 84.1 92.2\nCORE12+LMM 83.6 85.8 94.1 79.7 83.5 91.5\nCORE12+DET+LMM+PNG 84.8 86.9 94.9 81.1++ 84.4 92.3\nCORE12+DET+LMM+PERSON+FN*NG 84.9 86.9 94.8 81.4++ 84.7 92.4\nCORE12+DET+LMM+PERSON+FN*NGR 85.1 87.1 94.9 81.2++ 84.7 92.1\nCATIBEX 83.1 85.6 94.0 81.2 84.6 92.5\nCATIBEX+DET+LMM+PERSON+FN*NG 83.5 85.8 94.2 81.4 84.6 92.7\nCATIBEX+DET+LMM+PERSON+FN*NGR 83.9 85.9 94.7 81.1 84.6 92.5\nThe first two parts of Table 10 show that, as expected, training on gold feature\n"},{"#tail":"\n","@confidence":"0.936681523809524","#text":"\nMaltParser:\nCORE12 (gold train, repeated) 82.9 85.4 93.5 78.7 82.5 90.6\nCORE12 predicted train 82.4 85.0 93.2 79.8++ 83.2 91.4\nCORE12 g+p 82.7 85.2 93.5 80.0++ 83.4 91.6\nCORE12+DET+LMM+PNG (gold train, repeated) 84.4 86.4 94.6 80.2++ 83.3 91.9\nCORE12+DET+LMM+PNG predicted train 84.1 86.1 94.3 81.6++ 84.4 92.8\nCORE12+DET+LMM+PNG g+p 84.2 86.1 94.5 81.7++ 84.5 92.9\nCORE12+DET2+LMM+PERSON+FN*NGR 85.0 86.7 94.9 80.3++ 83.7 91.6\n(gold train, repeated)\nCORE12+DET2+LMM+PERSON+FN*NG 84.6 86.5 94.7 80.4++ 83.5 91.9\n(gold train, repeated)\nCORE12+DET2+LMM+PERSON+FN*NG g+p 84.4 86.3 94.6 81.8++ 84.6 93.0\nCORE12+DET2+LMM+PERSON+FN*NGR g+p 84.7 86.5 94.7 81.9++ 84.7 93.0\nEasy-First Parser:\nCORE12 (gold train, repeated) 83.5 86.0 93.9 79.6 83.5 91.3\nCORE12 g+p 83.6 86.1 94.1 80.8++ 84.4 92.3\nCORE12+DET+LMM+PNG g+p 84.8 86.9 94.7 82.5++ 85.5 93.3\nCORE12+DET+LMM+PERSON+FN*NG g+p 84.9 86.9 94.9 82.7++ 85.7 93.3\nCORE12+DET+LMM+PERSON+FN*NGR g+p 85.2 87.2 95.0 82.6++ 85.7 93.2\n5% (absolute) for LAS on predicted input with MaltParser (and over 3% with Easy-\nFirst Parser). Although much improved, BW models? performance still lags behind the\n"},{"#tail":"\n","@confidence":"0.948361351351351","#text":"\nMaltParser:\nCATIBEX (gold train, repeated) 82.5 85.0 93.4 79.7 83.3 91.4\nCATIBEX g+p 82.3 84.8 93.3 80.4 83.6 92.0\nCATIBEX+DET2+LMM+PERSON+FN*NG 83.5 85.4 94.1 80.7 83.7 92.2\n(gold train, repeated)\nCATIBEX+DET2+LMM+PERSON+FN*NGR 84.1 85.9 94.4 80.7 84.0 91.9\n(gold train)\nCATIBEX+DET2+LMM+PERSON+FN*NG g+p 83.2 85.3 93.9 81.3 84.2 92.6\nCATIBEX+DET2+LMM+PERSON+FN*NGR g+p 83.7 85.7 94.2 81.4 84.3 92.6\nBW (gold train, repeated) 84.0 85.8 94.8 72.6 77.9 86.5\nBW g+p 83.9 85.7 94.7 77.8 81.4 90.3\nBW+DET2+LMM+PERSON+FN*NG g+p 84.8 86.4 95.1 79.4 82.6 91.2\nBW+DET2+LMM+PERSON+FN*NGR g+p 85.1 86.6 95.2 79.5 82.7 91.2\nEasy-First Parser:\nCATIBEX (gold train, repeated) 83.1 85.6 94.0 81.2 84.6 92.5\nCATIBEX g+p 82.5 85.1 93.8 81.2 84.4 92.9\nCATIBEX+DET+LMM+PERSON+FN*NG 83.5 85.8 94.2 81.4 84.6 92.7\n(gold train, repeated)\nCATIBEX+DET+LMM+PERSON+FN*NGR 83.9 85.9 94.7 81.1 84.6 92.5\n(gold train, repeated)\nCATIBEX+DET+LMM+PNG g+p 83.4 85.7 94.3 82.1 85.0 93.3\nCATIBEX+DET+LMM+PERSON+FN*NG g+p 83.6 85.8 94.4 82.0 84.9 93.4\nCATIBEX+DET+LMM+PERSON+FN*NGR g+p 83.9 86.0 94.6 82.2 85.3 93.2\nBW (gold train, repeated) 84.9 86.6 95.6 77.5 82.2 90.1\nBW g+p 84.4 86.2 95.3 80.7 84.1 92.5\nBW+DET+LMM+PERSON+FN*NG g+p 84.8 86.5 95.5 81.1 84.2 92.8\nBW+DET+LMM+PERSON+FN*NGR g+p 85.1 86.7 95.6 81.2 84.4 92.9\nTable 12\nResults on PATB3-TEST for form-based models which performed best on PATB3-DEV ?\npredicted input. Statistical significance tested on the PATB3-TEST set, only for MaltParser\nCORE12+. . . models against the MaltParser CORE12 baseline model output.\nmodel (POS tag set and morph. features) LAS UAS LS\nCORE12 77.3 81.0 90.1\nCORE12+DET+PNG 78.6 81.7 91.1\nCORE12+DET+LMM+PNG 79.1++ 82.1 91.4\nCATIBEX 78.5 81.8 91.0\nCATIBEX+DET+LMM+PNG 79.3 82.4 91.6\n"},{"#tail":"\n","@confidence":"0.993376066666667","#text":"\nCORE12 (repeated) 77.3 81.0 90.1\nCORE12+DET2+LMM+PG+FNNUMDGTBIN 79.3++ 82.3 91.4\nCORE12+DET2+LMM+PERSON +FN*NGR 78.9+ 82.3 91.0\nCORE12+DET2+LMM+PERSON +FN*NG 79.1++ 82.1 91.4\nCORE12+DET2+LMM+PERSON +FN*NGR g+p, Easy-First Parser 81.0++ 84.0 92.7\nCORE12+DET2+LMM+PERSON +FN*NG g+p, Easy-First Parser 80.9++ 83.9 92.8\nCATIBEX 78.5 81.8 91.0\nCATIBEX+DET2+LMM+PG+FNNUMDGTBIN 79.4 82.5 91.6\nCATIBEX+DET2+LMM+PERSON +FN*NGR 79.3 82.6 91.3\nCATIBEX+DET2+LMM+PERSON +FN*NG 79.3 82.4 91.5\nCATIBEX+DET2+LMM+PERSON +FN*NGR g+p, Easy-First Parser 79.5 83.0 91.9\nCATIBEX+DET2+LMM+PERSON +FN*NG g+p, Easy-First Parser 79.6 82.8 92.1\nBW 72.1 77.2 86.3\nBW+DET2+LMM+PERSON +FN*NGR g+p, Easy-First Parser 79.6 82.7 92.2\nBW+DET2+LMM+PERSON +FN*NG g+p, Easy-First Parser 79.7 82.9 92.3\n"},{"#tail":"\n","@confidence":"0.57312275","#text":"\nCORE12+DET+LMM+PERSON+FN*NGR g+p, PATB3-DEV 83.6 86.5 93.5\nEasy-First Parser\nCORE12+DET+LMM+PERSON+FN*NGR g+p, PATB3-TEST 81.7 84.6 92.8\nEasy-First Parser\n"},{"#tail":"\n","@confidence":"0.9738796","#text":"\nCORE12 67.9 90.4 72.0 70.3 94.5 78.7\nCORE12 + LMM 68.8 90.4 72.6 70.9 94.6 79.0\nCORE12 + DET2+LMM+PNG 71.7 91.0 74.9 72.4 95.5 80.2\nCORE12 + DET2+LMM+PERSON +FN*NG 72.3 91.0 75.6 72.7 95.5 80.4\nCORE12 + DET2+LMM+PERSON +FN*NGR 71.9 91.2 74.5 73.2 95.3 80.2\n"},{"#tail":"\n","@confidence":"0.852574888888889","#text":"\nCORE12 67.9 90.4 72.0 70.3 94.5 78.7\nCORE12 g+p 70.9 91.0 73.5 70.2 94.7 80.0\nCORE12 + DET+LMM+PNG g+p 73.7 91.6 76.6 72.8 96.3 81.7\nCORE12 + DET2+LMM+PERSON 74.3 91.8 77.4 72.9 95.2 81.8\n+FN*NG g+p\nCORE12 + DET2+LMM+PERSON 74.8 91.6 77.4 73.5 95.5 81.9\n+FN*NGR g+p\n187\nComputational Linguistics Volume 39, Number 1\n"},{"#tail":"\n","@confidence":"0.98026","#text":"\nCORE12 70.8 90.7 73.1 71.4 94.2 79.6\nCORE12 g+p 73.3 91.2 74.6 71.4 95.0 80.8\nCORE12 + DET+LMM+PERSON+FN*NG g+p 76.4 91.9 77.9 73.2 96.2 82.7\nCORE12 + DET+LMM+PERSON+FN*NGR g+p 76.2 91.9 78.1 73.2 95.9 82.6\n"},{"#tail":"\n","@confidence":"0.896321","#text":"\nGold 97.8 98.1\nMaltParser using CORE12 95.2 88.6\nEasy-First Parser using CORE12 + 96.8 95.8\nDET+LMM+PERSON+FN*NGR g+p\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.819214","#tail":"\n","@no":"0","#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.991643","#text":"Center for Computational Learning Systems, Columbia University"},{"#tail":"\n","@confidence":"0.9958095","#text":"Center for Computational Learning Systems, Columbia University"}],"author":[{"#tail":"\n","@confidence":"0.995613","#text":"Nizar Habash"},{"#tail":"\n","@confidence":"0.989908","#text":"Owen Rambow"}],"abstract":{"#tail":"\n","@confidence":"0.996754833333333","#text":"We explore the contribution of lexical and inflectional morphology features to dependency parsing of Arabic, a morphologically rich language with complex agreement patterns. Using controlled experiments, we contrast the contribution of different part-of-speech (POS) tag sets and morphological features in two input conditions: machine-predicted condition (in which POS tags and morphological feature values are automatically assigned), and gold condition (in which their true values are known). We find that more informative (fine-grained) tag sets are useful in the gold condition, but may be detrimental in the predicted condition, where they are outperformed by simpler but more accurately predicted tag sets. We identify a set of features (definiteness, person, number, gender, and undiacritized lemma) that improve parsing quality in the predicted condition, whereas other features are more useful in gold. We are the first to show that functional features for gender and number (e.g., ?broken plurals?), and optionally the related rationality (?humanness?) feature, are more helpful for parsing than form-based gender and number. We finally show that parsing quality in the predicted condition can dramatically improve by training in a combined gold+predicted condition. We experimented with two transition-based parsers, MaltParser and Easy-First Parser. Our findings are robust across parsers, models, and input conditions. This suggests that the contribution of the linguistic knowledge in the tag sets and features we identified goes beyond particular experimental settings, and may be informative for other parsers and morphologically rich languages."},"title":{"#tail":"\n","@confidence":"0.9701625","#text":"Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features Yuval Marton? Nuance Communications"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Alkuhlani, Sarah and Nizar Habash. 2011. A corpus for modeling morpho-syntactic agreement in Arabic: Gender, number and rationality. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 357?362, Portland, OR."},"#text":"\n","pages":{"#tail":"\n","#text":"357--362"},"marker":{"#tail":"\n","#text":"Alkuhlani, Habash, 2011"},"location":{"#tail":"\n","#text":"Portland, OR."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"best model on predicted input. We then modified the ElixirFMbased best model to use the enhanced DET2 feature. This variation yielded a similarly small gain, altogether less than 0.2% from its ElixirFM-free counterparts. 5.2 Functional Gender and Number Features, and the Rationality Feature The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011).18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features, and with predicted features, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012).19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gend","@endWordPosition":"9251","@position":"58727","annotationId":"T1","@startWordPosition":"9248","@citStr":"Alkuhlani and Habash 2011"},{"#tail":"\n","#text":"accuracy again. The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX?the best performing tag set with predicted input. Note, however, that the 1% (absolute) advantage of CATIBEX (without additional features) over the morphology-free CORE12 on machine-predicted input (Table 2) has 16 http://sourceforge.net/projects/elixir-fm. 17 We also applied the manipulations described in Section A.3 to FNNUM, giving us the variants FNNUMDGT and FNNUMDGTBIN, which we tested similarly. 18 In this article, we use a newer version of the corpus by Alkuhlani and Habash (2011) than the one we used in Marton, Habash, and Rambow (2011). 19 The paper by Alkuhlani and Habash (2012) presents additional, more sophisticated models that we do not use in this article. 178 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). FN* = functional feature(s) based on Alkuhlani and Habash (2011); GN = GENDER+NUMBER; GNR = GENDER+ NUMBER+RAT. Statistical significance tested only for CORE12+. . . models on predicted input, against the CORE12 baseline. model (POS tag set and feature","@endWordPosition":"9527","@position":"60554","annotationId":"T2","@startWordPosition":"9524","@citStr":"Alkuhlani and Habash (2011)"},{"#tail":"\n","#text":"erform identically with or without rationality, but subject and idafa perform worse; only nominal modification performs better (with overall performance decreasing). If we inspect the unlabeled attachment scores for subjects, we do detect an increase in accuracy (from 85.0% to 85.4%); perhaps the parser can exploit the rationality feature, but the labeler cannot. Grammaticality of parse trees. We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by Alkuhlani and Habash (2011) and evaluate our baseline (MaltParser using only CORE12), best performing model (Easy-First Parser using CORE12 + DET+LMM+PERSON+FN*NGR g+p), and the gold reference. The agreement checker verifies, for all verb?nominal subject relations and noun?adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria. Note that we use the syntax given by the tree, not the gold syntax. For all three trees, however, we used gold morphological features for this evaluation even whe","@endWordPosition":"14250","@position":"90698","annotationId":"T3","@startWordPosition":"14247","@citStr":"Alkuhlani and Habash (2011)"}]},"title":{"#tail":"\n","#text":"A corpus for modeling morpho-syntactic agreement in Arabic: Gender, number and rationality."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sarah Alkuhlani"},{"#tail":"\n","#text":"Nizar Habash"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Alkuhlani, Sarah and Nizar Habash. 2012. Identifying broken plurals, irregular gender, and rationality in Arabic text. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 675?685, Avignon."},"#text":"\n","pages":{"#tail":"\n","#text":"675--685"},"marker":{"#tail":"\n","#text":"Alkuhlani, Habash, 2012"},"location":{"#tail":"\n","#text":"Avignon."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"th 2012). The Elixir-FM analyzer (Smr? 2007) readily provides the 4 PATB-tokenized words; see Section 2.5. 5 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity. 6 Note that the functional and form-based feature values for verbs always coincide. 165 Computational Linguistics Volume 39, Number 1 functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012). See Section 5.2 for more details. 2.4 Morpho-Syntactic Interactions Inflectional features and rationality interact with syntax in two ways. In agreement relations, two words in a specific syntactic configuration have coordinated values for specific sets of features. MSA has standard (i.e., matching value) agreement for subject? verb pairs on PERSON, GENDER, and NUMBER, and for noun?adjective pairs on NUMBER, GENDER, CASE, and DET. There are, however, three very common cases of exceptional agreement: Verbs preceding subjects are always singular, adjectives of irrational plural nouns are alway","@endWordPosition":"2746","@position":"17853","annotationId":"T4","@startWordPosition":"2743","@citStr":"Alkuhlani and Habash 2012"},{"#tail":"\n","#text":"lues but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011).18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features, and with predicted features, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012).19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts. The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and pre","@endWordPosition":"9305","@position":"59102","annotationId":"T5","@startWordPosition":"9302","@citStr":"Alkuhlani and Habash 2012"},{"#tail":"\n","#text":"ure combination, using CATIBEX?the best performing tag set with predicted input. Note, however, that the 1% (absolute) advantage of CATIBEX (without additional features) over the morphology-free CORE12 on machine-predicted input (Table 2) has 16 http://sourceforge.net/projects/elixir-fm. 17 We also applied the manipulations described in Section A.3 to FNNUM, giving us the variants FNNUMDGT and FNNUMDGTBIN, which we tested similarly. 18 In this article, we use a newer version of the corpus by Alkuhlani and Habash (2011) than the one we used in Marton, Habash, and Rambow (2011). 19 The paper by Alkuhlani and Habash (2012) presents additional, more sophisticated models that we do not use in this article. 178 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). FN* = functional feature(s) based on Alkuhlani and Habash (2011); GN = GENDER+NUMBER; GNR = GENDER+ NUMBER+RAT. Statistical significance tested only for CORE12+. . . models on predicted input, against the CORE12 baseline. model (POS tag set and features) gold predictedLAS UAS LS LAS UAS LS CORE12 (repeated) 82.9 85.4 93.5 78.7 82.5 90.6 +FN*RATIONAL 83.","@endWordPosition":"9546","@position":"60657","annotationId":"T6","@startWordPosition":"9543","@citStr":"Alkuhlani and Habash (2012)"}]},"title":{"#tail":"\n","#text":"Identifying broken plurals, irregular gender, and rationality in Arabic text."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sarah Alkuhlani"},{"#tail":"\n","#text":"Nizar Habash"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of Computational Natural Language Learning (CoNLL), pages 149?164, New York, NY."},"#text":"\n","pages":{"#tail":"\n","#text":"149--164"},"marker":{"#tail":"\n","#text":"Buchholz, Marsi, 2006"},"location":{"#tail":"\n","#text":"New York, NY."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" Rambow Arabic Parsing with Lexical and Inflectional Features comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima?an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser. Our results agree with previou","@endWordPosition":"4383","@position":"27925","annotationId":"T7","@startWordPosition":"4380","@citStr":"Buchholz and Marsi 2006"}},"title":{"#tail":"\n","#text":"CoNLL-X shared task on multilingual dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Computational Natural Language Learning (CoNLL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sabine Buchholz"},{"#tail":"\n","#text":"Erwin Marsi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"institution":{"#tail":"\n","#text":"University of Pennsylvania,"},"rawString":{"#tail":"\n","#text":"Buckwalter, Timothy A. 2004. Buckwalter Arabic Morphological Analyzer Version 2.0. Linguistic Data Consortium, University of Pennsylvania, 2002. LDC Catalog No.: LDC2004L02, ISBN 1-58563-324-0."},"#text":"\n","pages":{"#tail":"\n","#text":"1--58563"},"marker":{"#tail":"\n","#text":"Buckwalter, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ngular adjective *    ?zraq+? (?blue?) is  +  zarqA?+? not &+   * *?zraq+a. To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smr? (2007), we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features.6 Most available Arabic NLP tools and resources model morphology using formbased (?surface?) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smr? 2007) readily provides the 4 PATB-tokenized words; see Section 2.5. 5 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity. 6 Note that the functional and form-based feature values for verbs always coincide. 165 Computational Linguistics Volume 39, Number 1 functional inflectional number feature, but not full functional gender (only for adjectives and verb","@endWordPosition":"2631","@position":"17077","annotationId":"T8","@startWordPosition":"2630","@citStr":"Buckwalter 2004"},{"#tail":"\n","#text":"ich we only briefly touch on here (in Section 4.1). An example CATiB dependency tree is shown in Figure 1. For the corpus statistics, see Table 1. For more information on CATiB, see Habash and Roth (2009) and Habash, Faraj, and Roth (2009). 2.6 Core POS Tag Sets Linguistically, words have associated POS tags, e.g., ?verb? or ?noun,? which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer (Buckwalter 2004) used in the PATB has a core POS set of 44 tags (CORE44) before morphological extension.8 Cross-linguistically, a core set containing around 12 tags is often 7 We ignore the rare ?false idafa? construction (Habash 2010, p. 102). 8 The 44 tags in CORE44 are based on the tokenized version of Arabic words. There are 34 untokenized core tags as used in MADA+TOKAN (Habash, Rambow, and Roth 2012). 167 Computational Linguistics Volume 39, Number 1 assumed as a ?universal tag set? (Rambow et al 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al (2006) for Arabic, and cal","@endWordPosition":"3590","@position":"23149","annotationId":"T9","@startWordPosition":"3589","@citStr":"Buckwalter 2004"},{"#tail":"\n","#text":"various available Arabic tag sets can be found in Habash (2010). The following are the various tag sets we use in this article: (a) the core POS tag sets CORE44 and the newly introduced CORE12; (b) CATiB Treebank tag set (CATIB6) (Habash and Roth 2009) and its newly introduced extension of CATIBEX created using simple regular expressions on word form, indicating particular morphemes such as the prefix  Al+ or the suffix  +wn; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4; (c) the PATB full tag set with complete morphological tag (BW) (Buckwalter 2004); and two extensions of the PATB reduced tag set (PENN POS, a.k.a. RTS, size 24 [Diab, Hacioglu, and Jurafsky 2004]), both outperforming it: (d) Kulick, Gabbard, and Marcus (2006)?s tag set (KULICK), size 43, one of whose most important extensions is the marking of the definite article clitic, and (e) Diab and Benajiba?s (in preparation) EXTENDED RTS tag set (ERTS), which marks gender, number, and definiteness, size 134. 3. Related Work Much work has been done on the use of morphological features for parsing of morphologically rich languages. Collins et al (1999) report that an optimal tag set","@endWordPosition":"4054","@position":"25912","annotationId":"T10","@startWordPosition":"4053","@citStr":"Buckwalter 2004"}]},"title":{"#tail":"\n","#text":"Buckwalter Arabic Morphological Analyzer Version 2.0. Linguistic Data Consortium,"},"booktitle":{"#tail":"\n","#text":"LDC Catalog No.: LDC2004L02, ISBN"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Timothy A Buckwalter"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Collins, Michael, Jan Hajic, Lance Ramshaw, and Christoph Tillmann. 1999. A statistical parser for Czech. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pages 505?512, College Park, MD."},"#text":"\n","pages":{"#tail":"\n","#text":"505--512"},"marker":{"#tail":"\n","#text":"Collins, Hajic, Ramshaw, Tillmann, 1999"},"location":{"#tail":"\n","#text":"College Park, MD."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ill be of little or no help for parsing, even if helpful when its gold values are provided. As we will see, the CASE feature is very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is not useful. Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009). In this article we investigate morphological features for dependency parsing of Modern Standard Arabic (MSA). For MSA, the space of possible morphological features is fairly large. We determine which morphological features help and why. We further determine the upper bound for thei","@endWordPosition":"816","@position":"5430","annotationId":"T11","@startWordPosition":"813","@citStr":"Collins et al 1999"},{"#tail":"\n","#text":"h complete morphological tag (BW) (Buckwalter 2004); and two extensions of the PATB reduced tag set (PENN POS, a.k.a. RTS, size 24 [Diab, Hacioglu, and Jurafsky 2004]), both outperforming it: (d) Kulick, Gabbard, and Marcus (2006)?s tag set (KULICK), size 43, one of whose most important extensions is the marking of the definite article clitic, and (e) Diab and Benajiba?s (in preparation) EXTENDED RTS tag set (ERTS), which marks gender, number, and definiteness, size 134. 3. Related Work Much work has been done on the use of morphological features for parsing of morphologically rich languages. Collins et al (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable). This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ?3000+). They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic? and Vidov?-Hladk? 1998) compared with Arabic (?14.0%, see Table 3). Similarly, Cowan and Collins (2005) r","@endWordPosition":"4147","@position":"26481","annotationId":"T12","@startWordPosition":"4144","@citStr":"Collins et al (1999)"}]},"title":{"#tail":"\n","#text":"A statistical parser for Czech."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michael Collins"},{"#tail":"\n","#text":"Jan Hajic"},{"#tail":"\n","#text":"Lance Ramshaw"},{"#tail":"\n","#text":"Christoph Tillmann"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Cowan, Brooke and Michael Collins. 2005. Morphology and reranking for the statistical parsing of Spanish. In Proceedings of Human Language Technology (HLT) and the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 795?802, Morristown, NJ."},"#text":"\n","pages":{"#tail":"\n","#text":"795--802"},"marker":{"#tail":"\n","#text":"Cowan, Collins, 2005"},"location":{"#tail":"\n","#text":"Morristown, NJ."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ages. Collins et al (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable). This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ?3000+). They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic? and Vidov?-Hladk? 1998) compared with Arabic (?14.0%, see Table 3). Similarly, Cowan and Collins (2005) report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations. Our approach is 168 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima?an (2007) report that extending POS and phrase structure tags with definiteness i","@endWordPosition":"4253","@position":"27079","annotationId":"T13","@startWordPosition":"4250","@citStr":"Cowan and Collins (2005)"}},"title":{"#tail":"\n","#text":"Morphology and reranking for the statistical parsing of Spanish."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Human Language Technology (HLT) and the Conference on Empirical Methods in Natural Language Processing (EMNLP),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Brooke Cowan"},{"#tail":"\n","#text":"Michael Collins"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Dada, Ali. 2007. Implementation of Arabic numerals and their syntax in GF. In Proceedings of the Workshop on Computational Approaches to Semitic Languages, pages 9?16, Prague."},"#text":"\n","pages":{"#tail":"\n","#text":"9--16"},"marker":{"#tail":"\n","#text":"Dada, 2007"},"location":{"#tail":"\n","#text":"Prague."},"title":{"#tail":"\n","#text":"Implementation of Arabic numerals and their syntax in GF."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Workshop on Computational Approaches to Semitic Languages,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Ali Dada"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Diab, Mona. 2007. Towards an optimal POS tag set for modern standard Arabic processing. In Proceedings of Recent Advances in Natural Language Processing (RANLP), pages 91?96, Borovets. Diab, Mona and Yassine Benajiba. (in preparation). From raw text to base phrase chunks: The new generation of AMIRA Tools for the processing of Modern Standard Arabic."},"#text":"\n","pages":{"#tail":"\n","#text":"91--96"},"marker":{"#tail":"\n","#text":"Diab, 2007"},"location":{"#tail":"\n","#text":"Borovets. Diab, Mona"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"bs) outperforms other combinations. Our approach is 168 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima?an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the expe","@endWordPosition":"4369","@position":"27836","annotationId":"T14","@startWordPosition":"4368","@citStr":"Diab 2007"}},"title":{"#tail":"\n","#text":"Towards an optimal POS tag set for modern standard Arabic processing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Recent Advances in Natural Language Processing (RANLP),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Mona Diab"}}},{"date":{"#tail":"\n","#text":"2004"},"title":{"#tail":"\n","#text":"Automatic tagging of Arabic text: From raw text to base phrase 23 Available for downloading at http://www1.ccls.columbia.edu/ CATiB/parser."},"volume":{"#tail":"\n","#text":"39"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Diab, Mona, Kadri Hacioglu, and Daniel Jurafsky. 2004. Automatic tagging of Arabic text: From raw text to base phrase 23 Available for downloading at http://www1.ccls.columbia.edu/ CATiB/parser. Computational Linguistics Volume 39, Number 1 chunks. In Proceedings of the 4th Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL) -Human Language Technology (HLT), pages 149?152, Boston, MA."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"pages":{"#tail":"\n","#text":"149--152"},"#text":"\n","marker":{"#tail":"\n","#text":"Diab, Hacioglu, Jurafsky, 2004"},"location":{"#tail":"\n","#text":"Boston, MA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 4th Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL) -Human Language Technology (HLT),"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mona Diab"},{"#tail":"\n","#text":"Kadri Hacioglu"},{"#tail":"\n","#text":"Daniel Jurafsky"}]}},{"volume":{"#tail":"\n","#text":"34"},"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Eryigit, G?lsen, Joakim Nivre, and Kemal Oflazer. 2008. Dependency parsing of Turkish. Computational Linguistics, 34(3):357?389."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Eryigit, Nivre, Oflazer, 2008"},"title":{"#tail":"\n","#text":"Dependency parsing of Turkish."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Glsen Eryigit"},{"#tail":"\n","#text":"Joakim Nivre"},{"#tail":"\n","#text":"Kemal Oflazer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Goldberg, Yoav and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Proceedings of Human Language Technology (HLT): The North American Chapter of the Association for Computational Linguistics (NAACL), pages 742?750, Los Angeles, CA."},"#text":"\n","pages":{"#tail":"\n","#text":"742--750"},"marker":{"#tail":"\n","#text":"Goldberg, Elhadad, 2010"},"location":{"#tail":"\n","#text":"Los Angeles, CA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb?subject and noun?adjective relations. The result holds for both the MaltParser (Nivre 2008) and the Easy-First Parser (Goldberg and Elhadad 2010). Additionally, almost all work to date in MSA morphological analysis and part-ofspeech (POS) tagging has concentrated on the morphemic form of the words. Often, however, the functional morphology (which is relevant to agreement, and relates to the meaning of the word) is at odds with the ?surface? (form-based) morphology; a well-known example of this are the ?broken? (irregular) plurals of nominals. We show that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance (again, both when using gold and when using predicted ","@endWordPosition":"1129","@position":"7387","annotationId":"T15","@startWordPosition":"1126","@citStr":"Goldberg and Elhadad 2010"},{"#tail":"\n","#text":"nite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6). Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.9 We expect this kind of feature to yield lower gains for Arabic, unless:  one uses functional feature values (such as those used here for the first time in Arabic NLP),  one uses yet another representation l","@endWordPosition":"4574","@position":"29203","annotationId":"T16","@startWordPosition":"4571","@citStr":"Goldberg and Elhadad 2010"},{"#tail":"\n","#text":"++ 83.5 91.9 CATIBEX+DET2+LMM+PERSON+FN*NGR 84.1 85.9 94.4 80.7 84.0 91.9 CATIBEX+DET2+LMM+PERSON+FN*NG 83.5 85.4 94.1 80.7 83.7 92.2 shrunk with these functional feature combinations to 0.3%. We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags. 6. Evaluation of Results with Easy-First Parser In this section, we validate the contribution of key tag sets and morphological features? and combinations thereof?using a different parser: the Easy-First Parser (Goldberg and Elhadad 2010). As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values. The Easy-First Parser is a shift-reduce parser (as is MaltParser). Unlike MaltParser, however, it does not attempt to attach arcs ?eagerly? as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments). Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ?easy? attachment, to low, as estimated by the classifier).","@endWordPosition":"9781","@position":"62249","annotationId":"T17","@startWordPosition":"9778","@citStr":"Goldberg and Elhadad 2010"},{"#tail":"\n","#text":"ree CORE12 on machine-predicted input (compare with only 1% in MaltParser in Table 2) has shrunk completely with these functional feature combinations. This suggests that Easy-First Parser is more resilient to accuracy errors (presumably due to its design to make less ambiguous decisions earlier), and hence can take better advantage of the relevant information encoded in our functional morphology features. 7. Combined Gold and Predicted Features for Training So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers, however, including Goldberg and Elhadad (2010), train on predicted feature values instead. It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold).21 To test our hypothesis, we start this secti","@endWordPosition":"10332","@position":"65948","annotationId":"T18","@startWordPosition":"10329","@citStr":"Goldberg and Elhadad (2010)"}]},"title":{"#tail":"\n","#text":"An efficient algorithm for easy-first non-directional dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Human Language Technology (HLT): The North American Chapter of the Association for Computational Linguistics (NAACL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yoav Goldberg"},{"#tail":"\n","#text":"Michael Elhadad"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Green, Spence and Christopher D. Manning. 2010. Better Arabic parsing: Baselines, evaluations, and analysis. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 394?402, Beijing."},"#text":"\n","pages":{"#tail":"\n","#text":"394--402"},"marker":{"#tail":"\n","#text":"Green, Manning, 2010"},"location":{"#tail":"\n","#text":"Beijing."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"orms other combinations. Our approach is 168 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima?an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here wer","@endWordPosition":"4373","@position":"27861","annotationId":"T19","@startWordPosition":"4370","@citStr":"Green and Manning 2010"},{"#tail":"\n","#text":" 78.5 81.8 91.0 CATIBEX+DET+LMM+PNG 79.3 82.4 91.6 183 Computational Linguistics Volume 39, Number 1 is the test split of part 3 of the PATB (hereafter PATB3-TEST; see Table 1 for details). Table 12 shows that the same trends held on this set too, with even greater relative gains, up to almost 2% absolute gains. We then also revalidated the contribution of the best performing models from Sections 5?7 on PATB3-TEST. Here, too, the same trends held. Results are shown in Table 13. 8.2 Best Results on Length-Filtered Input For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long. We report these filtered results in Table 14. Filtered results are consistently higher (as expected). Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set. The contribution of the RAT feature across sets is negligible (or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but about 0.15% gain on the test set. For clarity and conciseness, we only show the best model (with RAT) in Table 14. 8.3 Error Analysis We perform two types of error analyses. First","@endWordPosition":"11829","@position":"75723","annotationId":"T20","@startWordPosition":"11826","@citStr":"Green and Manning (2010)"}]},"title":{"#tail":"\n","#text":"Better Arabic parsing: Baselines, evaluations, and analysis."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Spence Green"},{"#tail":"\n","#text":"Christopher D Manning"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Habash, Nizar. 2010. Introduction to Arabic Natural Language Processing. Morgan & Claypool Publishers."},"#text":"\n","marker":{"#tail":"\n","#text":"Habash, 2010"},"publisher":{"#tail":"\n","#text":"Morgan & Claypool Publishers."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" and Roth (2009). 2.6 Core POS Tag Sets Linguistically, words have associated POS tags, e.g., ?verb? or ?noun,? which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer (Buckwalter 2004) used in the PATB has a core POS set of 44 tags (CORE44) before morphological extension.8 Cross-linguistically, a core set containing around 12 tags is often 7 We ignore the rare ?false idafa? construction (Habash 2010, p. 102). 8 The 44 tags in CORE44 are based on the tokenized version of Arabic words. There are 34 untokenized core tags as used in MADA+TOKAN (Habash, Rambow, and Roth 2012). 167 Computational Linguistics Volume 39, Number 1 assumed as a ?universal tag set? (Rambow et al 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al (2006) for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation","@endWordPosition":"3627","@position":"23367","annotationId":"T21","@startWordPosition":"3626","@citStr":"Habash 2010"},{"#tail":"\n","#text":"ely rare (in our training corpus of about 300,000 words, we encounter only 430 POS tags with complete morphology). Therefore, researchers have proposed tag sets for MSA whose size is similar to that of the English PTB tag set, as this has proven to be a useful size computationally. These tag sets are hybrids in the sense that they are neither simply the core POS, nor the complete morphologically enriched tag set, but instead they selectively enrich the core POS tag set with only certain morphological features. A more detailed discussion of the various available Arabic tag sets can be found in Habash (2010). The following are the various tag sets we use in this article: (a) the core POS tag sets CORE44 and the newly introduced CORE12; (b) CATiB Treebank tag set (CATIB6) (Habash and Roth 2009) and its newly introduced extension of CATIBEX created using simple regular expressions on word form, indicating particular morphemes such as the prefix  Al+ or the suffix  +wn; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4; (c) the PATB full tag set with complete morphological tag (BW) (Buckwalter 2004); and two extensions of the PATB reduced tag se","@endWordPosition":"3960","@position":"25359","annotationId":"T22","@startWordPosition":"3959","@citStr":"Habash (2010)"}]},"title":{"#tail":"\n","#text":"Introduction to Arabic Natural Language Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Nizar Habash"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Habash, Nizar, Reem Faraj, and Ryan Roth. 2009. Syntactic Annotation in the Columbia Arabic Treebank. In Proceedings of MEDAR International Conference on Arabic Language Resources and Tools, pages 125?135, Cairo."},"#text":"\n","pages":{"#tail":"\n","#text":"125--135"},"marker":{"#tail":"\n","#text":"Habash, Faraj, Roth, 2009"},"location":{"#tail":"\n","#text":"Cairo."},"title":{"#tail":"\n","#text":"Syntactic Annotation in the Columbia Arabic Treebank."},"booktitle":{"#tail":"\n","#text":"In Proceedings of MEDAR International Conference on Arabic Language Resources and Tools,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Reem Faraj"},{"#tail":"\n","#text":"Ryan Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Habash, Nizar, Ryan Gabbard, Owen Rambow, Seth Kulick, and Mitch Marcus. 2007. Determining case in Arabic: Learning complex linguistic behavior requires complex linguistic features. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1,084?1,092, Prague."},"#text":"\n","pages":{"#tail":"\n","#text":"1--084"},"marker":{"#tail":"\n","#text":"Habash, Gabbard, Rambow, Kulick, Marcus, 2007"},"location":{"#tail":"\n","#text":"Prague."},"title":{"#tail":"\n","#text":"Determining case in Arabic: Learning complex linguistic behavior requires complex linguistic features."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Ryan Gabbard"},{"#tail":"\n","#text":"Owen Rambow"},{"#tail":"\n","#text":"Seth Kulick"},{"#tail":"\n","#text":"Mitch Marcus"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Habash, Nizar and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 573?580, Ann Arbor, MI."},"#text":"\n","pages":{"#tail":"\n","#text":"573--580"},"marker":{"#tail":"\n","#text":"Habash, Rambow, 2005"},"location":{"#tail":"\n","#text":"Ann Arbor, MI."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"n the correspondence between inflectional features and morphemes, and inspired by Smr? (2007), we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features.6 Most available Arabic NLP tools and resources model morphology using formbased (?surface?) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smr? 2007) readily provides the 4 PATB-tokenized words; see Section 2.5. 5 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity. 6 Note that the functional and form-based feature values for verbs always coincide. 165 Computational Linguistics Volume 39, Number 1 functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, an","@endWordPosition":"2650","@position":"17203","annotationId":"T23","@startWordPosition":"2647","@citStr":"Habash and Rambow 2005"},{"#tail":"\n","#text":"umn shows the number of tag types actually occurring in the training data. Predicted POS tag values. So far we discussed optimal (gold) conditions. But in practice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14 The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear. Put differently, we are interested in the tradeoff between relevance and accuracy. Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012)15 (see Table 2, 14 Some parsers predict POS tags internally, instead of receiving them as input, but this is not the case in this article. 15 We use MADA v3.1 in all of our experiments. We note that MADA v3.1 was tuned on the same development set that we use for making our parsing model choices; ideally, we would have chosen a different development set for our work on parsing, but we thought it would be best to use MADA as a black box component (for past and future comparability), and did not have sufficient data to carve out from a second development set (whil","@endWordPosition":"6254","@position":"39631","annotationId":"T24","@startWordPosition":"6251","@citStr":"Habash and Rambow 2005"}]},"title":{"#tail":"\n","#text":"Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Owen Rambow"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"MADA+TOKAN Manual. Technical Report CCLS-12-01,"},"date":{"#tail":"\n","#text":"2012"},"institution":{"#tail":"\n","#text":"Columbia University,"},"rawString":{"#tail":"\n","#text":"Habash, Nizar, Owen Rambow, and Ryan Roth. 2012. MADA+TOKAN Manual. Technical Report CCLS-12-01, Columbia University, New York, NY."},"#text":"\n","marker":{"#tail":"\n","#text":"Habash, Rambow, Roth, 2012"},"location":{"#tail":"\n","#text":"New York, NY."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Owen Rambow"},{"#tail":"\n","#text":"Ryan Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Habash, Nizar and Ryan Roth. 2009. CATiB: The Columbia Arabic treebank. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221?224, Suntec."},"#text":"\n","pages":{"#tail":"\n","#text":"221--224"},"marker":{"#tail":"\n","#text":"Habash, Roth, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":", in which both elements generally exhibit agreement in definiteness (and agreement in other features, too). Although only N-N may be followed by additional N elements in Idafa relation, both constructions may be followed by one or more adjectival modifiers. Lexical features do not constrain syntactic structure as inflectional features do. Instead, bilexical dependencies are used to model semantic relations that often are the only way to disambiguate among different possible syntactic structures. 2.5 Corpus, CATiB Format, and the CATIB6 POS Tag Set We use the Columbia Arabic Treebank (CATiB) (Habash and Roth 2009). Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information. CATiB?s dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations. It has a reduced POS tag set consisting of six tags only (henceforth CATIB6). The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (","@endWordPosition":"3294","@position":"21200","annotationId":"T25","@startWordPosition":"3291","@citStr":"Habash and Roth 2009"},{"#tail":"\n","#text":"s here. For other PATB-based POS tag sets, see Sections 2.6 and 2.7. The CATiB Treebank uses the word segmentation of the PATB. It splits off several categories of orthographic clitics, but not the definite article + Al+ (?the?). In all of the experiments reported in this article, we use the gold segmentation. Tokenization involves further decisions on the segmented token forms, such as spelling normalization, which we only briefly touch on here (in Section 4.1). An example CATiB dependency tree is shown in Figure 1. For the corpus statistics, see Table 1. For more information on CATiB, see Habash and Roth (2009) and Habash, Faraj, and Roth (2009). 2.6 Core POS Tag Sets Linguistically, words have associated POS tags, e.g., ?verb? or ?noun,? which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer (Buckwalter 2004) used in the PATB has a core POS set of 44 tags (CORE44) before morphological extension.8 Cross-linguistically, a core set containing around 12 tags is often 7 We ignore the rare ?false id","@endWordPosition":"3533","@position":"22737","annotationId":"T26","@startWordPosition":"3530","@citStr":"Habash and Roth (2009)"},{"#tail":"\n","#text":"s similar to that of the English PTB tag set, as this has proven to be a useful size computationally. These tag sets are hybrids in the sense that they are neither simply the core POS, nor the complete morphologically enriched tag set, but instead they selectively enrich the core POS tag set with only certain morphological features. A more detailed discussion of the various available Arabic tag sets can be found in Habash (2010). The following are the various tag sets we use in this article: (a) the core POS tag sets CORE44 and the newly introduced CORE12; (b) CATiB Treebank tag set (CATIB6) (Habash and Roth 2009) and its newly introduced extension of CATIBEX created using simple regular expressions on word form, indicating particular morphemes such as the prefix  Al+ or the suffix  +wn; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4; (c) the PATB full tag set with complete morphological tag (BW) (Buckwalter 2004); and two extensions of the PATB reduced tag set (PENN POS, a.k.a. RTS, size 24 [Diab, Hacioglu, and Jurafsky 2004]), both outperforming it: (d) Kulick, Gabbard, and Marcus (2006)?s tag set (KULICK), size 43, one of whose most important","@endWordPosition":"3994","@position":"25548","annotationId":"T27","@startWordPosition":"3991","@citStr":"Habash and Roth 2009"},{"#tail":"\n","#text":" Features comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima?an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser. Our results agree with previous work on Arabic and Hebrew in that marking the de","@endWordPosition":"4392","@position":"27975","annotationId":"T28","@startWordPosition":"4389","@citStr":"Habash and Roth 2009"}]},"title":{"#tail":"\n","#text":"CATiB: The Columbia Arabic treebank."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Ryan Roth"}]}},{"date":{"#tail":"\n","#text":"2007"},"editor":{"#tail":"\n","#text":"In A. van den Bosch and A. Soudi, editors,"},"title":{"#tail":"\n","#text":"On Arabic transliteration."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Habash, Nizar, Abdelhadi Soudi, and Tim Buckwalter. 2007. On Arabic transliteration. In A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology: Knowledge-based and Empirical Methods, pages 15?22. Springer, Berlin."},"#text":"\n","pages":{"#tail":"\n","#text":"15--22"},"marker":{"#tail":"\n","#text":"Habash, Soudi, Buckwalter, 2007"},"publisher":{"#tail":"\n","#text":"Springer,"},"location":{"#tail":"\n","#text":"Berlin."},"booktitle":{"#tail":"\n","#text":"Arabic Computational Morphology: Knowledge-based and Empirical Methods,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Abdelhadi Soudi"},{"#tail":"\n","#text":"Tim Buckwalter"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Hajic?, Jan and Barbora Vidov?-Hladk?. 1998. Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset. In Proceedings of the International Conference on Computational Linguistics (COLING) - the Association for Computational Linguistics (ACL), pages 483?490, Stroudsburg, PA."},"#text":"\n","pages":{"#tail":"\n","#text":"483--490"},"marker":{"#tail":"\n","#text":"Hajic, Vidov-Hladk, 1998"},"location":{"#tail":"\n","#text":"Stroudsburg, PA."},"title":{"#tail":"\n","#text":"Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the International Conference on Computational Linguistics (COLING) - the Association for Computational Linguistics (ACL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jan Hajic"},{"#tail":"\n","#text":"Barbora Vidov-Hladk"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Hohensee, Matt and Emily M. Bender. 2012. Getting more from morphology in multilingual dependency parsing."},"#text":"\n","marker":{"#tail":"\n","#text":"Hohensee, Bender, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6). Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.9 We expect this kind of feature to yield lower gains for Arabic, unless:  one uses functional feature values (such as those used here for the first time in Arabic NLP),  one uses yet another representation level to account for the otherwise non-id","@endWordPosition":"4580","@position":"29243","annotationId":"T29","@startWordPosition":"4577","@citStr":"Hohensee and Bender (2012)"},{"#tail":"\n","#text":"atterns of irrational plurals,  one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and  one adequately represents the otherwise ?inverse? number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too). 4. Basic Parsing Experiments We examined a large space of settings. In all our experiments, we contrasted the results obtained using machine-predicted input with the results obtained using gold input (the 9 We do not relate to specific results in their study because it has been brought to our attention that Hohensee and Bender (2012) are in the process of rechecking their code for errors, and rerunning their experiments (personal communication). 169 Computational Linguistics Volume 39, Number 1 upper bound for using these features). We started by looking at individual features (including POS tag sets) and their prediction accuracy. We then explored various feature combinations in a hill-climbing fashion. We examined these issues in the following order: 1. the contribution of POS tag sets to the parsing quality, as a function of the amount of information encoded in the tag set, using (a) gold input, and (b) machinepredicte","@endWordPosition":"4782","@position":"30487","annotationId":"T30","@startWordPosition":"4779","@citStr":"Hohensee and Bender (2012)"}]},"title":{"#tail":"\n","#text":"Getting more from morphology in multilingual dependency parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Matt Hohensee"},{"#tail":"\n","#text":"Emily M Bender"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 315?326, Montr?al."},"#text":"\n","pages":{"#tail":"\n","#text":"315--326"},"marker":{"#tail":"\n"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"K?bler, Sandra, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Synthesis Lectures on Human Language Technologies. Morgan and Claypool Publishers."},"#text":"\n","marker":{"#tail":"\n","#text":"Kbler, McDonald, Nivre, 2009"},"publisher":{"#tail":"\n","#text":"Morgan and Claypool Publishers."},"title":{"#tail":"\n","#text":"Dependency Parsing. Synthesis Lectures on Human Language Technologies."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sandra Kbler"},{"#tail":"\n","#text":"Ryan McDonald"},{"#tail":"\n","#text":"Joakim Nivre"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Kulick, Seth, Ryan Gabbard, and Mitch Marcus. 2006. Parsing the Arabic Treebank: Analysis and improvements. In Proceedings of the Treebanks and Linguistic Theories Conference, pages 31?42, Prague."},"#text":"\n","pages":{"#tail":"\n","#text":"31--42"},"marker":{"#tail":"\n","#text":"Kulick, Gabbard, Marcus, 2006"},"location":{"#tail":"\n","#text":"Prague."},"title":{"#tail":"\n","#text":"Parsing the Arabic Treebank: Analysis and improvements."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Treebanks and Linguistic Theories Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Seth Kulick"},{"#tail":"\n","#text":"Ryan Gabbard"},{"#tail":"\n","#text":"Mitch Marcus"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Maamouri, Mohamed, Ann Bies, Timothy A. Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic Treebank: Building a large-scale annotated Arabic corpus. In Proceedings of the NEMLAR Conference on Arabic Language Resources and Tools, pages 102?109, Cairo."},"#text":"\n","pages":{"#tail":"\n","#text":"102--109"},"marker":{"#tail":"\n","#text":"Maamouri, Bies, Buckwalter, Mekki, 2004"},"location":{"#tail":"\n","#text":"Cairo."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" suffixes, for example, the feminine form of the masculine singular adjective *    ?zraq+? (?blue?) is  +  zarqA?+? not &+   * *?zraq+a. To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smr? (2007), we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features.6 Most available Arabic NLP tools and resources model morphology using formbased (?surface?) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smr? 2007) readily provides the 4 PATB-tokenized words; see Section 2.5. 5 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity. 6 Note that the functional and form-based feature values for verbs always coincide. 165 Computational Linguistics Volume 39, Number 1 functional inflectional number feature, but","@endWordPosition":"2625","@position":"17020","annotationId":"T31","@startWordPosition":"2622","@citStr":"Maamouri et al 2004"},{"#tail":"\n","#text":"accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given. For statistical significance, we use McNemar?s test on non-gold LAS, as implemented by Nilsson and Nivre (2008). We denote p < 0.05 and p < 0.01 with + and ++, respectively. 4.1 Data Sets and Parser For all the experiments reported in this article, we used the training portion of PATB Part 3 v3.1 (Maamouri et al 2004), converted to the CATiB Treebank format, as mentioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set.10 We kept the test unseen (?blind?) during training and model development. Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1. For all experiments reported in this section we used the syntactic dependency parser MaltParser","@endWordPosition":"5118","@position":"32608","annotationId":"T32","@startWordPosition":"5115","@citStr":"Maamouri et al 2004"}]},"title":{"#tail":"\n","#text":"The Penn Arabic Treebank: Building a large-scale annotated Arabic corpus."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the NEMLAR Conference on Arabic Language Resources and Tools,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mohamed Maamouri"},{"#tail":"\n","#text":"Ann Bies"},{"#tail":"\n","#text":"Timothy A Buckwalter"},{"#tail":"\n","#text":"Wigdan Mekki"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Marton, Yuval, Nizar Habash, and Owen Rambow. 2010. Improving Arabic dependency parsing with inflectional and lexical morphological features. In Proceedings of Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL) at the 11th Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL) -Human Language Technology (HLT), pages 13?21, Los Angeles, CA."},"#text":"\n","pages":{"#tail":"\n","#text":"13--21"},"marker":{"#tail":"\n","#text":"Marton, Habash, Rambow, 2010"},"location":{"#tail":"\n","#text":"Los Angeles, CA."},"title":{"#tail":"\n","#text":"Improving Arabic dependency parsing with inflectional and lexical morphological features."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL) at the 11th Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL) -Human Language Technology (HLT),"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yuval Marton"},{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Owen Rambow"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"Marton, Yuval, Nizar Habash, and Owen Rambow. 2011. Improving Arabic dependency parsing with lexical and inflectional surface and functional features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1,586?1,596, Portland, OR."},"#text":"\n","pages":{"#tail":"\n","#text":"1--586"},"marker":{"#tail":"\n","#text":"Marton, Habash, Rambow, 2011"},"location":{"#tail":"\n","#text":"Portland, OR."},"title":{"#tail":"\n","#text":"Improving Arabic dependency parsing with lexical and inflectional surface and functional features."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yuval Marton"},{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Owen Rambow"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features McClosky, David, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL) -Human Language Technology (HLT), pages 152?159, Brooklyn, New York."},"#text":"\n","pages":{"#tail":"\n","#text":"152--159"},"marker":{"#tail":"\n","#text":"Marton, 2006"},"location":{"#tail":"\n","#text":"Brooklyn, New York."},"title":{"#tail":"\n","#text":"and Rambow Arabic Parsing with Lexical and Inflectional"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL) -Human Language Technology (HLT),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Habash Marton"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Nilsson, Jens and Joakim Nivre. 2008. MaltEval: An evaluation and visualization tool for dependency parsing. In Proceedings of the sixth Conference on Language Resources and Evaluation (LREC), pages 161?166, Marrakech."},"#text":"\n","pages":{"#tail":"\n","#text":"161--166"},"marker":{"#tail":"\n","#text":"Nilsson, Nivre, 2008"},"location":{"#tail":"\n","#text":"Marrakech."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rating the robustness of our findings across these frameworks. In Section 7 we explore alternative training methods, and their impact on key models. All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given. For statistical significance, we use McNemar?s test on non-gold LAS, as implemented by Nilsson and Nivre (2008). We denote p < 0.05 and p < 0.01 with + and ++, respectively. 4.1 Data Sets and Parser For all the experiments reported in this article, we used the training portion of PATB Part 3 v3.1 (Maamouri et al 2004), converted to the CATiB Treebank format, as mentioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set.10 We kept the test unseen (?blind?) during training and model ","@endWordPosition":"5077","@position":"32400","annotationId":"T33","@startWordPosition":"5074","@citStr":"Nilsson and Nivre (2008)"}},"title":{"#tail":"\n","#text":"MaltEval: An evaluation and visualization tool for dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the sixth Conference on Language Resources and Evaluation (LREC),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jens Nilsson"},{"#tail":"\n","#text":"Joakim Nivre"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Nivre, Joakim. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Conference on Parsing Technologies (IWPT), pages 149?160, Nancy."},"#text":"\n","pages":{"#tail":"\n","#text":"149--160"},"marker":{"#tail":"\n","#text":"Nivre, 2003"},"location":{"#tail":"\n","#text":"Nancy."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rted to the CATiB Treebank format, as mentioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set.10 We kept the test unseen (?blind?) during training and model development. Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1. For all experiments reported in this section we used the syntactic dependency parser MaltParser v1.3 (Nivre 2003, 2008; K?bler, McDonald, and Nivre 2009), a transition-based parser with an input buffer and a stack, which uses SVM classifiers 10 We use the term ?dev set? to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate ?tuning set?). 170 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features Table 1 Penn Arabic Treebank part 3 v3.1 data split. split # tokens # sentences sentence length (avg. # tokens) training 341,094 11,476 29.7 dev 31,208 ","@endWordPosition":"5220","@position":"33225","annotationId":"T34","@startWordPosition":"5219","@citStr":"Nivre 2003"}},"title":{"#tail":"\n","#text":"An efficient algorithm for projective dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 8th International Conference on Parsing Technologies (IWPT),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Joakim Nivre"}}},{"volume":{"#tail":"\n","#text":"34"},"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Nivre, Joakim. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513?553."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Nivre, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"lectional Features results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb?subject and noun?adjective relations. The result holds for both the MaltParser (Nivre 2008) and the Easy-First Parser (Goldberg and Elhadad 2010). Additionally, almost all work to date in MSA morphological analysis and part-ofspeech (POS) tagging has concentrated on the morphemic form of the words. Often, however, the functional morphology (which is relevant to agreement, and relates to the meaning of the word) is at odds with the ?surface? (form-based) morphology; a well-known example of this are the ?broken? (irregular) plurals of nominals. We show that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance ","@endWordPosition":"1121","@position":"7333","annotationId":"T35","@startWordPosition":"1120","@citStr":"Nivre 2008"},{"#tail":"\n","#text":"th Lexical and Inflectional Features comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima?an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser. Our results agree with previous work on Ara","@endWordPosition":"4385","@position":"27938","annotationId":"T36","@startWordPosition":"4384","@citStr":"Nivre 2008"},{"#tail":"\n","#text":"obustness of our findings across these frameworks. In Section 7 we explore alternative training methods, and their impact on key models. All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given. For statistical significance, we use McNemar?s test on non-gold LAS, as implemented by Nilsson and Nivre (2008). We denote p < 0.05 and p < 0.01 with + and ++, respectively. 4.1 Data Sets and Parser For all the experiments reported in this article, we used the training portion of PATB Part 3 v3.1 (Maamouri et al 2004), converted to the CATiB Treebank format, as mentioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set.10 We kept the test unseen (?blind?) during training and model ","@endWordPosition":"5077","@position":"32400","annotationId":"T37","@startWordPosition":"5076","@citStr":"Nivre (2008)"},{"#tail":"\n","#text":"za removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms. All experiments reported here were conducted using this new configuration. To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, 7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ?eager? algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE). 11 Nivre (2008) reports that non-projective and pseudo-projective algorithms outperform the ?eager? projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. The Nivre ?standard? algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the ?eager? one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set?an observation already noted in Nivre (2008). 12 The terms feature and attribute are overloaded in the literature. We use them in the linguist","@endWordPosition":"5683","@position":"36233","annotationId":"T38","@startWordPosition":"5682","@citStr":"Nivre (2008)"}]},"title":{"#tail":"\n","#text":"Algorithms for deterministic incremental dependency parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Joakim Nivre"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Nivre, Joakim. 2009. Parsing Indian languages with MaltParser. In Proceedings of the ICON09 NLP Tools Contest: Indian Language Dependency Parsing, pages 12?18, Hyderabad, India."},"#text":"\n","pages":{"#tail":"\n","#text":"12--18"},"marker":{"#tail":"\n","#text":"Nivre, 2009"},"location":{"#tail":"\n","#text":"Hyderabad, India."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"us tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009). In this article we investigate morphological features for dependency parsing of Modern Standard Arabic (MSA). For MSA, the space of possible morphological features is fairly large. We determine which morphological features help and why. We further determine the upper bound for their contribution to parsing quality. Similar to previous 1 Other morphological features, such as MOOD or ASPECT, do not interact with syntax at all. Note also that we do not commit to a specific linguistic theory with these terms; hence, other theoretical terms such as the Minimalist feature checking may be used here","@endWordPosition":"861","@position":"5746","annotationId":"T39","@startWordPosition":"860","@citStr":"Nivre 2009"},{"#tail":"\n","#text":"ed on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser. Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6). Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of ","@endWordPosition":"4528","@position":"28908","annotationId":"T40","@startWordPosition":"4527","@citStr":"Nivre 2009"},{"#tail":"\n","#text":"ntioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set.10 We kept the test unseen (?blind?) during training and model development. Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1. For all experiments reported in this section we used the syntactic dependency parser MaltParser v1.3 (Nivre 2003, 2008; K?bler, McDonald, and Nivre 2009), a transition-based parser with an input buffer and a stack, which uses SVM classifiers 10 We use the term ?dev set? to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate ?tuning set?). 170 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features Table 1 Penn Arabic Treebank part 3 v3.1 data split. split # tokens # sentences sentence length (avg. # tokens) training 341,094 11,476 29.7 dev 31,208 1,043 29.9 unseen test 29,944 1,007 29.7 ","@endWordPosition":"5226","@position":"33266","annotationId":"T41","@startWordPosition":"5225","@citStr":"Nivre 2009"},{"#tail":"\n","#text":"ependency relation between the current word and its parent). There are default MaltParser features (in the machine learning sense),12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers. The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K?bler, McDonald, and Nivre (2009) describe a ?typical? MaltParser model configuration of attributes and features.13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0, 1] + stk[0, 1] for word-forms, and buf[0, 1, 2, 3] + stk[0, 1, 2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0]. We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument). This new MaltParser configuration resulted in gains of ","@endWordPosition":"5474","@position":"34814","annotationId":"T42","@startWordPosition":"5473","@citStr":"Nivre (2009)"}]},"title":{"#tail":"\n","#text":"Parsing Indian languages with MaltParser."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ICON09 NLP Tools Contest: Indian Language Dependency Parsing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Joakim Nivre"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Nivre, Joakim, Igor M. Boguslavsky, and Leonid K. Iomdin. 2008. Parsing the SynTagRus Treebank of Russian. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 641?648, Manchester."},"#text":"\n","pages":{"#tail":"\n","#text":"641--648"},"marker":{"#tail":"\n","#text":"Nivre, Boguslavsky, Iomdin, 2008"},"location":{"#tail":"\n","#text":"Manchester."},"title":{"#tail":"\n","#text":"Parsing the SynTagRus Treebank of Russian."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 22nd International Conference on Computational Linguistics (COLING),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Joakim Nivre"},{"#tail":"\n","#text":"Igor M Boguslavsky"},{"#tail":"\n","#text":"Leonid K Iomdin"}]}},{"volume":{"#tail":"\n","#text":"13"},"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95?135."},"journal":{"#tail":"\n","#text":"Natural Language Engineering,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Nivre, Hall, Nilsson, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ness information helps unlexicalized PCFG parsing. As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser. Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin","@endWordPosition":"4435","@position":"28290","annotationId":"T43","@startWordPosition":"4432","@citStr":"Nivre et al 2007"}},"title":{"#tail":"\n","#text":"Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov, and Erwin Marsi."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Joakim Nivre"},{"#tail":"\n","#text":"Johan Hall"},{"#tail":"\n","#text":"Jens Nilsson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"Petrov, Slav, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the Conference on Language Resources and Evaluation (LREC), pages 2,089?2,096."},"#text":"\n","pages":{"#tail":"\n","#text":"2--089"},"marker":{"#tail":"\n","#text":"Petrov, Das, McDonald, 2012"},"title":{"#tail":"\n","#text":"A universal part-of-speech tagset."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Conference on Language Resources and Evaluation (LREC),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Slav Petrov"},{"#tail":"\n","#text":"Dipanjan Das"},{"#tail":"\n","#text":"Ryan McDonald"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Rambow, Owen, Bonnie Dorr, David Farwell, Rebecca Green, Nizar Habash, Stephen Helmreich, Eduard Hovy, Lori Levin, Keith J. Miller, Teruko Mitamura, Florence Reeder, and Siddharthan Advaith. 2006. Parallel syntactic annotation of multiple languages. In Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC), pages 559?564, Genoa."},"#text":"\n","pages":{"#tail":"\n","#text":"559--564"},"marker":{"#tail":"\n","#text":"Rambow, Dorr, Farwell, Green, Habash, Helmreich, Hovy, Levin, Miller, 2006"},"location":{"#tail":"\n","#text":"Genoa."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nto verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer (Buckwalter 2004) used in the PATB has a core POS set of 44 tags (CORE44) before morphological extension.8 Cross-linguistically, a core set containing around 12 tags is often 7 We ignore the rare ?false idafa? construction (Habash 2010, p. 102). 8 The 44 tags in CORE44 are based on the tokenized version of Arabic words. There are 34 untokenized core tags as used in MADA+TOKAN (Habash, Rambow, and Roth 2012). 167 Computational Linguistics Volume 39, Number 1 assumed as a ?universal tag set? (Rambow et al 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al (2006) for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX). The CATIB6 tag set can be viewed as a further reduction, with the exception that CATIB6 contains a passive voice tag (a morphological feature); this tag constitutes only 0.5% of the tags in the training, however. 2.7 Extended POS Tag Sets The notio","@endWordPosition":"3676","@position":"23645","annotationId":"T44","@startWordPosition":"3673","@citStr":"Rambow et al 2006"}},"title":{"#tail":"\n","#text":"Teruko Mitamura, Florence Reeder, and Siddharthan Advaith."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Owen Rambow"},{"#tail":"\n","#text":"Bonnie Dorr"},{"#tail":"\n","#text":"David Farwell"},{"#tail":"\n","#text":"Rebecca Green"},{"#tail":"\n","#text":"Nizar Habash"},{"#tail":"\n","#text":"Stephen Helmreich"},{"#tail":"\n","#text":"Eduard Hovy"},{"#tail":"\n","#text":"Lori Levin"},{"#tail":"\n","#text":"Keith J Miller"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"2007"},"institution":{"#tail":"\n","#text":"Charles University,"},"rawString":{"#tail":"\n","#text":"Smr?, Otakar. 2007. Functional Arabic Morphology. Formal System and Implementation. Ph.D. thesis, Charles University, Prague."},"#text":"\n","marker":{"#tail":"\n","#text":"Smr, 2007"},"location":{"#tail":"\n","#text":"Prague."},"title":{"#tail":"\n","#text":"Functional Arabic Morphology. Formal System and Implementation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Otakar Smr"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Tsarfaty, Reut and Khalil Sima?an. 2007. Three-dimensional parametrization for parsing morphologically rich languages. In Proceedings of the 10th International Conference on Parsing Technologies (IWPT), pages 156?167, Morristown, NJ."},"#text":"\n","pages":{"#tail":"\n","#text":"156--167"},"marker":{"#tail":"\n","#text":"Tsarfaty, Simaan, 2007"},"location":{"#tail":"\n","#text":"Morristown, NJ."},"title":{"#tail":"\n","#text":"Three-dimensional parametrization for parsing morphologically rich languages."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 10th International Conference on Parsing Technologies (IWPT),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Reut Tsarfaty"},{"#tail":"\n","#text":"Khalil Simaan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Zitouni, Imed, Jeffrey S. Sorensen, and Ruhi Sarikaya. 2006. Maximum entropy based restoration of Arabic diacritics. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and the 44th Annual Meeting of the Association for Computational Linguistics (ACL), pages 577?584, Sydney. Computational Linguistics Volume 39, Number 1 A. Appendix: Additional Feature Engineering The following sections describe additional experiments, with negative or small gains, presented here for completeness. A.1 Embedding Morphological Features Within the POS Tags After discovering our best form-based feature combination, we explored whether morphological data should be added to an Arabic parsing model as stand-alone machine learning features, or whether they should be used to enhance and extend a POS tag set. We created a new POS tag set, CORE12EX, size 81 (and 96.0% prediction accuracy), by extending the CORE12 tag set with the features that most improved the CORE12 baseline: DET and the PNG-features. But CORE12EX did worse than its non-extended (but featureenhanced) counterpart, CORE12+DET+PNG. Another variant, CORE12EX+DET+PNG, which used both the extended tag set and the additional DET and PNG-features, did not improve over CORE12+DET+PNG either. A.2 Extended PERSON Feature After extending the determiner feature (DET2), the next gainful feature that we could alter was PERSON. We changed the values of proper names from ?N/A? to ?3? (third-person). But this change resulted in a slight decrease in performance, so it was abandoned."},"#text":"\n","pages":{"#tail":"\n","#text":"577--584"},"marker":{"#tail":"\n","#text":"Zitouni, Sorensen, Sarikaya, 2006"},"title":{"#tail":"\n","#text":"Maximum entropy based restoration of Arabic diacritics."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and the 44th Annual Meeting of the Association for Computational Linguistics (ACL),"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Imed Zitouni"},{"#tail":"\n","#text":"Jeffrey S Sorensen"},{"#tail":"\n","#text":"Ruhi Sarikaya"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"A.3 Digit Tokens and Number Binning Digit tokens (e.g., 4, as opposed to four) are marked singular by default. They don?t show surface agreement with a noun, even though the corresponding number-word token would. Therefore we replaced the digit tokens? NUMBER value with ?N,? and denoted these experiments with NUMDGT.24 We further observe that MSA displays complex agreement patterns with numbers (Dada 2007). Therefore, we alternatively experimented with binning the digit tokens? NUMBER value accordingly:  the number 0 and numbers ending with 00  the number 1 and numbers ending with 01  the number 2 and numbers ending with 02  the numbers 3?10 and those ending with 03?10  the numbers, and numbers ending with, 11?99  all other number tokens (e.g., 0.35 or 7/16) We denoted these experiments with NUMDGTBIN. Almost 1.5% of the tokens are digit tokens in the training set, and 1.2% in the dev set. Number binning did not have a consistent contribution in either gold or predicted value conditions (results not shown), so it was abandoned as well. 24 We didn?t mark the number-words because in our training data there were fewer than 30 lemmas of fewer than 2,000 such tokens, and hence presumably their agreement patterns can be more easily learned."},"#text":"\n","marker":{"#tail":"\n","#text":"Digit, 2007"},"title":{"#tail":"\n","#text":"Tokens and Number Binning Digit tokens (e.g., 4, as opposed to four) are marked singular by default. They don?t show surface agreement with a noun, even though the corresponding number-word token would. Therefore we replaced the digit tokens? NUMBER value with ?N,? and denoted these experiments with NUMDGT.24 We further observe that MSA displays complex agreement patterns with numbers (Dada"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A 3 Digit"}}}]}}]}}
