it the properties of CCG. Our experiments show that our model outperforms a baseline in which this context information is not captured. 1 Introduction Learning parsers from incomplete or indirect supervision is an important component of moving NLP research toward new domains and languages. But with less information, it becomes necessary to devise ways of making better use of the information that is available. In general, this means constructing inductive biases that take advantage of unannotated data to train probabilistic models. One important example is the constituentcontext model (CCM) of Klein and Manning (2002), which was specifically designed to capture the linguistic observation made by Radford (1988) that there are regularities to the contexts in which constituents appear. This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts. For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB. This DET—VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN. From this, we might deduce that DET— VERB is a likely context for a noun phrase. CCM is able to learn which POS context
 were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim22 Proceedings of the 19th Conference on Computational Language Learning, pages 22–31, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger (Garrette et al., 2014). In this paper, we present a novel parsing model that is designed specifically for the capacity to capture both of these universal, intrinsic properties of CCG. We do so by extending our previous, PCFG-based parsing model to include parameters that govern the relationship between constituent categories and the preterminal categories (also known as supertags) to the left and right. The advantage of modeling context within a CCG framework is that while CCM must learn which contexts are likely purely from the data, the CCG categories give us obvious a priori information about whether a context i
 design of models and learning procedures that result in better parsing tools. Given our desire to train NLP models in low-supervision scenarios, the possibility of constructing inductive biases out of universal properties of language is enticing: if we can do this well, then it only needs to be done once, and can be applied to any language or domain without adaptation. In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model, we develop a blocked sampler based on that of Johnson et al. (2007) to sample parse trees for sentences in the raw training corpus according to their posterior probabilities. However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios. 2 Combinatory Categorial Grammar In the CCG formalism
ehavior of the function. A category (s\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\) with a noun phrase (np) that serves as its subject. 23 n np/n n/n n s\np The lazy dog sleeps s s\np pp pp/np np The man walks to work np np/n n (s\np)/pp Figure 2: Higher-level category n subsumes the categories of its constituents. Thus, n should have a strong prior on combinability with its adjacent supertags np/n and s\np. Figure 1: CCG parse for “The man walks to work.” We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007). 3 Generative Model In this section, we present our novel supertagcontext model (SCM) that augments a standard PCFG with parameters governing the supertags to the left and right of each constituent. The CCG formalism is said to be naturally associative since a constituent label is often able to combine on either the left or the right. As a motivating example, consid
np) that serves as its subject. 23 n np/n n/n n s\np The lazy dog sleeps s s\np pp pp/np np The man walks to work np np/n n (s\np)/pp Figure 2: Higher-level category n subsumes the categories of its constituents. Thus, n should have a strong prior on combinability with its adjacent supertags np/n and s\np. Figure 1: CCG parse for “The man walks to work.” We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007). 3 Generative Model In this section, we present our novel supertagcontext model (SCM) that augments a standard PCFG with parameters governing the supertags to the left and right of each constituent. The CCG formalism is said to be naturally associative since a constituent label is often able to combine on either the left or the right. As a motivating example, consider the sentence “The lazy dog sleeps”, as shown in Figure 2. The word lazy, with category n/n, can either combine with dog (n) via the Forward Application rule (>), or with The (np/n) via the Forward Composition (>B) rule. Baldridg
 A tree is complete when all branches end in terminal words. See Figure 3 for a graphical depiction of the generative behavior of the process. Finally, since it is possible to generate a supertag context category that does not match the actual category generated by the neighboring constituent, we must allow our process to reject such invalid trees and re-attempt to sample. Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid. Since we are not generating from the model, this does not introduce difficulties (Klein and Manning, 2002). One additional complication that must be addressed is that left-frontier non-terminal categories — those whose subtree span includes the first word of the sentence — do not have a left-side supertag to use as context. For these cases, we use the special sentence-start symbol (S) to serve as context. Similarly, we use the end symbol (E) for the right-side context of the right-frontier. We next discuss how the prior distributions are constructed to encode desirable biases, using universal CCG properties. 3.1 Non-terminal production prior means For the root, binary, and unary parameters, we wan
milarly, we use the end symbol (E) for the right-side context of the right-frontier. We next discuss how the prior distributions are constructed to encode desirable biases, using universal CCG properties. 3.1 Non-terminal production prior means For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross-linguistically-plausible categories. To formalize the notion of what it means for a category to be more “plausible”, we extend the category generator of our previous work, which we will call PCAT. We can define PCAT using a probabilistic grammar (Garrette et al., 2014). The grammar may first generate a start or end category ((S),(E)) with probability pse or a special tokendeletion category ((D); explained in §5) with probability pdel, or a standard CCG category C: X—(S) |(E) pse X—(D) pdel X—C (1— (2pse + pdel)) ' PC(C) For each sentence s, there will be one (S) and one (E), so we set pse = 1/(25 + 2), since the average sentence length in the corpora is roughly 25. To discourage the model from deleting tokens (only applies during testing), we set pdel = 10−100. For PC, the distribution over standard categories, we use a recursive definition based on the str
 since the probability of the depth of a tree will decrease geometrically (Chi, 1999). Bik Ckj ti-1 ti tk-1 tk tj-1 tj 25 that result in a priori more likely categories:3 θROOT-0 (t) = PC AT (t) θBIN-0((u, v)/\ CAT(u) · PCAT(v) θUN-0((u)/) = For simplicity, we assume the production-type mixture prior to be uniform: λ0 =(13, 13, 13). 3.2 Terminal production prior means We employ the same procedure as our previous work for setting the terminal production prior distributions θTERM-0 t (w) by estimating word-givencategory relationships from the weak supervision: the tag dictionary and raw corpus (Garrette and Baldridge, 2012; Garrette et al., 2015).4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags. These counts are then combined with estimates of the “openness” of each tag in order to assess its likelihood of appearing with new words. 3.3 Context parameter prior means In order to encourage our model to choose trees in which the constituent labels “fit” into their supertag contexts, we want to bias our context parameters toward context categories that
d with estimates of the “openness” of each tag in order to assess its likelihood of appearing with new words. 3.3 Context parameter prior means In order to encourage our model to choose trees in which the constituent labels “fit” into their supertag contexts, we want to bias our context parameters toward context categories that are combinable with the constituent label. The right-side context of a non-terminal category — the probability of generating a category to the right of the current constituent’s category — corresponds directly to the category transitions used for the HMM supertagger of Garrette et al. (2014). Thus, the right-side context prior mean θRCTX-0 can be biased in exactly the same way as t the HMM supertagger’s transitions: toward context supertags that connect to the constituent label. To encode a notion of combinability, we follow Baldridge’s (2008) definition. Briefly, let κ(t, u) E {0, 1} be an indicator of whether t combines with u (in that order). For any binary rule that can combine t to u, κ(t, u)=1. To ensure that our prior captures the natural associativity of CCG, we define combinability in this context to include composition rules as well as application rules. If 3For our exp
: { σ · 1/|T |if κ(t, r) σ > 1 Pright(r |t) = 1/|T PCATt(r |t) = { PCAT(r) (r) otherwiseσ > 1 Distributions Pleft(.e |t) and P left CAT(.e |t) are defined in the same way, but with the combinability direction flipped: κ(.e, t), since the left context supertag precedes the constituent category. 4 Posterior Inference We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences. Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. Our strategy is based on the approach presented by Johnson et al. (2007). At a high level, we alternate between resampling model parameters (θROOT, θBIN, θUN, θTERM, λ, θLCTX, θRCTX) given the current set of parse trees and resampling those trees given the PCAT(u) otherwise 26 current model parameters and observed word sequences. To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy. By repeating these alternating steps and accumulating the productions, we obtain an approximation of the required posterior quantities. Our inference procedure takes as input the distribution prior means, along with the raw corpus and tag dictionary. D
es the full distribution and for which direct sampling is tractable, and then choose to accept or reject those trees based on the true distribution P. For our model, there is a straightforward and intuitive choice for the proposal distribution: the PCFG model without our context parameters: (θROOT, θBIN, θUN, θTERM, λ), which is known to have an efficient sampling method. Our acceptance step is therefore based on the remaining parameters: the context (θLCTX, θRCTX). To sample from our proposal distribution, we use a blocked Gibbs sampler based on the one proposed by Goodman (1998) and used by Johnson et al. (2007) that samples entire parse trees. For a sentence w, the strategy is to use the Inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non-terminal position spanning words wi through wj−1 and category t, going “up” the tree, the probability of generating wi, ... , wj−1 via any arrangement of productions that is rooted by yij = t. p(wi |yi,i+1 = t) = λt(T) · θTERM t (wi) +Et→u λt(U) · θUN t (hui) · p(wi:j−1 |yij = u) p(wi:j−1 |yij = t) = Et→u λt(U) · θUN t (hui) · p(wi:j−1 |yij = u) + Et→u v Ei<k<j λt (B) · θtIN(hu, vi) · p(wi:k−1 |yik = u) · p(wk:j−1 |ykj = v) We the
wing a Dirichlet sample based on the vector of counts, we simply normalize those counts. However, since we require a final model that can parse sentences efficiently, we drop the context parameters, making the model a standard PCFG, which allows us to use the probabilistic CKY algorithm. 5 Experiments In our evaluation we compared our supertagcontext approach to (our reimplementation of) the best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\X)/X rather than introd
us work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Gigaword 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999). For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k). For Chinese and Itali
n we would have to take a score of zero for that sentence: every dependency would be “wrong”. Thus, it is important that we make a best effort to find a parse. To accomplish this, we implemented a parsing backoff strategy. The parser first tries to find a valid parse that has either sdcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the “deletion” strategy employed by Zettlemoyer and Collins (2007), but we do it directly in the grammar. We add unary rules of the form (D)-*u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t-*((D), v) and t-*(v, (D)). Recall that in §3.1, we stated that (D) is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary. Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents. For each language and level of supervision, we executed four experimen
 adjacent supertags frequently helps the model. However, for English, we found that additionally biasing context priors toward simpler categories using P left CAT/P right CAT degraded performance. This is likely due to the fact that the priors on production parameters (BBIN, BUN) are already biasing the model toward likely categories, and that having the context parameters do the same ends up over-emphasizing the need for simple categories, preventing the model from choosing more complex categories when they are needed. On the other hand, this bias helps in Chinese and Italian. 6 Related Work Klein and Manning (2002)’s CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span (as well as the spans and contexts of each non-constituent). They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents. While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the rel
