ed user satisfaction as process evolving over time with Hidden Markov Models (HMM). In the experiment, users were asked to interact with a Wizard-of-Oz restaurant information system. Each participant followed dialogues which have previously been defined following predefined scripts, i.e. specific scenarios. This resulted in equally long dialogue transcripts for each scenario. The users were constrained to rate their satisfaction on a 5-point scale with “bad”, “poor”, “fair”, “good” and “excellent” after each dialogue step. The interaction was halted while the user voted. In a similar spirit, (Higashinaka et al., 2010a) developed a model for predicting turn-wise ratings, which was evaluated on human-machine and human-human dialogues. The data employed was not spoken dialogue but text dialogues from a chat system and a transcribed conversation between humans. The labels in the model originated from two expert raters that listened to the recorded interactions and provided turn-wise scores from 1-7 on smoothness (“Smoothness of the conversation”), closeness (“Closeness perceived by the user towards the system”) and willingness (“Willingness to continue the conversation”). Rater-independent performance scores 
 users measuring their satisfaction on a five point scale after the interaction. The best result could be achieved with a 3-gram model that reached 34% accuracy in distinguishing between six classes at any point in the dialogue. It seems that the prediction of turn-level user satisfaction scores given only one overall dialogue-level score seems hardly possible and is close to random: The prediction of the five user satisfaction classes reach an average F-score as low as 0.252, which is only 0.052 score points above the baseline of 0.20. A similar result as (Hara et al., 2010) was obtained by (Higashinaka et al., 2010b). Using HMMs they derived turn-level ratings from dialogue-wide ratings. The model’s performance when trained on dialogue-level ratings was closer to random than when trained on turn-level ratings. The open issues that arise from the cited work are addressed in the following. 3 Issues Our aim is to create a general model that may be used to predict the quality of the interaction - or ideally the actual satisfaction of the user - at arbitrary system-user exchanges in an SDS. It has become obvious from the cited work that current models are not suited for deployment due to low prediction accur
esult, general prediction models that mirror a universal, unbiased understanding of satisfaction can presumably hardly be derived from user’s impressions. Large influence of subjectivity - and also randomness in assigning the scores - would prevent such a general model. Consequently, it seems unavoidable to employ expert annotations. In the proper meaning of the word, the scores then do not exactly mirror the subjective impression of users but the more objective impression of expert raters. Thus we decide against the use of the term user satisfaction in the course of this work in contrast to (Higashinaka et al., 2010a) and instead opt for the expression Interaction Quality. It can be assumed that basic attitudes towards dialogue systems in general, opinions about the TTS voice, environmental factors etc. that would typically influence user satisfaction scores, and which are not of interest for our prediction, are not dominant in expert satisfaction scores in a series of annotated dialogues. Experts are expected to fade out such system-dependent and environment-dependent influences and instead focus on the dialogue behavior (i.e. the Interaction Quality) only. As a result, two key issues are addressed in t
 annotate the quality of the interaction at each system-user exchange with the scores 5 (very good), 4 (good), 3 (fair), 2 (poor) and 1 (very poor). Every dialogue is initially rated with a score of 5 since every interaction at the beginning can be considered as good until the opposite eventuates. Our model assumes that users are initially interacting with an SDS without bias, i.e. the basic attitude towards a dialogue system is positive. Other assumptions would not be statistically predictable. An example dialogue is depicted in Table 5 along with the ratings (cf. Figure 2 in the Appendix). (Higashinaka et al., 2010b) and (Higashinaka et al., 2010a) report low correlation among the ratings (Spearman’s p 0.04-0.32), which motivated us to develop a set of basic guidelines that should be used by the raters (cf. Table 6 in the Appendix). The guidelines have been designed in such a way that the raters still have sufficient level of freedom when choosing the labels but preventing them from too strong variations among the neighboring system-user exchanges. The distribution of the labels provided by the single raters is depicted in Figure 3. As expected, the distribution is skew towards label “5” since every dia
ith the prefix {#} for a number and {Mean} for the mean value. A number of successive barge-ins or recognition problems might indicate a low IQ. Thus we add {MEAN}ASRCONFIDENCE, the mean confidence of the ASR within the window, {#}ASRSUCCESS, {#}ASRREJECTIONS and {#}TIME-OUTPROMPTS, i.e. the number of successfully and unsuccessfully parsed utterances within the window and the number of time-outs. The other counters are calculated likewise: {#}BARGEINS; {#}UNEXMO, {#}HELPREQUESTS, {#}OPERATORREQUESTS, {#}REPROMPT, {#}CONFIRMATIONS, {#}SYSTEMQUESTIONS. To provide comparability to previous work (Higashinaka et al., 2010a), we further introduce a dialogue act feature group that we create semiautomatically: DAct SYSTEMDIALOGUEACT: one of 28 distinct dialogue acts, such as greeting,offer help, ask bus, confirm departure, deliver result, etc. USERDIALOGUEACT: one of 22 distinct DAs, such as confirm departure, place information, polite, reject time, request help, etc. To create an upper baseline of our model we further introduce the negative emotional state of the user that is manually annotated by a human rater who chooses one of the labels garbage, non-angry, slightly angry, very angry for each single user turn
ALOGUEACT: one of 22 distinct DAs, such as confirm departure, place information, polite, reject time, request help, etc. To create an upper baseline of our model we further introduce the negative emotional state of the user that is manually annotated by a human rater who chooses one of the labels garbage, non-angry, slightly angry, very angry for each single user turn: Emo EMOTIONALSTATE: emotional state of the caller in the current exchange. One of garbage, non-angry, slightly angry, very angry. The same annotation scheme as in our previous work on anger detection has been applied, see e.g. (Schmitt et al., 2009). From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events. In total, the number of interaction parameters servings as input variables for the model amounts to 52. 6 Non-Linear Modeling with Support Vector Machines The IQ scores are classified with Support Vector Machines (Bennett and Campbell, 2000). In short, an SVM uses a set of training examples (x1, yl) ... (xn, yn)|xi ∈ X, yi ∈ {−1, 1} to create a hyperplane that separates two classes {−1, 1} in such a manner that the smallest margin between all training sam
t, ASR, SLU, DM, Emo}. The target variable is the IQ score. 177 7 Feature Group Evaluation The skew distribution of the five classes requires the employment of an evaluation metric that weights the prediction of all classes equally. Hence, a performance metric, such as accuracy, would not be a reliable measurement. We select the unweighted average recall (UAR) to assess the model performance. Although it does not consider the severity of the error, i.e. predicting “1” for an IQ of “5” is considered as fatal as predicting “4”, it has been proven to be superior to other evaluation metrics, see (Higashinaka et al., 2010a), where the UAR is called Match Rate per Rating (MR/R). It is defined as follows: x 1 MR/R(R, H) = K r=1 where K is the number of classes, here “5”, and ’match’ is either ’1’ or ’0’ depending on whether the classifier’s hypothesis Hi for the class r matches the reference label Ri. In the course of this work we will stick to the expression MR/R by reason of clearness. We further list Cohen’s n and Spearman’s p to make our work comparable to other studies but will use MR/R as central evaluation criterion and for feature selection. We have split all available data into two disjoint subsets cons
.606 0.549 0.785 ALL no 0.619 0.559 0.800 DAct – - - - ASR 13/25 0.598 0.545 0.730 SLU 4/5 0.250 0.083 0.293 DM 10/17 0.436 0.338 0.649 AUTO 20/47 0.616 0.563 0.786 AUTOEMO 31/48 0.604 0.545 0.785 ALL 23/52 0.625 0.575 0.795 Table 2: Model performance after 10-fold cross validation on training/test set. The first half comprises results when all features of a group are employed. The second half contains results after feature selection on the optimization set ((x/y)=where x is the number of features used from all y available features.) As can be seen, the model reaches a similar performance as (Higashinaka et al., 2010a) with MR/R=0.26, when trained with dialogue act features alone. The slightly higher performance of our model can potentially be explained by the lower number of classes (5 vs. 7), a different definition of the dialogue act set, the employment of Support Vector Machines instead of Hidden Markov Models or the difference in the target variable (IQ vs. closeness/smoothness/willingness). It can be noted that the utilization of other features considerably outperforms dialogue act features. Particularly the group of the ASR features alone reaches a performance of 60.5%. The employment of all AUTO f
