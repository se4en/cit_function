{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-2005), pages 99\u2013106, Ann Arbor, Michigan, USA, June 25-30."},"#text":"\n","pages":{"#tail":"\n","#text":"99--106"},"marker":{"#tail":"\n","#text":"Nivre, Nilsson, 2005"},"location":{"#tail":"\n","#text":"Ann Arbor, Michigan, USA,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"in a parsing way. 3.1 Formularization Using a character-level dependency representation, we first show how a word segmentation task can be transformed into a dependency parsing problem. Since word segmentation is traditionally formularized as an unlabeled character chunking task since (Xue, 2003), only unlabeled dependencies are concerned in the transformation. There are many ways to transform chunks in a sequence into dependency representation. However, for the sake of simplicity, only well-formed and projective output sequences are considered for our processing. Borrowing the notation from (Nivre and Nilsson, 2005), an unlabeled dependency graph is formally defined as follows: An unlabeled dependency graph for a string of cliques (i.e., words and characters) W = 880 Figure 1: Two character dependency schemes w1...wn is an unlabeled directed graph D = (W, A), where (a) W is the set of ordered nodes, i.e. clique tokens in the input string, ordered by a linear precedence relation <, (b) A is a set of unlabeled arcs (wi, wj), where wi, wj E W, If (wi, wj) E A, wi is called the head of wj and wj a dependent of wi. Traditionally, the notation wi \u2014* wj means (wi, wj) E A; wi \u2014** wj denotes the reflexive and tr","@endWordPosition":"1180","@position":"7622","annotationId":"T1","@startWordPosition":"1176","@citStr":"Nivre and Nilsson, 2005"}},"title":{"#tail":"\n","#text":"Pseudoprojective dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-2005),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Joakim Nivre"},{"#tail":"\n","#text":"Jens Nilsson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Hai Zhao and Chunyu Kit. 2008b. Parsing syntactic and semantic dependencies with two singlestage maximum entropy models. In Twelfth Conference on Computational Natural Language Learning (CoNLL-2008), pages 203\u2013207, Manchester, UK, August 16-17."},"#text":"\n","pages":{"#tail":"\n","#text":"203--207"},"marker":{"#tail":"\n","#text":"Zhao, Kit, 2008"},"location":{"#tail":"\n","#text":"Manchester, UK,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"dency parsing, dprel means the arc direction from the head, either left or right. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature cnseq returns a substring started from a given character. It checks the direction of the arc that passes the given character and collects all characters with the same arc direction to yield an output substring until the arc direction is changed. Note that all combinational features concerned with this one can be regarded as word-level features. The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al., 2004) is adopted as the unsupervised segmentation criterion. The AV value of a substring s is defined as AV (s) = min{Lav(s), Rav(s)}, where the left and right AV values Lav(s) and Rav(s) are defined, respectively, as the numbers of its distinct predecessor and successor characters. In this work, AV values for substrings are derived from unlabeled training and test corpora by substring counting. Multiple features are used to represent substrings of various lengths identified by the AV criterion. Formally put, the feature function for a n-characte","@endWordPosition":"2016","@position":"12393","annotationId":"T2","@startWordPosition":"2013","@citStr":"Zhao and Kit, 2008"},{"#tail":"\n","#text":"arch algorithm with width 5 is used to decode, otherwise, a simple shift-reduce decoding is used. We see that the performance given by Scheme E is much better than that by Scheme B. The results of character-based classification and tagging methods are at the bottom of Table 46. It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence. As full features are used, the former and the latter provide the similar performance. Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now7. Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus. 4 Character Dependencies inside a Word We further consider exploiting annotated character dependencies inside a word (internal dependencies). A parsing task for these internal dependencies incorporated with trivial external dependencies 8 that are transformed from common word boundaries are cor","@endWordPosition":"3332","@position":"20306","annotationId":"T3","@startWordPosition":"3329","@citStr":"Zhao and Kit, 2008"}]},"title":{"#tail":"\n","#text":"Parsing syntactic and semantic dependencies with two singlestage maximum entropy models."},"booktitle":{"#tail":"\n","#text":"In Twelfth Conference on Computational Natural Language Learning (CoNLL-2008),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hai Zhao"},{"#tail":"\n","#text":"Chunyu Kit"}]}}]}}}}
