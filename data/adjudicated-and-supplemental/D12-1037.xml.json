{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.968910142857143","#text":"\n1: for all sentence ti such that 1 ? i ? N do\n2: Retrieve the training examples Di with size\nK for ti from D according to a similarity;\n3: Train a local weight W i based on Dev and\nDi;\n4: Decode ti with W i;\n5: end for\n"},{"#tail":"\n","@confidence":"0.902254571428571","#text":"\nOutput: Translation results of T\n1: Run global Training (such as MERT) on Dev to\nget a baseline weight Wb; // Phase 1\n2: Decode each sentence in D to get\nD = {?fs, cs, rs?}s=Ss=1 ;\n3: for all sentence ti such that 1 ? i ? N do\n4: Retrieve K training examples Di =\n"},{"#tail":"\n","@confidence":"0.9455435","#text":"\nj=1 for ti from D according to\na similarity;\n5: Incrementally train a local weight W i based\non Wb and Di; // Phase 2\n6: Decode ti with W i;\n7: end for\n"}],"figure":[{"#tail":"\n","@confidence":"0.940760041666666","#text":"\nSource Candidate Translation\ni\ni\nf j\nij\ne h score\n1 ? ? ?? ? 1 I am students . <2, 1> 0.5\n2 I was students . <1,1> 0.2\n2 ?? ?? ? ? 1 week several today ? <1,2> 0.3\n2 today several weeks . <3,2> 0.1\n(a) (b)\n2 21 2 222,0 ( , ) ( , )h f e h f e? ? ?? ?\n2 22 2 212,0 ( , ) ( , )h f e h f e? ?? ?1 11 1 11, 0 ( , ) ( , )h f e h f e? ?? ?\n1 12 1 111,0 ( , ) ( , )h f e h f e? ? ?? ?\n2 22 2 21( , ) ( , )h f e h f e?\n1 11 1 12( , ) ( , )h f e h f e?\n<-2,0>\n<-1,0>\n<1,0>\n<2,0>\n0h1h\n. .* *\n2 21 2 22( , ) ( , )h f e h f e?\n1 12 1 11( , ) ( , )h f e h f e?\n"},{"#tail":"\n","@confidence":"0.80771075","#text":"\n1\n2\n?W?Wb?\n2+\n?\nK\nK?\nj=1\n?\ne\nError(rj ; e)P?(e|fj ;W ),\n(6)\n"}],"address":{"#tail":"\n","@confidence":"0.8262","#text":"\n3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan\n"},"equation":[{"#tail":"\n","@confidence":"0.98489235","#text":"\ne?(f ;W ) = arg max\ne\nP(e|f ;W )\n= arg max\ne\nexp\n{\nW ? h(f, e)\n}\n?\ne? exp\n{\nW ? h(f, e?)\n}\n= arg max\ne\n{\nW ? h(f, e)\n}\n, (1)\n"},{"#tail":"\n","@confidence":"0.955538","#text":"\n{?f ij , c\ni\nj , r\ni\nj?}\nj=K\n"},{"#tail":"\n","@confidence":"0.7276325","#text":"\ndist(f1, f2) = ? ? edit-dist(f1, f2)+\n(1? ?)? tf-idf(f1, f2), (2)\nwhere ?(0 ? ? ? 1) is an interpolation weight,\nfi(i = 1, 2) is a word sequence and can be also\n"},{"#tail":"\n","@confidence":"0.995334666666667","#text":"\ni\nj , r\ni\nj?}\nK\nj=1 denotes\n"},{"#tail":"\n","@confidence":"0.964898142857143","#text":"\nmin\nW\n{\nd(W,Wb) + ? ? Loss(D\ni,W )\n}\n, (3)\n"},{"#tail":"\n","@confidence":"0.9161702","#text":"\n1\n2\n||W ?Wb||\n2+\n?\nK\nK?\nj=1\nmax\n1?n?|cj |\n(\n`jn?W ??h(fj , ejn)\n)\nwith\n?h(fj , ejn) = h(fj , ej?)? h(fj , ejn), (4)\n"},{"#tail":"\n","@confidence":"0.747553846153846","#text":"\n1\n2\n?W ?Wb?\n2 +\n?\nK\nK?\nj=1\nError(rj ; e?(fj ;W )), (5)\nwhere e?(fj ;W ) is defined in Equation (1), and\nError(rj , e) is the sentence-wise minus BLEU (Pa-\npineni et al2002) of a candidate e with respect to\nrj .\n"},{"#tail":"\n","@confidence":"0.973394857142857","#text":"\nwith\nP?(e|fj ;W ) =\nexp[?W ? h(fj , e)]\n?\ne??cj exp[?W ? h(fj , e\n?)]\n, (7)\n"},{"#tail":"\n","@confidence":"0.995760666666667","#text":"\n0 . 0 0 0 . 0 2 0 . 0 4 0 . 0 6 0 . 0 8 0 . 1 01 82 02 2\n2 42 62 8\nN I S T 0 5 N I S T 0 6 N I S T 0 8BLEU l\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.994785","#text":"\n4.1 Margin Based Ultraconservative Update\n"},{"#tail":"\n","@confidence":"0.944438","#text":"\n4.2 Error Rate Based Ultraconservative\nUpdate\n"},{"#tail":"\n","@confidence":"0.946732","#text":"\n5.1 Setting\n"},{"#tail":"\n","@confidence":"0.968743","#text":"\n5.2 Runtime Results\n"},{"#tail":"\n","@confidence":"0.866757","#text":"\n5.3 Results and Analysis\n"}],"footnote":[{"#tail":"\n","@confidence":"0.895645428571428","#text":"\nProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural\nLanguage Learning, pages 402?411, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics\nLocally Training the Log-Linear Model for SMT\nLemao Liu1, Hailong Cao1, Taro Watanabe2, Tiejun Zhao1, Mo Yu1, CongHui Zhu1\n1School of Computer Science and Technology\nHarbin Institute of Technology, Harbin, China\n2National Institute of Information and Communication Technology\n"},{"#tail":"\n","@confidence":"0.999908","#text":"\n2See web: http://www.statmt.org\n3Wb is exactly the weight of In-Hiero in Table 1.\n"},{"#tail":"\n","@confidence":"0.918498","#text":"\n4The runtime excludes the time of tuning and decoding on D\nin Algorithm 2, since both of them can be performanced offline.\n"}],"construct":{"#tail":"\n","@confidence":"0.3634518","#text":"\nAlgorithm 2 Local Training Method Based on In-\ncremental Training\nInput: T = {ti}Ni=1 (test set), K (retrieval size),\nDev (development set),\nD = {?fs, rs?}s=Ss=1 (retrieval data),\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.977743988888889","#text":"\nPhil Blunsom, Trevor Cohn, and Miles Osborne. 2008.\nA discriminative latent variable model for statisti-\ncal machine translation. In Proceedings of ACL,\npages 200?208, Columbus, Ohio, June. Association\nfor Computational Linguistics.\nLe?on Bottou and Vladimir Vapnik. 1992. Local learning\nalgorithms. Neural Comput., 4:888?900, November.\nG. Cauwenberghs and T. Poggio. 2001. Incremental\nand decremental support vector machine learning. In\nAdvances in Neural Information Processing Systems\n(NIPS*2000), volume 13.\nStanley F Chen and Joshua Goodman. 1998. An empir-\nical study of smoothing techniques for language mod-\neling. In Technical Report TR-10-98. Harvard Univer-\nsity.\nHaibin Cheng, Pang-Ning Tan, and Rong Jin. 2010. Ef-\nficient algorithm for localized support vector machine.\nIEEE Trans. on Knowl. and Data Eng., 22:537?549,\nApril.\nDavid Chiang, Yuval Marton, and Philip Resnik. 2008.\nOnline large-margin training of syntactic and struc-\ntural translation features. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP ?08, pages 224?233, Strouds-\nburg, PA, USA. Association for Computational Lin-\nguistics.\nDavid Chiang. 2005. A hierarchical phrase-based model\nfor statistical machine translation. In Proceedings of\nthe 43rd Annual Meeting on Association for Computa-\ntional Linguistics, ACL ?05, pages 263?270, Strouds-\nburg, PA, USA. Association for Computational Lin-\nguistics.\nKoby Crammer and Yoram Singer. 2003. Ultraconser-\nvative online algorithms for multiclass problems. J.\nMach. Learn. Res., 3:951?991, March.\nKoby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-\nShwartz, and Yoram Singer. 2006. Online passive-\naggressive algorithms. J. Mach. Learn. Res., 7:551?\n585, December.\nMichel Galley and Chris Quirk. 2011. Optimal search\nfor minimum error rate training. In Proceedings of\nthe 2011 Conference on Empirical Methods in Natural\nLanguage Processing, pages 38?49, Edinburgh, Scot-\nland, UK., July. Association for Computational Lin-\nguistics.\nYifan He, Yanjun Ma, Josef van Genabith, and Andy\nWay. 2010. Bridging smt and tm with translation\nrecommendation. In Proceedings of the 48th Annual\nMeeting of the Association for Computational Linguis-\ntics, pages 622?630, Uppsala, Sweden, July. Associa-\ntion for Computational Linguistics.\nS. Hildebrand, M. Eck, S. Vogel, and Alex Waibel. 2005.\nAdaptation of the translation model for statistical ma-\nchine translation based on information retrieval. In\nProceedings of EAMT. Association for Computational\nLinguistics.\nMark Hopkins and Jonathan May. 2011. Tuning as rank-\ning. In Proceedings of the 2011 Conference on Empir-\nical Methods in Natural Language Processing, pages\n1352?1362, Edinburgh, Scotland, UK., July. Associa-\ntion for Computational Linguistics.\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu.\n2003. Statistical phrase-based translation. In Proc.\nof HLT-NAACL. ACL.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran, Richard\nZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-\nstantin, and Evan Herbst. 2007. Moses: open source\ntoolkit for statistical machine translation. In Proceed-\nings of the 45th Annual Meeting of the ACL on Inter-\nactive Poster and Demonstration Sessions, ACL ?07,\npages 177?180, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\nPhilipp Koehn. 2004. Statistical significance tests for\nmachine translation evaluation. In Proc. of EMNLP.\nACL.\nMu Li, Yinggong Zhao, Dongdong Zhang, and Ming\nZhou. 2010. Adaptive development data selection for\nlog-linear model in statistical machine translation. In\nProceedings of the 23rd International Conference on\nComputational Linguistics, COLING ?10, pages 662?\n670, Stroudsburg, PA, USA. Association for Compu-\ntational Linguistics.\nYajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving\nstatistical machine translation performance by train-\ning data selection and optimization. In Proceedings\nof the 2007 Joint Conference on Empirical Methods\nin Natural Language Processing and Computational\nNatural Language Learning (EMNLP-CoNLL), pages\n"},{"#tail":"\n","@confidence":"0.999812409638554","#text":"\n343?350, Prague, Czech Republic, June. Association\nfor Computational Linguistics.\nYanjun Ma, Yifan He, Andy Way, and Josef van Gen-\nabith. 2011. Consistent translation using discrim-\ninative learning - a translation memory-inspired ap-\nproach. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1239?1248, Port-\nland, Oregon, USA, June. Association for Computa-\ntional Linguistics.\nChristopher D. Manning and Hinrich Schu?tze. 1999.\nFoundations of statistical natural language process-\ning. MIT Press, Cambridge, MA, USA.\nRobert C. Moore and Chris Quirk. 2008. Random\nrestarts in minimum error rate training for statistical\nmachine translation. In Proceedings of the 22nd Inter-\nnational Conference on Computational Linguistics -\nVolume 1, COLING ?08, pages 585?592, Stroudsburg,\nPA, USA. Association for Computational Linguistics.\nFranz Josef Och and Hermann Ney. 2000. Improved\nstatistical alignment models. In Proceedings of the\n38th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ?00, pages 440?447, Strouds-\nburg, PA, USA. Association for Computational Lin-\nguistics.\nFranz Josef Och and Hermann Ney. 2002. Discrimi-\nnative training and maximum entropy models for sta-\ntistical machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ?02, pages 295?302, Strouds-\nburg, PA, USA. Association for Computational Lin-\nguistics.\nFranz Josef Och. 2003. Minimum error rate training in\nstatistical machine translation. In Proceedings of the\n41st Annual Meeting of the Association for Compu-\ntational Linguistics, pages 160?167, Sapporo, Japan,\nJuly. Association for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of 40th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 311?318, Philadelphia, Pennsylva-\nnia, USA, July. Association for Computational Lin-\nguistics.\nAdam Pauls, John Denero, and Dan Klein. 2009. Con-\nsensus training for consensus decoding in machine\ntranslation. In Proceedings of the 2009 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1418?1427, Singapore, August. Association for\nComputational Linguistics.\nAlistair Shilton, Marimuthu Palaniswami, Daniel Ralph,\nand Ah Chung Tsoi. 2005. Incremental training of\nsupport vector machines. IEEE Transactions on Neu-\nral Networks, 16(1):114?131.\nAndreas Stolcke. 2002. Srilm - an extensible language\nmodeling toolkit. In Proc. of ICSLP.\nTaro Watanabe and Eiichiro Sumita. 2003. Example-\nbased decoding for statistical machine translation. In\nProc. of MT Summit IX, pages 410?417.\nTaro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki\nIsozaki. 2007. Online large-margin training for sta-\ntistical machine translation. In Proceedings of the\n2007 Joint Conference on Empirical Methods in Nat-\nural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pages 764?\n773, Prague, Czech Republic, June. Association for\nComputational Linguistics.\nHao Zhang, Alexander C. Berg, Michael Maire, and Ji-\ntendra Malik. 2006. Svm-knn: Discriminative near-\nest neighbor classification for visual category recog-\nnition. In Proceedings of the 2006 IEEE Computer\nSociety Conference on Computer Vision and Pattern\nRecognition - Volume 2, CVPR ?06, pages 2126?2136,\nWashington, DC, USA. IEEE Computer Society.\nBing Zhao and Shengyuan Chen. 2009. A simplex\narmijo downhill algorithm for optimizing statistical\nmachine translation decoding parameters. In Proceed-\nings of Human Language Technologies: The 2009 An-\nnual Conference of the North American Chapter of the\nAssociation for Computational Linguistics, Compan-\nion Volume: Short Papers, NAACL-Short ?09, pages\n21?24, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.995375923076923","#text":"\nIn statistical machine translation, minimum\nerror rate training (MERT) is a standard\nmethod for tuning a single weight with regard\nto a given development data. However, due to\nthe diversity and uneven distribution of source\nsentences, there are two problems suffered by\nthis method. First, its performance is highly\ndependent on the choice of a development set,\nwhich may lead to an unstable performance\nfor testing. Second, translations become in-\nconsistent at the sentence level since tuning is\nperformed globally on a document level. In\nthis paper, we propose a novel local training\nmethod to address these two problems. Un-\nlike a global training method, such as MERT,\nin which a single weight is learned and used\nfor all the input sentences, we perform training\nand testing in one step by learning a sentence-\nwise weight for each input sentence. We pro-\npose efficient incremental training methods to\nput the local training into practice. In NIST\nChinese-to-English translation tasks, our lo-\ncal training method significantly outperforms\nMERT with the maximal improvements up to\n2.0 BLEU points, meanwhile its efficiency is\ncomparable to that of the global method.\n"},{"#tail":"\n","@confidence":"0.96306925","#text":"\nOch and Ney (2002) introduced the log-linear model\nfor statistical machine translation (SMT), in which\ntranslation is considered as the following optimiza-\ntion problem:\n"},{"#tail":"\n","@confidence":"0.998623115384615","#text":"\nwhere f and e (e?) are source and target sentences,\nrespectively. h is a feature vector which is scaled\nby a weight W . Parameter estimation is one of\nthe most important components in SMT, and var-\nious training methods have been proposed to tune\nW . Some methods are based on likelihood (Och and\nNey, 2002; Blunsom et al2008), error rate (Och,\n2003; Zhao and Chen, 2009; Pauls et al2009; Gal-\nley and Quirk, 2011), margin (Watanabe et al2007;\nChiang et al2008) and ranking (Hopkins and May,\n2011), and among which minimum error rate train-\ning (MERT) (Och, 2003) is the most popular one.\nAll these training methods follow the same\npipeline: they train only a single weight on a given\ndevelopment set, and then use it to translate all the\nsentences in a test set. We call them a global train-\ning method. One of its advantages is that it allows us\nto train a single weight offline and thereby it is effi-\ncient. However, due to the diversity and uneven dis-\ntribution of source sentences(Li et al2010), there\nare some shortcomings in this pipeline.\nFirstly, on the document level, the performance of\nthese methods is dependent on the choice of a devel-\nopment set, which may potentially lead to an unsta-\nble translation performance for testing. As referred\nin our experiment, the BLEU points on NIST08 are\n"},{"#tail":"\n","@confidence":"0.995462034482758","#text":"\nlinearly separable classification problem transformed from (a) via tuning as ranking (Hopkins and May, 2011). Since\nscore of e11 is greater than that of e12, ?1, 0? corresponds to a possitive example denoted as ???, and ??1, 0? corre-\nsponds to a negative example denoted as ?*?. Since the transformed classification problem is not linearly separable,\nthere does not exist a single weight which can obtain e11 and e21 as translation results meanwhile. However, one can\nobtain e11 and e21 with weights: ?1, 1? and ??1, 1?, respectively.\n19.04 when the Moses system is tuned on NIST02\nby MERT. However, its performance is improved to\n21.28 points when tuned on NIST06. The automatic\nselection of a development set may partially address\nthe problem. However it is inefficient since tuning\nrequires iteratively decoding an entire development\nset, which is impractical for an online service.\nSecondly, translation becomes inconsistent on the\nsentence level (Ma et al2011). Global training\nmethod such as MERT tries to optimize the weight\ntowards the best performance for the whole set, and\nit can not necessarily always obtain good translation\nfor every sentence in the development set. The rea-\nson is that different sentences may need different\noptimal weights, and MERT can not find a single\nweight to satisfy all of the sentences. Figure 1(a)\nshows such an example, in which a development set\ncontains two sentences f1 and f2 with translations e\nand feature vectors h. When we tune examples in\nFigure 1(a) by MERT, it can be regarded as a non-\nlinearly separable classification problem illustrated\nin Figure 1(b). Therefore, there exists no single\nweightW which simultaneously obtains e11 and e21\nas translation for f1 and f2 via Equation (1). How-\never, we can achieve this with two weights: ?1, 1?\nfor f1 and ??1, 1? for f2.\nIn this paper, inspired by KNN-SVM (Zhang et\nal., 2006), we propose a local training method,\nwhich trains sentence-wise weights instead of a sin-\ngle weight, to address the above two problems.\nCompared with global training methods, such as\nMERT, in which training and testing are separated,\nour method works in an online fashion, in which\ntraining is performed during testing. This online\nfashion has an advantage in that it can adapt the\nweights for each of the test sentences, by dynam-\nically tuning the weights on translation examples\nwhich are similar to these test sentences. Similar\nto the method of development set automatical selec-\ntion, the local training method may also suffer the\nproblem of efficiency. To put it into practice, we\npropose incremental training methods which avoid\nretraining and iterative decoding on a development\nset.\nOur local training method has two advantages:\nfirstly, it significantly outperforms MERT, especially\nwhen test set is different from the development set;\nsecondly, it improves the translation consistency.\nExperiments on NIST Chinese-to-English transla-\ntion tasks show that our local training method sig-\nnificantly gains over MERT, with the maximum im-\nprovements up to 2.0 BLEU, and its efficiency is\ncomparable to that of the global training method.\n"},{"#tail":"\n","@confidence":"0.998392142857143","#text":"\nThe local training method (Bottou and Vapnik,\n1992) is widely employed in computer vision\n(Zhang et al2006; Cheng et al2010). Compared\nwith the global training method which tries to fit\na single weight on the training data, the local one\nlearns weights based on the local neighborhood in-\nformation for each test example. It is superior to\n"},{"#tail":"\n","@confidence":"0.851639857142857","#text":"\nthe global one when the data sets are not evenly\ndistributed (Bottou and Vapnik, 1992; Zhang et al\n2006).\nAlgorithm 1 Naive Local Training Method\nInput: T = {ti}Ni=1(test set), K (retrieval size),\nDev(development set), D(retrieval data)\nOutput: Translation results of T\n"},{"#tail":"\n","@confidence":"0.99553576744186","#text":"\nSuppose T be a test set, Dev a development set,\nand D a retrieval data. The local training in SMT\nis described in the Algorithm 1. For each sentence\nti in test set, training examples Di is retrieved from\nD using a similarity measure (line 2), a weight W i\nis optimized on Dev and Di (line 3)1, and, finally,\nti is decoded with W i for testing (line 4). At the\nend of this algorithm, it returns the translation re-\nsults for T . Note that weights are adapted for each\ntest sentence ti in line 3 by utilizing the translation\nexamples Di which are similar to ti. Thus, our local\ntraining method can be considered as an adaptation\nof translation weights.\nAlgorithm 1 suffers a problem of training effi-\nciency in line 3. It is impractical to train a weight\nW i on Dev and Di from scratch for every sen-\ntence, since iteratively decodingDev andDi is time\nconsuming when we apply MERT. To address this\nproblem, we propose a novel incremental approach\nwhich is based on a two-phase training.\nOn the first phase, we use a global training\nmethod, like MERT, to tune a baseline weight on\nthe development set Dev in an offline manner. On\nthe second phase, we utilize the retrieved examples\nto incrementally tune sentence-wise local weights\nbased on the baseline weight. This method can\nnot only consider the common characteristics learnt\nfrom the Dev, but also take into account the knowl-\n1Usually, the quality of development set Dev is high, since\nit is manually produced with multiple references. This is the\nmain reason why Dev is used as a part of new development set\nto train W i.\nedge for each individual sentence learnt from sim-\nilar examples during testing. On the phase of in-\ncremental training, we perform decoding only once\nfor retrieved examples Di, though several rounds of\ndecoding are possible and potentially better if one\ndoes not seriously care about training speed. Fur-\nthermore, instead of on-the-fly decoding, we decode\nthe retrieval data D offline using the parameter from\nour baseline weight and its nbest translation candi-\ndates are saved with training examples to increase\nthe training efficiency.\n"},{"#tail":"\n","@confidence":"0.997766941176471","#text":"\nThe two-phase local training algorithm is de-\nscribed in Algorithm 2, where cs and rs denote the\ntranslation candidate set and reference set for each\nsentence fs in retrieval data, respectively, and K is\nthe retrieval size. It globally trains a baseline weight\nWb (line 1), and decodes each sentence in retrieval\ndata D with the weight Wb (line 2). For each sen-\ntence ti in test set T , it first retrieves training exam-\nples Di from D (line 4), and then it runs local train-\ning to tune a local weight W i (line 5) and performs\ntesting with W i for ti (line 6). Please note that the\ntwo-phase training contains global training in line 1\nand local training in line 5.\nFrom Algorithm 2, one can see that our method is\neffective even if the test set is unknow, for example,\nin the scenario of online translation services, since\nthe global training on development set and decoding\n"},{"#tail":"\n","@confidence":"0.99781825","#text":"\non retrieval data can be performed offline.\nIn the next two sections, we will discuss the de-\ntails about the similarity metric in line 4 and the in-\ncremental training in line 5 of Algorithm 2.\n"},{"#tail":"\n","@confidence":"0.981293","#text":"\nIn line 4 of Algorithm 2, to retrieve training exam-\nples for the sentence ti , we first need a metric to\nretrieve similar translation examples. We assume\nthat the metric satisfy the property: more similar the\ntest sentence and translation examples are, the better\ntranslation result one obtains when decoding the test\nsentence with the weight trained on the translation\nexamples.\nThe metric we consider here is derived from\nan example-based machine translation. To retrieve\ntranslation examples for a test sentence, (Watanabe\nand Sumita, 2003) defined a metric based on the\ncombination of edit distance and TF-IDF (Manning\nand Schu?tze, 1999) as follows:\n"},{"#tail":"\n","@confidence":"0.998548833333333","#text":"\nconsidered as a document. In this paper, we extract\nsimilar examples from training data. Like example-\nbased translation in which similar source sentences\nhave similar translations, we assume that the optimal\ntranslation weights of the similar source sentences\nare closer.\n"},{"#tail":"\n","@confidence":"0.976359733333333","#text":"\nUltraconservative Update\nCompared with retraining mode, incremental train-\ning can improve the training efficiency. In the field\nof machine learning research, incremental training\nhas been employed in the work (Cauwenberghs and\nPoggio, 2001; Shilton et al2005), but there is lit-\ntle work for tuning parameters of statistical machine\ntranslation. The biggest difficulty lies in that the fea-\nture vector of a given training example, i.e. transla-\ntion example, is unavailable until actually decoding\nthe example, since the derivation is a latent variable.\nIn this section, we will investigate the incremental\ntraining methods in SMT scenario.\nFollowing the notations in Algorithm 2, Wb is\nthe baseline weight, Di = {?f ij , c\n"},{"#tail":"\n","@confidence":"0.99613885","#text":"\ntraining examples for ti. For the sake of brevity, we\nwill drop the index i, Di = {?fj , cj , rj?}Kj=1, in the\nrest of this paper. Our goal is to find an optimal\nweight, denoted by W i, which is a local weight and\nused for decoding the sentence ti. Unlike the global\nmethod which performs tuning on the whole devel-\nopment set Dev +Di as in Algorithm 1, W i can be\nincrementally learned by optimizing onDi based on\nWb. We employ the idea of ultraconservative update\n(Crammer and Singer, 2003; Crammer et al2006)\nto propose two incremental methods for local train-\ning in Algorithm 2 as follows.\nUltraconservative update is an efficient way to\nconsider the trade-off between the progress made on\ndevelopment set Dev and the progress made on Di.\nIt desires that the optimal weight W i is not only\nclose to the baseline weight Wb, but also achieves\nthe low loss over the retrieved examples Di. The\nidea of ultraconservative update can be formalized\nas follows:\n"},{"#tail":"\n","@confidence":"0.9950213","#text":"\nwhere d(W,Wb) is a distance metric over a pair\nof weights W and Wb. It penalizes the weights\nfar away from Wb and it is L2 norm in this paper.\nLoss(Di,W ) is a loss function of W defined on Di\nand it evaluates the performance of W over Di. ?\nis a positive hyperparameter. If Di is more similar\nto the test sentence ti, the better performance will be\nachieved for the larger ?. In particular, ifDi consists\nof only a single sentence ti, the best performance\nwill be obtained when ? goes to infinity.\n"},{"#tail":"\n","@confidence":"0.9868402","#text":"\nMIRA(Crammer and Singer, 2003; Crammer et al\n2006) is a form of ultraconservative update in (3)\nwhoseLoss is defined as hinge loss based on margin\nover the pairwise translation candiates in Di. It tries\nto minimize the following quadratic program:\n"},{"#tail":"\n","@confidence":"0.99247465","#text":"\nwhere h(fj , e) is the feature vector of candidate e,\nejn is a translation member of fj in cj , ej? is the\noracle one in cj , `jn is a loss between ej? and ejn\nand it is the same as referred in (Chiang et al2008),\nand |cj  |denotes the number of members in cj .\nDifferent from (Watanabe et al2007; Chiang\net al2008) employing the MIRA to globally train\nSMT, in this paper, we apply MIRA as one of local\ntraining method for SMT and we call it as margin\nbased ultraconservative update (MBUU for shortly)\nto highlight its advantage of incremental training in\nline 5 of Algorithm 2.\nFurther, there is another difference between\nMBUU and MIRA in (Watanabe et al2007; Chi-\nang et al2008). MBUU is a batch update mode\nwhich updates the weight with all training examples,\nbut MIRA is an online one which updates with each\nexample (Watanabe et al2007) or part of examples\n(Chiang et al2008). Therefore, MBUU is more ul-\ntraconservative.\n"},{"#tail":"\n","@confidence":"0.966055","#text":"\nInstead of taking into account the margin-based\nhinge loss between a pair of translations as the Loss\nin (3), we directly optimize the error rate of trans-\nlation candidates with respect to their references in\nDi. Formally, the objective function of error rate\nbased ultraconservative update (EBUU) is as fol-\nlows:\n"},{"#tail":"\n","@confidence":"0.950867428571429","#text":"\nDue to the existence of L2 norm in objective\nfunction (5), the optimization algorithm MERT can\nnot be applied for this question since the exact line\nsearch routine does not hold here. Motivated by\n(Och, 2003; Smith and Eisner, 2006), we approxi-\nmate the Error in (5) by the expected loss, and then\nderive the following function:\n"},{"#tail":"\n","@confidence":"0.961602111111111","#text":"\nwhere ? > 0 is a real number valued smoother. One\ncan see that, in the extreme case, for ? ? ?, (6)\nconverges to (5).\nWe apply the gradient decent method to minimize\nthe function (6), as it is smooth with respect to ?.\nSince the function (6) is non-convex, the solution\nobtained by gradient descent method may depend on\nthe initial point. In this paper, we set the initial point\nas Wb in order to achieve a desirable solution.\n"},{"#tail":"\n","@confidence":"0.999320260869565","#text":"\nWe conduct our experiments on the Chinese-to-\nEnglish translation task. The training data is FBIS\ncorpus consisting of about 240k sentence pairs. The\ndevelopment set is NIST02 evaluation data, and the\ntest datasets are NIST05, NIST06,and NIST08.\nWe run GIZA++ (Och and Ney, 2000) on the\ntraining corpus in both directions (Koehn et al\n2003) to obtain the word alignment for each sen-\ntence pair. We train a 4-gram language model on\nthe Xinhua portion of the English Gigaword cor-\npus using the SRILM Toolkits (Stolcke, 2002) with\nmodified Kneser-Ney smoothing (Chen and Good-\nman, 1998). In our experiments the translation per-\nformances are measured by case-insensitive BLEU4\nmetric (Papineni et al2002) and we use mteval-\nv13a.pl as the evaluation tool. The significance test-\ning is performed by paired bootstrap re-sampling\n(Koehn, 2004).\nWe use an in-house developed hierarchical\nphrase-based translation (Chiang, 2005) as our base-\nline system, and we denote it as In-Hiero. To ob-\ntain satisfactory baseline performance, we tune In-\nHiero system for 5 times using MERT, and then se-\n"},{"#tail":"\n","@confidence":"0.98486595","#text":"\n(MERT). NIST05 is the set used to tune ? for MBUU and\nEBUU, and NIST06 and NIST08 are test sets. + means\nthe local method is significantly better than MERT with\np < 0.05.\nlect the best-performing one as our baseline for the\nfollowing experiments. As Table 1 indicates, our\nbaseline In-Hiero is comparable to the phrase-based\nMT (Moses) and the hierarchical phrase-based MT\n(Moses hier) implemented in Moses, an open source\nMT toolkit2 (Koehn et al2007). Both of these sys-\ntems are with default setting. All three systems are\ntrained by MERT with 100 best candidates.\nTo compare the local training method in Algo-\nrithm 2, we use a standard global training method,\nMERT, as the baseline training method. We do not\ncompare with Algorithm 1, in which retraining is\nperformed for each input sentence, since retraining\nfor the whole test set is impractical given that each\nsentence-wise retraining may take some hours or\neven days. Therefore, we just compare Algorithm\n"},{"#tail":"\n","@confidence":"0.980563857142857","#text":"\nTo run the Algorithm 2, we tune the baseline weight\nWb on NIST02 by MERT3. The retrieval data is set\nas the training data, i.e. FBIS corpus, and the re-\ntrieval size is 100. We translate retrieval data with\nWb to obtain their 100 best translation candidates.\nWe use the simple linear interpolated TF-IDF met-\nric with ? = 0.1 in Section 3 as the retrieval metric.\n"},{"#tail":"\n","@confidence":"0.987639090909091","#text":"\ndatasets.\nFor an efficient tuning, the retrieval process is par-\nallelized as follows: the examples are assigned to 4\nCPUs so that each CPU accepts a query and returns\nits top-100 results, then all these top-100 results are\nmerged into the final top-100 retrieved examples to-\ngether with their translation candidates. In our ex-\nperiments, we employ the two incremental training\nmethods, i.e. MBUU and EBUU. Both of the hyper-\nparameters ? are tuned on NIST05 and set as 0.018\nand 0.06 for MBUU and EBUU, respectively. In\nthe incremental training step, only one CPU is em-\nployed.\nTable 2 depicts that testing each sentence with lo-\ncal training method takes 2.9 seconds, which is com-\nparable to the testing time 2.0 seconds with global\ntraining method4. This shows that the local method\nis efficient. Further, compared to the retrieval, the\nlocal training is not the bottleneck. Actually, if we\nuse LSH technique (Andoni and Indyk, 2008) in re-\ntrieval process, the local method can be easily scaled\nto a larger training data.\n"},{"#tail":"\n","@confidence":"0.9982419375","#text":"\nTable 3 shows the main results of our local train-\ning methods. The EBUU training method signifi-\ncantly outperforms the MERT baseline, and the im-\nprovement even achieves up to 2.0 BLEU points on\nNIST08. We can also see that EBUU and MBUU are\ncomparable on these three test sets. Both of these\ntwo local training methods achieve significant im-\nprovements over the MERT baseline, which proves\nthe effectiveness of our local training method over\nglobal training method.\nAlthough both local methods MBUU and EBUU\nachieved improvements on all the datasets, their\ngains on NIST06 and NIST08 are significantly\nhigher than those achieved on NIST05 test dataset.\nWe conjecture that, the more different a test set and\na development set are, the more potential improvem-\n"},{"#tail":"\n","@confidence":"0.946146578947368","#text":"\nEBUU.\nnts local training has for the sentences in this test set.\nTo test our hypothesis, we measured the similarity\nbetween the development set and a test set by the\naverage value5 of accumulated TF-IDF scores of de-\nvelopment dataset and each sentence in test datasets.\nTable 4 shows that NIST06 and NIST08 are more\ndifferent from NIS02 than NIST05, thus, this is po-\ntentially the reason why local training is more effec-\ntive on NIST06 and NIST08.\nAs mentioned in Section 1, the global training\nmethods such as MERT are highly dependent on de-\nvelopment sets, which can be seen in Table 5. There-\nfore, the translation performance will be degraded if\none chooses a development data which is not close\n5Instead of using the similarity between two documents de-\nvelopment and test datasets, we define the similarity as the av-\nerage similarity of the development set and the sentences in test\nset. The reason is that it reduces its dependency on the number\n"},{"#tail":"\n","@confidence":"0.998793048780488","#text":"\nlevel BLEU points over three test datasets.\nto the test data. We can see that, with the help of the\nlocal training, we still gain much even if we selected\nan unsatisfactory development data.\nAs also mentioned in Section 1, the global meth-\nods do not care about the sentence level perfor-\nmance. Table 6 depicts that there are 1735 sentences\nwith zero BLEU points in all the three test datasets\nfor MERT. Besides obtaining improvements on doc-\nument level as referred in Table 3, the local training\nmethods can also achieve consistent improvements\non sentence level and thus can improve the users?\nexperiences.\nThe hyperparameters ? in both MBUU (4) and\nEBUU (6) has an important influence on transla-\ntion performance. Figure 2 shows such influence\nfor EBUU on the test datasets. We can see that, the\nperformances on all these datasets improve as ? be-\ncomes closer to 0.06 from 0, and the performance\ncontinues improving when ? passes over 0.06 on\nNIST08 test set, where the performance constantly\nimproves up to 2.6 BLEU points over baseline. As\nmentioned in Section 4, if the retrieved examples are\nvery similar to the test sentence, the better perfor-\nmance will be achieved with the larger ?. There-\nfore, it is reasonable that the performances improved\nwhen ? increased from 0 to 0.06. Further, the turn-\ning point appearing at 0.06 proves that the ultra-\nconservative update is necessary. We can also see\nthat the performance on NIST08 consistently im-\nproves and achieves the maximum gain when ? ar-\nrives at 0.1, but those on both NIST05 and NIST06\nachieves the best when it arrives at 0.06. This\nphenomenon can also be interpreted in Table 4 as\nthe lowest similarity between the development and\nNIST08 datasets.\nGenerally, the better performance may be\nachieved when more examples are retrieved. Actu-\nally, in Table 7 there seems to be little dependency\nbetween the numbers of examples retrieved and the\ntranslation qualities, although they are positively re-\n"},{"#tail":"\n","@confidence":"0.823950142857143","#text":"\nwhich consist of 1-best resluts of MERT and 1-best\nresluts of EBUU.\nlated approximately.\nTable 8 presents the performance of the oracle\ntranslations selected from the 1-best translation re-\nsults of MERT and EBUU. Clearly, there exists more\npotential improvement for local training method.\n"},{"#tail":"\n","@confidence":"0.999280945945946","#text":"\nSeveral works have proposed discriminative tech-\nniques to train log-linear model for SMT. (Och and\nNey, 2002; Blunsom et al2008) used maximum\nlikelihood estimation to learn weights for MT. (Och,\n2003; Moore and Quirk, 2008; Zhao and Chen,\n2009; Galley and Quirk, 2011) employed an eval-\nuation metric as a loss function and directly opti-\nmized it. (Watanabe et al2007; Chiang et al2008;\nHopkins and May, 2011) proposed other optimiza-\ntion objectives by introducing a margin-based and\nranking-based indirect loss functions.\nAll the methods mentioned above train a single\nweight for the whole development set, whereas our\nlocal training method learns a weight for each sen-\ntence. Further, our translation framework integrates\nthe training and testing into one unit, instead of treat-\ning them separately. One of the advantages is that it\ncan adapt the weights for each of the test sentences.\nOur method resorts to some translation exam-\nples, which is similar as example-based translation\nor translation memory (Watanabe and Sumita, 2003;\nHe et al2010; Ma et al2011). Instead of using\ntranslation examples to construct translation rules\nfor enlarging the decoding space, we employed them\nto discriminatively learn local weights.\nSimilar to (Hildebrand et al2005; Lu? et al\n2007), our method also employes IR methods to re-\ntrieve examples for a given test set. Their methods\nutilize the retrieved examples to acquire translation\nmodel and can be seen as the adaptation of trans-\nlation model. However, ours uses the retrieved ex-\namples to tune the weights and thus can be consid-\nered as the adaptation of tuning. Furthermore, since\nours does not change the translation model which\nneeds to run GIZA++ and it incrementally trains lo-\ncal weights, our method can be applied for online\ntranslation service.\n"},{"#tail":"\n","@confidence":"0.99939412","#text":"\nThis paper proposes a novel local training frame-\nwork for SMT. It has two characteristics, which\nare different from global training methods such as\nMERT. First, instead of training only one weight for\ndocument level, it trains a single weight for sentence\nlevel. Second, instead of considering the training\nand testing as two separate units, we unify the train-\ning and testing into one unit, which can employ the\ninformation of test sentences and perform sentence-\nwise local adaptation of weights.\nLocal training can not only alleviate the prob-\nlem of the development data selection, but also re-\nduce the risk of sentence-wise bad translation re-\nsults, thus consistently improve the translation per-\nformance. Experiments show gains up to 2.0 BLEU\npoints compared with a MERT baseline. With the\nhelp of incremental training methods, the time in-\ncurred by local training was negligible and the local\ntraining and testing totally took 2.9 seconds for each\nsentence.\nIn the future work, we will further investigate the\nlocal training method, since there are more room for\nimprovements as observed in our experiments. We\nwill test our method on other translation models and\nlarger training data6.\n"},{"#tail":"\n","@confidence":"0.9999386","#text":"\nWe would like to thank Hongfei Jiang and Shujie\nLiu for many valuable discussions and thank three\n6Intuitionally, when the corpus of translation examples is\nlarger, the retrieval results in Algorithm 2 are much similar as\nthe test sentence. Therefore our method may favor this.\n"},{"#tail":"\n","@confidence":"0.815222875","#text":"\nanonymous reviewers for many valuable comments\nand helpful suggestions. This work was supported\nby National Natural Science Foundation of China\n(61173073,61100093), and the Key Project of the\nNational High Technology Research and Develop-\nment Program of China (2011AA01A207), and the\nFundamental Research Funds for Central Univer-\nsites (HIT.NSRIF.2013065).\n"},{"#tail":"\n","@confidence":"0.83337025","#text":"\nAlexandr Andoni and Piotr Indyk. 2008. Near-optimal\nhashing algorithms for approximate nearest neighbor\nin high dimensions. Commun. ACM, 51(1):117?122,\nJanuary.\n"}],"#text":"\n","sectionHeader":[{"#tail":"\n","@confidence":"0.989371","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.996822","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.886932","@genericHeader":"introduction","#text":"\n2 Local Training and Testing\n"},{"#tail":"\n","@confidence":"0.901328","@genericHeader":"method","#text":"\n3 Acquiring Training Examples\n"},{"#tail":"\n","@confidence":"0.956947","@genericHeader":"method","#text":"\n4 Incremental Training Based on\n"},{"#tail":"\n","@confidence":"0.981524","@genericHeader":"method","#text":"\n5 Experiments and Results\n"},{"#tail":"\n","@confidence":"0.817073","@genericHeader":"method","#text":"\n2 with MERT.\n"},{"#tail":"\n","@confidence":"0.999434","@genericHeader":"method","#text":"\n6 Related Work\n"},{"#tail":"\n","@confidence":"0.984369","@genericHeader":"conclusions","#text":"\n7 Conclusion and Future Work\n"},{"#tail":"\n","@confidence":"0.966928","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.910527","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.7749885","#text":"\nTable 1: The performance comparison of the baseline In-\nHiero VS Moses and Moses hier.\n"},{"#tail":"\n","@confidence":"0.8646575","#text":"\nTable 2: The efficiency of the local training and testing\nmeasured by sentence averaged runtime.\n"},{"#tail":"\n","@confidence":"0.9227175","#text":"\nTable 3: The performance comparison of local train-\ning methods (MBUU and EBUU) and a global method\n"},{"#tail":"\n","@confidence":"0.995717","#text":"\nTable 4: The similarity of development and three test\n"},{"#tail":"\n","@confidence":"0.939423","#text":"\nTable 5: The comparison of MERT with different de-\nvelopment datasets and local training method based on\n"},{"#tail":"\n","@confidence":"0.999179","#text":"\nTable 6: The statistics of sentences with 0.0 sentence-\n"},{"#tail":"\n","@confidence":"0.8665295","#text":"\nTable 7: The performance comparison by varying re-\ntrieval size in Algorithm 2 based on EBUU.\n"},{"#tail":"\n","@confidence":"0.999816","#text":"\nTable 8: The performance of Oracle of 2-best results\n"}],"page":[{"#tail":"\n","@confidence":"0.99392","#text":"\n402\n"},{"#tail":"\n","@confidence":"0.998311","#text":"\n403\n"},{"#tail":"\n","@confidence":"0.99783","#text":"\n404\n"},{"#tail":"\n","@confidence":"0.966445","#text":"\n405\n"},{"#tail":"\n","@confidence":"0.998856","#text":"\n406\n"},{"#tail":"\n","@confidence":"0.982419","#text":"\n407\n"},{"#tail":"\n","@confidence":"0.998123","#text":"\n408\n"},{"#tail":"\n","@confidence":"0.98423","#text":"\n409\n"},{"#tail":"\n","@confidence":"0.865518","#text":"\n410\n"},{"#tail":"\n","@confidence":"0.998585","#text":"\n411\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.999949","#text":"\nFigure 1: (a). An Example candidate space of dimensionality two. score is a evaluation metric of e. (b). The non-\n"},{"#tail":"\n","@confidence":"0.63513025","#text":"\nFigure 2: The peformance of EBUU for different ? over\nall the test datasets. The horizontal axis denotes the val-\nues of ? in function (6), and the vertical one denotes the\nBLEU points.\n"}],"table":[{"#tail":"\n","@confidence":"0.99293425","#text":"\nSystems NIST02 NIST05 NIST06 NIST08\nMoses 30.39 26.31 25.34 19.07\nMoses hier 33.68 26.94 26.28 18.65\nIn-Hiero 31.24 27.07 26.32 19.03\n"},{"#tail":"\n","@confidence":"0.98854375","#text":"\nMethods Steps Seconds\nGlobal method Decoding 2.0\nLocal method Retrieval +0.6\nLocal training +0.3\n"},{"#tail":"\n","@confidence":"0.99861925","#text":"\nMethods NIST05 NIST06 NIST08\nGlobal MERT 27.07 26.32 19.03\nLocal MBUU 27.75+ 27.88+ 20.84+\nEBUU 27.85+ 27.99+ 21.08+\n"},{"#tail":"\n","@confidence":"0.991351","#text":"\nNIST05 NIST06 NIST08\nNIST02 0.665 0.571 0.506\n"},{"#tail":"\n","@confidence":"0.9444422","#text":"\nMetthods Dev NIST08\nNIST02 19.03\nMERT NIST05 20.06\nNIST06 21.28\nEBUU NIST02 21.08\n"},{"#tail":"\n","@confidence":"0.87980225","#text":"\nof sentences in test dataset, which may cause a bias.\nMethods Number Percents\nMERT 1735 42.3%\nEBUU 1606 39.1%\n"},{"#tail":"\n","@confidence":"0.9902175","#text":"\nRetrieval Size NIST05 NIST06 NIST08\n40 27.66 27.81 20.87\n70 27.77 27.93 21.08\n100 27.85 27.99 21.08\n"},{"#tail":"\n","@confidence":"0.9995475","#text":"\nMethods NIST05 NIST06 NIST08\nMERT 27.07 26.32 19.03\nEBUU 27.85 27.99 21.08\nOracle 29.46 29.35 22.09\n"}],"email":{"#tail":"\n","@confidence":"0.963736","#text":"\n{lmliu,hailong,tjzhao,yumo,chzhu}@mtlab.hit.edu.cn\ntaro.watanabe@nict.go.jp\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.511310","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.91245","#text":"Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 402?411, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics"},"address":{"#tail":"\n","@confidence":"0.999038","#text":"3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.996736","#text":"1School of Computer Science and Technology Harbin Institute of Technology, Harbin, China 2National Institute of Information and Communication Technology"},"author":[{"#tail":"\n","@confidence":"0.921786","#text":"Lemao Liu"},{"#tail":"\n","@confidence":"0.921786","#text":"Hailong Cao"},{"#tail":"\n","@confidence":"0.921786","#text":"Taro Watanabe"},{"#tail":"\n","@confidence":"0.921786","#text":"Tiejun Zhao"},{"#tail":"\n","@confidence":"0.921786","#text":"Mo Yu"},{"#tail":"\n","@confidence":"0.921786","#text":"CongHui Zhu"}],"abstract":{"#tail":"\n","@confidence":"0.997270592592593","#text":"In statistical machine translation, minimum error rate training (MERT) is a standard method for tuning a single weight with regard to a given development data. However, due to the diversity and uneven distribution of source sentences, there are two problems suffered by this method. First, its performance is highly dependent on the choice of a development set, which may lead to an unstable performance for testing. Second, translations become inconsistent at the sentence level since tuning is performed globally on a document level. In this paper, we propose a novel local training method to address these two problems. Unlike a global training method, such as MERT, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentencewise weight for each input sentence. We propose efficient incremental training methods to put the local training into practice. In NIST Chinese-to-English translation tasks, our local training method significantly outperforms MERT with the maximal improvements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method."},"title":{"#tail":"\n","@confidence":"0.912907","#text":"Locally Training the Log-Linear Model for SMT"},"email":[{"#tail":"\n","@confidence":"0.8103335","#text":"lmliu@mtlab.hit.edu.cntaro.watanabe@nict.go.jp"},{"#tail":"\n","@confidence":"0.8103335","#text":"hailong@mtlab.hit.edu.cntaro.watanabe@nict.go.jp"},{"#tail":"\n","@confidence":"0.8103335","#text":"tjzhao@mtlab.hit.edu.cntaro.watanabe@nict.go.jp"},{"#tail":"\n","@confidence":"0.8103335","#text":"yumo@mtlab.hit.edu.cntaro.watanabe@nict.go.jp"},{"#tail":"\n","@confidence":"0.8103335","#text":"chzhu@mtlab.hit.edu.cntaro.watanabe@nict.go.jp"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"volume":{"#tail":"\n","#text":"51"},"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Alexandr Andoni and Piotr Indyk. 2008. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Commun. ACM, 51(1):117?122, January."},"journal":{"#tail":"\n","#text":"Commun. ACM,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Andoni, Indyk, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"candidates. In our experiments, we employ the two incremental training methods, i.e. MBUU and EBUU. Both of the hyperparameters ? are tuned on NIST05 and set as 0.018 and 0.06 for MBUU and EBUU, respectively. In the incremental training step, only one CPU is employed. Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method4. This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually, if we use LSH technique (Andoni and Indyk, 2008) in retrieval process, the local method can be easily scaled to a larger training data. 5.3 Results and Analysis Table 3 shows the main results of our local training methods. The EBUU training method significantly outperforms the MERT baseline, and the improvement even achieves up to 2.0 BLEU points on NIST08. We can also see that EBUU and MBUU are comparable on these three test sets. Both of these two local training methods achieve significant improvements over the MERT baseline, which proves the effectiveness of our local training method over global training method. Although both local metho","@endWordPosition":"3943","@position":"22218","annotationId":"T1","@startWordPosition":"3940","@citStr":"Andoni and Indyk, 2008"}},"title":{"#tail":"\n","#text":"Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alexandr Andoni"},{"#tail":"\n","#text":"Piotr Indyk"}]}},{"date":{"#tail":"\n","#text":"2008"},"title":{"#tail":"\n","#text":"A discriminative latent variable model for statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL, pages 200?208, Columbus, Ohio, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"200--208"},"marker":{"#tail":"\n","#text":"Blunsom, Cohn, Osborne, 2008"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Columbus, Ohio,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Phil Blunsom"},{"#tail":"\n","#text":"Trevor Cohn"},{"#tail":"\n","#text":"Miles Osborne"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1992"},"rawString":{"#tail":"\n","#text":"Le?on Bottou and Vladimir Vapnik. 1992. Local learning algorithms. Neural Comput., 4:888?900, November."},"journal":{"#tail":"\n","#text":"Neural Comput.,"},"#text":"\n","pages":{"#tail":"\n","#text":"4--888"},"marker":{"#tail":"\n","#text":"Bottou, Vapnik, 1992"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"incremental training methods which avoid retraining and iterative decoding on a development set. Our local training method has two advantages: firstly, it significantly outperforms MERT, especially when test set is different from the development set; secondly, it improves the translation consistency. Experiments on NIST Chinese-to-English translation tasks show that our local training method significantly gains over MERT, with the maximum improvements up to 2.0 BLEU, and its efficiency is comparable to that of the global training method. 2 Local Training and Testing The local training method (Bottou and Vapnik, 1992) is widely employed in computer vision (Zhang et al2006; Cheng et al2010). Compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example. It is superior to 403 the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992; Zhang et al 2006). Algorithm 1 Naive Local Training Method Input: T = {ti}Ni=1(test set), K (retrieval size), Dev(development set), D(retrieval data) Output: Translation results of T 1: for all sentence ti such that 1 ? i ","@endWordPosition":"1288","@position":"7266","annotationId":"T2","@startWordPosition":"1285","@citStr":"Bottou and Vapnik, 1992"}},"title":{"#tail":"\n","#text":"Local learning algorithms."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Leon Bottou"},{"#tail":"\n","#text":"Vladimir Vapnik"}]}},{"volume":{"#tail":"\n","#text":"13"},"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"G. Cauwenberghs and T. Poggio. 2001. Incremental and decremental support vector machine learning. In Advances in Neural Information Processing Systems (NIPS*2000), volume 13."},"#text":"\n","marker":{"#tail":"\n","#text":"Cauwenberghs, Poggio, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"(0 ? ? ? 1) is an interpolation weight, fi(i = 1, 2) is a word sequence and can be also considered as a document. In this paper, we extract similar examples from training data. Like examplebased translation in which similar source sentences have similar translations, we assume that the optimal translation weights of the similar source sentences are closer. 4 Incremental Training Based on Ultraconservative Update Compared with retraining mode, incremental training can improve the training efficiency. In the field of machine learning research, incremental training has been employed in the work (Cauwenberghs and Poggio, 2001; Shilton et al2005), but there is little work for tuning parameters of statistical machine translation. The biggest difficulty lies in that the feature vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable. In this section, we will investigate the incremental training methods in SMT scenario. Following the notations in Algorithm 2, Wb is the baseline weight, Di = {?f ij , c i j , r i j?} K j=1 denotes training examples for ti. For the sake of brevity, we will drop the index i, Di = {?fj , cj ","@endWordPosition":"2347","@position":"13222","annotationId":"T3","@startWordPosition":"2344","@citStr":"Cauwenberghs and Poggio, 2001"}},"title":{"#tail":"\n","#text":"Incremental and decremental support vector machine learning."},"booktitle":{"#tail":"\n","#text":"In Advances in Neural Information Processing Systems (NIPS*2000),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"G Cauwenberghs"},{"#tail":"\n","#text":"T Poggio"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical Report TR-10-98."},"date":{"#tail":"\n","#text":"1998"},"institution":{"#tail":"\n","#text":"Harvard University."},"rawString":{"#tail":"\n","#text":"Stanley F Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. In Technical Report TR-10-98. Harvard University."},"#text":"\n","marker":{"#tail":"\n","#text":"Chen, Goodman, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ution. 5 Experiments and Results 5.1 Setting We conduct our experiments on the Chinese-toEnglish translation task. The training data is FBIS corpus consisting of about 240k sentence pairs. The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al2002) and we use mtevalv13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline system, and we denote it as In-Hiero. To obtain satisfactory baseline performance, we tune InHiero system for 5 times using MERT, and then se406 Methods Steps Seconds Global method Decoding 2.0 Local method Retrieval +0.6 Local training +0.3 Table 2: ","@endWordPosition":"3380","@position":"18853","annotationId":"T4","@startWordPosition":"3376","@citStr":"Chen and Goodman, 1998"}},"title":{"#tail":"\n","#text":"An empirical study of smoothing techniques for language modeling. In"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Stanley F Chen"},{"#tail":"\n","#text":"Joshua Goodman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Haibin Cheng, Pang-Ning Tan, and Rong Jin. 2010. Efficient algorithm for localized support vector machine. IEEE Trans. on Knowl. and Data Eng., 22:537?549, April."},"journal":{"#tail":"\n","#text":"IEEE Trans. on Knowl. and Data Eng.,"},"#text":"\n","pages":{"#tail":"\n","#text":"22--537"},"marker":{"#tail":"\n","#text":"Cheng, Tan, Jin, 2010"},"title":{"#tail":"\n","#text":"Efficient algorithm for localized support vector machine."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Haibin Cheng"},{"#tail":"\n","#text":"Pang-Ning Tan"},{"#tail":"\n","#text":"Rong Jin"}]}},{"date":{"#tail":"\n","#text":"2008"},"title":{"#tail":"\n","#text":"Online large-margin training of syntactic and structural translation features."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 224?233, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"224--233"},"marker":{"#tail":"\n","#text":"Chiang, Marton, Resnik, 2008"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Chiang"},{"#tail":"\n","#text":"Yuval Marton"},{"#tail":"\n","#text":"Philip Resnik"}]}},{"date":{"#tail":"\n","#text":"2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"pus in both directions (Koehn et al 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al2002) and we use mtevalv13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline system, and we denote it as In-Hiero. To obtain satisfactory baseline performance, we tune InHiero system for 5 times using MERT, and then se406 Methods Steps Seconds Global method Decoding 2.0 Local method Retrieval +0.6 Local training +0.3 Table 2: The efficiency of the local training and testing measured by sentence averaged runtime. Methods NIST05 NIST06 NIST08 Global MERT 27.07 26.32 19.03 Local MBUU 27.75+ 27.88+ 20.84+ EBUU 27.85+ 27.99+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EBUU) and a global method (MERT). NIST05 is the set use","@endWordPosition":"3427","@position":"19185","annotationId":"T5","@startWordPosition":"3426","@citStr":"Chiang, 2005"}},"title":{"#tail":"\n","#text":"A hierarchical phrase-based model for statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 263?270, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"263--270"},"marker":{"#tail":"\n","#text":"Chiang, 2005"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David Chiang"}}},{"volume":{"#tail":"\n","#text":"3"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach. Learn. Res., 3:951?991, March."},"journal":{"#tail":"\n","#text":"J. Mach. Learn. Res.,"},"#text":"\n","marker":{"#tail":"\n","#text":"Crammer, Singer, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ethods in SMT scenario. Following the notations in Algorithm 2, Wb is the baseline weight, Di = {?f ij , c i j , r i j?} K j=1 denotes training examples for ti. For the sake of brevity, we will drop the index i, Di = {?fj , cj , rj?}Kj=1, in the rest of this paper. Our goal is to find an optimal weight, denoted by W i, which is a local weight and used for decoding the sentence ti. Unlike the global method which performs tuning on the whole development set Dev +Di as in Algorithm 1, W i can be incrementally learned by optimizing onDi based on Wb. We employ the idea of ultraconservative update (Crammer and Singer, 2003; Crammer et al2006) to propose two incremental methods for local training in Algorithm 2 as follows. Ultraconservative update is an efficient way to consider the trade-off between the progress made on development set Dev and the progress made on Di. It desires that the optimal weight W i is not only close to the baseline weight Wb, but also achieves the low loss over the retrieved examples Di. The idea of ultraconservative update can be formalized as follows: min W { d(W,Wb) + ? ? Loss(D i,W ) } , (3) where d(W,Wb) is a distance metric over a pair of weights W and Wb. It penalizes the weights","@endWordPosition":"2529","@position":"14219","annotationId":"T6","@startWordPosition":"2526","@citStr":"Crammer and Singer, 2003"}},"title":{"#tail":"\n","#text":"Ultraconservative online algorithms for multiclass problems."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Koby Crammer"},{"#tail":"\n","#text":"Yoram Singer"}]}},{"volume":{"#tail":"\n","#text":"7"},"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. J. Mach. Learn. Res., 7:551? 585, December."},"journal":{"#tail":"\n","#text":"J. Mach. Learn. Res.,"},"#text":"\n","pages":{"#tail":"\n","#text":"585"},"marker":{"#tail":"\n","#text":"Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" + ? ? Loss(D i,W ) } , (3) where d(W,Wb) is a distance metric over a pair of weights W and Wb. It penalizes the weights far away from Wb and it is L2 norm in this paper. Loss(Di,W ) is a loss function of W defined on Di and it evaluates the performance of W over Di. ? is a positive hyperparameter. If Di is more similar to the test sentence ti, the better performance will be achieved for the larger ?. In particular, ifDi consists of only a single sentence ti, the best performance will be obtained when ? goes to infinity. 4.1 Margin Based Ultraconservative Update MIRA(Crammer and Singer, 2003; Crammer et al 2006) is a form of ultraconservative update in (3) whoseLoss is defined as hinge loss based on margin over the pairwise translation candiates in Di. It tries to minimize the following quadratic program: 1 2 ||W ?Wb|| 2+ ? K K? j=1 max 1?n?|cj | ( `jn?W ??h(fj , ejn) ) with ?h(fj , ejn) = h(fj , ej?)? h(fj , ejn), (4) 405 where h(fj , e) is the feature vector of candidate e, ejn is a translation member of fj in cj , ej? is the oracle one in cj , `jn is a loss between ej? and ejn and it is the same as referred in (Chiang et al2008), and |cj | denotes the number of members in cj . Different from (Wata","@endWordPosition":"2730","@position":"15318","annotationId":"T7","@startWordPosition":"2727","@citStr":"Crammer et al 2006"}},"title":{"#tail":"\n","#text":"Online passiveaggressive algorithms."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Koby Crammer"},{"#tail":"\n","#text":"Ofer Dekel"},{"#tail":"\n","#text":"Joseph Keshet"},{"#tail":"\n","#text":"Shai ShalevShwartz"},{"#tail":"\n","#text":"Yoram Singer"}]}},{"date":{"#tail":"\n","#text":"2011"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"lation (SMT), in which translation is considered as the following optimization problem: e?(f ;W ) = arg max e P(e|f ;W ) = arg max e exp { W ? h(f, e) } ? e? exp { W ? h(f, e?) } = arg max e { W ? h(f, e) } , (1) where f and e (e?) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al2009; Galley and Quirk, 2011), margin (Watanabe et al2007; Chiang et al2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al2010), there are some shor","@endWordPosition":"408","@position":"2533","annotationId":"T8","@startWordPosition":"404","@citStr":"Galley and Quirk, 2011"},{"#tail":"\n","#text":"Table 8: The performance of Oracle of 2-best results which consist of 1-best resluts of MERT and 1-best resluts of EBUU. lated approximately. Table 8 presents the performance of the oracle translations selected from the 1-best translation results of MERT and EBUU. Clearly, there exists more potential improvement for local training method. 6 Related Work Several works have proposed discriminative techniques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al2007; Chiang et al2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions. All the methods mentioned above train a single weight for the whole development set, whereas our local training method learns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is that it can adapt the weights for each of the t","@endWordPosition":"4900","@position":"27662","annotationId":"T9","@startWordPosition":"4897","@citStr":"Galley and Quirk, 2011"}]},"title":{"#tail":"\n","#text":"Optimal search for minimum error rate training."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Michel Galley and Chris Quirk. 2011. Optimal search for minimum error rate training. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 38?49, Edinburgh, Scotland, UK., July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"38--49"},"marker":{"#tail":"\n","#text":"Galley, Quirk, 2011"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Edinburgh, Scotland, UK.,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michel Galley"},{"#tail":"\n","#text":"Chris Quirk"}]}},{"date":{"#tail":"\n","#text":"2010"},"title":{"#tail":"\n","#text":"Bridging smt and tm with translation recommendation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Yifan He, Yanjun Ma, Josef van Genabith, and Andy Way. 2010. Bridging smt and tm with translation recommendation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622?630, Uppsala, Sweden, July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"622--630"},"marker":{"#tail":"\n","#text":"He, Ma, van Genabith, Way, 2010"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Uppsala, Sweden,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yifan He"},{"#tail":"\n","#text":"Yanjun Ma"},{"#tail":"\n","#text":"Josef van Genabith"},{"#tail":"\n","#text":"Andy Way"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"S. Hildebrand, M. Eck, S. Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of EAMT. Association for Computational Linguistics."},"#text":"\n","marker":{"#tail":"\n","#text":"Hildebrand, Eck, Vogel, Waibel, 2005"},"title":{"#tail":"\n","#text":"Adaptation of the translation model for statistical machine translation based on information retrieval."},"booktitle":{"#tail":"\n","#text":"In Proceedings of EAMT. Association for Computational Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Hildebrand"},{"#tail":"\n","#text":"M Eck"},{"#tail":"\n","#text":"S Vogel"},{"#tail":"\n","#text":"Alex Waibel"}]}},{"date":{"#tail":"\n","#text":"2011"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"em: e?(f ;W ) = arg max e P(e|f ;W ) = arg max e exp { W ? h(f, e) } ? e? exp { W ? h(f, e?) } = arg max e { W ? h(f, e) } , (1) where f and e (e?) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al2009; Galley and Quirk, 2011), margin (Watanabe et al2007; Chiang et al2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al2010), there are some shortcomings in this pipeline. Firstly, on the document level, the performance of these","@endWordPosition":"421","@position":"2616","annotationId":"T10","@startWordPosition":"418","@citStr":"Hopkins and May, 2011"},{"#tail":"\n","#text":" today ? <1,2> 0.3 2 today several weeks . <3,2> 0.1 (a) (b) 2 21 2 222,0 ( , ) ( , )h f e h f e? ? ?? ? 2 22 2 212,0 ( , ) ( , )h f e h f e? ?? ?1 11 1 11, 0 ( , ) ( , )h f e h f e? ?? ? 1 12 1 111,0 ( , ) ( , )h f e h f e? ? ?? ? 2 22 2 21( , ) ( , )h f e h f e? 1 11 1 12( , ) ( , )h f e h f e? <-2,0> <-1,0> <1,0> <2,0> 0h1h . .* * 2 21 2 22( , ) ( , )h f e h f e? 1 12 1 11( , ) ( , )h f e h f e? Figure 1: (a). An Example candidate space of dimensionality two. score is a evaluation metric of e. (b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (Hopkins and May, 2011). Since score of e11 is greater than that of e12, ?1, 0? corresponds to a possitive example denoted as ???, and ??1, 0? corresponds to a negative example denoted as ?*?. Since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile. However, one can obtain e11 and e21 with weights: ?1, 1? and ??1, 1?, respectively. 19.04 when the Moses system is tuned on NIST02 by MERT. However, its performance is improved to 21.28 points when tuned on NIST06. The automatic selection of a development se","@endWordPosition":"784","@position":"4185","annotationId":"T11","@startWordPosition":"781","@citStr":"Hopkins and May, 2011"},{"#tail":"\n","#text":"y. Table 8 presents the performance of the oracle translations selected from the 1-best translation results of MERT and EBUU. Clearly, there exists more potential improvement for local training method. 6 Related Work Several works have proposed discriminative techniques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al2007; Chiang et al2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions. All the methods mentioned above train a single weight for the whole development set, whereas our local training method learns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is that it can adapt the weights for each of the test sentences. Our method resorts to some translation examples, which is similar as example-based translation or translation memory (Watan","@endWordPosition":"4924","@position":"27800","annotationId":"T12","@startWordPosition":"4921","@citStr":"Hopkins and May, 2011"}]},"title":{"#tail":"\n","#text":"Tuning as ranking."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352?1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"1352--1362"},"marker":{"#tail":"\n","#text":"Hopkins, May, 2011"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Edinburgh, Scotland, UK.,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mark Hopkins"},{"#tail":"\n","#text":"Jonathan May"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL. ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Koehn, Och, Marcu, 2003"},"publisher":{"#tail":"\n","#text":"ACL."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" it is smooth with respect to ?. Since the function (6) is non-convex, the solution obtained by gradient descent method may depend on the initial point. In this paper, we set the initial point as Wb in order to achieve a desirable solution. 5 Experiments and Results 5.1 Setting We conduct our experiments on the Chinese-toEnglish translation task. The training data is FBIS corpus consisting of about 240k sentence pairs. The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al2002) and we use mtevalv13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline system, and","@endWordPosition":"3339","@position":"18613","annotationId":"T13","@startWordPosition":"3336","@citStr":"Koehn et al 2003"}},"title":{"#tail":"\n","#text":"Statistical phrase-based translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of HLT-NAACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Franz Josef Och"},{"#tail":"\n","#text":"Daniel Marcu"}]}},{"date":{"#tail":"\n","#text":"2007"},"title":{"#tail":"\n","#text":"Moses: open source toolkit for statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"177--180"},"marker":{"#tail":"\n","#text":"Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07,"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Hieu Hoang"},{"#tail":"\n","#text":"Alexandra Birch"},{"#tail":"\n","#text":"Chris Callison-Burch"},{"#tail":"\n","#text":"Marcello Federico"},{"#tail":"\n","#text":"Nicola Bertoldi"},{"#tail":"\n","#text":"Brooke Cowan"},{"#tail":"\n","#text":"Wade Shen"},{"#tail":"\n","#text":"Christine Moran"},{"#tail":"\n","#text":"Richard Zens"},{"#tail":"\n","#text":"Chris Dyer"},{"#tail":"\n","#text":"Ondrej Bojar"},{"#tail":"\n","#text":"Alexandra Constantin"},{"#tail":"\n","#text":"Evan Herbst"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP. ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Koehn, 2004"},"publisher":{"#tail":"\n","#text":"ACL."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e NIST05, NIST06,and NIST08. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al2002) and we use mtevalv13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline system, and we denote it as In-Hiero. To obtain satisfactory baseline performance, we tune InHiero system for 5 times using MERT, and then se406 Methods Steps Seconds Global method Decoding 2.0 Local method Retrieval +0.6 Local training +0.3 Table 2: The efficiency of the local training and testing measured by sentence averaged runtime. Methods NIST05 NIST06 NIST08 Global MERT 27.07 26.32 19.03 Local MBUU 27.75+ 27.88+ 20.84+ EBUU 27.85+ 27.99+ 21.08+ Table 3: The performance comparison of local","@endWordPosition":"3417","@position":"19102","annotationId":"T14","@startWordPosition":"3416","@citStr":"Koehn, 2004"}},"title":{"#tail":"\n","#text":"Statistical significance tests for machine translation evaluation."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Philipp Koehn"}}},{"date":{"#tail":"\n","#text":"2010"},"title":{"#tail":"\n","#text":"Adaptive development data selection for log-linear model in statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming Zhou. 2010. Adaptive development data selection for log-linear model in statistical machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ?10, pages 662? 670, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"662--670"},"marker":{"#tail":"\n","#text":"Li, Zhao, Zhang, Zhou, 2010"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ?10,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mu Li"},{"#tail":"\n","#text":"Yinggong Zhao"},{"#tail":"\n","#text":"Dongdong Zhang"},{"#tail":"\n","#text":"Ming Zhou"}]}},{"date":{"#tail":"\n","#text":"2007"},"title":{"#tail":"\n","#text":"Improving statistical machine translation performance by training data selection and optimization."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 343?350, Prague, Czech Republic, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"343--350"},"marker":{"#tail":"\n","#text":"Lu, Huang, Liu, 2007"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Prague, Czech Republic,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yajuan Lu"},{"#tail":"\n","#text":"Jin Huang"},{"#tail":"\n","#text":"Qun Liu"}]}},{"date":{"#tail":"\n","#text":"2011"},"title":{"#tail":"\n","#text":"Consistent translation using discriminative learning - a translation memory-inspired approach."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Yanjun Ma, Yifan He, Andy Way, and Josef van Genabith. 2011. Consistent translation using discriminative learning - a translation memory-inspired approach. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1239?1248, Portland, Oregon, USA, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"1239--1248"},"marker":{"#tail":"\n","#text":"Ma, He, Way, van Genabith, 2011"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Portland, Oregon, USA,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yanjun Ma"},{"#tail":"\n","#text":"Yifan He"},{"#tail":"\n","#text":"Andy Way"},{"#tail":"\n","#text":"Josef van Genabith"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Christopher D. Manning and Hinrich Schu?tze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, MA, USA."},"#text":"\n","marker":{"#tail":"\n","#text":"Manning, Schutze, 1999"},"publisher":{"#tail":"\n","#text":"MIT Press,"},"location":{"#tail":"\n","#text":"Cambridge, MA, USA."},"title":{"#tail":"\n","#text":"Foundations of statistical natural language processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Christopher D Manning"},{"#tail":"\n","#text":"Hinrich Schutze"}]}},{"date":{"#tail":"\n","#text":"2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" 27.85 27.99 21.08 Oracle 29.46 29.35 22.09 Table 8: The performance of Oracle of 2-best results which consist of 1-best resluts of MERT and 1-best resluts of EBUU. lated approximately. Table 8 presents the performance of the oracle translations selected from the 1-best translation results of MERT and EBUU. Clearly, there exists more potential improvement for local training method. 6 Related Work Several works have proposed discriminative techniques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al2007; Chiang et al2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions. All the methods mentioned above train a single weight for the whole development set, whereas our local training method learns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is t","@endWordPosition":"4892","@position":"27616","annotationId":"T15","@startWordPosition":"4889","@citStr":"Moore and Quirk, 2008"}},"title":{"#tail":"\n","#text":"Random restarts in minimum error rate training for statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Robert C. Moore and Chris Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In Proceedings of the 22nd International Conference on Computational Linguistics -Volume 1, COLING ?08, pages 585?592, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"585--592"},"marker":{"#tail":"\n","#text":"Moore, Quirk, 2008"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 22nd International Conference on Computational Linguistics -Volume 1, COLING ?08,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Robert C Moore"},{"#tail":"\n","#text":"Chris Quirk"}]}},{"date":{"#tail":"\n","#text":"2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ly the gradient decent method to minimize the function (6), as it is smooth with respect to ?. Since the function (6) is non-convex, the solution obtained by gradient descent method may depend on the initial point. In this paper, we set the initial point as Wb in order to achieve a desirable solution. 5 Experiments and Results 5.1 Setting We conduct our experiments on the Chinese-toEnglish translation task. The training data is FBIS corpus consisting of about 240k sentence pairs. The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al2002) and we use mtevalv13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase","@endWordPosition":"3328","@position":"18552","annotationId":"T16","@startWordPosition":"3325","@citStr":"Och and Ney, 2000"}},"title":{"#tail":"\n","#text":"Improved statistical alignment models."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ?00, pages 440?447, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"440--447"},"marker":{"#tail":"\n","#text":"Och, Ney, 2000"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ?00,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Franz Josef Och"},{"#tail":"\n","#text":"Hermann Ney"}]}},{"date":{"#tail":"\n","#text":"2002"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"aining method to address these two problems. Unlike a global training method, such as MERT, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentencewise weight for each input sentence. We propose efficient incremental training methods to put the local training into practice. In NIST Chinese-to-English translation tasks, our local training method significantly outperforms MERT with the maximal improvements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method. 1 Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: e?(f ;W ) = arg max e P(e|f ;W ) = arg max e exp { W ? h(f, e) } ? e? exp { W ? h(f, e?) } = arg max e { W ? h(f, e) } , (1) where f and e (e?) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al2008),","@endWordPosition":"270","@position":"1848","annotationId":"T17","@startWordPosition":"267","@citStr":"Och and Ney (2002)"},{"#tail":"\n","#text":" varying retrieval size in Algorithm 2 based on EBUU. Methods NIST05 NIST06 NIST08 MERT 27.07 26.32 19.03 EBUU 27.85 27.99 21.08 Oracle 29.46 29.35 22.09 Table 8: The performance of Oracle of 2-best results which consist of 1-best resluts of MERT and 1-best resluts of EBUU. lated approximately. Table 8 presents the performance of the oracle translations selected from the 1-best translation results of MERT and EBUU. Clearly, there exists more potential improvement for local training method. 6 Related Work Several works have proposed discriminative techniques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al2007; Chiang et al2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions. All the methods mentioned above train a single weight for the whole development set, whereas our local training method learns a weight for each sentence. Further, our translation framework ","@endWordPosition":"4874","@position":"27502","annotationId":"T18","@startWordPosition":"4871","@citStr":"Och and Ney, 2002"}]},"title":{"#tail":"\n","#text":"Discriminative training and maximum entropy models for statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ?02, pages 295?302, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"295--302"},"marker":{"#tail":"\n","#text":"Och, Ney, 2002"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ?02,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Franz Josef Och"},{"#tail":"\n","#text":"Hermann Ney"}]}},{"date":{"#tail":"\n","#text":"2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"he log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: e?(f ;W ) = arg max e P(e|f ;W ) = arg max e exp { W ? h(f, e) } ? e? exp { W ? h(f, e?) } = arg max e { W ? h(f, e) } , (1) where f and e (e?) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al2009; Galley and Quirk, 2011), margin (Watanabe et al2007; Chiang et al2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven dist","@endWordPosition":"396","@position":"2470","annotationId":"T19","@startWordPosition":"395","@citStr":"Och, 2003"},{"#tail":"\n","#text":"ly optimize the error rate of translation candidates with respect to their references in Di. Formally, the objective function of error rate based ultraconservative update (EBUU) is as follows: 1 2 ?W ?Wb? 2 + ? K K? j=1 Error(rj ; e?(fj ;W )), (5) where e?(fj ;W ) is defined in Equation (1), and Error(rj , e) is the sentence-wise minus BLEU (Papineni et al2002) of a candidate e with respect to rj . Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here. Motivated by (Och, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: 1 2 ?W?Wb? 2+ ? K K? j=1 ? e Error(rj ; e)P?(e|fj ;W ), (6) Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with P?(e|fj ;W ) = exp[?W ? h(fj , e)] ? e??cj exp[?W ? h(fj , e ?)] , (7) where ? > 0 is a real number valued smoother. One can see that, in the extreme case, for ? ? ?, (6) converges to (5). We app","@endWordPosition":"3104","@position":"17334","annotationId":"T20","@startWordPosition":"3103","@citStr":"Och, 2003"},{"#tail":"\n","#text":" 19.03 EBUU 27.85 27.99 21.08 Oracle 29.46 29.35 22.09 Table 8: The performance of Oracle of 2-best results which consist of 1-best resluts of MERT and 1-best resluts of EBUU. lated approximately. Table 8 presents the performance of the oracle translations selected from the 1-best translation results of MERT and EBUU. Clearly, there exists more potential improvement for local training method. 6 Related Work Several works have proposed discriminative techniques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al2007; Chiang et al2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions. All the methods mentioned above train a single weight for the whole development set, whereas our local training method learns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One","@endWordPosition":"4888","@position":"27593","annotationId":"T21","@startWordPosition":"4887","@citStr":"Och, 2003"}]},"title":{"#tail":"\n","#text":"Minimum error rate training in statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160?167, Sapporo, Japan, July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"160--167"},"marker":{"#tail":"\n","#text":"Och, 2003"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Sapporo, Japan,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Franz Josef Och"}}},{"date":{"#tail":"\n","#text":"2002"},"title":{"#tail":"\n","#text":"Bleu: a method for automatic evaluation of machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311?318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"311--318"},"marker":{"#tail":"\n","#text":"Papineni, Roukos, Ward, Zhu, 2002"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Philadelphia, Pennsylvania, USA,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kishore Papineni"},{"#tail":"\n","#text":"Salim Roukos"},{"#tail":"\n","#text":"Todd Ward"},{"#tail":"\n","#text":"WeiJing Zhu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Adam Pauls, John Denero, and Dan Klein. 2009. Consensus training for consensus decoding in machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1418?1427, Singapore, August. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"1418--1427"},"marker":{"#tail":"\n","#text":"Pauls, Denero, Klein, 2009"},"publisher":{"#tail":"\n","#text":"Association"},"title":{"#tail":"\n","#text":"Consensus training for consensus decoding in machine translation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Adam Pauls"},{"#tail":"\n","#text":"John Denero"},{"#tail":"\n","#text":"Dan Klein"}]}},{"volume":{"#tail":"\n","#text":"16"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Alistair Shilton, Marimuthu Palaniswami, Daniel Ralph, and Ah Chung Tsoi. 2005. Incremental training of support vector machines. IEEE Transactions on Neural Networks, 16(1):114?131."},"journal":{"#tail":"\n","#text":"IEEE Transactions on Neural Networks,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Shilton, Palaniswami, Ralph, Tsoi, 2005"},"title":{"#tail":"\n","#text":"Incremental training of support vector machines."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alistair Shilton"},{"#tail":"\n","#text":"Marimuthu Palaniswami"},{"#tail":"\n","#text":"Daniel Ralph"},{"#tail":"\n","#text":"Ah Chung Tsoi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proc. of ICSLP."},"#text":"\n","marker":{"#tail":"\n","#text":"Stolcke, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ial point as Wb in order to achieve a desirable solution. 5 Experiments and Results 5.1 Setting We conduct our experiments on the Chinese-toEnglish translation task. The training data is FBIS corpus consisting of about 240k sentence pairs. The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al2002) and we use mtevalv13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline system, and we denote it as In-Hiero. To obtain satisfactory baseline performance, we tune InHiero system for 5 times using MERT, and then se406 Methods Steps Seconds Global method Decoding 2","@endWordPosition":"3371","@position":"18793","annotationId":"T22","@startWordPosition":"3370","@citStr":"Stolcke, 2002"}},"title":{"#tail":"\n","#text":"Srilm - an extensible language modeling toolkit."},"booktitle":{"#tail":"\n","#text":"In Proc. of ICSLP."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Andreas Stolcke"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Taro Watanabe and Eiichiro Sumita. 2003. Examplebased decoding for statistical machine translation. In Proc. of MT Summit IX, pages 410?417."},"#text":"\n","pages":{"#tail":"\n","#text":"410--417"},"marker":{"#tail":"\n","#text":"Watanabe, Sumita, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"nd the incremental training in line 5 of Algorithm 2. 3 Acquiring Training Examples In line 4 of Algorithm 2, to retrieve training examples for the sentence ti , we first need a metric to retrieve similar translation examples. We assume that the metric satisfy the property: more similar the test sentence and translation examples are, the better translation result one obtains when decoding the test sentence with the weight trained on the translation examples. The metric we consider here is derived from an example-based machine translation. To retrieve translation examples for a test sentence, (Watanabe and Sumita, 2003) defined a metric based on the combination of edit distance and TF-IDF (Manning and Schu?tze, 1999) as follows: dist(f1, f2) = ? ? edit-dist(f1, f2)+ (1? ?)? tf-idf(f1, f2), (2) where ?(0 ? ? ? 1) is an interpolation weight, fi(i = 1, 2) is a word sequence and can be also considered as a document. In this paper, we extract similar examples from training data. Like examplebased translation in which similar source sentences have similar translations, we assume that the optimal translation weights of the similar source sentences are closer. 4 Incremental Training Based on Ultraconservative Update","@endWordPosition":"2217","@position":"12408","annotationId":"T23","@startWordPosition":"2214","@citStr":"Watanabe and Sumita, 2003"},{"#tail":"\n","#text":"2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions. All the methods mentioned above train a single weight for the whole development set, whereas our local training method learns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is that it can adapt the weights for each of the test sentences. Our method resorts to some translation examples, which is similar as example-based translation or translation memory (Watanabe and Sumita, 2003; He et al2010; Ma et al2011). Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights. Similar to (Hildebrand et al2005; Lu? et al 2007), our method also employes IR methods to retrieve examples for a given test set. Their methods utilize the retrieved examples to acquire translation model and can be seen as the adaptation of translation model. However, ours uses the retrieved examples to tune the weights and thus can be considered as the adaptation of tuning. Furthermore, since ours does n","@endWordPosition":"5020","@position":"28420","annotationId":"T24","@startWordPosition":"5017","@citStr":"Watanabe and Sumita, 2003"}]},"title":{"#tail":"\n","#text":"Examplebased decoding for statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In Proc. of MT Summit IX,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Taro Watanabe"},{"#tail":"\n","#text":"Eiichiro Sumita"}]}},{"date":{"#tail":"\n","#text":"2007"},"title":{"#tail":"\n","#text":"Online large-margin training for statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764? 773, Prague, Czech Republic, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"764--773"},"marker":{"#tail":"\n","#text":"Watanabe, Suzuki, Tsukada, Isozaki, 2007"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Prague, Czech Republic,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Taro Watanabe"},{"#tail":"\n","#text":"Jun Suzuki"},{"#tail":"\n","#text":"Hajime Tsukada"},{"#tail":"\n","#text":"Hideki Isozaki"}]}},{"date":{"#tail":"\n","#text":"2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" and MERT can not find a single weight to satisfy all of the sentences. Figure 1(a) shows such an example, in which a development set contains two sentences f1 and f2 with translations e and feature vectors h. When we tune examples in Figure 1(a) by MERT, it can be regarded as a nonlinearly separable classification problem illustrated in Figure 1(b). Therefore, there exists no single weightW which simultaneously obtains e11 and e21 as translation for f1 and f2 via Equation (1). However, we can achieve this with two weights: ?1, 1? for f1 and ??1, 1? for f2. In this paper, inspired by KNN-SVM (Zhang et al., 2006), we propose a local training method, which trains sentence-wise weights instead of a single weight, to address the above two problems. Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing. This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences. Similar to the method of development set automatical selection, the local tra","@endWordPosition":"1081","@position":"5950","annotationId":"T25","@startWordPosition":"1078","@citStr":"Zhang et al., 2006"},{"#tail":"\n","#text":"cantly gains over MERT, with the maximum improvements up to 2.0 BLEU, and its efficiency is comparable to that of the global training method. 2 Local Training and Testing The local training method (Bottou and Vapnik, 1992) is widely employed in computer vision (Zhang et al2006; Cheng et al2010). Compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example. It is superior to 403 the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992; Zhang et al 2006). Algorithm 1 Naive Local Training Method Input: T = {ti}Ni=1(test set), K (retrieval size), Dev(development set), D(retrieval data) Output: Translation results of T 1: for all sentence ti such that 1 ? i ? N do 2: Retrieve the training examples Di with size K for ti from D according to a similarity; 3: Train a local weight W i based on Dev and Di; 4: Decode ti with W i; 5: end for Suppose T be a test set, Dev a development set, and D a retrieval data. The local training in SMT is described in the Algorithm 1. For each sentence ti in test set, training examples Di is retrieved from D using a s","@endWordPosition":"1357","@position":"7661","annotationId":"T26","@startWordPosition":"1354","@citStr":"Zhang et al 2006"}]},"title":{"#tail":"\n","#text":"Svm-knn: Discriminative nearest neighbor classification for visual category recognition."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Hao Zhang, Alexander C. Berg, Michael Maire, and Jitendra Malik. 2006. Svm-knn: Discriminative nearest neighbor classification for visual category recognition. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2, CVPR ?06, pages 2126?2136, Washington, DC, USA. IEEE Computer Society."},"#text":"\n","pages":{"#tail":"\n","#text":"2126--2136"},"marker":{"#tail":"\n","#text":"Zhang, Berg, Maire, Malik, 2006"},"publisher":{"#tail":"\n","#text":"IEEE Computer Society."},"location":{"#tail":"\n","#text":"Washington, DC, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2, CVPR ?06,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Hao Zhang"},{"#tail":"\n","#text":"Alexander C Berg"},{"#tail":"\n","#text":"Michael Maire"},{"#tail":"\n","#text":"Jitendra Malik"}]}},{"date":{"#tail":"\n","#text":"2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ar model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: e?(f ;W ) = arg max e P(e|f ;W ) = arg max e exp { W ? h(f, e) } ? e? exp { W ? h(f, e?) } = arg max e { W ? h(f, e) } , (1) where f and e (e?) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al2009; Galley and Quirk, 2011), margin (Watanabe et al2007; Chiang et al2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source se","@endWordPosition":"400","@position":"2491","annotationId":"T27","@startWordPosition":"397","@citStr":"Zhao and Chen, 2009"},{"#tail":"\n","#text":"le 29.46 29.35 22.09 Table 8: The performance of Oracle of 2-best results which consist of 1-best resluts of MERT and 1-best resluts of EBUU. lated approximately. Table 8 presents the performance of the oracle translations selected from the 1-best translation results of MERT and EBUU. Clearly, there exists more potential improvement for local training method. 6 Related Work Several works have proposed discriminative techniques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al2007; Chiang et al2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions. All the methods mentioned above train a single weight for the whole development set, whereas our local training method learns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is that it can adapt the ","@endWordPosition":"4896","@position":"27637","annotationId":"T28","@startWordPosition":"4893","@citStr":"Zhao and Chen, 2009"}]},"title":{"#tail":"\n","#text":"A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Bing Zhao and Shengyuan Chen. 2009. A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short ?09, pages 21?24, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"21--24"},"marker":{"#tail":"\n","#text":"Zhao, Chen, 2009"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short ?09,"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Bing Zhao"},{"#tail":"\n","#text":"Shengyuan Chen"}]}}]}}]}}
