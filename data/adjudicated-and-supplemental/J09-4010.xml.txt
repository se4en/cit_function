stralia. E-mail: Ingrid.Zukerman@infotech.monash.edu.au. 1 http ://customercare.telephonyonline.com/ar/telecom next generation customer. Submission received: 7 November 2007; revised submission received: 20 March 2009; accepted for publication: 3 June 2009. ? 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven (Barr and Tessler 1995; Watson 1997; Delic and Lahaix 1998). These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998). In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk cus
.Zukerman@infotech.monash.edu.au. 1 http ://customercare.telephonyonline.com/ar/telecom next generation customer. Submission received: 7 November 2007; revised submission received: 20 March 2009; accepted for publication: 3 June 2009. ? 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven (Barr and Tessler 1995; Watson 1997; Delic and Lahaix 1998). These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998). In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may ha
otech.monash.edu.au. 1 http ://customercare.telephonyonline.com/ar/telecom next generation customer. Submission received: 7 November 2007; revised submission received: 20 March 2009; accepted for publication: 3 June 2009. ? 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven (Barr and Tessler 1995; Watson 1997; Delic and Lahaix 1998). These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998). In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tole
er systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this, to date, there has been little work on corpus-based approaches to help-desk response automation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). A major factor limiting this work is the dearth of corpora?help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of request? response e-mail dialogues between customers and operators at Hewlett-Packard. Observations from this corpu
nes, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this, to date, there has been little work on corpus-based approaches to help-desk response automation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). A major factor limiting this work is the dearth of corpora?help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of request? response e-mail dialogues between customers and operators at Hewlett-Packard. Observations from this corpus have led us to consider 
. This suggests that different response-generation strategies may be suitable, depending on the content of the initiating request and how well it matches previous requests or responses. In our work, we focus on the first two of these situations, where either complete existing responses or parts of responses are reused to address a new request. The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response. This situation suggests a response-automation approach that follows the document retrieval paradigm (Salton and McGill 1983), where a new request is matched with existing response documents (e-mails). However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively. Sometimes requests match each other quite well, suggesting an approach where a new request is matched with an old one, and the corresponding response is reused. However, analysis of our corpus shows that this does not occur very often, because unlike response e-mails, request e-mails exhibit a high language variability: There a
operator is likely to be both coherent and complete. Therefore, if a particular request can be addressed with a single existing response document, then a document reuse approach would be preferred. An important capability of a response-generation system is to be able to determine when such an approach is appropriate, and when there is insufficient evidence to reuse a complete response document. As stated herein, we studied two document-based methods: Document Retrieval andDocument Prediction. 3.1.1 Document Retrieval (Doc-Ret). This method follows a traditional Information Retrieval paradigm (Salton and McGill 1983), where a query is represented by the content terms it contains, and the system retrieves from the corpus a set of documents that best match this query. In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus: (1) previous response e-mails, (2) previous request e-mails, or (3) previous request? response pairs. The first alternative corresponds to the more traditional view of retrieval as applied in question-answering tasks, where the terms in the question are matched to those in the answer documents. We con
imilar responses in the corpus, an appropriate response can still be retrieved). The results of this experiment are shown in Table 1. The first column shows which document retrieval variant is being evaluated. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request?response pairs yields even more retrieved documents. For the cases where retrieval took place, we used F-score (van Rijsbergen 1979; Salton and McGill 1983) to determine the similarity between the response from the top-ranked document and the real response (the formulas for F-score and its contributing factors, recall and precision, appear in Section 4.2). The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request?response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took p
iability. Hence, we keep their representation at a low level of abstraction (bag-of-lemmas). The idea behind the Doc-Pred method is similar to Bickel and Scheffer?s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request?s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case, the clustering is performed by the program Snob, which implements mixture modeling combined with model selection based on the Minimum Message Length (MML) criterion (Wallace and Boulton 1968; Wallace 2005). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the
heir representation at a low level of abstraction (bag-of-lemmas). The idea behind the Doc-Pred method is similar to Bickel and Scheffer?s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request?s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case, the clustering is performed by the program Snob, which implements mixture modeling combined with model selection based on the Minimum Message Length (MML) criterion (Wallace and Boulton 1968; Wallace 2005). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.
ength (MML) criterion (Wallace and Boulton 1968; Wallace 2005). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle. The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request. As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirical
to strongly match a previous request or response e-mail (Doc-Ret), or requests contain terms that are predictive of complete template response e-mails (Doc-Pred). 4 We used a binary representation, rather than a representation based on TF.IDF scores, because important domain-related words, such as monitor and network, are actually quite frequent. Thus, their low TF.IDF score may have an adverse influence on clustering performance. Nonetheless, in the future, it may be worth investigating a TF.IDF-based representation. 5 Significant bigrams are obtained using the n-gram statistics package NSP (Banerjee and Pedersen 2003), which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram (that it is not a collocation). 603 Computational Linguistics Volume 35, Number 4 As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-
 approach, because requests only predict or match portions of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization (Goldstein et al 2000; Barzilay, Elhadad, andMcKeown 2001; Barzilay and McKeown 2005). The appeal of a sentence-level approach is that it supports the generation of a ?combination response? in situations where there is insufficient evidence for a single document containing a full response, but there is enough evidence for parts of responses. Although such a combined response is generally less satisfactory than a full response, the information included in it may address a user?s problem or point the user in the right direction. As argued in the Introduction, when it comes to obtaining information quickly on-line, t
 of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization (Goldstein et al 2000; Barzilay, Elhadad, andMcKeown 2001; Barzilay and McKeown 2005). The appeal of a sentence-level approach is that it supports the generation of a ?combination response? in situations where there is insufficient evidence for a single document containing a full response, but there is enough evidence for parts of responses. Although such a combined response is generally less satisfactory than a full response, the information included in it may address a user?s problem or point the user in the right direction. As argued in the Introduction, when it comes to obtaining information quickly on-line, this option may be preferable to having to wait for a human-gener
elect more than one sentence (see the subsequent discussion on removing redundant sentences). We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users? requests.7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package (Chang and Lin 2001). 605 Computational Linguistics Volume 35, Number 4 prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps. 1. Calculate the scores of the sentences in the predicted SCs. 2. Remove redundant sentences from cohesive SCs; these are SCs which contain similar sentences. 3. Calculate the confidence of the generated response. Calculating the score of a sentence. The score of each sentence sj is calcul
 a Radial Basis Function kernel to predict SCs from users? requests.7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package (Chang and Lin 2001). 605 Computational Linguistics Volume 35, Number 4 prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps. 1. Calculate the scores of the sentences in the predicted SCs. 2. Remove redundant sentences from cohesive SCs; these are SCs which contain similar sentences. 3. Calculate the confidence of the generated response. Calculating the score of a sentence. The score of each sentence sj is calculated using the following formula. Score(sj) = m ? i=1 Pr(SCi)? Pr(sj|SCi) (2) where m is the number of SCs, Pr(sj|SCi) is the proba
nd is independent of particular requests. Thus, the SVM for SC1 has a higher reliability than that for SC3, because it is easier for an SVM to learn when SC1 is appropriate (predominantly from the presence of the words faulty and repair). In order to ensure the relevance of the generated replies, we have placed tight restrictions on prediction probability and cluster cohesion (Table 3), which cause the Sent-Pred method to often return partial responses. Removing redundant sentences. After calculating the raw score of each sentence, we use a modified version of the Adaptive Greedy Algorithm by Filatova and Hatzivassiloglou (2004) to penalize redundant sentences in cohesive clusters. This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence?i.e., its score is not decremented). Specifically, given a sentence sk in cluster SCl which contains a sentence with a higher or equal score, the contribution of SCl to Score(sk) (= Pr(SCl)? Pr(sk|SCl)) is subtracted from Score(sk). After applying these penalties, we retain only the sentences whose adjusted score is gr
 exceeds an applicability threshold. As for Sent-Pred, confidence is calculated using Equation (6). Both applicability thresholds (confidence and number of retrieved sentences) are set to 0 (Table 3). 3.3 Summary The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically, we used Decision Graphs (Oliver 1993) for Doc-Pred, and SVMs (Vapnik 1998) for Sent-Pred.11 Additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters (Sections 3.1.2 and 3.2.2). Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2. As indicated at the beginning of this section, the implementation of these methods requires the selection of different thresholds, which are subjective and application dependent. Table 3 sum
As for Sent-Pred, confidence is calculated using Equation (6). Both applicability thresholds (confidence and number of retrieved sentences) are set to 0 (Table 3). 3.3 Summary The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically, we used Decision Graphs (Oliver 1993) for Doc-Pred, and SVMs (Vapnik 1998) for Sent-Pred.11 Additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters (Sections 3.1.2 and 3.2.2). Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2. As indicated at the beginning of this section, the implementation of these methods requires the selection of different thresholds, which are subjective and application dependent. Table 3 summarizes the thresholds required for t
 determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods. Our evaluation is performed by measuring the quality of the generated responses. Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators). In Section 5, we discuss the difficulties associated with such user studies, and describe a human-based evaluation we conducted for a small subset of the responses generated by our system (Marom and Zukerman 2007b). However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones. We employ 10-fold cross-validation, where we split each data set in the corpus into 10 test sets, each comprising 10% of the e-mail dialogues; the remaining 90% of the dialogues constitute the training set. For each of the cross-validation folds, the responses generated for the requests in the test split are compared against the actual responses 
ing set. For each of the cross-validation folds, the responses generated for the requests in the test split are compared against the actual responses generated by help-desk operators for these requests. Although thismethod of assessment is less informative than human-based evaluations, it enables us to evaluate the performance of our systemwith substantial amounts of data, and produce representative results for a large corpus such as ours. We use two measures from Information Retrieval to determine the quality of an automatically generated response: precision and F-score (van Rijsbergen 1979; Salton and McGill 1983). Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response. F-score is the harmonic mean of precision and recall, which measures how much of the information in the model response appears in the generated response. We consider precision separately because it does not penalize missing 612 Marom and Zukerman Empirical Study of Response Automation Methods information, enabling us to better assess our sentence-
ge exceeds some minimum (e.g., 10%), and chose the method(s) which could adequately answer the largest number of queries in the data set (based on coverage combined with F-score and precision). Table 5 presents the coverage and unique/best coverage of each method (the percentage of queries covered only by this method or for which this method produces a better reply than other methods), and the average and standard deviation of the precision and F-score obtained by each method (calculated over the requests that are covered). 13 We also employed sequence-based measures using the ROUGE tool set (Lin and Hovy 2003), with similar results to those obtained with the word-by-word measures. 613 Computational Linguistics Volume 35, Number 4 Figure 4 Performance of the different methods for each data set: (a) coverage, (b) F-score, and (c) precision. 614 Marom and Zukerman Empirical Study of Response Automation Methods Table 5 Coverage, uniqueness, precision, and F-score for the response-generation methods. Method Coverage Unique Avg. (St dev.) or best Precision F-score Doc-Ret 43% 22% 0.37 (0.34) 0.35 (0.33) Doc-Pred 29% 3% 0.82 (0.21) 0.82 (0.24) Sent-Ret 9% 0% 0.19 (0.19) 0.12 (0.11) Sent-Pred 34% 5% 0.94 (
t match precisely the model response. However, it is often the case that there is not one single appropriate response to a query, and even a help-desk operator may respond to the same question in different ways on different occasions.  The relationship between the results obtained by the automatic evaluation of the responses generated by our system and people?s assessments of these responses is unclear, in particular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itsel
btained by the automatic evaluation of the responses generated by our system and people?s assessments of these responses is unclear, in particular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaning
valuation of the responses generated by our system and people?s assessments of these responses is unclear, in particular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system.
ticular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in o
onses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user stud
esemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request?response pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read re
hat they provide answers to queries. These systems addressed the evaluation issue as follows.  Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006).  Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al 2000).  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (Feng et al 2006; Leuski et al 2006). The representativeness of the sample size was not discussed in any of these studies. There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request?response pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read relatively long reques
 that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request?response pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read relatively long request? response e-mails, which quickly becomes tedious. In order to address these limitations in a practical way, we conducted a small user study where we asked four judges (graduate students from the Faculty of Information Technology at Monash University) to assess the responses generated by our system (Marom and Zukerman 2007a). Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator. Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precisionmeasures obtained in the automatic evaluation.  Informativeness: Is there anything useful in the response that would make it a good automatic response, given that otherwise the customer has 619 Computational Linguistics
s being compared. Each judgewas given 20 of these cases, and was asked to assess the generated responses on the four criteria listed previously.14 Wemaximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges? assessments would be comparable. Because the judges do not evaluate the same cases, we could not employ standard inter-annotator agreement measures (Carletta 1996). However, it is still necessary to 14 We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur. 620 Marom and Zukerman Empirical Study of Response Automation Methods have some measure of agreement, and control for bias from specific judges or specific cases. This was done by performing pairwise significance testing, treating the data from two judges as independent samples (we used the Wilcoxon Rank-Sum Test for equal medians). We conducted this significance test separately for each method and each of t
ining whether a particular method is favored for specific data sets. 6. Meta-Learning In Section 4, we employed empirically determined applicability thresholds to circumscribe the coverage of the different response-generation methods. However, as shown by our results, these thresholds were sometimes sub-optimal. In this section, we describe a meta-level process which can automatically select a response-generation method to address a new request without using such thresholds. A common way to combine different models consists of selecting the model that is most confident regarding its decision (Burke 2002). However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance. In other words, our meta-level process learns to predict the performance of the different metho
ability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance. In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience. These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007). Following Lekakos and Giaglis (2007), one approach for achieving this objective consists of applying supervised learning, where a winning method is selected for each case in the training set, all the training cases are labeled accordingly, and then the system is trained to predict a winner for unseen cases. However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision). Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which
d only after the learning is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process. 6.1 Training We train the system by clustering the ?experiences? of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively). We then use the program Snob (Wallace and Boulton 1968; Wallace 2005) to cluster these experiences. Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)?these dimensions account for 95% of the variation in the data. The bottom part of Figure 8(b) 623 Computational Linguistics Volume 35, Number 4 Figure 8 Clusters of response-generation methods obtained from the training set: (a) dimensions produced by PCA and (b) sample clusters. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be di
is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process. 6.1 Training We train the system by clustering the ?experiences? of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively). We then use the program Snob (Wallace and Boulton 1968; Wallace 2005) to cluster these experiences. Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)?these dimensions account for 95% of the variation in the data. The bottom part of Figure 8(b) 623 Computational Linguistics Volume 35, Number 4 Figure 8 Clusters of response-generation methods obtained from the training set: (a) dimensions produced by PCA and (b) sample clusters. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequ
 for a given request, we need to combine our estimates of precision and recall into an overall estimate of performance, and then choose the method with the best estimated performance. The standard approach for combining precision and recall is to compute their harmonic mean, F-score, as we have done in our 16 In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases. We posit that this would not have a significant effect on the results, in particular for MML-based classification techniques, such as Decision Graphs (Oliver 1993). 625 Computational Linguistics Volume 35, Number 4 comparative evaluation in Section 4. However, in order to accommodate different levels of preference towards precision or recall, as discussed herein, we use the following weighted F-score calculation (van Rijsbergen 1979). F-score = { w Precision + 1? w Recall }?1 (12) where w is a weight between 0 and 1 given to precision. When w = 0.5 we have the standard usage of F-score (Equation (9)), and for w > 0.5, we have a preference for high precision. For example, for w = 0.5, the precision and recall values of Cluster 16 (Figure 8(b)) translate 
se, it is able to opt for a non-response rather than risk producing a bad one. The decision of what is a bad response should be made by the organization using the system. With the stringent criterion we have chosen (precision? 0.8), the system yields a good performance for approximately 57% of the requests. Figure 10 Example showing an appropriate response generated by the Sent-Hybrid method. 629 Computational Linguistics Volume 35, Number 4 8. Related Research The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, 
han risk producing a bad one. The decision of what is a bad response should be made by the organization using the system. With the stringent criterion we have chosen (precision? 0.8), the system yields a good performance for approximately 57% of the requests. Figure 10 Example showing an appropriate response generated by the Sent-Hybrid method. 629 Computational Linguistics Volume 35, Number 4 8. Related Research The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). eResponder, the syst
ng the system. With the stringent criterion we have chosen (precision? 0.8), the system yields a good performance for approximately 57% of the requests. Figure 10 Example showing an appropriate response generated by the Sent-Hybrid method. 629 Computational Linguistics Volume 35, Number 4 8. Related Research The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request?response pairs and presents a ranke
edge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request?response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. Bickel and Scheffer (2004) compared the performance of 
such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request?response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. Bickel and Scheffer (2004) compared the performance of document retrieval and doc
er and shorter than those in our corpus, and the replies shorter and more homogeneous. Malik, Subramaniam, and Kaushik (2007) developed a system that builds question? answer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their systemmatches response sentences to pre-existing templates and returns the templates. Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering. Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individu
ere data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question?answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to c
 case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question?answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient eviden
 individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question?answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses. In e-mail-thread summariza
pect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question?answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses. In e-mail-thread summarization, Dalli, Xia, and Wilks (2004) applied a procedu
n used high-level features for machine learning, as well as wordbased features. As indicated in Section 3.2.2, our Sent-Pred experiments with high-level features (specifically syntactic features) did not improve our results. Finally, Shrestha and McKeown used paragraphs as a unit of information?an approach we tried late in our project with encouraging results. This suggests that there are situations where one can generalize a response that is longer than a sentence but shorter than a whole document. Unfortunately, we could not pursue this avenue of research owing to time limitations. In FAQs, Berger and Mittal (2000) employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. Berger et al (2000) compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. Soricut and Brill (2006) com
aging results. This suggests that there are situations where one can generalize a response that is longer than a sentence but shorter than a whole document. Unfortunately, we could not pursue this avenue of research owing to time limitations. In FAQs, Berger and Mittal (2000) employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. Berger et al (2000) compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval. Two significant differences between help-desk and FAQs are the following.  The responses in the help-desk corpus are personalized, which means that on one
Qs, Berger and Mittal (2000) employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. Berger et al (2000) compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval. Two significant differences between help-desk and FAQs are the following.  The responses in the help-desk corpus are personalized, which means that on one hand, we must abstract from them sufficiently to obtain meaningful regularities, and on the other hand, we must be careful not to abstract away specific information that addresses particular issues.  Help-desk responses have much more repetition than 
 This motivates the use of multi-document summarization techniques, rather than question-answering approaches, to extract individual answers. These issues also differentiate the help-desk application from other types of questionanswering applications, specifically those found in the field of restricted domain question answering (Molla? and Vicedo 2007). 631 Computational Linguistics Volume 35, Number 4 In addition to the different response-generationmethods, we have proposed ametalevel strategy to combine them. This kind of meta-learning is referred to as stacking by the DataMining community (Witten and Frank 2000). Lekakos andGiaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems, merging and ensemble, each subdivided into themore specific subclasses suggested by Burke (2002) as follows. Themerging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke?s feature combination, cascade, feature augmentation, and meta-level sub-categories). The ensemble category co
icedo 2007). 631 Computational Linguistics Volume 35, Number 4 In addition to the different response-generationmethods, we have proposed ametalevel strategy to combine them. This kind of meta-learning is referred to as stacking by the DataMining community (Witten and Frank 2000). Lekakos andGiaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems, merging and ensemble, each subdivided into themore specific subclasses suggested by Burke (2002) as follows. Themerging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke?s feature combination, cascade, feature augmentation, and meta-level sub-categories). The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke?s weighted, switching, and mixed sub-categories). Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome. More specifi
 (this category encompasses Burke?s weighted, switching, and mixed sub-categories). Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome. More specifically, it belongs to Burke?s switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman?s (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by Chu-Carroll et al (2003) belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke?s cascade sub-category). Because the results of all the methods are comparable, no learning is required: At each stage of the ?cascade of methods,? the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method. 9. Conclusion Despite its theoretical importance and commer
