{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.843006416666667","#text":"\n? A deterministic algorithm for building labeled\nprojective dependency graphs (Nivre, 2006).\n? History-based feature models for predicting the\nnext parser action (Black et al, 1992).\n? Support vector machines for mapping histories\nto parser actions (Kudo and Matsumoto, 2002).\n? Graph transformations for recovering non-\nprojective structures (Nivre and Nilsson, 2005).\nAll experiments have been performed using Malt-\nParser (Nivre et al, 2006), version 0.4, which is\nmade available together with the suite of programs\nused for pre- and post-processing.1\n"},{"#tail":"\n","@confidence":"0.918059333333333","#text":"\non top of the stack and next is the next token):\n? SHIFT: Push next onto the stack.\n? REDUCE: Pop the stack.\n? RIGHT-ARC(r): Add an arc labeled r from top\nto next; push next onto the stack.\n? LEFT-ARC(r): Add an arc labeled r from next\n"}],"address":[{"#tail":"\n","@confidence":"0.933337","#text":"\n35195 Va?xjo?, Sweden\n"},{"#tail":"\n","@confidence":"0.970484","#text":"\n34469 Istanbul, Turkey\n"},{"#tail":"\n","@confidence":"0.963623","#text":"\nBox 408\n54128 Sko?vde, Sweden\n"}],"author":[{"#tail":"\n","@confidence":"0.525127","#text":"\nJohan Hall\nJens Nilsson\n"},{"#tail":"\n","@confidence":"0.745642","#text":"\nGu?ls?en Eryig?it\n"},{"#tail":"\n","@confidence":"0.815665","#text":"\nSvetoslav Marinov\n"}],"equation":{"#tail":"\n","@confidence":"0.970068","#text":"\nFO L C P FE D\nS: top + + + + + +\nS: top?1 +\nI: next + + + + +\nI: next+1 + +\nI: next+2 +\nI: next+3 +\n"},"subsectionHeader":[{"#tail":"\n","@confidence":"0.963737","#text":"\n2.1 Parsing Algorithm\n"},{"#tail":"\n","@confidence":"0.99357","#text":"\n2.2 History-Based Feature Models\n"},{"#tail":"\n","@confidence":"0.99949","#text":"\n2.3 Support Vector Machines\n"},{"#tail":"\n","@confidence":"0.911488","#text":"\n2.4 Pseudo-Projective Parsing\n"},{"#tail":"\n","@confidence":"0.998656","#text":"\n4.1 Swedish\n"},{"#tail":"\n","@confidence":"0.982528","#text":"\n4.2 Turkish\n"}],"footnote":[{"#tail":"\n","@confidence":"0.930425","#text":"\n1www.msi.vxu.se/users/nivre/research/MaltParser.html\n"},{"#tail":"\n","@confidence":"0.9961395","#text":"\n2The fields PHEAD and PDEPREL have not been used at all,\nsince we rely on pseudo-projective parsing for the treatment of\nnon-projective structures.\n3We also ran preliminary experiments with memory-based\n"}],"title":{"#tail":"\n","@confidence":"0.471788333333333","#text":"\nLabeled Pseudo-Projective Dependency Parsing\nwith Support Vector Machines\nJoakim Nivre\n"},"@confidence":"0.000001","reference":[{"#tail":"\n","@confidence":"0.37728","#text":"\n(Arabic, Czech, Portuguese, Slovene).\n"},{"#tail":"\n","@confidence":"0.998615506329114","#text":"\nA. Abeille?, editor. 2003. Treebanks: Building and Using\nParsed Corpora, volume 20 of Text, Speech and Language\nTechnology. Kluwer Academic Publishers, Dordrecht.\nS. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Floresta\nsinta?(c)tica?: a treebank for Portuguese. In Proc. of LREC-\n2002, pages 1698?1703.\nN. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation\nprocess in the Turkish treebank. In Proc. of LINC-2003.\nE. Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mer-\ncer, and S. Roukos. 1992. Towards history-based grammars:\nUsing richer models for probabilistic parsing. In Proc. of the\n5th DARPA Speech and Natural Language Workshop, pages\n31?37.\nA. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The\nPDT: a 3-level annotation scenario. In Abeille? (Abeille?,\n2003), chapter 7.\nS. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002.\nThe TIGER treebank. In Proc. of TLT-2002.\nC.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library\nfor Support Vector Machines. Software available at\nhttp://www.csie.ntu.edu.tw/ cjlin/libsvm.\nK. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and\nZ. Gao. 2003. Sinica treebank: Design criteria, representa-\ntional issues and implementation. In Abeille? (Abeille?, 2003),\nchapter 13, pages 231?248.\nM. Civit Torruella and Ma A. Mart?? Anton??n. 2002. Design\nprinciples for a Spanish treebank. In Proc. of TLT-2002.\nM. Collins. 1999. Head-Driven Statistical Models for Natural\nLanguage Parsing. Ph.D. thesis, University of Pennsylvania.\nS. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ?Zabokrtsky, and\nA. ?Zele. 2006. Towards a Slovene dependency treebank. In\nProc. of LREC-2006.\nG. Eryig?it and K. Oflazer. 2006. Statistical dependency parsing\nof Turkish. In Proc. of EACL-2006.\nJ. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka. 2004.\nPrague Arabic dependency treebank: Development in data\nand tools. In Proc. of NEMLAR-2004, pages 110?117.\nY. Kawata and J. Bartels. 2000. Stylebook for the Japanese\ntreebank in VERBMOBIL. Verbmobil-Report 240, Seminar\nfu?r Sprachwissenschaft, Universita?t Tu?bingen.\nM. T. Kromann. 2003. The Danish dependency treebank and\nthe underlying linguistic theory. In Proc. of TLT-2003.\nT. Kudo and Y. Matsumoto. 2002. Japanese dependency anal-\nysis using cascaded chunking. In Proc. of CoNLL-2002,\npages 63?69.\nJ. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER:\nReconstructing a Swedish treebank from antiquity. In Proc.\nof the NODALIDA Special Session on Treebanks.\nJ. Nivre and J. Nilsson. 2005. Pseudo-projective dependency\nparsing. In Proc. of ACL-2005, pages 99?106.\nJ. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-\ndency parsing. In Proc. CoNLL-2004, pages 49?56.\nJ. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A data-\ndriven parser-generator for dependency parsing. In Proc. of\nLREC-2006.\nJ. Nivre. 2003. An efficient algorithm for projective depen-\ndency parsing. In Proc. of IWPT-2003, pages 149?160.\nJ. Nivre. 2006. Inductive Dependency Parsing. Springer.\nK. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.\nBuilding a Turkish treebank. In Abeille? (Abeille?, 2003),\nchapter 15.\nK. Simov and P. Osenova. 2003. Practical annotation scheme\nfor an HPSG treebank of Bulgarian. In Proc. of LINC-2003,\npages 17?24.\nK. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2005.\nDesign and implementation of the Bulgarian HPSG-based\ntreebank. In Journal of Research on Language and Com-\nputation ? Special Issue, pages 495?522. Kluwer Academic\nPublishers.\nO. Smrz?, J. ?Snaidauf, and P. Zema?nek. 2002. Prague depen-\ndency treebank for Arabic: Multi-level annotation of Arabic\ncorpus. In Proc. of the Intern. Symposium on Processing of\nArabic, pages 147?155.\nL. van der Beek, G. Bouma, R. Malouf, and G. van Noord.\n2002. The Alpino dependency treebank. In Computational\nLinguistics in the Netherlands (CLIN).\nH. Yamada and Y. Matsumoto. 2003. Statistical dependency\nanalysis with support vector machines. In Proc. of IWPT-\n2003, pages 195?206.\n"}],"#tail":"\n","bodyText":[{"#tail":"\n","@confidence":"0.63085","#text":"\nProceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),\npages 221?225, New York City, June 2006. c?2006 Association for Computational Linguistics\n"},{"#tail":"\n","@confidence":"0.9990169","#text":"\nWe use SVM classifiers to predict the next\naction of a deterministic parser that builds\nlabeled projective dependency graphs in\nan incremental fashion. Non-projective\ndependencies are captured indirectly by\nprojectivizing the training data for the\nclassifiers and applying an inverse trans-\nformation to the output of the parser. We\npresent evaluation results and an error\nanalysis focusing on Swedish and Turkish.\n"},{"#tail":"\n","@confidence":"0.989195","#text":"\nThe CoNLL-X shared task consists in parsing texts\nin multiple languages using a single dependency\nparser that has the capacity to learn from treebank\ndata. Our methodology for performing this task is\nbased on four essential components:\n"},{"#tail":"\n","@confidence":"0.993793125","#text":"\nThe parsing algorithm used for all languages is the\ndeterministic algorithm first proposed for unlabeled\ndependency parsing by Nivre (2003) and extended\nto labeled dependency parsing by Nivre et al (2004).\nThe algorithm builds a labeled dependency graph in\none left-to-right pass over the input, using a stack\nto store partially processed tokens and adding arcs\nusing four elementary actions (where top is the token\n"},{"#tail":"\n","@confidence":"0.9776184375","#text":"\nto top; pop the stack.\nAlthough the parser only derives projective graphs,\nthe fact that graphs are labeled allows non-projective\ndependencies to be captured using the pseudo-\nprojective approach of Nivre and Nilsson (2005) .\nAnother limitation of the parsing algorithm is that\nit does not assign dependency labels to roots, i.e., to\ntokens having HEAD=0. To overcome this problem,\nwe have implemented a variant of the algorithm that\nstarts by pushing an artificial root token with ID=0\nonto the stack. Tokens having HEAD=0 can now\nbe attached to the artificial root in a RIGHT-ARC(r)\naction, which means that they can be assigned any\nlabel. Since this variant of the algorithm increases\nthe overall nondeterminism, it has only been used\nfor the data sets that include informative root labels\n"},{"#tail":"\n","@confidence":"0.998226","#text":"\nG: head of top +\nG: leftmost dep of top +\nG: rightmost dep of top +\nG: leftmost dep of next +\n"},{"#tail":"\n","@confidence":"0.5161605","#text":"\nFO: FORM, L: LEMMA , C: CPOS, P: POS,\nFE: FEATS, D: DEPREL\n"},{"#tail":"\n","@confidence":"0.998780095238095","#text":"\nHistory-based parsing models rely on features of the\nderivation history to predict the next parser action.\nThe features used in our system are all symbolic\nand extracted from the following fields of the data\nrepresentation: FORM, LEMMA, CPOSTAG, POSTAG,\nFEATS, and DEPREL. Features of the type DEPREL\nhave a special status in that they are extracted during\nparsing from the partially built dependency graph\nand may therefore contain errors, whereas all the\nother features have gold standard values during both\ntraining and parsing.2\nBased on previous research, we defined a base\nmodel to be used as a starting point for language-\nspecific feature selection. The features of this model\nare shown in Table 1, where rows denote tokens in\na parser configuration (defined relative to the stack,\nthe remaining input, and the partially built depen-\ndency graph), and where columns correspond to data\nfields. The base model contains twenty features, but\nnote that the fields LEMMA, CPOS and FEATS are not\navailable for all languages.\n"},{"#tail":"\n","@confidence":"0.993996166666667","#text":"\nWe use support vector machines3 to predict the next\nparser action from a feature vector representing the\nhistory. More specifically, we use LIBSVM (Chang\nand Lin, 2001) with a quadratic kernel K(xi, xj) =\n(?xTi xj +r)2 and the built-in one-versus-all strategy\nfor multi-class classification. Symbolic features are\n"},{"#tail":"\n","@confidence":"0.983852454545454","#text":"\nlearning but found that this gave consistently lower accuracy.\nconverted to numerical features using the standard\ntechnique of binarization, and we split values of the\nFEATS field into its atomic components.4\nFor some languages, we divide the training data\ninto smaller sets, based on some feature s (normally\nthe CPOS or POS of the next input token), which may\nreduce training times without a significant loss in\naccuracy (Yamada and Matsumoto, 2003). To avoid\ntoo small training sets, we pool together categories\nthat have a frequency below a certain threshold t.\n"},{"#tail":"\n","@confidence":"0.999567764705882","#text":"\nPseudo-projective parsing was proposed by Nivre\nand Nilsson (2005) as a way of dealing with\nnon-projective structures in a projective data-driven\nparser. We projectivize training data by a minimal\ntransformation, lifting non-projective arcs one step\nat a time, and extending the arc label of lifted arcs\nusing the encoding scheme called HEAD by Nivre\nand Nilsson (2005), which means that a lifted arc is\nassigned the label r?h, where r is the original label\nand h is the label of the original head in the non-\nprojective dependency graph.\nNon-projective dependencies can be recovered by\napplying an inverse transformation to the output of\nthe parser, using a left-to-right, top-down, breadth-\nfirst search, guided by the extended arc labels r?h\nassigned by the parser. This technique has been used\nwithout exception for all languages.\n"},{"#tail":"\n","@confidence":"0.9077415625","#text":"\nSince the projective parsing algorithm and graph\ntransformation techniques are the same for all data\nsets, our optimization efforts have been focused on\nfeature selection, using a combination of backward\nand forward selection starting from the base model\ndescribed in section 2.2, and parameter optimization\nfor the SVM learner, using grid search for an optimal\ncombination of the kernel parameters ? and r, the\npenalty parameter C and the termination criterion ?,\nas well as the splitting feature s and the frequency\nthreshold t. Feature selection and parameter opti-\nmization have to some extent been interleaved, but\nthe amount of work done varies between languages.\n4Preliminary experiments showed a slight improvement for\nmost languages when splitting the FEATS values, as opposed to\ntaking every combination of atomic values as a distinct value.\n"},{"#tail":"\n","@confidence":"0.9642824","#text":"\nLAcc = label accuracy score; total score excluding Bulgarian\nThe main optimization criterion has been labeled\nattachment score on held-out data, using ten-fold\ncross-validation for all data sets with 100k tokens\nor less, and an 80-20 split into training and devtest\nsets for larger datasets. The number of features in\nthe optimized models varies from 16 (Turkish) to 30\n(Spanish), but the models use all fields available for\na given language, except that FORM is not used for\nTurkish (only LEMMA). The SVM parameters fall\ninto the following ranges: ?: 0.12?0.20; r: 0.0?0.6;\nC: 0.1?0.7; ?: 0.01?1.0. Data has been split on the\nPOS of the next input token for Czech (t = 200),\nGerman (t = 1000), and Spanish (t = 1000), and\non the CPOS of the next input token for Bulgarian\n(t = 1000), Slovene (t = 600), and Turkish (t = 100).\n(For the remaining languages, the training data has\nnot been split at all.)5 A dry run at the end of the\ndevelopment phase gave a labeled attachment score\nof 80.46 over the twelve required languages.\nTable 2 shows final test results for each language\nand for the twelve required languages together. The\ntotal score is only 0.27 percentage points below the\nscore from the dry run, which seems to indicate that\nmodels have not been overfitted to the training data.\nThe labeled attachment score varies from 91.65 to\n65.68 but is above average for all languages. We\nhave the best reported score for Japanese, Swedish\nand Turkish, and the score for Arabic, Danish,\nDutch, Portuguese, Spanish, and overall does not\ndiffer significantly from the best one. The unlabeled\nscore is less competitive, with only Turkish having\nthe highest reported score, which indirectly indicates\nthat the integration of labels into the parsing process\nprimarily benefits labeled accuracy.\n"},{"#tail":"\n","@confidence":"0.997518311111111","#text":"\nAn overall error analysis is beyond the scope of this\npaper, but we will offer a few general observations\n5Detailed specifications of the feature models and learning\nalgorithm parameters can be found on the MaltParser web page.\nbefore we turn to Swedish and Turkish, focusing on\nrecall and precision of root nodes, as a reflection of\nglobal syntactic structure, and on attachment score\nas a function of arc length. If we start by considering\nlanguages with a labeled attachment score of 85% or\nhigher, they are characterized by high precision and\nrecall for root nodes, typically 95/90, and by a grace-\nful degradation of attachment score as arcs grow\nlonger, typically 95?90?85, for arcs of length 1, 2\nand 3?6. Typical examples are Bulgarian (Simov\net al, 2005; Simov and Osenova, 2003), Chinese\n(Chen et al, 2003), Danish (Kromann, 2003), and\nSwedish (Nilsson et al, 2005). Japanese (Kawata\nand Bartels, 2000), despite a very high accuracy, is\ndifferent in that attachment score drops from 98%\nto 85%, as we go from length 1 to 2, which may\nhave something to do with the data consisting of\ntranscribed speech with very short utterances.\nA second observation is that a high proportion of\nnon-projective structures leads to fragmentation in\nthe parser output, reflected in lower precision for\nroots. This is noticeable for German (Brants et al,\n2002) and Portuguese (Afonso et al, 2002), which\nstill have high overall accuracy thanks to very high\nattachment scores, but much more conspicuous for\nCzech (Bo?hmova? et al, 2003), Dutch (van der Beek\net al, 2002) and Slovene (Dz?eroski et al, 2006),\nwhere root precision drops more drastically to about\n69%, 71% and 41%, respectively, and root recall is\nalso affected negatively. On the other hand, all three\nlanguages behave like high-accuracy languages with\nrespect to attachment score. A very similar pattern\nis found for Spanish (Civit Torruella and Mart?? An-\nton??n, 2002), although this cannot be explained by\na high proportion of non-projective structures. One\npossible explanation in this case may be the fact that\ndependency graphs in the Spanish data are sparsely\nlabeled, which may cause problem for a parser that\nrelies on dependency labels as features.\nThe results for Arabic (Hajic? et al, 2004; Smrz?\net al, 2002) are characterized by low root accuracy\n"},{"#tail":"\n","@confidence":"0.969307222222222","#text":"\nas well as a rapid degradation of attachment score\nwith arc length (from about 93% for length 1 to 67%\nfor length 2). By contrast, Turkish (Oflazer et al,\n2003; Atalay et al, 2003) exhibits high root accu-\nracy but consistently low attachment scores (about\n88% for length 1 and 68% for length 2). It is note-\nworthy that Arabic and Turkish, being ?typological\noutliers?, show patterns that are different both from\neach other and from most of the other languages.\n"},{"#tail":"\n","@confidence":"0.999391483870968","#text":"\nA more fine-grained analysis of the Swedish results\nreveals a high accuracy for function words, which\nis compatible with previous studies (Nivre, 2006).\nThus, the labeled F-score is 100% for infinitive\nmarkers (IM) and subordinating conjunctions (UK),\nand above 95% for determiners (DT). In addition,\nsubjects (SS) have a score above 90%. In all these\ncases, the dependent has a configurationally defined\n(but not fixed) position with respect to its head.\nArguments of the verb, such as objects (DO, IO)\nand predicative complements (SP), have a slightly\nlower accuracy (about 85% labeled F-score), which\nis due to the fact that they ?compete? in the same\nstructural positions, whereas adverbials (labels that\nend in A) have even lower scores (often below 70%).\nThe latter result must be related both to the relatively\nfine-grained inventory of dependency labels for ad-\nverbials and to attachment ambiguities that involve\nprepositional phrases. The importance of this kind\nof ambiguity is reflected also in the drastic differ-\nence in accuracy between noun pre-modifiers (AT)\n(F > 97%) and noun post-modifiers (ET) (F ? 75%).\nFinally, it is worth noting that coordination, which\nis often problematic in parsing, has high accuracy.\nThe Swedish treebank annotation treats the second\nconjunct as a dependent of the first conjunct and as\nthe head of the coordinator, which seems to facil-\nitate parsing.6 The attachment of the second con-\njunct to the first (CC) has a labeled F-score above\n80%, while the attachment of the coordinator to the\nsecond conjunct (++) has a score well above 90%.\n"},{"#tail":"\n","@confidence":"0.96706888372093","#text":"\nIn Turkish, very essential syntactic information is\ncontained in the rich morphological structure, where\n6The analysis is reminiscent of the treatment of coordination\nin the Collins parser (Collins, 1999).\nconcatenated suffixes carry information that in other\nlanguages may be expressed by separate words. The\nTurkish treebank therefore divides word forms into\nsmaller units, called inflectional groups (IGs), and\nthe task of the parser is to construct dependencies\nbetween IGs, not (primarily) between word forms\n(Eryig?it and Oflazer, 2006). It is then important\nto remember that an unlabeled attachment score\nof 75.8% corresponds to a word-to-word score of\n82.7%, which puts Turkish on a par with languages\nlike Czech, Dutch and Spanish. Moreover, when\nwe break down the results according to whether the\nhead of a dependency is part of a multiple-IG word\nor a complete (single-IG) word, we observe a highly\nsignificant difference in accuracy, with only 53.2%\nunlabeled attachment score for multiple-IG heads\nversus 83.7% for single-IG heads. It is hard to say\nat this stage whether this means that our methods\nare ill-suited for IG-based parsing, or whether it is\nmainly a case of sparse data for multiple-IG words.\nWhen we break down the results by dependency\ntype, we can distinguish three main groups. The first\nconsists of determiners and particles, which have\nan unlabeled attachment score over 80% and which\nare found within a distance of 1?1.4 IGs from their\nhead.7 The second group mainly contains subjects,\nobjects and different kinds of adjuncts, with a score\nin the range 60?80% and a distance of 1.8?5.2 IGs to\ntheir head. In this group, information about case and\npossessive features of nominals is important, which\nis found in the FEATS field in the data representation.\nWe believe that one important explanation for our\nrelatively good results for Turkish is that we break\ndown the FEATS information into its atomic com-\nponents, independently of POS and CPOS tags, and\nlet the classifier decide which one to use in a given\nsituation. The third group contains distant depen-\ndencies, such as sentence modifiers, vocatives and\nappositions, which have a much lower accuracy.\n"},{"#tail":"\n","@confidence":"0.997563285714286","#text":"\nThe evaluation shows that labeled pseudo-projective\ndependency parsing, using a deterministic parsing\nalgorithm and SVM classifiers, gives competitive\nparsing accuracy for all languages involved in the\n7Given that the average IG count of a word is 1.26 in the\ntreebank, this means that they are normally adjacent to the head\nword.\n"},{"#tail":"\n","@confidence":"0.999698","#text":"\nshared task, although the level of accuracy varies\nconsiderably between languages. To analyze in\ndepth the factors determining this variation, and to\nimprove our parsing methods accordingly to meet\nthe challenges posed by the linguistic diversity, will\nbe an important research goal for years to come.\n"},{"#tail":"\n","@confidence":"0.993615142857143","#text":"\nWe are grateful for the support from T ?UB?ITAK\n(The Scientific and Technical Research Council of\nTurkey) and the Swedish Research Council. We also\nwant to thank Atanas Chanev for assistance with\nSlovene, the organizers of the shared task for all\ntheir hard work, and the creators of the treebanks\nfor making the data available.\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.677423","#text":"\nSchool of Mathematics\nand Systems Engineering\nVa?xjo? University\n"},{"#tail":"\n","@confidence":"0.741393333333333","#text":"\nDepartment of\nComputer Engineering\nIstanbul Technical University\n"},{"#tail":"\n","@confidence":"0.670624","#text":"\nSchool of Humanities\nand Informatics\nUniversity of Sko?vde\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.990598","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998254","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.571609","@genericHeader":"method","#text":"\n2 Parsing Methodology\n"},{"#tail":"\n","@confidence":"0.999497","@genericHeader":"method","#text":"\n3 Experiments\n"},{"#tail":"\n","@confidence":"0.998539","@genericHeader":"method","#text":"\n4 Error Analysis\n"},{"#tail":"\n","@confidence":"0.998398","@genericHeader":"conclusions","#text":"\n5 Conclusion\n"},{"#tail":"\n","@confidence":"0.966913","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.986321","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.812616","#text":"\nTable 1: Base model; S: stack, I: input, G: graph;\n"},{"#tail":"\n","@confidence":"0.995423","#text":"\nTable 2: Evaluation on final test set; LAS = labeled attachment score, UAS = unlabeled attachment score,\n"}],"page":[{"#tail":"\n","@confidence":"0.969081","#text":"\n221\n"},{"#tail":"\n","@confidence":"0.992079","#text":"\n222\n"},{"#tail":"\n","@confidence":"0.992193","#text":"\n223\n"},{"#tail":"\n","@confidence":"0.985818","#text":"\n224\n"},{"#tail":"\n","@confidence":"0.998405","#text":"\n225\n"}],"table":{"#tail":"\n","@confidence":"0.99785275","#text":"\nAra Bul Chi Cze Dan Dut Ger Jap Por Slo Spa Swe Tur Total\nLAS 66.71 87.41 86.92 78.42 84.77 78.59 85.82 91.65 87.60 70.30 81.29 84.58 65.68 80.19\nUAS 77.52 91.72 90.54 84.80 89.80 81.35 88.76 93.10 91.22 78.72 84.67 89.50 75.82 85.48\nLAcc 80.34 90.44 89.01 85.40 89.16 83.69 91.03 94.34 91.54 80.54 90.06 87.39 78.49 86.75\n"},"email":[{"#tail":"\n","@confidence":"0.937798","#text":"\n{nivre,jha,jni}@msi.vxu.se\n"},{"#tail":"\n","@confidence":"0.974368","#text":"\ngulsen@cs.itu.edu.tr\n"},{"#tail":"\n","@confidence":"0.990416","#text":"\nsvetoslav.marinov@his.se\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.449525","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.9863035","#text":"Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 221?225, New York City, June 2006. c?2006 Association for Computational Linguistics"},"address":[{"#tail":"\n","@confidence":"0.999718","#text":"35195 Va?xjo?, Sweden"},{"#tail":"\n","@confidence":"0.999996","#text":"34469 Istanbul, Turkey"},{"#tail":"\n","@confidence":"0.9967745","#text":"Box 408 54128 Sko?vde, Sweden"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.999158","#text":"School of Mathematics and Systems Engineering Va?xjo? University"},{"#tail":"\n","@confidence":"0.998313","#text":"Department of Computer Engineering Istanbul Technical University"},{"#tail":"\n","@confidence":"0.999442333333333","#text":"School of Humanities and Informatics University of Sko?vde"}],"author":[{"#tail":"\n","@confidence":"0.993093","#text":"Johan Hall Jens Nilsson"},{"#tail":"\n","@confidence":"0.973433","#text":"Gulsen Eryigit"},{"#tail":"\n","@confidence":"0.834831","#text":"Svetoslav Marinov"}],"abstract":{"#tail":"\n","@confidence":"0.997157545454545","#text":"We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion. Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish."},"title":{"#tail":"\n","@confidence":"0.866639666666667","#text":"Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines Joakim Nivre"},"email":[{"#tail":"\n","@confidence":"0.975376","#text":"nivre@msi.vxu.se"},{"#tail":"\n","@confidence":"0.975376","#text":"jha@msi.vxu.se"},{"#tail":"\n","@confidence":"0.975376","#text":"jni@msi.vxu.se"},{"#tail":"\n","@confidence":"0.991123","#text":"gulsen@cs.itu.edu.tr"},{"#tail":"\n","@confidence":"0.989869","#text":"svetoslav.marinov@his.se"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"date":{"#tail":"\n","#text":"2003"},"editor":{"#tail":"\n","#text":"A. Abeille?, editor."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":". ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? SHIFT: Push next onto the stack. ? REDUCE: Pop the stack. ? RIGHT-ARC(r): Add an arc labeled r from top to next; push next onto the stack. ? LEFT-ARC(r): Add an arc labeled r from next to top; pop the stack. Although the parser only derives projective graphs, the fact tha","@endWordPosition":"286","@position":"2123","annotationId":"T1","@startWordPosition":"286","@citStr":"(2003)"}},"title":{"#tail":"\n","#text":"Treebanks: Building and Using Parsed Corpora,"},"volume":{"#tail":"\n","#text":"20"},"#tail":"\n","rawString":{"#tail":"\n","#text":"A. Abeille?, editor. 2003. Treebanks: Building and Using Parsed Corpora, volume 20 of Text, Speech and Language Technology. Kluwer Academic Publishers, Dordrecht."},"#text":"\n","marker":{"#tail":"\n","#text":"2003"},"publisher":{"#tail":"\n","#text":"Kluwer Academic Publishers,"},"location":{"#tail":"\n","#text":"Dordrecht."},"booktitle":{"#tail":"\n","#text":"Text, Speech and Language Technology."},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Floresta sinta?(c)tica?: a treebank for Portuguese. In Proc. of LREC2002, pages 1698?1703."},"#text":"\n","pages":{"#tail":"\n","#text":"1698--1703"},"marker":{"#tail":"\n","#text":"Afonso, Bick, Haber, Santos, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"v and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech (Bo?hmova? et al, 2003), Dutch (van der Beek et al, 2002) and Slovene (Dz?eroski et al, 2006), where root precision drops more drastically to about 69%, 71% and 41%, respectively, and root recall is also affected negatively. On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Mart?? Anton??n, 2002), although this cannot be explained by a high proportion of non","@endWordPosition":"1818","@position":"11288","annotationId":"T2","@startWordPosition":"1815","@citStr":"Afonso et al, 2002"}},"title":{"#tail":"\n","#text":"Floresta sinta?(c)tica?: a treebank for Portuguese."},"booktitle":{"#tail":"\n","#text":"In Proc. of LREC2002,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Afonso"},{"#tail":"\n","#text":"E Bick"},{"#tail":"\n","#text":"R Haber"},{"#tail":"\n","#text":"D Santos"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation process in the Turkish treebank. In Proc. of LINC-2003."},"#text":"\n","marker":{"#tail":"\n","#text":"Atalay, Oflazer, Say, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" (Civit Torruella and Mart?? Anton??n, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features. The results for Arabic (Hajic? et al, 2004; Smrz? et al, 2002) are characterized by low root accuracy 223 as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast, Turkish (Oflazer et al, 2003; Atalay et al, 2003) exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). It is noteworthy that Arabic and Turkish, being ?typological outliers?, show patterns that are different both from each other and from most of the other languages. 4.1 Swedish A more fine-grained analysis of the Swedish results reveals a high accuracy for function words, which is compatible with previous studies (Nivre, 2006). Thus, the labeled F-score is 100% for infinitive markers (IM) and subordinating conjunctions (UK), and above 95% for determiners (DT). In addition, subjects","@endWordPosition":"2003","@position":"12400","annotationId":"T3","@startWordPosition":"2000","@citStr":"Atalay et al, 2003"}},"title":{"#tail":"\n","#text":"The annotation process in the Turkish treebank."},"booktitle":{"#tail":"\n","#text":"In Proc. of LINC-2003."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"N B Atalay"},{"#tail":"\n","#text":"K Oflazer"},{"#tail":"\n","#text":"B Say"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1992"},"rawString":{"#tail":"\n","#text":"E. Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mercer, and S. Roukos. 1992. Towards history-based grammars: Using richer models for probabilistic parsing. In Proc. of the 5th DARPA Speech and Natural Language Workshop, pages 31?37."},"#text":"\n","pages":{"#tail":"\n","#text":"31--37"},"marker":{"#tail":"\n","#text":"Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1992"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"raining data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. 1 Introduction The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ","@endWordPosition":"204","@position":"1517","annotationId":"T4","@startWordPosition":"201","@citStr":"Black et al, 1992"}},"title":{"#tail":"\n","#text":"Towards history-based grammars: Using richer models for probabilistic parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of the 5th DARPA Speech and Natural Language Workshop,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E Black"},{"#tail":"\n","#text":"F Jelinek"},{"#tail":"\n","#text":"J D Lafferty"},{"#tail":"\n","#text":"D M Magerman"},{"#tail":"\n","#text":"R L Mercer"},{"#tail":"\n","#text":"S Roukos"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"note":{"#tail":"\n","#text":"chapter 7."},"rawString":{"#tail":"\n","#text":"A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The PDT: a 3-level annotation scenario. In Abeille? (Abeille?, 2003), chapter 7."},"#text":"\n","marker":{"#tail":"\n","#text":"Bohmova, Hajic, Hajicova, Hladka, 2003"},"title":{"#tail":"\n","#text":"The PDT: a 3-level annotation scenario."},"booktitle":{"#tail":"\n","#text":"In Abeille? (Abeille?,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Bohmova"},{"#tail":"\n","#text":"J Hajic"},{"#tail":"\n","#text":"E Hajicova"},{"#tail":"\n","#text":"B Hladka"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002."},"#text":"\n","marker":{"#tail":"\n","#text":"Brants, Dipper, Hansen, Lezius, Smith, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech (Bo?hmova? et al, 2003), Dutch (van der Beek et al, 2002) and Slovene (Dz?eroski et al, 2006), where root precision drops more drastically to about 69%, 71% and 41%, respectively, and root recall is also affected negatively. On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Mart?? Anton??n, 2002), although this cannot be e","@endWordPosition":"1812","@position":"11252","annotationId":"T5","@startWordPosition":"1809","@citStr":"Brants et al, 2002"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Brants"},{"#tail":"\n","#text":"S Dipper"},{"#tail":"\n","#text":"S Hansen"},{"#tail":"\n","#text":"W Lezius"},{"#tail":"\n","#text":"G Smith"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"The TIGER treebank. In Proc. of TLT-2002."},"#text":"\n","marker":{"#tail":"\n"},"title":{"#tail":"\n","#text":"The TIGER treebank."},"booktitle":{"#tail":"\n","#text":"In Proc. of TLT-2002."},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm."},"#text":"\n","marker":{"#tail":"\n","#text":"Chang, Lin, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" a starting point for languagespecific feature selection. The features of this model are shown in Table 1, where rows denote tokens in a parser configuration (defined relative to the stack, the remaining input, and the partially built dependency graph), and where columns correspond to data fields. The base model contains twenty features, but note that the fields LEMMA, CPOS and FEATS are not available for all languages. 2.3 Support Vector Machines We use support vector machines3 to predict the next parser action from a feature vector representing the history. More specifically, we use LIBSVM (Chang and Lin, 2001) with a quadratic kernel K(xi, xj) = (?xTi xj +r)2 and the built-in one-versus-all strategy for multi-class classification. Symbolic features are 2The fields PHEAD and PDEPREL have not been used at all, since we rely on pseudo-projective parsing for the treatment of non-projective structures. 3We also ran preliminary experiments with memory-based learning but found that this gave consistently lower accuracy. converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.4 For some languages, we divide the training","@endWordPosition":"793","@position":"5024","annotationId":"T6","@startWordPosition":"790","@citStr":"Chang and Lin, 2001"}},"title":{"#tail":"\n","#text":"LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"C-C Chang"},{"#tail":"\n","#text":"C-J Lin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and Z. Gao. 2003. Sinica treebank: Design criteria, representational issues and implementation. In Abeille? (Abeille?, 2003), chapter 13, pages 231?248."},"#text":"\n","pages":{"#tail":"\n","#text":"231--248"},"marker":{"#tail":"\n","#text":"Chen, Luo, Chang, Chen, Chen, Huang, Gao, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nd on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overa","@endWordPosition":"1724","@position":"10717","annotationId":"T7","@startWordPosition":"1721","@citStr":"Chen et al, 2003"}},"title":{"#tail":"\n","#text":"Sinica treebank: Design criteria, representational issues and implementation."},"booktitle":{"#tail":"\n","#text":"In Abeille? (Abeille?,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Chen"},{"#tail":"\n","#text":"C Luo"},{"#tail":"\n","#text":"M Chang"},{"#tail":"\n","#text":"F Chen"},{"#tail":"\n","#text":"C Chen"},{"#tail":"\n","#text":"C Huang"},{"#tail":"\n","#text":"Z Gao"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"M. Civit Torruella and Ma A. Mart?? Anton??n. 2002. Design principles for a Spanish treebank. In Proc. of TLT-2002."},"#text":"\n","marker":{"#tail":"\n","#text":"Torruella, Antonn, 2002"},"title":{"#tail":"\n","#text":"Design principles for a Spanish treebank."},"booktitle":{"#tail":"\n","#text":"In Proc. of TLT-2002."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Civit Torruella"},{"#tail":"\n","#text":"Ma A Mart Antonn"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"University of Pennsylvania."},"rawString":{"#tail":"\n","#text":"M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"n problematic in parsing, has high accuracy. The Swedish treebank annotation treats the second conjunct as a dependent of the first conjunct and as the head of the coordinator, which seems to facilitate parsing.6 The attachment of the second conjunct to the first (CC) has a labeled F-score above 80%, while the attachment of the coordinator to the second conjunct (++) has a score well above 90%. 4.2 Turkish In Turkish, very essential syntactic information is contained in the rich morphological structure, where 6The analysis is reminiscent of the treatment of coordination in the Collins parser (Collins, 1999). concatenated suffixes carry information that in other languages may be expressed by separate words. The Turkish treebank therefore divides word forms into smaller units, called inflectional groups (IGs), and the task of the parser is to construct dependencies between IGs, not (primarily) between word forms (Eryig?it and Oflazer, 2006). It is then important to remember that an unlabeled attachment score of 75.8% corresponds to a word-to-word score of 82.7%, which puts Turkish on a par with languages like Czech, Dutch and Spanish. Moreover, when we break down the results according to whether t","@endWordPosition":"2339","@position":"14487","annotationId":"T8","@startWordPosition":"2338","@citStr":"Collins, 1999"}},"title":{"#tail":"\n","#text":"Head-Driven Statistical Models for Natural Language Parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ?Zabokrtsky, and A. ?Zele. 2006. Towards a Slovene dependency treebank. In Proc. of LREC-2006."},"#text":"\n","marker":{"#tail":"\n","#text":"Dzeroski, Erjavec, Ledinek, Pajas, Zabokrtsky, Zele, 2006"},"title":{"#tail":"\n","#text":"Towards a Slovene dependency treebank."},"booktitle":{"#tail":"\n","#text":"In Proc. of LREC-2006."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Dzeroski"},{"#tail":"\n","#text":"T Erjavec"},{"#tail":"\n","#text":"N Ledinek"},{"#tail":"\n","#text":"P Pajas"},{"#tail":"\n","#text":"Z Zabokrtsky"},{"#tail":"\n","#text":"A Zele"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"G. Eryig?it and K. Oflazer. 2006. Statistical dependency parsing of Turkish. In Proc. of EACL-2006."},"#text":"\n","marker":{"#tail":"\n","#text":"Eryigit, Oflazer, 2006"},"title":{"#tail":"\n","#text":"Statistical dependency parsing of Turkish."},"booktitle":{"#tail":"\n","#text":"In Proc. of EACL-2006."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"G Eryigit"},{"#tail":"\n","#text":"K Oflazer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka. 2004."},"#text":"\n","marker":{"#tail":"\n","#text":"Hajic, Smrz, Zemanek, Snaidauf, Beska, 2004"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Hajic"},{"#tail":"\n","#text":"O Smrz"},{"#tail":"\n","#text":"P Zemanek"},{"#tail":"\n","#text":"J Snaidauf"},{"#tail":"\n","#text":"E Beska"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"Prague Arabic dependency treebank: Development in data and tools. In Proc. of NEMLAR-2004, pages 110?117."},"#text":"\n","pages":{"#tail":"\n","#text":"110--117"},"marker":{"#tail":"\n"},"title":{"#tail":"\n","#text":"Prague Arabic dependency treebank: Development in data and tools."},"booktitle":{"#tail":"\n","#text":"In Proc. of NEMLAR-2004,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese treebank in VERBMOBIL. Verbmobil-Report 240, Seminar fu?r Sprachwissenschaft, Universita?t Tu?bingen."},"#text":"\n","marker":{"#tail":"\n","#text":"Kawata, Bartels, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech (Bo?hmova","@endWordPosition":"1738","@position":"10813","annotationId":"T9","@startWordPosition":"1735","@citStr":"Kawata and Bartels, 2000"}},"title":{"#tail":"\n","#text":"Stylebook for the Japanese treebank in VERBMOBIL. Verbmobil-Report 240, Seminar fu?r Sprachwissenschaft, Universita?t Tu?bingen."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Y Kawata"},{"#tail":"\n","#text":"J Bartels"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"M. T. Kromann. 2003. The Danish dependency treebank and the underlying linguistic theory. In Proc. of TLT-2003."},"#text":"\n","marker":{"#tail":"\n","#text":"Kromann, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ge. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to ve","@endWordPosition":"1727","@position":"10741","annotationId":"T10","@startWordPosition":"1726","@citStr":"Kromann, 2003"}},"title":{"#tail":"\n","#text":"The Danish dependency treebank and the underlying linguistic theory."},"booktitle":{"#tail":"\n","#text":"In Proc. of TLT-2003."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M T Kromann"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"T. Kudo and Y. Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proc. of CoNLL-2002, pages 63?69."},"#text":"\n","pages":{"#tail":"\n","#text":"63--69"},"marker":{"#tail":"\n","#text":"Kudo, Matsumoto, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"f the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. 1 Introduction The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds ","@endWordPosition":"218","@position":"1611","annotationId":"T11","@startWordPosition":"215","@citStr":"Kudo and Matsumoto, 2002"}},"title":{"#tail":"\n","#text":"Japanese dependency analysis using cascaded chunking."},"booktitle":{"#tail":"\n","#text":"In Proc. of CoNLL-2002,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"T Kudo"},{"#tail":"\n","#text":"Y Matsumoto"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity. In Proc. of the NODALIDA Special Session on Treebanks."},"#text":"\n","marker":{"#tail":"\n","#text":"Nilsson, Hall, Nivre, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to very high attachment scores, but much","@endWordPosition":"1733","@position":"10776","annotationId":"T12","@startWordPosition":"1730","@citStr":"Nilsson et al, 2005"}},"title":{"#tail":"\n","#text":"MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity."},"booktitle":{"#tail":"\n","#text":"In Proc. of the NODALIDA Special Session on Treebanks."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Nilsson"},{"#tail":"\n","#text":"J Hall"},{"#tail":"\n","#text":"J Nivre"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency parsing. In Proc. of ACL-2005, pages 99?106."},"#text":"\n","pages":{"#tail":"\n","#text":"99--106"},"marker":{"#tail":"\n","#text":"Nivre, Nilsson, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"rkish. 1 Introduction The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store","@endWordPosition":"230","@position":"1702","annotationId":"T13","@startWordPosition":"227","@citStr":"Nivre and Nilsson, 2005"},{"#tail":"\n","#text":"istently lower accuracy. converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.4 For some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t. 2.4 Pseudo-Projective Parsing Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser. We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r?h, where r is the original label and h is the label of the original head in the nonprojective dependency graph. Non-projective dependencies can be recovered by applying an inverse transformation to the output of the parser, using a left-to-r","@endWordPosition":"947","@position":"6035","annotationId":"T14","@startWordPosition":"944","@citStr":"Nivre and Nilsson (2005)"}]},"title":{"#tail":"\n","#text":"Pseudo-projective dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL-2005,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Nivre"},{"#tail":"\n","#text":"J Nilsson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proc. CoNLL-2004, pages 49?56."},"#text":"\n","pages":{"#tail":"\n","#text":"49--56"},"marker":{"#tail":"\n","#text":"Nivre, Hall, Nilsson, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? SHIFT: Push next onto the stack. ? REDUCE: Pop the stack. ? RIGHT-ARC(r): Add an arc labeled r from top to next; push next onto the stack. ? LEFT-ARC(r): Add an arc labeled r from next to top; pop the stack. Although the parser only derives projective graphs, the fact that graphs are labeled allows non-projective dependencies to be cap","@endWordPosition":"297","@position":"2188","annotationId":"T15","@startWordPosition":"294","@citStr":"Nivre et al (2004)"}},"title":{"#tail":"\n","#text":"Memory-based dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. CoNLL-2004,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Nivre"},{"#tail":"\n","#text":"J Hall"},{"#tail":"\n","#text":"J Nilsson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A datadriven parser-generator for dependency parsing. In Proc. of LREC-2006."},"#text":"\n","marker":{"#tail":"\n","#text":"Nivre, Hall, Nilsson, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and adding arcs using four elementary actions ","@endWordPosition":"242","@position":"1776","annotationId":"T16","@startWordPosition":"239","@citStr":"Nivre et al, 2006"}},"title":{"#tail":"\n","#text":"MaltParser: A datadriven parser-generator for dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of LREC-2006."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Nivre"},{"#tail":"\n","#text":"J Hall"},{"#tail":"\n","#text":"J Nilsson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proc. of IWPT-2003, pages 149?160."},"#text":"\n","pages":{"#tail":"\n","#text":"149--160"},"marker":{"#tail":"\n","#text":"Nivre, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? SHIFT: Push next onto the stack. ? REDUCE: Pop the stack. ? RIGHT-ARC(r): Add an arc labeled r from top to next; push next onto the stack. ? LEFT-ARC(r): Add an arc labeled r from next to top; pop the stack. Although the parser only derives projective graphs, the fact tha","@endWordPosition":"286","@position":"2123","annotationId":"T17","@startWordPosition":"285","@citStr":"Nivre (2003)"}},"title":{"#tail":"\n","#text":"An efficient algorithm for projective dependency parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of IWPT-2003,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Nivre"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"J. Nivre. 2006. Inductive Dependency Parsing. Springer."},"#text":"\n","marker":{"#tail":"\n","#text":"Nivre, 2006"},"publisher":{"#tail":"\n","#text":"Springer."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"fashion. Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. 1 Introduction The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all language","@endWordPosition":"190","@position":"1427","annotationId":"T18","@startWordPosition":"189","@citStr":"Nivre, 2006"},{"#tail":"\n","#text":" a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast, Turkish (Oflazer et al, 2003; Atalay et al, 2003) exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). It is noteworthy that Arabic and Turkish, being ?typological outliers?, show patterns that are different both from each other and from most of the other languages. 4.1 Swedish A more fine-grained analysis of the Swedish results reveals a high accuracy for function words, which is compatible with previous studies (Nivre, 2006). Thus, the labeled F-score is 100% for infinitive markers (IM) and subordinating conjunctions (UK), and above 95% for determiners (DT). In addition, subjects (SS) have a score above 90%. In all these cases, the dependent has a configurationally defined (but not fixed) position with respect to its head. Arguments of the verb, such as objects (DO, IO) and predicative complements (SP), have a slightly lower accuracy (about 85% labeled F-score), which is due to the fact that they ?compete? in the same structural positions, whereas adverbials (labels that end in A) have even lower scores (often be","@endWordPosition":"2075","@position":"12842","annotationId":"T19","@startWordPosition":"2074","@citStr":"Nivre, 2006"}]},"title":{"#tail":"\n","#text":"Inductive Dependency Parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Nivre"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003."},"#text":"\n","marker":{"#tail":"\n","#text":"Oflazer, Say, Hakkani-Tur, Tur, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" is found for Spanish (Civit Torruella and Mart?? Anton??n, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features. The results for Arabic (Hajic? et al, 2004; Smrz? et al, 2002) are characterized by low root accuracy 223 as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast, Turkish (Oflazer et al, 2003; Atalay et al, 2003) exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). It is noteworthy that Arabic and Turkish, being ?typological outliers?, show patterns that are different both from each other and from most of the other languages. 4.1 Swedish A more fine-grained analysis of the Swedish results reveals a high accuracy for function words, which is compatible with previous studies (Nivre, 2006). Thus, the labeled F-score is 100% for infinitive markers (IM) and subordinating conjunctions (UK), and above 95% for determiners (DT). ","@endWordPosition":"1999","@position":"12379","annotationId":"T20","@startWordPosition":"1996","@citStr":"Oflazer et al, 2003"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Oflazer"},{"#tail":"\n","#text":"B Say"},{"#tail":"\n","#text":"D Zeynep Hakkani-Tur"},{"#tail":"\n","#text":"G Tur"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"note":{"#tail":"\n","#text":"chapter 15."},"rawString":{"#tail":"\n","#text":"Building a Turkish treebank. In Abeille? (Abeille?, 2003), chapter 15."},"#text":"\n","marker":{"#tail":"\n","#text":"2003"},"location":{"#tail":"\n","#text":"Abeille?,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":". ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? SHIFT: Push next onto the stack. ? REDUCE: Pop the stack. ? RIGHT-ARC(r): Add an arc labeled r from top to next; push next onto the stack. ? LEFT-ARC(r): Add an arc labeled r from next to top; pop the stack. Although the parser only derives projective graphs, the fact tha","@endWordPosition":"286","@position":"2123","annotationId":"T21","@startWordPosition":"286","@citStr":"(2003)"}},"title":{"#tail":"\n","#text":"Building a Turkish treebank. In Abeille?"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"K. Simov and P. Osenova. 2003. Practical annotation scheme for an HPSG treebank of Bulgarian. In Proc. of LINC-2003, pages 17?24."},"#text":"\n","pages":{"#tail":"\n","#text":"17--24"},"marker":{"#tail":"\n","#text":"Simov, Osenova, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ing algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002),","@endWordPosition":"1719","@position":"10689","annotationId":"T22","@startWordPosition":"1716","@citStr":"Simov and Osenova, 2003"}},"title":{"#tail":"\n","#text":"Practical annotation scheme for an HPSG treebank of Bulgarian."},"booktitle":{"#tail":"\n","#text":"In Proc. of LINC-2003,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Simov"},{"#tail":"\n","#text":"P Osenova"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2005. Design and implementation of the Bulgarian HPSG-based treebank. In Journal of Research on Language and Computation ? Special Issue, pages 495?522. Kluwer Academic Publishers."},"journal":{"#tail":"\n","#text":"In Journal of Research on Language and Computation ? Special Issue,"},"#text":"\n","pages":{"#tail":"\n","#text":"495--522"},"marker":{"#tail":"\n","#text":"Simov, Osenova, Simov, Kouylekov, 2005"},"publisher":{"#tail":"\n","#text":"Kluwer Academic Publishers."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"re models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portug","@endWordPosition":"1715","@position":"10663","annotationId":"T23","@startWordPosition":"1712","@citStr":"Simov et al, 2005"}},"title":{"#tail":"\n","#text":"Design and implementation of the Bulgarian HPSG-based treebank."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Simov"},{"#tail":"\n","#text":"P Osenova"},{"#tail":"\n","#text":"A Simov"},{"#tail":"\n","#text":"M Kouylekov"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"O. Smrz?, J. ?Snaidauf, and P. Zema?nek. 2002. Prague dependency treebank for Arabic: Multi-level annotation of Arabic corpus. In Proc. of the Intern. Symposium on Processing of Arabic, pages 147?155."},"#text":"\n","pages":{"#tail":"\n","#text":"147--155"},"marker":{"#tail":"\n","#text":"Smrz, Snaidauf, Zemanek, 2002"},"title":{"#tail":"\n","#text":"Prague dependency treebank for Arabic: Multi-level annotation of Arabic corpus."},"booktitle":{"#tail":"\n","#text":"In Proc. of the Intern. Symposium on Processing of Arabic,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"O Smrz"},{"#tail":"\n","#text":"J Snaidauf"},{"#tail":"\n","#text":"P Zemanek"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"L. van der Beek, G. Bouma, R. Malouf, and G. van Noord. 2002. The Alpino dependency treebank. In Computational Linguistics in the Netherlands (CLIN)."},"#text":"\n","marker":{"#tail":"\n","#text":"van der Beek, Bouma, Malouf, van Noord, 2002"},"title":{"#tail":"\n","#text":"The Alpino dependency treebank."},"booktitle":{"#tail":"\n","#text":"In Computational Linguistics in the Netherlands (CLIN)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L van der Beek"},{"#tail":"\n","#text":"G Bouma"},{"#tail":"\n","#text":"R Malouf"},{"#tail":"\n","#text":"G van Noord"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT2003, pages 195?206."},"#text":"\n","pages":{"#tail":"\n","#text":"195--206"},"marker":{"#tail":"\n","#text":"Yamada, Matsumoto, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"EL have not been used at all, since we rely on pseudo-projective parsing for the treatment of non-projective structures. 3We also ran preliminary experiments with memory-based learning but found that this gave consistently lower accuracy. converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.4 For some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t. 2.4 Pseudo-Projective Parsing Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser. We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r?h, where r is the original lab","@endWordPosition":"916","@position":"5824","annotationId":"T24","@startWordPosition":"913","@citStr":"Yamada and Matsumoto, 2003"}},"title":{"#tail":"\n","#text":"Statistical dependency analysis with support vector machines."},"booktitle":{"#tail":"\n","#text":"In Proc. of IWPT2003,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"H Yamada"},{"#tail":"\n","#text":"Y Matsumoto"}]}}]}}]}}
