rticle concludes with a discussion of salience and pointing, which are analyzed as if they were gradable adjectives. 1. Introduction: Vagueness of Gradable Adjectives 1.1 Vague Descriptions Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees. This article focuses on gradable adjectives, also called degree adjectives.1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following Pinkal (1979), such expressions will be called vague descriptions even though, as we shall see, the vagueness of the adjective does not extend to the description as a whole. It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the dif
 indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean. Gradability is especially widespread in adjectives. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words (Peccei 1994) and understand some of their intricacies as early as their 24th month (Ebeling and Gelman 1994). These ? Computing Science Department, King?s College, University of Aberdeen, United Kingdom, E-mail: kvdeemter@csd.abdn.ac.uk. 1 We take such adjectives to be ones that have comparative and superlative forms, and which can be premodified by intensifiers such as very (Quirk et al 1972, Section 5.4). Submission received: 7 July 2004; revised submission received: 19 October 2005; accepted for publication: 24 November 2005. ? 2006 Association for Computational Linguistics Computational Linguistics Vo
cult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean. Gradability is especially widespread in adjectives. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words (Peccei 1994) and understand some of their intricacies as early as their 24th month (Ebeling and Gelman 1994). These ? Computing Science Department, King?s College, University of Aberdeen, United Kingdom, E-mail: kvdeemter@csd.abdn.ac.uk. 1 We take such adjectives to be ones that have comparative and superlative forms, and which can be premodified by intensifiers such as very (Quirk et al 1972, Section 5.4). Submission received: 7 July 2004; revised submission received: 19 October 2005; accepted for publication: 24 November 2005. ? 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 2 intricacies include what Ebeling and Gelman call perceptual context dependence
, for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words (Peccei 1994) and understand some of their intricacies as early as their 24th month (Ebeling and Gelman 1994). These ? Computing Science Department, King?s College, University of Aberdeen, United Kingdom, E-mail: kvdeemter@csd.abdn.ac.uk. 1 We take such adjectives to be ones that have comparative and superlative forms, and which can be premodified by intensifiers such as very (Quirk et al 1972, Section 5.4). Submission received: 7 July 2004; revised submission received: 19 October 2005; accepted for publication: 24 November 2005. ? 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 2 intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible). 1.2 Vagueness in NLG Some NLG systems produce gradable adjectives. The FOG weather
n element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible). 1.2 Vagueness in NLG Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by Reiter and Sripada (2002), where users can specify boundary values for attributes like rainfall, specifying, for example, rain counts as moderate above 7 mm/h, as heavy above 20 mm/h, and so on. A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart?s piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number 
 adjectives, letting these values depend on the context in which the adjective is used. Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact. Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair, since vague expressions tend to be interpreted in different ways by different people (Toogood 1980), sometimes in stark contrast with the intention of the speaker/writer (Berry, Knapp, and Raynor 2002). We shall therefore focus?unlike earlier computational accounts?on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective. Suppose you enter a vet?s surgery in the company of two dogs: a big one on a leash, and a tiny one in your arms. The vet asks ?Who?s the patient??, and you answer ?the big dog.? This answer will allow the vet to pick out the patient just as reliably as if you had said ?the 
 means to be large: larger than average, larger than most, larger than some given baseline, and so on. It is doubtful that any one of these analyses makes sense for all definite descriptions. To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm) Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000; DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In 
all definite descriptions. To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm) Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000; DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of 2 The reader is asked to focus on any reasonable si
. To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm) Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000; DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of 2 The reader is asked to focus on any reasonable size measurement, for examp
rge. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of 2 The reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or vertical distance, or some combination of dimensions (Kamp 1975; also Section 8.1 of the present article). 197 Computational Linguistics Volume 32, Number 2 the adjective to be semantically equivalent to the superlative form (and, analogously, the comparative): The n large mice = The largest n mice The large mice = The largest mice The large mouse = The largest mouse. Viewed in this way, gradable adjectives are an extreme example of the ?efficiency of language? (Barwise and Perry 1983): Far from meaning something concrete like ?larger than 8 cm??a concept that would have very limited applicability?or even something more general like ?larger than the avera
, we take the base form of 2 The reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or vertical distance, or some combination of dimensions (Kamp 1975; also Section 8.1 of the present article). 197 Computational Linguistics Volume 32, Number 2 the adjective to be semantically equivalent to the superlative form (and, analogously, the comparative): The n large mice = The largest n mice The large mice = The largest mice The large mouse = The largest mouse. Viewed in this way, gradable adjectives are an extreme example of the ?efficiency of language? (Barwise and Perry 1983): Far from meaning something concrete like ?larger than 8 cm??a concept that would have very limited applicability?or even something more general like ?larger than the average N,? a word like large is applicable across a wide range of different situations. 2.2 Caveat: Full NP Anaphora Having said this, there are pragmatic differences between the base form and the superlative (Section 5). For example, the equivalence does not take anaphoric uses into account, such as when the large mouse is legitimized by the fact that the mouse has been called large before, as in (5) I was transfixed by a larg
where the mouse on the chimney may be smaller than those on the ground. We focus on Ebeling and Gelman?s (1994) perceptual context dependence (Section 1), pretending that the only contextually relevant factor is the ?comparison set?: those elements of the noun denotation that are perceptually available. We disregard functional context dependence, as when the small hat is the one too small to fit on your head. 2.3 Caveat: Evaluative Adjectives What we said above has also disregarded elements of the ?global? (i.e., not immediately available) context. For some adjectives, including the ones that Bierwisch (1989) called evaluative (as opposed to dimensional), this is clearly inadequate. He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms. For example (after Bierwisch 1989), (6a) Hans is taller than Fritz ? Fritz is shorter than Hans. (6b) Hans is smarter than Fritz ? Fritz is more stupid than Hans. We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension. (For Fritz to be the stupid man, it is not enough for him t
eferent of an evaluative description fall into the correct segment of the relevant dimension. (For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans?s and Fritz?s heights are 210 and 198 van Deemter GRE with Gradable Properties 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see Sedivy et al 1999, discussed in Section 7.2). Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary. With these qualifications in place, let us say more precisely what we will assume the different types of expressions to mean. For ease of reading, concrete examples (e.g., large) will replace abstract labels (e.g., ?Adj?), but the analysis is meant to be general. The largest n mouse/mice; The n large mice. Imagine a set C of contextually relevant animals. Then these noun phrases (NPs) presuppose that there is a subset 
C := Domain For each Ai  A do Vi = FindBestValue(S, Ai) If S ? [[?Ai, Vi?]] & C ? [[?Ai, Vi?]] then do L := L ? {?Ai, Vi?} C := C ? [[?Ai, Vi?]] If C = S then Return L Return Failure FindBestValue selects the ?best value? from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity. The function selects the Value that removes most distractors, but in case of a tie, the least specific contestant is chosen, as long as it is not less specific than the basic-level Value (i.e., the most commonly occurring and psychologically most fundamental level, Rosch 1978). IAPlur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}. 3.2 The Existing Treatment of Gradables IAPlur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description. In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example. This approach does not do justice to gradable ad
are used in the base form, the superlative, or the comparative. Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it. Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless. In other words, existing treatments of gradables in GRE fail to take the ?efficiency of language? into account (Barwise and Perry 1983; see our Section 2). 200 van Deemter GRE with Gradable Properties 4. The Generation of Vague Descriptions We now turn to the question of how vague descriptions may be generated from numerical data. We focus on semantic issues, postponing discussion of pragmatics until Section 5, and linguistic realization until Section 6. We shall make occasional reference to a PROLOG program called VAGUE, designed by Richard Power, which implements a version of the algorithm described in this section. Code and documentation for VAGUE can be found at http://www.csd.abdn.ac.uk/?kvdeemte/vague.html. 4.1 Express
llow mouse, or the large yellow mouse). 4.1.2 Exploiting Numerical Properties, Singular. To (almost4) ensure that every description contains a property expressible as a noun, we shall assume that the type Attribute is more highly preferred than all others. Suppose also, for now, that properties related to size are less preferred than others. As a result, all other properties that turn up in the NP are already in the list L when size is added. Suppose the target is c4: type(c1) = type(c2) = type(c3) = type(c4) = mouse type(p5) = rat size(c1) = 6 cm 3 The degree of precision of the measurement (James et al 1996, Section 1.5) determines which objects can be described by the GRE algorithm, since it determines which objects count as having the same size. 4 To turn this likelihood into a certainty, one can add a test at the end of the algorithm, which adds a type-related property if none is present yet (cf., Dale and Reiter 1995). VAGUE uses both of these devices. 201 Computational Linguistics Volume 32, Number 2 size(c2) = 10 cm size(c3) = 12 cm size(c4) = size(p5) = 14 cm Since gradable properties are (for now at least) assumed to be dispreferred, the first property that makes it into L is ?mouse,? wh
less preferred than others. As a result, all other properties that turn up in the NP are already in the list L when size is added. Suppose the target is c4: type(c1) = type(c2) = type(c3) = type(c4) = mouse type(p5) = rat size(c1) = 6 cm 3 The degree of precision of the measurement (James et al 1996, Section 1.5) determines which objects can be described by the GRE algorithm, since it determines which objects count as having the same size. 4 To turn this likelihood into a certainty, one can add a test at the end of the algorithm, which adds a type-related property if none is present yet (cf., Dale and Reiter 1995). VAGUE uses both of these devices. 201 Computational Linguistics Volume 32, Number 2 size(c2) = 10 cm size(c3) = 12 cm size(c4) = size(p5) = 14 cm Since gradable properties are (for now at least) assumed to be dispreferred, the first property that makes it into L is ?mouse,? which removes p5 from the context set. (Result: C = {c1, ..., c4}.) Now size is taken into account, and size(x) = 14 cm singles out c4. The resulting list is L = {mouse, 14 cm} This might be considered the end of the matter, since the target has been singled out. But we are interested in alternative lists, to enable later
ation of inequalities is not entirely trivial. For one thing, it is convenient to view properties of the form size(x) < ? as belonging to a different Attribute than those of the form size(x) > ?, because this causes the Values of an Attribute to be linearly ordered: Being larger than 12 cm implies being larger than 10 cm, and so on. More importantly, it will now become normal for an object to have many Values for the same Attribute; c4, for example, has the Values > 6 cm, > 10 cm, and > 12 cm. Each of these Values has equal status, so the notion of a basic-level Value cannot play a role (cf., Dale and Reiter 1995). If we abstract away from the role of basic-level Values, then Dale and Reiter?s FindBestValue chooses the most general Value that removes the maximal 202 van Deemter GRE with Gradable Properties number of distractors, as we have seen. The problem at hand suggests a simpler approach that will always prefer logically stronger inequalities over logically weaker ones, even when they do not remove more distractors.5 (Thus, size(x) > m is preferred over size(x) > n iff m > n; conversely, size(x) < m is preferred over size(x) < n iff m < n.) This is reflected by the order in which the properties ar
s ?being a set of cardinality greater than 1, all of whose elements are larger than all other elements in C.? The result may be realized as the largest mice. L3 = ?mouse, size(x) = max?. 4.1.4 Ordering of Properties. Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch (1976; also reported in Levelt 1989) show that greater differences are most likely to be chosen, presumably because they are more striking. In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say ?the tall candle? when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to ?the fat candle.? Hermann and Deutsch?s findings may be implemented as follows. First, the Values of the different Attribute
, all of whose elements are larger than all other elements in C.? The result may be realized as the largest mice. L3 = ?mouse, size(x) = max?. 4.1.4 Ordering of Properties. Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch (1976; also reported in Levelt 1989) show that greater differences are most likely to be chosen, presumably because they are more striking. In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say ?the tall candle? when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to ?the fat candle.? Hermann and Deutsch?s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make 
able properties are less preferred than crisp ones (point 3) then this algorithm will only use gradable properties if an entirely crisp distinguishing description is impossible. This may well cause gradable properties to be underused. For this and other reasons, we shall consider non-incremental versions of these ideas in Section 8. 4.3 Computational Complexity We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects (Beun and Cremers 1998, Krahmer and Theune 2002). Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, Ai) is a constant times the number of Values of the Attribute Ai, then the worst-case running time of IA (and IAPlur) is O(nvna), where na equals the number of Attributes in the language and nv the average number of Values of all Attributes. This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002). As for the new algorithm, we focus on the crucial phases 2, 4, and 5. 206 van Deemter GRE with Gradable Properties Phas
s preferred than crisp ones (point 3) then this algorithm will only use gradable properties if an entirely crisp distinguishing description is impossible. This may well cause gradable properties to be underused. For this and other reasons, we shall consider non-incremental versions of these ideas in Section 8. 4.3 Computational Complexity We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects (Beun and Cremers 1998, Krahmer and Theune 2002). Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, Ai) is a constant times the number of Values of the Attribute Ai, then the worst-case running time of IA (and IAPlur) is O(nvna), where na equals the number of Attributes in the language and nv the average number of Values of all Attributes. This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002). As for the new algorithm, we focus on the crucial phases 2, 4, and 5. 206 van Deemter GRE with Gradable Properties Phase 2: Recompilation of the 
 ?Pi? (the extension of the next property in the description). If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used. In a vague description, the property last added to the description is context dependent. Worst case, calculating the set corresponding with such a property, of the form size(x) = maxm, for example, involves sorting the distractors as to their size, which may amount to O(n2d) or O(nd log nd) calculations (depending on the sorting algorithm: cf. [Aho et al 1983] Chapter 8). Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions. Thus, the worst-case time complexity of interpretation is as follows: The part that can be computed off-line takes O(nd log nd) calculations. The part that has to be computed for each referring expression separately takes O(nd) calculations. Once again, there is a difference with the nongradable case, but the difference is modest, especially regarding the part that cannot be done off-line. One should bear in mind that worst-case theoretical co
done off-line. One should bear in mind that worst-case theoretical complexity is not always a good measure of the time that a program takes in the kinds of cases that occur most commonly, let alne the difficulty for a person. For example, it seems likely that hearers and speakers will have most difficulty dealing with differences that are too small to be obvious (e.g., two mice that are very similar in size). 207 Computational Linguistics Volume 32, Number 2 5. Pragmatic Constraints NLG has to do more than select a distinguishing description (i.e., one that unambiguously denotes its referent; Dale 1989): The selected expression should also be felicitous. Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between ?observationally indifferent? entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects th
?observationally indifferent? entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it. This is the strongest version of the sorites paradox (e.g., Hyde 2002). Our approach to vague descriptions allows a subtle response: that the offending statement may be correct yet infelicitous. This shifts the problem from asking when vague descriptions are ?correct? to the question of when they are used felicitously. Felicity is naturally thought of as a gradable concept. There is therefore no need for a generator to demarcate precisely between felicitous and infelicitous expressions, as long as all the utterances generated are felicitous enough. When in doubt, a generator should avoid the expression in question. If x and y are mice of sizes 10 and 9.9 cm, for
or should avoid the expression in question. If x and y are mice of sizes 10 and 9.9 cm, for example, then it is probably better to describe x as the largest mouse than as the large mouse. Prior to carrying out the experiments to be reported in Section 7, we believed that the following constraints should be taken into account: Small Gaps. Expressions of the form the (n) large [N] are infelicitous when the gap between (1) the smallest element of the designated set S (henceforth, s?) and (2) the largest N smaller than all elements of S (henceforth, s+) is small in comparison with the other gaps (Thorisson 1994; Funakoshi et al 2004). If this gap is so small as to make the difference between the sizes of s? and s+ impossible to perceive, then the expression is also infelicitous. Dichotomy. When separating one single referent from one distractor, the comparative form is often said to be favored (Use the comparative form to compare two things). We expected this to generalize to situations where all the referents are of one size, and all the distractors of another. Minimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English, where the 
o perceive, then the expression is also infelicitous. Dichotomy. When separating one single referent from one distractor, the comparative form is often said to be favored (Use the comparative form to compare two things). We expected this to generalize to situations where all the referents are of one size, and all the distractors of another. Minimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English, where the base form is morphologically simpler than the other two, this rule could be argued to follow from Gricean principles (Grice 1975). To keep matters simple, linguistic realization could choose the base form if and only if the gap between s? and s+ surpasses a certain value, which is specified interactively by the user. (This approach was chosen for the VAGUE program.) As for the presence/absence of the numeral in the description, there appear to be different ?believable? patterns of linguistic behavior. A cautious generator might only omit the numeral when the pragmatic principles happen to enforce a specific extension (e.g., the large mice, when the mice are sized 3, 2.8, 2.499, and 2.498 cm). This would allow the genera
en to enforce a specific extension (e.g., the large mice, when the mice are sized 3, 2.8, 2.499, and 2.498 cm). This would allow the generator to use vague expressions, but only where they result in a description that is itself unambiguous. We shall see in Section 7 that it has not been easy to confirm the pragmatic constraints of the present section experimentally. 208 van Deemter GRE with Gradable Properties 6. Linguistic Realization Some recent GRE algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead (Stone and Webber 1998; Krahmer and Theune 2002). We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = ?size > 3 cm, size < 9 cm?. If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm). Clearly, sophisticated use of gradable adjectives requires a separation bet
c extension (e.g., the large mice, when the mice are sized 3, 2.8, 2.499, and 2.498 cm). This would allow the generator to use vague expressions, but only where they result in a description that is itself unambiguous. We shall see in Section 7 that it has not been easy to confirm the pragmatic constraints of the present section experimentally. 208 van Deemter GRE with Gradable Properties 6. Linguistic Realization Some recent GRE algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead (Stone and Webber 1998; Krahmer and Theune 2002). We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = ?size > 3 cm, size < 9 cm?. If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm). Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic rea
ent, logical aspect of referring expressions generation from its language-dependent, linguistic aspect. Our algorithm suggests a distinction into three phases, the first two of which can be thought of as part of CD: 1. CD proper, that is, the production of a distinguishing list of properties L; 2. An inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives? relative position. Interestingly, vague properties tend to be realized before others. Quirk et al (1985), for example, report that ?adjectives denoting size, length, and height normally precede other nonderived adjectives? (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small(-est) [N], for exampl
 expressions generation from its language-dependent, linguistic aspect. Our algorithm suggests a distinction into three phases, the first two of which can be thought of as part of CD: 1. CD proper, that is, the production of a distinguishing list of properties L; 2. An inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives? relative position. Interestingly, vague properties tend to be realized before others. Quirk et al (1985), for example, report that ?adjectives denoting size, length, and height normally precede other nonderived adjectives? (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small(-est) [N], for example, the words p
inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives? relative position. Interestingly, vague properties tend to be realized before others. Quirk et al (1985), for example, report that ?adjectives denoting size, length, and height normally precede other nonderived adjectives? (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small(-est) [N], for example, the words preceding N select the three smallest elements of [N]. It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables. The latter would mean something else, nam
estions: (1) When is it natural to generate a vague description (i.e., a 209 Computational Linguistics Volume 32, Number 2 qualitative description as opposed to a purely quantitative one)? (2) Given that a vague description is used, which form of the description is most natural? and (3) Are the generated descriptions properly understood by hearers and readers? Much is unknown, but we shall summarize the available results in these three areas very briefly, referring readers to the literature for details. 7.1 Human Speakers? Use of Vague Descriptions Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication. We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. Dale and Reiter (1995), for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner). They found that, while instructional texts tended to use numerical descriptions lik
aders? Much is unknown, but we shall summarize the available results in these three areas very briefly, referring readers to the literature for details. 7.1 Human Speakers? Use of Vague Descriptions Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication. We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. Dale and Reiter (1995), for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner). They found that, while instructional texts tended to use numerical descriptions like the 3 14 ? bolt, human assemblers ?unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt? (Dale and Reiter 1995).6 Our own experiments (van Deemter 2004) point in the same direction. In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each 
timation of their absolute values. Further experiments are needed before we can say with more confidence under what circumstances vague descriptions are favored over absolute ones. It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input. Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. 6 Presumably, Beun and Cremers (1998) found vague adjectives to be rare because, in their experiments, referents could always be identified using nongradable dimensions. 210 van Deemter GRE with Gradable Properties 7.2 Testing the Correctness of the Generated Expressions Sedivy et al (1999) asked subjects to identify the target of a vague description in a visual scene. Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pi
the kind of numerical representations that our algorithm has used as input. Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. 6 Presumably, Beun and Cremers (1998) found vague adjectives to be rare because, in their experiments, referents could always be identified using nongradable dimensions. 210 van Deemter GRE with Gradable Properties 7.2 Testing the Correctness of the Generated Expressions Sedivy et al (1999) asked subjects to identify the target of a vague description in a visual scene. Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key). Across the different conditions under which the experiment was done (e
ng ??The two high numbers appear in brackets??? The outcomes of the experiment suggested that readers understand plural vague descriptions in accordance with the semantics of Section 2 (van Deemter 2000). In other words, they judged the description to be correct if and only if the two highest numbers in the sequence appeared in brackets. Assessing the evidence, it seems that vague descriptions are largely unproblematic from the point of view of interpretation. 7.3 Testing the Felicity of the Generated Expressions How can we choose between the different forms that a vague description can take? Reiter and Sripada (2002) showed that the variation in corpora based on expert authors 211 Computational Linguistics Volume 32, Number 2 can be considerable, especially in their use of vague expressions (e.g., by evening, by late evening, around midnight). We confirmed these findings using experiments with human subjects (van Deemter 2004), focusing on the choice between the different forms of the adjective. Informally: 1. The dichotomy constraint of Section 5 did not hold up well: Even when comparing two things, the superlative form was often preferred over the comparative. 2. When base forms were used, the gap was a
r example, one might use the superlative all the time, since this was?surprisingly?the most frequent form overall. Based on point (2), however, one might also defend using the base form whenever the gap is large enough (as was done in the VAGUE program). Future experiments should allow us to refine this position, perhaps depending on factors such as genre, communicative goal, and type of audience. 8. Incrementality: Help or Hindrance? The account sketched in Section 4 was superimposed on an incremental GRE algorithm, partly because incrementality is well established in this area (Appelt 1985; Dale and Reiter 1995). But IA may be replaced by any other reasonable7 GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always ?greedily? selects the property that removes the maximum number of distractors. Let G be any such GRE algorithm, then we can proceed as follows: GRE for Vague Descriptions (version not relying on IA): 1. Construct KB using Attributes and Values, assigning numerical Values to gradable Attributes. 2. Recompile the KB, replacing equalities by inequalities. 3. Let G deliver an unordered set of properties which jointly distinguish the
ry preparation for (5) because the superlative properties resulting from (5), unlike the inequalities resulting from (4), are context dependent. For example, ?mouse, size(x) = max2? (the largest two mice, {c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two elements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the short(est) black mouse if there is only one black mouse, because this might invite false implicatures. 8.1 Problems with Incrementality While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates. (Wildly redundant descriptions can result if the ?wrong? preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same ord
ause the superlative properties resulting from (5), unlike the inequalities resulting from (4), are context dependent. For example, ?mouse, size(x) = max2? (the largest two mice, {c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two elements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the short(est) black mouse if there is only one black mouse, because this might invite false implicatures. 8.1 Problems with Incrementality While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates. (Wildly redundant descriptions can result if the ?wrong? preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which t
rlative properties resulting from (5), unlike the inequalities resulting from (4), are context dependent. For example, ?mouse, size(x) = max2? (the largest two mice, {c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two elements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the short(est) black mouse if there is only one black mouse, because this might invite false implicatures. 8.1 Problems with Incrementality While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates. (Wildly redundant descriptions can result if the ?wrong? preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were select
ies resulting from (5), unlike the inequalities resulting from (4), are context dependent. For example, ?mouse, size(x) = max2? (the largest two mice, {c3, c4}) does not equal ?size(x) = max2, mouse? (the mouse among the largest two elements, {c4}). Deletion of superfluous inequalities avoids saying, for example, the short(est) black mouse if there is only one black mouse, because this might invite false implicatures. 8.1 Problems with Incrementality While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976; Levelt 1989; Pechmann 1989; Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates. (Wildly redundant descriptions can result if the ?wrong? preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected? This question is
emental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected? This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989). A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known. Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said. 8.2 Low Preference for Gradable Properties? It has been argued that, in an incremental 
ty, since an adjective in a vague description can only be fully interpreted when its comparison set is known. Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said. 8.2 Low Preference for Gradable Properties? It has been argued that, in an incremental approach, gradable properties should be given a low preference ranking because they are difficult to process (Krahmer and Theune 2002). We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions. Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes). Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking. Let us see how things would work if they were ranked more highly. Suppo
t word this as the two heaviest brown ones among the smallest four mice. To avoid such awkward expressions, one can change the order of properties after CD (mirroring step 4 above), moving the inequalities to the end of the list before they are transformed into the appropriate superlatives. The effect would be to boost the number of occurrences of gradable properties in generated descriptions while keeping CD incremental. 9. Extensions of the Approach 9.1 Relational Descriptions Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2): Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed. Note that this is felicitous even if the shed is not the largest one i
re transformed into the appropriate superlatives. The effect would be to boost the number of occurrences of gradable properties in generated descriptions while keeping CD incremental. 9. Extensions of the Approach 9.1 Relational Descriptions Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2): Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed. Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b): type(d1) = type(d2) = dog type(c) = cat type(s1) = type(s2) = type(s3) = shed size(d1) = size(d2) = size(c) = 1m size(s1) = 3m size(s2) =
, to say the dog in the largest shed. 9.2 Boolean Combinations Generalizations to complex Boolean descriptions involving negation and disjunction (van Deemter 2004) appear to be largely straightforward, except for issues to do with 214 van Deemter GRE with Gradable Properties opposites and markedness. For example, the generator will have to decide whether to say the patients that are old or the patients that are not young. 9.3 Multidimensionality 9.3.1 Combinations of Adjectives. When objects are compared in terms of several dimensions, these dimensions can be weighed in different ways (e.g., Rasmusen 1989). Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: height(a) = 5 m height(b) = height(c) = 15 m width(a) = width(b) = 3 m width(c) = 2 m Cases like this would be covered if the decision-theoretic property of
es to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: height(a) = 5 m height(b) = height(c) = 15 m width(a) = width(b) = 3 m width(c) = 2 m Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r ? C has a Pareto-optimal combination of Values V iff there is no other x ? C such that 1. ?Vi ? V : Vi(x) > Vi(r) and 2. ??Vj ? V : Vj(x) < Vj(r) In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall
he only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan, for example, would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors (Nash 1950; cf. Gorniak and Roy 2003; Thorisson 1994, for other plans). 9.3.2 Multidimensional Adjectives (and Color). Multidimensionality can also slip in through the backdoor. Consider big, for example, when applied to 3D shapes. If there exists a formula for mapping three dimensions into one (e.g., length ? width ? height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and
that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan, for example, would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors (Nash 1950; cf. Gorniak and Roy 2003; Thorisson 1994, for other plans). 9.3.2 Multidimensional Adjectives (and Color). Multidimensionality can also slip in through the backdoor. Consider big, for example, when applied to 3D shapes. If there exists a formula for mapping three dimensions into one (e.g., length ? width ? height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situ
mal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan, for example, would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors (Nash 1950; cf. Gorniak and Roy 2003; Thorisson 1994, for other plans). 9.3.2 Multidimensional Adjectives (and Color). Multidimensionality can also slip in through the backdoor. Consider big, for example, when applied to 3D shapes. If there exists a formula for mapping three dimensions into one (e.g., length ? width ? height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. Similar t
 Color). Multidimensionality can also slip in through the backdoor. Consider big, for example, when applied to 3D shapes. If there exists a formula for mapping three dimensions into one (e.g., length ? width ? height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. Similar things hold for multifaceted properties like intelligence (Kamp 1975). Color terms are a case apart. If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue). This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly 215 Computational Linguistics Volume 32, Number 2 straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.) A further complication is that differ
 greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue). This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly 215 Computational Linguistics Volume 32, Number 2 straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.) A further complication is that different speakers can regard very different values as prototypical, making it difficult to assess which of two objects is greener even on one dimension (Berlin and Kay 1969, pages 10?12). (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair.) Different attitudes towards multidimensionality are possible. One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense. In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values. Alternatively, one could allow referring expressions to be ambiguous. 
ty are possible. One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense. In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values. Alternatively, one could allow referring expressions to be ambiguous. It would be consistent with this attitude, for example, to map multiple dimensions into one overall dimension, perhaps by borrowing from principles applied in perceptual grouping, where different perceptual dimensions are mapped into one (e.g., Thorisson 1994). The empirical basis of this line of work, however, is still somewhat weak, so the risk of referential unclarity looms large. Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing. 9.4 Salience as a Gradable Property We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions. As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are ac
 still somewhat weak, so the risk of referential unclarity looms large. Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing. 9.4 Salience as a Gradable Property We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions. As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous. 9.4.1 A New Perspective on Salience. Krahmer and Theune (2002) have argued that Dale and Reiter?s (1995) dichotomy between salient and nonsalient objects (where the objects in the domain are the salient ones) should be replaced by an account that takes degrees of salience into account: No object can be too unsalient to be referred to, as long as the right properties are available. In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice
 degrees of salience (i.e., the closer to the center of pointing, the higher the value for the attribute SALIENCE) in the style of Section 9.2. 218 van Deemter GRE with Gradable Properties 10. Conclusion If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle, this might be done by providing the generator with vague input?in which case no special algorithms are needed?but suitably contextualized vague input is often not available (Mellish 2000). The only practical alternative is to provide the generator with ?crisp? (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE. Far from being a peculiarity of a few adjectives, vagueness is widespread. We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example,  color terms (Section 9.3);  nouns that allow different degrees of strictness (
 so on, each of which may single out the same individuals. Section 7.3 has summarized some experimental evidence related to such choices, focusing on the different forms of the adjective, but the evidence is far from conclusive. Much is still unknown, differences between speakers abound, and the experimental methodology for advancing the state of the art in this area is not without its problems (van Deemter 2004). Architecture (Section 6). The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model (e.g., Reiter and Dale 2000). An example of such an inference rule is the one that transforms a list of the form ?mouse, >10 cm? into one of the form ?mouse, size(x) = max2? if only two mice are larger than 10 cm. The same issues also make it difficult to interleave CD and linguistic realization as proposed by various authors, because properties may need to be combined before they are expressed. Incrementality (Section 8). Gradable adjectives complicate the notion of incrementality, in generation as well as interpretation. Focusing on generation, for example, they force us to reexamine the idea that properties can be put
g that a generator would ideally be aware of. Multidimensionality (Section 9.3). We know roughly how to deal with one gradable dimension: the short man, for example, is the shortest man around. But in practice, we often juggle several dimensions. This happens, for example, when two adjectives are used (the short thin man), or when salience is taken into account (e.g., the short man, when the shortest man is not the most salient one), threatening to make irrefutably distinguishing descriptions something of an exception. (For a study of approaches to multidimensionality in a different area, see Masthoff 2004.) At some point, GRE may have to abandon the strategy of aiming for unambiguous descriptions in all situations. Acknowledgments I thank Hua Cheng, Roger Evans, Albert Gatt, Markus Guhe, Imtiaz Khan, Emiel Krahmer, Judith Masthoff, Chris Mellish, Oystein Nilsen, Manfred Pinkal, Paul Piwek, Ehud Reiter, Graeme Ritchie, Ielka van der Sluis, Rosemary Stevenson, Matthew Stone, and Sebastian Varges for helpful comments. I am especially grateful to Richard Power, for inspiration as well as for implementing the VAGUE program at great speed. Thanks are due to four anonymous reviewers for some very sub
