<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000972">
<title confidence="0.9996825">
Understanding Differences in Perceived Peer-Review Helpfulness using
Natural Language Processing
</title>
<author confidence="0.998411">
Wenting Xiong
</author>
<affiliation confidence="0.999787">
University of Pittsburgh
Department of Computer Science
</affiliation>
<address confidence="0.772685">
Pittsburgh, PA, 15260
</address>
<email confidence="0.997878">
wex12@cs.pitt.edu
</email>
<author confidence="0.995677">
Diane Litman
</author>
<affiliation confidence="0.908362">
University of Pittsburgh
Department of Computer Science &amp;
Learning Research and Development Center
</affiliation>
<address confidence="0.803084">
Pittsburgh, PA, 15260
</address>
<email confidence="0.999149">
litman@cs.pitt.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999485647058823">
Identifying peer-review helpfulness is an im-
portant task for improving the quality of feed-
back received by students, as well as for help-
ing students write better reviews. As we tailor
standard product review analysis techniques to
our peer-review domain, we notice that peer-
review helpfulness differs not only between
students and experts but also between types
of experts. In this paper, we investigate how
different types of perceived helpfulness might
influence the utility of features for automatic
prediction. Our feature selection results show
that certain low-level linguistic features are
more useful for predicting student perceived
helpfulness, while high-level cognitive con-
structs are more effective in modeling experts’
perceived helpfulness.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999265">
Peer review of writing is a commonly recommended
technique to include in good writing instruction. It
not only provides more feedback compared to what
students might get from their instructors, but also
provides opportunities for students to practice writ-
ing helpful reviews. While existing web-based peer-
review systems facilitate peer review from the logis-
tic aspect (e.g. collecting papers from authors, as-
signing reviewers, and sending reviews back), there
still remains the problem that the quality of peer
reviews varies, and potentially good feedback is
not written in a helpful way. To address this is-
sue, we propose to add a peer-review helpfulness
model to current peer-review systems, to automat-
</bodyText>
<page confidence="0.968807">
10
</page>
<bodyText confidence="0.999947931034483">
ically predict peer-review helpfulness based on fea-
tures mined from textual reviews using Natural Lan-
guage Processing (NLP) techniques. Such an intel-
ligent component could enable peer-review systems
to 1) control the quality of peer reviews that are sent
back to authors, so authors can focus on the help-
ful ones; and 2) provide feedback to reviewers with
respect to their reviewing performance, so students
can learn to write better reviews.
In our prior work (Xiong and Litman, 2011), we
examined whether techniques used for predicting the
helpfulness of product reviews (Kim et al., 2006)
could be tailored to our peer-review domain, where
the definition of helpfulness is largely influenced by
the educational context of peer review. While previ-
ously we used the average of two expert-provided
ratings as our gold standard of peer-review help-
fulness1, there are other types of helpfulness rating
(e.g. author perceived helpfulness) that could be the
gold standard, and that could potentially impact the
features used to build the helpfulness model. In fact,
we observe that peer-review helpfulness seems to
differ not only between students and experts (exam-
ple 1), but also between types of experts (example
2).
In the following examples, students judge helpful-
ness with discrete ratings from one to seven; experts
judge it using a one to five scale. Higher ratings on
both scales correspond to the most helpful reviews.
</bodyText>
<sectionHeader confidence="0.775227" genericHeader="introduction">
Example 1:
</sectionHeader>
<bodyText confidence="0.7318">
Student rating = 7, Average expert rating = 2 The
</bodyText>
<footnote confidence="0.989385">
1Averaged ratings are considered more reliable since they
are less noisy.
</footnote>
<note confidence="0.8884405">
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10–19,
Portland, Oregon, 24 June 2011. c�2011 Association for Computational Linguistics
author also has great logic in this paper. How can
we consider the United States a great democracy
when everyone is not treated equal. All of the main
points were indeed supported in this piece.
</note>
<bodyText confidence="0.8307505">
Student rating = 3, Average expert rating = 5 I
thought there were some good opportunities to
provide further data to strengthen your argument.
For example the statement “These methods of
intimidation, and the lack of military force offered
by the government to stop the KKK, led to the
rescinding of African American democracy.”
Maybe here include data about how ... (126 words)
</bodyText>
<sectionHeader confidence="0.548571" genericHeader="method">
Example 2:
</sectionHeader>
<construct confidence="0.985469125">
Writing-expert rating = 2, Content-expert rating = 5
Your over all arguements were organized in some
order but was unclear due to the lack of thesis in
the paper. Inside each arguement, there was no
order to the ideas presented, they went back and
forth between ideas. There was good support to
the arguements but yet some of it didnt not fit your
arguement.
Writing-expert rating = 5, Content-expert rating = 2
First off, it seems that you have difficulty writing
transitions between paragraphs. It seems that you
end your paragraphs with the main idea of each
paragraph. That being said, ... (173 words) As a
final comment, try to continually move your paper,
that is, have in your mind a logical flow with every
paragraph having a purpose.
</construct>
<bodyText confidence="0.99995605882353">
To better understand such differences and inves-
tigate their impact on automatically assessing peer-
review helpfulness, in this paper, we compare help-
fulness predictions using our many different pos-
sibilities for gold standard ratings. In particular,
we compare the predictive ability of features across
gold standard ratings by examining the most use-
ful features and feature ranks using standard feature
selection techniques. We show that paper ratings
and lexicon categories that suggest clear transitions
and opinions are most useful in predicting helpful-
ness as perceived by students, while review length
is generally effective in predicting expert helpful-
ness. While the presence of praise and summary
comments are more effective in modeling writing-
expert helpfulness, providing solutions is more use-
ful in predicting content-expert helpfulness.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="method">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999992456521739">
To our knowledge, no prior work on peer review
from the NLP community has attempted to auto-
matically predict peer-review helpfulness. Instead,
the NLP community has focused on issues such as
highlighting key sentences in papers (Sandor and
Vorndran, 2009), detecting important feedback fea-
tures in reviews (Cho, 2008; Xiong and Litman,
2010), and adapting peer-review assignment (Gar-
cia, 2010). However, many NLP studies have been
done on the helpfulness of other types of reviews,
such as product reviews (Kim et al., 2006; Ghose
and Ipeirotis, 2010), movie reviews (Liu et al.,
2008), book reviews (Tsur and Rappoport, 2009),
etc. Kim et al. (2006) used regression to predict the
helpfulness ranking of product reviews based on var-
ious classes of linguistic features. Ghose and Ipeiro-
tis (2010) further examined the socio-economic im-
pact of product reviews using a similar approach
and suggested the usefulness of subjectivity analy-
sis. Another study (Liu et al., 2008) of movie re-
views showed that helpfulness depends on review-
ers’ expertise, their writing style, and the timeliness
of the review. Tsur and Rappoport (2009) proposed
RevRank to select the most helpful book reviews in
an unsupervised fashion based on review lexicons.
To tailor the utility of this prior work on help-
fulness prediction to educational peer reviews, we
will draw upon research on peer review in cognitive
science. One empirical study of the nature of peer-
review feedback (Nelson and Schunn, 2009) found
that feedback implementation likelihood is signif-
icantly correlated with five feedback features. Of
these features, problem localization —pinpointing
the source of the problem and/or solution in the orig-
inal paper— and solution —providing a solution to
the observed problem— were found to be most im-
portant. Researchers (Cho, 2008; Xiong and Lit-
man, 2010) have already shown that some of these
constructs can be automatically learned from tex-
tual input using Machine Learning and NLP tech-
niques. In addition to investigating what proper-
ties of textual comments make peer-review helpful,
researchers also examined how the comments pro-
duced by students versus by different types of ex-
perts differ (Patchan et al., 2009). Though focusing
on differences between what students and experts
</bodyText>
<page confidence="0.998558">
11
</page>
<bodyText confidence="0.999829636363636">
produce, such work sheds light on our study of stu-
dents’ and experts’ helpfulness ratings of the same
student comments (i.e. what students and experts
value).
Our work in peer-review helpfulness prediction
integrates the NLP techniques and cognitive-science
approaches mentioned above. We will particularly
focus on examining the utility of features motivated
by related work from both areas, with respect to dif-
ferent types of gold standard ratings of peer-review
helpfulness for automatic prediction.
</bodyText>
<sectionHeader confidence="0.99616" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999502066666667">
In this study, we use a previously annotated peer-
review corpus (Nelson and Schunn, 2009; Patchan
et al., 2009) that was collected in an introduc-
tory college history class using the freely available
web-based peer-review SWoRD (Scaffolded Writ-
ing and Rewriting in the Discipline) system (Cho
and Schunn, 2007). The corpus consists of 16 pa-
pers (about six pages each) and 189 reviews (vary-
ing from twenty words to about two hundred words)
accompanied by numeric ratings of the papers. Each
review was manually segmented into idea units (de-
fined as contiguous feedback referring to a single
topic) (Nelson and Schunn, 2009), and these idea
units were then annotated by two independent an-
notators for various coding categories, such as feed-
back type (praise, problem, and summary), problem
localization, solution, etc. For example, the sec-
ond case in Example 1, which only has one idea
unit, was annotated as feedbackType = problem,
problemlocalization = True, and solution =
True. The agreement (Kappa) between the two an-
notators is 0.92 for FeedbackType, 0.69 for localiza-
tion, and 0.89 for solution.2
Our corpus also contains author provided back
evaluations. At the end of the peer-review assign-
ment, students were asked to provide back evalu-
ation on each review that they received by rating
review helpfulness using a discrete scale from one
to seven. After the corpus was collected, one writ-
2For Kappa value interpretation, Landis and Koch (1977)
propose the following agreement standard: 0.21-0.40 = “Fair”;
0.41-0.60 = “Moderate”; 0.61-0.80 = “Substantial”; 0.81-1.00
= “Almost Perfect”. Thus, while localization signals are more
difficult to annotate, the inter-annotator agreement is still sub-
stantial.
ing expert and one content expert were also asked to
rate review helpfulness with a slightly different scale
from one to five. For our study, we will also com-
pute the average ratings given by the two experts,
yielding four types of possible gold-standard ratings
of peer-review helpfulness for each review. Figure 1
shows the rating distribution of each type. Interest-
ingly, we observed that expert ratings roughly follow
a normal distribution, while students are more likely
to give higher ratings (as illustrated in Figure 1).
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999960142857143">
Our features are motivated by the prior work in-
troduced in Section 2, in particular, NLP work on
predicting product-review helpfulness (Kim et al.,
2006), as well as work on automatically learning
cognitive-science constructs (Nelson and Schunn,
2009) using NLP (Cho, 2008; Xiong and Litman,
2010). The complete list of features is shown in Ta-
ble 3 and described below. The computational lin-
guistic features are automatically extracted based
on the output of syntactic analysis of reviews and
papers3. These features represent structural, lexi-
cal, syntactic and semantic information of the tex-
tual content, and also include information for identi-
fying certain important cognitive constructs:
</bodyText>
<listItem confidence="0.999652588235294">
• Structural features consider the general struc-
ture of reviews, which includes review length in
terms of tokens (reviewLength), number of sen-
tences (sentNum), the average sentence length
(sentLengthAve), percentage of sentences that
end with question marks (question%), and
number of exclamatory sentences (exclams).
• Lexical features are counts of ten lexical cat-
egories (Table 1), where the categories were
learned in a semi-supervised way from review
lexicons in a pilot study. We first manually cre-
ated a list of words that were specified as signal
words for annotating feedbackType and prob-
lem localization in the coding manual; then
we supplemented the list with words selected
by a decision tree model learned using a Bag-
of-Words representation of the peer reviews.
</listItem>
<footnote confidence="0.936523">
3We used MSTParser (McDonald et al., 2005) for syntactic
analysis.
</footnote>
<page confidence="0.995895">
12
</page>
<figureCaption confidence="0.999977">
Figure 1: Distribution of peer-review helpfulness when rated by students and experts
</figureCaption>
<bodyText confidence="0.976363909090909">
Tag Meaning Word list
SUG suggestion should, must, might, could, need, needs, maybe, try, revision, want
LOC location page, paragraph, sentence
ERR problem error, mistakes, typo, problem, difficulties, conclusion
IDE idea verb consider, mention
LNK transition however, but
NEG negative words fail, hard, difficult, bad, short, little, bit, poor, few, unclear, only, more
POS positive words great, good, well, clearly, easily, effective, effectively, helpful, very
SUM summarization main, overall, also, how, job
NOT negation not, doesn’t, don’t
SOL solution revision specify correction
</bodyText>
<tableCaption confidence="0.996267">
Table 1: Ten lexical categories
</tableCaption>
<bodyText confidence="0.997821">
Compared with commonly used lexical uni-
grams and bigrams (Kim et al., 2006), these
lexical categories are equally useful in model-
ing peer-review helpfulness, and significantly
reduce the feature space.4
</bodyText>
<listItem confidence="0.9997485">
• Syntactic features mainly focus on nouns and
verbs, and include percentage of tokens that are
nouns, verbs, verbs conjugated in the first per-
son (1stPUerb%), adjectives/adverbs, and open
classes, respectively.
• Semantic features capture two important peer-
</listItem>
<footnote confidence="0.7677835">
4Lexical categories help avoid the risk of over-fitting, given
only 189 peer reviews in our case compared to more than ten
thousand Amazon.com reviews used for predicting product re-
view helpfulness (Kim et al., 2006).
</footnote>
<bodyText confidence="0.999698923076923">
review properties: their relevance to the main
topics in students’ papers, and their opinion
sentiment polarities. Kim et al. (2006) ex-
tracted product property keywords from exter-
nal resources based on their hypothesis that
helpful product reviews refer frequently to cer-
tain product properties. Similarly, we hypothe-
size that helpful peer reviews are closely related
to domain topics that are shared by all students
papers in an assignment. Our domain topic set
contains 288 words extracted from the collec-
tion of student papers using topic-lexicon ex-
traction software5; our feature (domainWord)
</bodyText>
<footnote confidence="0.974642">
5The software extracts topic words based on topic signa-
tures (Lin and Hovy, 2000), and was kindly provided by Annie
Louis.
</footnote>
<page confidence="0.989847">
13
</page>
<table confidence="0.504416666666667">
Feature Description
regTag% The percentage of problems in reviews that could be matched with a localization pattern.
soDomain% The percentage of sentences where any domain word appears between the subject and the object.
dDeterminer The number of demonstrative determiners.
windowSize For each review sentence, we search for the most likely referred window of words in the related
paper, and windowSize is the average number of words of all windows.
</table>
<tableCaption confidence="0.980397">
Table 2: Localization features
</tableCaption>
<bodyText confidence="0.998815">
counts how many words of a given review be-
long to the extracted set. For sentiment po-
larities, we extract positive and negative sen-
timent words from the General Inquirer Dictio-
naries 6, and count their appearance in reviews
in terms of their sentiment polarity (posWord,
negWord).
</bodyText>
<listItem confidence="0.634099875">
• Localization features are motivated by lin-
guistic features that are used for automatically
predicting problem localization (an important
cognitive construct for feedback understand-
ing and implementation) (Nelson and Schunn,
2009), and are presented in Table 2. To illus-
trate how these features are computed, consider
the following critique:
</listItem>
<bodyText confidence="0.99972615">
The section of the essay on African
Americans needs more careful at-
tention to the timing and reasons
for the federal governments decision
to stop protecting African American
civil and political rights.
The review has only one sentence, in which one
regular expression is matched with “the section
of” thus regTag% = 1; no demonstrative de-
terminer, thus dDeterminer = 0; “African”
and “Americans” are domain words appearing
between the subject “section” and the object
“attention”, so soDomain is true for this sen-
tence and thus soDomain% = 1 for the given
review.
In addition to the low-level linguistic features pre-
sented above, we also examined non-linguistic fea-
tures that are derived from the ratings and prior
manual annotations of the corpus, described in Sec-
tion 3.
</bodyText>
<footnote confidence="0.980392">
6http://www.wjh.harvard.edu/ inquirer/homecat.htm
</footnote>
<listItem confidence="0.99239">
• Cognitive-science features are motivated by
</listItem>
<bodyText confidence="0.808571047619048">
an empirical study (Nelson and Schunn, 2009)
which suggests significant correlation between
certain cognitive constructs (e.g. feedbackType,
problem localization, solution) and review im-
plementation likelihood. Intuitively, helpful
reviews are more likely to get implemented,
thus we introduced these features to capture
desirable high-level characteristics of peer re-
views. Note that in our corpus these cogni-
tive constructs are manually coded at the idea-
unit level (Nelson and Schunn, 2009), how-
ever, peer-review helpfulness is rated at the re-
view level.7 Our cognitive-science features ag-
gregate the annotations up to the review-level
by reporting the percentage of idea-units in
a review that exhibit each characteristic: the
distribution of review types (praise%, prob-
lem%, summary%), the percentage of problem-
localized critiques (localization%), as well as
the percentage of solution-provided ones (solu-
tion%).
</bodyText>
<listItem confidence="0.998130538461538">
• Social-science features introduce elements re-
flecting interactions between students in a peer-
review assignment. As suggested in related
work on product review helpfulness (Kim
et al., 2006; Danescu-Niculescu-Mizil et al.,
2009), some social dimensions (e.g. customer
opinion on related product quality) are of great
influence in the perceived helpfulness of prod-
uct reviews. Similarly, in our case, we intro-
duced related paper ratings (pRating) — to con-
sider whether and how helpfulness ratings are
affected by the rating that the paper receives8
— and the absolute difference between the rat-
</listItem>
<footnote confidence="0.9232285">
7Details of different granularity levels of annotation can be
found in (Nelson and Schunn, 2009).
8That is, to examine whether students give higher ratings to
peers who gave them higher paper ratings in the first place.
</footnote>
<page confidence="0.998565">
14
</page>
<bodyText confidence="0.999974333333333">
ing and the average score given by all review-
ers (pRatingDiff) — to measure the variation in
perceived helpfulness of a given review.
</bodyText>
<sectionHeader confidence="0.998731" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.992417339285715">
We take a machine learning approach to model dif-
ferent types of perceived helpfulness (student help-
fulness, writing-expert helpfulness, content-expert
helpfulness, average-expert helpfulness) based on
combinations of linguistic and non-linguistic fea-
tures extracted from our peer-review corpus. Then
we compare the different helpfulness types in terms
of the predictive power of features used in their cor-
responding models. For comparison purpose, we
consider the linguistic and non-linguistic features
both separately and in combination, which generates
three set of features: 1) linguistic features, 2) non-
linguistic features, and 3) all features. For each set
of features, we train four models, each correspond-
ing to a different kind of helpfulness rating. For each
learning task (three by four), we use two standard
feature selection algorithms to find the most useful
features based on 10-fold cross validation. First, we
perform Linear Regression with Greedy Stepwise
search (stepwise LR) to select the most useful fea-
tures when testing in each of the ten folds, and count
how many times each features is selected in the ten
trials. Second, we use Relief Feature Evaluation9
with Ranker (Relief) (Kira and Rendell, 1992; Wit-
ten and Frank, 2005) to rank all used features based
on their average merits (the ability of the given fea-
ture to differentiate between two example pairs) of
ten trials.10
Although both methods are supervised, the wrap-
per is “more aggressive” because its feature evalu-
ation is based on the performance of the regression
model and thus the resulting feature set is tailored
to the learning algorithm. In contrast, Relief does
not optimize feature sets directly for classifier per-
formance, thus it takes into account class informa-
tion in a “less aggressive” manner than the Wrapper
method. We use both methods in our experiment to
9Relief evaluates the worth of an attribute by repeatedly
sampling an instance and changing the value of the given at-
tribute based on the nearest instance of the same and different
class.
10Both algorithms are provided by Weka
(http://www.cs.waikato.ac.nz/ml/weka/).
provide complementary perspectives. While the for-
mer can directly tell us what features are most use-
ful, the latter gives feature ranks which provide more
detailed information about differences between fea-
tures. To compare the feature selection results, we
examine the four kind of helpfulness models for
each of the three feature sets separately, as presented
below. Note that the focus of this paper is compar-
ing feature utilities in different helpfulness models
rather than predicting those types of helpfulness rat-
ings. (Details of how the average-expert model per-
forms can be found in our prior work (Xiong and
Litman, 2011).)
</bodyText>
<subsectionHeader confidence="0.977027">
5.1 Feature Selection of Linguistic Features
</subsectionHeader>
<bodyText confidence="0.999993606060606">
Table 4 presents the feature selection results of com-
putational linguistic features used in modeling the
four different types of peer-review helpfulness. The
first row lists the four sources of helpfulness ratings,
and each column represents a corresponding model.
The second row presents the most useful features
in each model selected by stepwise LR, where “#
of folds” refers to the number of trials in which the
given feature appears in the resulting feature set dur-
ing the 10-fold cross validation. Here we only report
features that are selected by no less than five folds
(half the time). The third row presents feature ranks
computed using Relief, where we only report the top
six features due to the space limit. Features are or-
dered in descending ranks, and the average merit and
its standard deviation is reported for each one of the
features.
The selection result of stepwise LR shows that
reviewLength is most useful for predicting expert
helpfulness in general, while specific lexicon cate-
gories (i.e. LNK, and NOT) and positive words (pos-
Word) are more useful in predicting student helpful-
ness. When looking at the ranking result, we observe
that transition cues (LNK) and posWord are also
ranked high in the student-helpfulness model, al-
though question% and suggestion words (SUG) are
ranked highest. For expert-helpfulness models, win-
dowSize and posWord, which are not listed in the se-
lected features for expert helpfulness (although they
are selected for students), are actually ranked high
for modeling average-expert helpfulness. While ex-
clamatory sentence number (exclams) and summa-
rization cues are ranked top for the writing expert,
</bodyText>
<page confidence="0.992194">
15
</page>
<table confidence="0.999449">
Type Features
Structural reviewLength, sentNum, sentLengthAve, question%, exclams
Lexical SUG, LOC, ERR, IDE, LNK, NEG, POS, SUM, NOT, SOL (Table 1)
Syntactic noun%, verb%, 1stPVerb%, adj+adv%, opClass%
Semantic domainWord, posWord, negWord
Localization regTag%, soDomain%, dDeterminer, windowSize (Table 2)
Cognitive-science praise%, problem%, summary%, localization%, solution%
Social-science pRating, pRatingDiff
</table>
<tableCaption confidence="0.985495">
Table 3: Summary of features
</tableCaption>
<table confidence="0.998922142857143">
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
LNK 9 reviewLength 8 reviewLength 10 reviewLength 10
Stepwise posWord 8 question% 6 sentNum 8
LR NOT 6 sentNum 5 question% 8
windowSize 6 1stPVerb% 5
POS 5
Feature Merit Feature Merit Feature Merit Feature Merit
question% .019 ± .002 exclams .010 ± .003 question% .010 ± .004 exclams .010 ± .003
Relief SUG .015 ± .003 SUM .008 ± .004 ERR .009 ± .003 question% .011 ± .004
LNK .014 ± .003 NEG .006 ± .004 SUG .009 ± .004 windowSize .008 ± .002
sentLengthAve .012 ± .003 negWord .005 ± .002 posWord .007 ± .002 posWord .006 ± .002
POS .011 ± .002 windowSize .004 ± .002 exclams .006 ± .001 reviewLength .004 ± .001
posWord .010 ± .001 sentNum .003 ± .001 1stPVerb% .007 ± .004 sentLengthAve .004 ± .001
</table>
<tableCaption confidence="0.99923">
Table 4: Feature selection based on linguistic features
</tableCaption>
<bodyText confidence="0.999923444444444">
the percentage of questions (question%) and error
cues (ERR) are ranked top for the content-expert. In
addition, the percentage of words that are verbs con-
jugated in the first person (1stPVerb%) is both se-
lected and ranked high in the content-expert helpful-
ness model. Out of the four models, SUG are ranked
high for predicting both students and content-expert
helpfulness. These observations indicate that both
students and experts value questions (question%)
and suggestions (SUG) in reviews, and students par-
ticularly favor clear signs of logic flow in review ar-
guments (LNK), positive words (posWord), as well
as reference of their paper content which provides
explicit context information (windowSize). In addi-
tion, experts in general prefer longer reviews (re-
viewLength), and the writing expert thinks clear
summary signs (SUM) are important indicators of
helpful peer reviews.
</bodyText>
<subsectionHeader confidence="0.997279">
5.2 Feature Selection of non-Linguistic
Features
</subsectionHeader>
<bodyText confidence="0.9999247">
When switching to the high-level non-linguistic fea-
tures (Table 5), we find that solution% is always se-
lected (in all ten trials) as a most useful feature for
predicting all four kind of helpfulness, and is also
ranked high for content-expert and student helpful-
ness. Especially for the content-expert, solution%
has a much higher merit (0.013) compared to all the
other features (G 0.03). This agrees with our ob-
servation in section 5.1 that SUG are ranked high in
both cases. localization% is selected as one of the
most useful features in the content-expert helpful-
ness model, which is also ranked top in the student
model (though not selected frequently by stepwise
LR). For modeling the writing-expert helpfulness,
praise (praise%) is more important than problem
and summary, and the paper rating (pRating) loses
its predictive power compared to how it works in the
other models. In contrast, pRating is both selected
and ranked high for predicting students’ perceived
helpfulness.
</bodyText>
<subsectionHeader confidence="0.997712">
5.3 Feature Selection of All Features
</subsectionHeader>
<bodyText confidence="0.9999422">
When considering all features together as reported
in Table 6, pRating is only selected in the student-
helpfulness model, and still remains to be the most
important feature for predicting students’ perceived
helpfulness. As for experts, the structural feature
</bodyText>
<page confidence="0.992496">
16
</page>
<table confidence="0.99660675">
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
Stepwise pRating 10 solution% 10 localization% 10 solution% 10
LR solution% 10 solution% 10 pRating 10
problem% 9 pRating 10 localization% 9
Feature Merit Feature Merit Feature Merit Feature Merit
localization% .012 ± .003 praise% .008 ± .002 solution% .013 ± .005 problem% .004 ± .002
Relief pRatingDiff .010 ± .002 problem% .007 ± .002 pRating .003 ± .002 localization% .004 ± .006
pRating .007 ± .002 summary% .001 ± .004 praise% .001 ± .002 praise% .003 ± .003
solution% .006 ± .005 localization% .001 ± .005 localization% .001 ± .004 solution% .002 ± .004
problem% .004 ± .002 pRating .004 ± .004 problem% .001 ± .002 pRating .005 ± .003
summary% .004 ± .003 pRatingDiff .007 ± .002 pRating .002 ± .003 pRatingDiff .006 ± .005
</table>
<tableCaption confidence="0.943601">
Table 5: Feature selection based on non-linguistic features
</tableCaption>
<table confidence="0.998869">
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
Stepwise pRating 10 reviewLength 10 reviewLength 10 reviewLength 10
LR dDeterminer 7 problem% 8 problem% 6
pRatingDiff 5
sentNum 5
Feature Merit Feature Merit Feature Merit Feature Merit
pRating .030 ± .006 exclams .016 ± .003 solution% .025 ± .003 exclams .015 ± .004
Relief NOT .019 ± .004 praise% .015 ± .003 domainWord .012 ± .002 question% .012 ± .004
pRatingDiff .019 ± .005 SUM .013 ± .004 regTag% .012 ± .007 LOC .007 ± .002
sentNum .014 ± .002 summary% .008 ± .003 reviewLength .009 ± .002 sentNum .007 ± .002
question% .014 ± .003 problem% .009 ± .003 question% .010 ± .003 reviewLength .007 ± .001
NEG .013 ± .002 reviewLength .004 ± .001 sentNum .008 ± .002 praise% .008 ± .004
</table>
<tableCaption confidence="0.997005">
Table 6: Feature selection based on all features
</tableCaption>
<bodyText confidence="0.999970842105263">
reviewLength stands out from all other features in
both the writing-expert and the content-expert mod-
els. Interestingly, it is the number of sentences (sent-
Num) rather than review length of structure features
that is useful in the student-helpfulness model. And
demonstrative determiners (dDeterminer) is also se-
lected, which indicates that having a clear sign of
comment targets is considered important from the
students’ perspective. When examining the model’s
ranking result, we find that more lexicon categories
are ranked high for students compared to other kind
of helpfulness. Specifically, NOT appears high
again, suggesting clear expression of opinion is im-
portant in predicting student-helpfulness. Across
four types of helpfulness, again, we observed that
the writing expert tends to value praise and summary
(indicated by both SUM and summary%) in reviews
while the content-expert favors critiques, especially
solution provided critiques.
</bodyText>
<subsectionHeader confidence="0.72611">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999963370370371">
Based on our observations from the above three
comparisons, we summarize our findings with re-
spect to different feature types and provide inter-
pretation: 1) review length (in tokens) is generally
effective in predicting expert perceived helpfulness,
while number of sentences is more useful in mod-
eling student perceived helpfulness. Interestingly,
there is a strong correlation between these two fea-
tures (r = 0.91, p ≤ 0.001), and why one is selected
over the other in different helpfulness models needs
further investigation. 2) Lexical categories such as
transition cues, negation, and suggestion words are
of more importance in modeling student perceived
helpfulness. This might indicate that students pre-
fer clear expression of problem, reference and even
opinion in terms of specific lexicon clues, the lack of
which is likely to result in difficulty in their under-
standing of the reviews. 3) As for cognitive-science
features, solution is generally an effective indica-
tor of helpful peer reviews. Within the three feed-
back types of peer reviews, praise is valued high
by the writing expert. (It is interesting to notice
that although praise is shown to be more impor-
tant than problem and summary for modeling the
writing-expert helpfulness, positive sentiment words
do not appear to be more predictive than negative
sentiments.) In contrast, problem is more desirable
</bodyText>
<page confidence="0.996737">
17
</page>
<bodyText confidence="0.999963533333333">
from the content expert’s point of view. Although
students assign less importance to the problem them-
selves, solution provided peer reviews could be help-
ful for them with respect to the learning goal of peer-
review assignments. 4) Paper rating is a very ef-
fective feature for predicting review helpfulness per-
ceived by students, which is not the case for either
expert. This supports the argument of social aspects
in people’s perception of review helpfulness, and it
also reflects the fact that students tend to be nice to
each other in such peer-review interactions. How-
ever, this dimension might not correspond with the
real helpfulness of the reviews, at least from the per-
spective of both the writing expert and content ex-
pert.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999983631578947">
We have shown that the type of helpfulness to be
predicted does indeed influence the utility of dif-
ferent feature types for automatic prediction. Low-
level general linguistic features are more predic-
tive when modeling students’ perceived helpfulness;
high-level theory supported constructs are more use-
ful in experts’ models. However, in the related area
of automated essay scoring (Attali and Burstein,
2006), others have suggested the need for the use
of validated features related to meaningful dimen-
sions of writing, rather than low-level (but easy to
automate) features. In this perspective, our work
similarly poses challenge to the NLP community in
terms of how to take into account the education-
oriented dimensions of helpfulness when applying
traditional NLP techniques of automatically pred-
icating review helpfulness. In addition, it is im-
portant to note that predictive features of perceived
helpfulness are not guaranteed to capture the nature
of “truly” helpful peer reviews (in contrast to the
perceived ones).
In the future, we would like to investigate how
to integrate useful dimensions of helpfulness per-
ceived by different audiences in order to come up
with a “true” helpfulness gold standard. We would
also like to explore more sophisticated features and
other NLP techniques to improve our model of peer-
review helpfulness. As we have already built models
to automatically predict certain cognitive constructs
(problem localization and solution), we will replace
the annotated cognitive-science features used here
with their automatic predictions, so that we can build
our helpfulness model fully automatically. Finally,
we would like to integrate our helpfulness model
into a real peer-review system and evaluate its per-
formance extrinsically in terms of improving stu-
dents’ learning and reviewing performance in future
peer-review assignments.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999942571428572">
This work was supported by the Learning Research
and Development Center at the University of Pitts-
burgh. We thank Melissa Patchan and Chris Schunn
for generously providing the manually annotated
peer-review corpus. We are also grateful to Michael
Lipschultz and Chris Schunn for their feedback
while writing this paper.
</bodyText>
<sectionHeader confidence="0.999344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997012166666667">
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning and Assessment (JTLA), 4(3), February.
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A web-
based reciprocal peer review system. Computers and
Education, 48:409–426.
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings of the First In-
ternational Conference on Educational Data Mining
(EDM2008), pages 192–196.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opin-
ions are received by online communities: A case study
on Amazon.com helpfulness votes. In Proceedings of
WWW, pages 141–150.
Raquel M. Crespo Garcia. 2010. Exploring document
clustering techniques for personalized peer assessment
in exploratory courses. In Proceedings of Computer-
Supported Peer Review in Education (CSPRED) Work-
shop in the Tenth International Conference on Intelli-
gent Tutoring Systems (ITS 2010).
Anindya Ghose and Panagiotis G. Ipeirotis. 2010. Esti-
mating the helpfunless and economic impact of prod-
uct reviews: Mining text and reviewer characteristics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 99.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference
</reference>
<page confidence="0.984968">
18
</page>
<reference confidence="0.99964309375">
on Empirical Methods in Natural Language Process-
ing (EMNLP2006), pages 423–430, Sydney, Australia,
July.
Kenji Kira and Larry A. Rendell. 1992. A practical
approach to feature selection. In Derek H. Sleeman
and Peter Edwards, editors, ML92: Proceedings of
the Ninth International Conference on Machine Learn-
ing, pages 249–256, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159–174.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th conference on
Computational linguistics, volume 1 of COLING ’00,
pages 495–501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu.
2008. Modeling and predicting the helpfulness of on-
line reviews. In Proceedings of the Eighth IEEE Inter-
national Conference on Data Mining, pages 443–452,
Los Alamitos, CA, USA.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 91–98, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. Instructional Sci-
ence, 37(4):375–401.
Melissa M. Patchan, Davida Charney, and Christian D.
Schunn. 2009. A validation study of students’ end
comments: Comparing comments by students, a writ-
ing instructor, and a content instructor. Journal of
Writing Research, 1(2):124–152.
Agnes Sandor and Angela Vorndran. 2009. Detect-
ing key sentences for automatic assistance in peer-
reviewing research articles in educational sciences. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP), pages 36–44.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most helpful
book reviews. In Proceedings of the Third Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM2009), pages 36–44.
IH Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, San Francisco, CA.
Wenting Xiong and Diane Litman. 2010. Identifying
problem localization in peer-review feedback. In Pro-
ceedings of Tenth International Conference on Intelli-
gent Tutoring Systems (ITS2010), volume 6095, pages
429–431.
Wenting Xiong and Diane Litman. 2011. Automatically
predicting peer-review helpfulness. In Proceedings
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL/HLT), Portland, Oregon, June.
</reference>
<page confidence="0.999332">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.074286">
<title confidence="0.998117">Understanding Differences in Perceived Peer-Review Helpfulness Natural Language Processing</title>
<author confidence="0.617607">Wenting</author>
<affiliation confidence="0.830610666666667">University of Department of Computer Pittsburgh, PA,</affiliation>
<email confidence="0.997512">wex12@cs.pitt.edu</email>
<affiliation confidence="0.803503">Diane University of Department of Computer Science Learning Research and Development Pittsburgh, PA,</affiliation>
<email confidence="0.999788">litman@cs.pitt.edu</email>
<abstract confidence="0.9949525">Identifying peer-review helpfulness is an important task for improving the quality of feedback received by students, as well as for helping students write better reviews. As we tailor standard product review analysis techniques to our peer-review domain, we notice that peerreview helpfulness differs not only between students and experts but also between types of experts. In this paper, we investigate how different types of perceived helpfulness might influence the utility of features for automatic prediction. Our feature selection results show that certain low-level linguistic features are more useful for predicting student perceived helpfulness, while high-level cognitive constructs are more effective in modeling experts’ perceived helpfulness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater v.2.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning and Assessment (JTLA),</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="31663" citStr="Attali and Burstein, 2006" startWordPosition="4958" endWordPosition="4961">eer-review interactions. However, this dimension might not correspond with the real helpfulness of the reviews, at least from the perspective of both the writing expert and content expert. 6 Conclusion and Future Work We have shown that the type of helpfulness to be predicted does indeed influence the utility of different feature types for automatic prediction. Lowlevel general linguistic features are more predictive when modeling students’ perceived helpfulness; high-level theory supported constructs are more useful in experts’ models. However, in the related area of automated essay scoring (Attali and Burstein, 2006), others have suggested the need for the use of validated features related to meaningful dimensions of writing, rather than low-level (but easy to automate) features. In this perspective, our work similarly poses challenge to the NLP community in terms of how to take into account the educationoriented dimensions of helpfulness when applying traditional NLP techniques of automatically predicating review helpfulness. In addition, it is important to note that predictive features of perceived helpfulness are not guaranteed to capture the nature of “truly” helpful peer reviews (in contrast to the p</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater v.2. The Journal of Technology, Learning and Assessment (JTLA), 4(3), February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kwangsu Cho</author>
<author>Christian D Schunn</author>
</authors>
<title>Scaffolded writing and rewriting in the discipline: A webbased reciprocal peer review system.</title>
<date>2007</date>
<booktitle>Computers and Education,</booktitle>
<pages>48--409</pages>
<contexts>
<context position="8884" citStr="Cho and Schunn, 2007" startWordPosition="1380" endWordPosition="1383">ediction integrates the NLP techniques and cognitive-science approaches mentioned above. We will particularly focus on examining the utility of features motivated by related work from both areas, with respect to different types of gold standard ratings of peer-review helpfulness for automatic prediction. 3 Data In this study, we use a previously annotated peerreview corpus (Nelson and Schunn, 2009; Patchan et al., 2009) that was collected in an introductory college history class using the freely available web-based peer-review SWoRD (Scaffolded Writing and Rewriting in the Discipline) system (Cho and Schunn, 2007). The corpus consists of 16 papers (about six pages each) and 189 reviews (varying from twenty words to about two hundred words) accompanied by numeric ratings of the papers. Each review was manually segmented into idea units (defined as contiguous feedback referring to a single topic) (Nelson and Schunn, 2009), and these idea units were then annotated by two independent annotators for various coding categories, such as feedback type (praise, problem, and summary), problem localization, solution, etc. For example, the second case in Example 1, which only has one idea unit, was annotated as fee</context>
</contexts>
<marker>Cho, Schunn, 2007</marker>
<rawString>Kwangsu Cho and Christian D. Schunn. 2007. Scaffolded writing and rewriting in the discipline: A webbased reciprocal peer review system. Computers and Education, 48:409–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kwangsu Cho</author>
</authors>
<title>Machine classification of peer comments in physics.</title>
<date>2008</date>
<booktitle>In Proceedings of the First International Conference on Educational Data Mining (EDM2008),</booktitle>
<pages>192--196</pages>
<contexts>
<context position="6109" citStr="Cho, 2008" startWordPosition="946" endWordPosition="947">ed by students, while review length is generally effective in predicting expert helpfulness. While the presence of praise and summary comments are more effective in modeling writingexpert helpfulness, providing solutions is more useful in predicting content-expert helpfulness. 2 Related Work To our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of </context>
<context position="7609" citStr="Cho, 2008" startWordPosition="1184" endWordPosition="1185">ion based on review lexicons. To tailor the utility of this prior work on helpfulness prediction to educational peer reviews, we will draw upon research on peer review in cognitive science. One empirical study of the nature of peerreview feedback (Nelson and Schunn, 2009) found that feedback implementation likelihood is significantly correlated with five feedback features. Of these features, problem localization —pinpointing the source of the problem and/or solution in the original paper— and solution —providing a solution to the observed problem— were found to be most important. Researchers (Cho, 2008; Xiong and Litman, 2010) have already shown that some of these constructs can be automatically learned from textual input using Machine Learning and NLP techniques. In addition to investigating what properties of textual comments make peer-review helpful, researchers also examined how the comments produced by students versus by different types of experts differ (Patchan et al., 2009). Though focusing on differences between what students and experts 11 produce, such work sheds light on our study of students’ and experts’ helpfulness ratings of the same student comments (i.e. what students and </context>
<context position="11097" citStr="Cho, 2008" startWordPosition="1733" endWordPosition="1734">erts, yielding four types of possible gold-standard ratings of peer-review helpfulness for each review. Figure 1 shows the rating distribution of each type. Interestingly, we observed that expert ratings roughly follow a normal distribution, while students are more likely to give higher ratings (as illustrated in Figure 1). 4 Features Our features are motivated by the prior work introduced in Section 2, in particular, NLP work on predicting product-review helpfulness (Kim et al., 2006), as well as work on automatically learning cognitive-science constructs (Nelson and Schunn, 2009) using NLP (Cho, 2008; Xiong and Litman, 2010). The complete list of features is shown in Table 3 and described below. The computational linguistic features are automatically extracted based on the output of syntactic analysis of reviews and papers3. These features represent structural, lexical, syntactic and semantic information of the textual content, and also include information for identifying certain important cognitive constructs: • Structural features consider the general structure of reviews, which includes review length in terms of tokens (reviewLength), number of sentences (sentNum), the average sentence</context>
</contexts>
<marker>Cho, 2008</marker>
<rawString>Kwangsu Cho. 2008. Machine classification of peer comments in physics. In Proceedings of the First International Conference on Educational Data Mining (EDM2008), pages 192–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Gueorgi Kossinets</author>
<author>Jon Kleinberg</author>
<author>Lillian Lee</author>
</authors>
<title>How opinions are received by online communities: A case study on Amazon.com helpfulness votes.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="17587" citStr="Danescu-Niculescu-Mizil et al., 2009" startWordPosition="2707" endWordPosition="2710">eview helpfulness is rated at the review level.7 Our cognitive-science features aggregate the annotations up to the review-level by reporting the percentage of idea-units in a review that exhibit each characteristic: the distribution of review types (praise%, problem%, summary%), the percentage of problemlocalized critiques (localization%), as well as the percentage of solution-provided ones (solution%). • Social-science features introduce elements reflecting interactions between students in a peerreview assignment. As suggested in related work on product review helpfulness (Kim et al., 2006; Danescu-Niculescu-Mizil et al., 2009), some social dimensions (e.g. customer opinion on related product quality) are of great influence in the perceived helpfulness of product reviews. Similarly, in our case, we introduced related paper ratings (pRating) — to consider whether and how helpfulness ratings are affected by the rating that the paper receives8 — and the absolute difference between the rat7Details of different granularity levels of annotation can be found in (Nelson and Schunn, 2009). 8That is, to examine whether students give higher ratings to peers who gave them higher paper ratings in the first place. 14 ing and the </context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Kossinets, Kleinberg, Lee, 2009</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinberg, and Lillian Lee. 2009. How opinions are received by online communities: A case study on Amazon.com helpfulness votes. In Proceedings of WWW, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel M Crespo Garcia</author>
</authors>
<title>Exploring document clustering techniques for personalized peer assessment in exploratory courses.</title>
<date>2010</date>
<booktitle>In Proceedings of ComputerSupported Peer Review in Education (CSPRED) Workshop in the Tenth International Conference on Intelligent Tutoring Systems (ITS</booktitle>
<contexts>
<context position="6186" citStr="Garcia, 2010" startWordPosition="956" endWordPosition="958">expert helpfulness. While the presence of praise and summary comments are more effective in modeling writingexpert helpfulness, providing solutions is more useful in predicting content-expert helpfulness. 2 Related Work To our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews show</context>
</contexts>
<marker>Garcia, 2010</marker>
<rawString>Raquel M. Crespo Garcia. 2010. Exploring document clustering techniques for personalized peer assessment in exploratory courses. In Proceedings of ComputerSupported Peer Review in Education (CSPRED) Workshop in the Tenth International Conference on Intelligent Tutoring Systems (ITS 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anindya Ghose</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Estimating the helpfunless and economic impact of product reviews: Mining text and reviewer characteristics.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>99</volume>
<contexts>
<context position="6344" citStr="Ghose and Ipeirotis, 2010" startWordPosition="982" endWordPosition="985">ns is more useful in predicting content-expert helpfulness. 2 Related Work To our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to sele</context>
</contexts>
<marker>Ghose, Ipeirotis, 2010</marker>
<rawString>Anindya Ghose and Panagiotis G. Ipeirotis. 2010. Estimating the helpfunless and economic impact of product reviews: Mining text and reviewer characteristics. IEEE Transactions on Knowledge and Data Engineering, 99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Patrick Pantel</author>
<author>Tim Chklovski</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006),</booktitle>
<pages>423--430</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2439" citStr="Kim et al., 2006" startWordPosition="357" endWordPosition="360">iew systems, to automat10 ically predict peer-review helpfulness based on features mined from textual reviews using Natural Language Processing (NLP) techniques. Such an intelligent component could enable peer-review systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews. In our prior work (Xiong and Litman, 2011), we examined whether techniques used for predicting the helpfulness of product reviews (Kim et al., 2006) could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced by the educational context of peer review. While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness1, there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types</context>
<context position="6316" citStr="Kim et al., 2006" startWordPosition="978" endWordPosition="981"> providing solutions is more useful in predicting content-expert helpfulness. 2 Related Work To our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (20</context>
<context position="10978" citStr="Kim et al., 2006" startWordPosition="1714" endWordPosition="1717"> with a slightly different scale from one to five. For our study, we will also compute the average ratings given by the two experts, yielding four types of possible gold-standard ratings of peer-review helpfulness for each review. Figure 1 shows the rating distribution of each type. Interestingly, we observed that expert ratings roughly follow a normal distribution, while students are more likely to give higher ratings (as illustrated in Figure 1). 4 Features Our features are motivated by the prior work introduced in Section 2, in particular, NLP work on predicting product-review helpfulness (Kim et al., 2006), as well as work on automatically learning cognitive-science constructs (Nelson and Schunn, 2009) using NLP (Cho, 2008; Xiong and Litman, 2010). The complete list of features is shown in Table 3 and described below. The computational linguistic features are automatically extracted based on the output of syntactic analysis of reviews and papers3. These features represent structural, lexical, syntactic and semantic information of the textual content, and also include information for identifying certain important cognitive constructs: • Structural features consider the general structure of revie</context>
<context position="13140" citStr="Kim et al., 2006" startWordPosition="2038" endWordPosition="2041">, maybe, try, revision, want LOC location page, paragraph, sentence ERR problem error, mistakes, typo, problem, difficulties, conclusion IDE idea verb consider, mention LNK transition however, but NEG negative words fail, hard, difficult, bad, short, little, bit, poor, few, unclear, only, more POS positive words great, good, well, clearly, easily, effective, effectively, helpful, very SUM summarization main, overall, also, how, job NOT negation not, doesn’t, don’t SOL solution revision specify correction Table 1: Ten lexical categories Compared with commonly used lexical unigrams and bigrams (Kim et al., 2006), these lexical categories are equally useful in modeling peer-review helpfulness, and significantly reduce the feature space.4 • Syntactic features mainly focus on nouns and verbs, and include percentage of tokens that are nouns, verbs, verbs conjugated in the first person (1stPUerb%), adjectives/adverbs, and open classes, respectively. • Semantic features capture two important peer4Lexical categories help avoid the risk of over-fitting, given only 189 peer reviews in our case compared to more than ten thousand Amazon.com reviews used for predicting product review helpfulness (Kim et al., 200</context>
<context position="17548" citStr="Kim et al., 2006" startWordPosition="2703" endWordPosition="2706">), however, peer-review helpfulness is rated at the review level.7 Our cognitive-science features aggregate the annotations up to the review-level by reporting the percentage of idea-units in a review that exhibit each characteristic: the distribution of review types (praise%, problem%, summary%), the percentage of problemlocalized critiques (localization%), as well as the percentage of solution-provided ones (solution%). • Social-science features introduce elements reflecting interactions between students in a peerreview assignment. As suggested in related work on product review helpfulness (Kim et al., 2006; Danescu-Niculescu-Mizil et al., 2009), some social dimensions (e.g. customer opinion on related product quality) are of great influence in the perceived helpfulness of product reviews. Similarly, in our case, we introduced related paper ratings (pRating) — to consider whether and how helpfulness ratings are affected by the rating that the paper receives8 — and the absolute difference between the rat7Details of different granularity levels of annotation can be found in (Nelson and Schunn, 2009). 8That is, to examine whether students give higher ratings to peers who gave them higher paper rati</context>
</contexts>
<marker>Kim, Pantel, Chklovski, Pennacchiotti, 2006</marker>
<rawString>Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pennacchiotti. 2006. Automatically assessing review helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006), pages 423–430, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Kira</author>
<author>Larry A Rendell</author>
</authors>
<title>A practical approach to feature selection.</title>
<date>1992</date>
<booktitle>ML92: Proceedings of the Ninth International Conference on Machine Learning,</booktitle>
<pages>249--256</pages>
<editor>In Derek H. Sleeman and Peter Edwards, editors,</editor>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="19548" citStr="Kira and Rendell, 1992" startWordPosition="3014" endWordPosition="3017">) nonlinguistic features, and 3) all features. For each set of features, we train four models, each corresponding to a different kind of helpfulness rating. For each learning task (three by four), we use two standard feature selection algorithms to find the most useful features based on 10-fold cross validation. First, we perform Linear Regression with Greedy Stepwise search (stepwise LR) to select the most useful features when testing in each of the ten folds, and count how many times each features is selected in the ten trials. Second, we use Relief Feature Evaluation9 with Ranker (Relief) (Kira and Rendell, 1992; Witten and Frank, 2005) to rank all used features based on their average merits (the ability of the given feature to differentiate between two example pairs) of ten trials.10 Although both methods are supervised, the wrapper is “more aggressive” because its feature evaluation is based on the performance of the regression model and thus the resulting feature set is tailored to the learning algorithm. In contrast, Relief does not optimize feature sets directly for classifier performance, thus it takes into account class information in a “less aggressive” manner than the Wrapper method. We use </context>
</contexts>
<marker>Kira, Rendell, 1992</marker>
<rawString>Kenji Kira and Larry A. Rendell. 1992. A practical approach to feature selection. In Derek H. Sleeman and Peter Edwards, editors, ML92: Proceedings of the Ninth International Conference on Machine Learning, pages 249–256, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>33--159</pages>
<contexts>
<context position="10024" citStr="Landis and Koch (1977)" startWordPosition="1567" endWordPosition="1570">, the second case in Example 1, which only has one idea unit, was annotated as feedbackType = problem, problemlocalization = True, and solution = True. The agreement (Kappa) between the two annotators is 0.92 for FeedbackType, 0.69 for localization, and 0.89 for solution.2 Our corpus also contains author provided back evaluations. At the end of the peer-review assignment, students were asked to provide back evaluation on each review that they received by rating review helpfulness using a discrete scale from one to seven. After the corpus was collected, one writ2For Kappa value interpretation, Landis and Koch (1977) propose the following agreement standard: 0.21-0.40 = “Fair”; 0.41-0.60 = “Moderate”; 0.61-0.80 = “Substantial”; 0.81-1.00 = “Almost Perfect”. Thus, while localization signals are more difficult to annotate, the inter-annotator agreement is still substantial. ing expert and one content expert were also asked to rate review helpfulness with a slightly different scale from one to five. For our study, we will also compute the average ratings given by the two experts, yielding four types of possible gold-standard ratings of peer-review helpfulness for each review. Figure 1 shows the rating distri</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<volume>1</volume>
<pages>495--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14422" citStr="Lin and Hovy, 2000" startWordPosition="2233" endWordPosition="2236">udents’ papers, and their opinion sentiment polarities. Kim et al. (2006) extracted product property keywords from external resources based on their hypothesis that helpful product reviews refer frequently to certain product properties. Similarly, we hypothesize that helpful peer reviews are closely related to domain topics that are shared by all students papers in an assignment. Our domain topic set contains 288 words extracted from the collection of student papers using topic-lexicon extraction software5; our feature (domainWord) 5The software extracts topic words based on topic signatures (Lin and Hovy, 2000), and was kindly provided by Annie Louis. 13 Feature Description regTag% The percentage of problems in reviews that could be matched with a localization pattern. soDomain% The percentage of sentences where any domain word appears between the subject and the object. dDeterminer The number of demonstrative determiners. windowSize For each review sentence, we search for the most likely referred window of words in the related paper, and windowSize is the average number of words of all windows. Table 2: Localization features counts how many words of a given review belong to the extracted set. For s</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th conference on Computational linguistics, volume 1 of COLING ’00, pages 495–501, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Xiangji Guang</author>
<author>Aijun An</author>
<author>Xiaohui Yu</author>
</authors>
<title>Modeling and predicting the helpfulness of online reviews.</title>
<date>2008</date>
<booktitle>In Proceedings of the Eighth IEEE International Conference on Data Mining,</booktitle>
<pages>443--452</pages>
<location>Los Alamitos, CA, USA.</location>
<contexts>
<context position="6378" citStr="Liu et al., 2008" startWordPosition="988" endWordPosition="991">ert helpfulness. 2 Related Work To our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews i</context>
</contexts>
<marker>Liu, Guang, An, Yu, 2008</marker>
<rawString>Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu. 2008. Modeling and predicting the helpfulness of online reviews. In Proceedings of the Eighth IEEE International Conference on Data Mining, pages 443–452, Los Alamitos, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12334" citStr="McDonald et al., 2005" startWordPosition="1922" endWordPosition="1925">ngthAve), percentage of sentences that end with question marks (question%), and number of exclamatory sentences (exclams). • Lexical features are counts of ten lexical categories (Table 1), where the categories were learned in a semi-supervised way from review lexicons in a pilot study. We first manually created a list of words that were specified as signal words for annotating feedbackType and problem localization in the coding manual; then we supplemented the list with words selected by a decision tree model learned using a Bagof-Words representation of the peer reviews. 3We used MSTParser (McDonald et al., 2005) for syntactic analysis. 12 Figure 1: Distribution of peer-review helpfulness when rated by students and experts Tag Meaning Word list SUG suggestion should, must, might, could, need, needs, maybe, try, revision, want LOC location page, paragraph, sentence ERR problem error, mistakes, typo, problem, difficulties, conclusion IDE idea verb consider, mention LNK transition however, but NEG negative words fail, hard, difficult, bad, short, little, bit, poor, few, unclear, only, more POS positive words great, good, well, clearly, easily, effective, effectively, helpful, very SUM summarization main,</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 91–98, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melissa M Nelson</author>
<author>Christian D Schunn</author>
</authors>
<title>The nature of feedback: how different types of peer feedback affect writing performance.</title>
<date>2009</date>
<journal>Instructional Science,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="7272" citStr="Nelson and Schunn, 2009" startWordPosition="1132" endWordPosition="1135">ews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews in an unsupervised fashion based on review lexicons. To tailor the utility of this prior work on helpfulness prediction to educational peer reviews, we will draw upon research on peer review in cognitive science. One empirical study of the nature of peerreview feedback (Nelson and Schunn, 2009) found that feedback implementation likelihood is significantly correlated with five feedback features. Of these features, problem localization —pinpointing the source of the problem and/or solution in the original paper— and solution —providing a solution to the observed problem— were found to be most important. Researchers (Cho, 2008; Xiong and Litman, 2010) have already shown that some of these constructs can be automatically learned from textual input using Machine Learning and NLP techniques. In addition to investigating what properties of textual comments make peer-review helpful, resear</context>
<context position="8663" citStr="Nelson and Schunn, 2009" startWordPosition="1346" endWordPosition="1349">t students and experts 11 produce, such work sheds light on our study of students’ and experts’ helpfulness ratings of the same student comments (i.e. what students and experts value). Our work in peer-review helpfulness prediction integrates the NLP techniques and cognitive-science approaches mentioned above. We will particularly focus on examining the utility of features motivated by related work from both areas, with respect to different types of gold standard ratings of peer-review helpfulness for automatic prediction. 3 Data In this study, we use a previously annotated peerreview corpus (Nelson and Schunn, 2009; Patchan et al., 2009) that was collected in an introductory college history class using the freely available web-based peer-review SWoRD (Scaffolded Writing and Rewriting in the Discipline) system (Cho and Schunn, 2007). The corpus consists of 16 papers (about six pages each) and 189 reviews (varying from twenty words to about two hundred words) accompanied by numeric ratings of the papers. Each review was manually segmented into idea units (defined as contiguous feedback referring to a single topic) (Nelson and Schunn, 2009), and these idea units were then annotated by two independent annot</context>
<context position="11076" citStr="Nelson and Schunn, 2009" startWordPosition="1727" endWordPosition="1730">average ratings given by the two experts, yielding four types of possible gold-standard ratings of peer-review helpfulness for each review. Figure 1 shows the rating distribution of each type. Interestingly, we observed that expert ratings roughly follow a normal distribution, while students are more likely to give higher ratings (as illustrated in Figure 1). 4 Features Our features are motivated by the prior work introduced in Section 2, in particular, NLP work on predicting product-review helpfulness (Kim et al., 2006), as well as work on automatically learning cognitive-science constructs (Nelson and Schunn, 2009) using NLP (Cho, 2008; Xiong and Litman, 2010). The complete list of features is shown in Table 3 and described below. The computational linguistic features are automatically extracted based on the output of syntactic analysis of reviews and papers3. These features represent structural, lexical, syntactic and semantic information of the textual content, and also include information for identifying certain important cognitive constructs: • Structural features consider the general structure of reviews, which includes review length in terms of tokens (reviewLength), number of sentences (sentNum),</context>
<context position="15460" citStr="Nelson and Schunn, 2009" startWordPosition="2392" endWordPosition="2395">the related paper, and windowSize is the average number of words of all windows. Table 2: Localization features counts how many words of a given review belong to the extracted set. For sentiment polarities, we extract positive and negative sentiment words from the General Inquirer Dictionaries 6, and count their appearance in reviews in terms of their sentiment polarity (posWord, negWord). • Localization features are motivated by linguistic features that are used for automatically predicting problem localization (an important cognitive construct for feedback understanding and implementation) (Nelson and Schunn, 2009), and are presented in Table 2. To illustrate how these features are computed, consider the following critique: The section of the essay on African Americans needs more careful attention to the timing and reasons for the federal governments decision to stop protecting African American civil and political rights. The review has only one sentence, in which one regular expression is matched with “the section of” thus regTag% = 1; no demonstrative determiner, thus dDeterminer = 0; “African” and “Americans” are domain words appearing between the subject “section” and the object “attention”, so soDo</context>
<context position="16933" citStr="Nelson and Schunn, 2009" startWordPosition="2614" endWordPosition="2617">the corpus, described in Section 3. 6http://www.wjh.harvard.edu/ inquirer/homecat.htm • Cognitive-science features are motivated by an empirical study (Nelson and Schunn, 2009) which suggests significant correlation between certain cognitive constructs (e.g. feedbackType, problem localization, solution) and review implementation likelihood. Intuitively, helpful reviews are more likely to get implemented, thus we introduced these features to capture desirable high-level characteristics of peer reviews. Note that in our corpus these cognitive constructs are manually coded at the ideaunit level (Nelson and Schunn, 2009), however, peer-review helpfulness is rated at the review level.7 Our cognitive-science features aggregate the annotations up to the review-level by reporting the percentage of idea-units in a review that exhibit each characteristic: the distribution of review types (praise%, problem%, summary%), the percentage of problemlocalized critiques (localization%), as well as the percentage of solution-provided ones (solution%). • Social-science features introduce elements reflecting interactions between students in a peerreview assignment. As suggested in related work on product review helpfulness (K</context>
</contexts>
<marker>Nelson, Schunn, 2009</marker>
<rawString>Melissa M. Nelson and Christian D. Schunn. 2009. The nature of feedback: how different types of peer feedback affect writing performance. Instructional Science, 37(4):375–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melissa M Patchan</author>
<author>Davida Charney</author>
<author>Christian D Schunn</author>
</authors>
<title>A validation study of students’ end comments: Comparing comments by students, a writing instructor, and a content instructor.</title>
<date>2009</date>
<journal>Journal of Writing Research,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="7996" citStr="Patchan et al., 2009" startWordPosition="1245" endWordPosition="1248"> Of these features, problem localization —pinpointing the source of the problem and/or solution in the original paper— and solution —providing a solution to the observed problem— were found to be most important. Researchers (Cho, 2008; Xiong and Litman, 2010) have already shown that some of these constructs can be automatically learned from textual input using Machine Learning and NLP techniques. In addition to investigating what properties of textual comments make peer-review helpful, researchers also examined how the comments produced by students versus by different types of experts differ (Patchan et al., 2009). Though focusing on differences between what students and experts 11 produce, such work sheds light on our study of students’ and experts’ helpfulness ratings of the same student comments (i.e. what students and experts value). Our work in peer-review helpfulness prediction integrates the NLP techniques and cognitive-science approaches mentioned above. We will particularly focus on examining the utility of features motivated by related work from both areas, with respect to different types of gold standard ratings of peer-review helpfulness for automatic prediction. 3 Data In this study, we us</context>
</contexts>
<marker>Patchan, Charney, Schunn, 2009</marker>
<rawString>Melissa M. Patchan, Davida Charney, and Christian D. Schunn. 2009. A validation study of students’ end comments: Comparing comments by students, a writing instructor, and a content instructor. Journal of Writing Research, 1(2):124–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnes Sandor</author>
<author>Angela Vorndran</author>
</authors>
<title>Detecting key sentences for automatic assistance in peerreviewing research articles in educational sciences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP),</booktitle>
<pages>36--44</pages>
<contexts>
<context position="6048" citStr="Sandor and Vorndran, 2009" startWordPosition="935" endWordPosition="938"> transitions and opinions are most useful in predicting helpfulness as perceived by students, while review length is generally effective in predicting expert helpfulness. While the presence of praise and summary comments are more effective in modeling writingexpert helpfulness, providing solutions is more useful in predicting content-expert helpfulness. 2 Related Work To our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product revi</context>
</contexts>
<marker>Sandor, Vorndran, 2009</marker>
<rawString>Agnes Sandor and Angela Vorndran. 2009. Detecting key sentences for automatic assistance in peerreviewing research articles in educational sciences. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Revrank: A fully unsupervised algorithm for selecting the most helpful book reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third International AAAI Conference on Weblogs and Social Media (ICWSM2009),</booktitle>
<pages>36--44</pages>
<contexts>
<context position="6419" citStr="Tsur and Rappoport, 2009" startWordPosition="994" endWordPosition="997">o our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews in an unsupervised fashion based on review</context>
</contexts>
<marker>Tsur, Rappoport, 2009</marker>
<rawString>Oren Tsur and Ari Rappoport. 2009. Revrank: A fully unsupervised algorithm for selecting the most helpful book reviews. In Proceedings of the Third International AAAI Conference on Weblogs and Social Media (ICWSM2009), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IH Witten</author>
<author>E Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques, Second Edition.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="19573" citStr="Witten and Frank, 2005" startWordPosition="3018" endWordPosition="3022">, and 3) all features. For each set of features, we train four models, each corresponding to a different kind of helpfulness rating. For each learning task (three by four), we use two standard feature selection algorithms to find the most useful features based on 10-fold cross validation. First, we perform Linear Regression with Greedy Stepwise search (stepwise LR) to select the most useful features when testing in each of the ten folds, and count how many times each features is selected in the ten trials. Second, we use Relief Feature Evaluation9 with Ranker (Relief) (Kira and Rendell, 1992; Witten and Frank, 2005) to rank all used features based on their average merits (the ability of the given feature to differentiate between two example pairs) of ten trials.10 Although both methods are supervised, the wrapper is “more aggressive” because its feature evaluation is based on the performance of the regression model and thus the resulting feature set is tailored to the learning algorithm. In contrast, Relief does not optimize feature sets directly for classifier performance, thus it takes into account class information in a “less aggressive” manner than the Wrapper method. We use both methods in our exper</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>IH Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, Second Edition. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenting Xiong</author>
<author>Diane Litman</author>
</authors>
<title>Identifying problem localization in peer-review feedback.</title>
<date>2010</date>
<booktitle>In Proceedings of Tenth International Conference on Intelligent Tutoring Systems (ITS2010),</booktitle>
<volume>6095</volume>
<pages>429--431</pages>
<contexts>
<context position="6134" citStr="Xiong and Litman, 2010" startWordPosition="948" endWordPosition="951">nts, while review length is generally effective in predicting expert helpfulness. While the presence of praise and summary comments are more effective in modeling writingexpert helpfulness, providing solutions is more useful in predicting content-expert helpfulness. 2 Related Work To our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. An</context>
<context position="7634" citStr="Xiong and Litman, 2010" startWordPosition="1186" endWordPosition="1190">n review lexicons. To tailor the utility of this prior work on helpfulness prediction to educational peer reviews, we will draw upon research on peer review in cognitive science. One empirical study of the nature of peerreview feedback (Nelson and Schunn, 2009) found that feedback implementation likelihood is significantly correlated with five feedback features. Of these features, problem localization —pinpointing the source of the problem and/or solution in the original paper— and solution —providing a solution to the observed problem— were found to be most important. Researchers (Cho, 2008; Xiong and Litman, 2010) have already shown that some of these constructs can be automatically learned from textual input using Machine Learning and NLP techniques. In addition to investigating what properties of textual comments make peer-review helpful, researchers also examined how the comments produced by students versus by different types of experts differ (Patchan et al., 2009). Though focusing on differences between what students and experts 11 produce, such work sheds light on our study of students’ and experts’ helpfulness ratings of the same student comments (i.e. what students and experts value). Our work </context>
<context position="11122" citStr="Xiong and Litman, 2010" startWordPosition="1735" endWordPosition="1738">ing four types of possible gold-standard ratings of peer-review helpfulness for each review. Figure 1 shows the rating distribution of each type. Interestingly, we observed that expert ratings roughly follow a normal distribution, while students are more likely to give higher ratings (as illustrated in Figure 1). 4 Features Our features are motivated by the prior work introduced in Section 2, in particular, NLP work on predicting product-review helpfulness (Kim et al., 2006), as well as work on automatically learning cognitive-science constructs (Nelson and Schunn, 2009) using NLP (Cho, 2008; Xiong and Litman, 2010). The complete list of features is shown in Table 3 and described below. The computational linguistic features are automatically extracted based on the output of syntactic analysis of reviews and papers3. These features represent structural, lexical, syntactic and semantic information of the textual content, and also include information for identifying certain important cognitive constructs: • Structural features consider the general structure of reviews, which includes review length in terms of tokens (reviewLength), number of sentences (sentNum), the average sentence length (sentLengthAve), </context>
</contexts>
<marker>Xiong, Litman, 2010</marker>
<rawString>Wenting Xiong and Diane Litman. 2010. Identifying problem localization in peer-review feedback. In Proceedings of Tenth International Conference on Intelligent Tutoring Systems (ITS2010), volume 6095, pages 429–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenting Xiong</author>
<author>Diane Litman</author>
</authors>
<title>Automatically predicting peer-review helpfulness.</title>
<date>2011</date>
<booktitle>In Proceedings 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT),</booktitle>
<location>Portland, Oregon,</location>
<contexts>
<context position="2333" citStr="Xiong and Litman, 2011" startWordPosition="341" endWordPosition="344">n in a helpful way. To address this issue, we propose to add a peer-review helpfulness model to current peer-review systems, to automat10 ically predict peer-review helpfulness based on features mined from textual reviews using Natural Language Processing (NLP) techniques. Such an intelligent component could enable peer-review systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews. In our prior work (Xiong and Litman, 2011), we examined whether techniques used for predicting the helpfulness of product reviews (Kim et al., 2006) could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced by the educational context of peer review. While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness1, there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-re</context>
<context position="21078" citStr="Xiong and Litman, 2011" startWordPosition="3260" endWordPosition="3263">e complementary perspectives. While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features. To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. (Details of how the average-expert model performs can be found in our prior work (Xiong and Litman, 2011).) 5.1 Feature Selection of Linguistic Features Table 4 presents the feature selection results of computational linguistic features used in modeling the four different types of peer-review helpfulness. The first row lists the four sources of helpfulness ratings, and each column represents a corresponding model. The second row presents the most useful features in each model selected by stepwise LR, where “# of folds” refers to the number of trials in which the given feature appears in the resulting feature set during the 10-fold cross validation. Here we only report features that are selected b</context>
</contexts>
<marker>Xiong, Litman, 2011</marker>
<rawString>Wenting Xiong and Diane Litman. 2011. Automatically predicting peer-review helpfulness. In Proceedings 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT), Portland, Oregon, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>