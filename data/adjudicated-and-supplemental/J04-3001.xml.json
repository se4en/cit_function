{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.9062","#text":"\n1. Problem-space: Knowledge about the problem space may provide\n"},{"#tail":"\n","@confidence":"0.7993796","#text":"\n11 In this experiment, we have omitted the evaluation function for selecting novel lexical relationships,\nflex, because the grammar does not use actual lexical anchors.\n12 In terms of the number of sentences, the baseline frand selected 2,600 sentences; flen selected 1,300\nsentences; and ferr and func each selected 900 sentences.\n13 F = 2?LR?LPLR+LP , where LR is the labeled recall score and LP is the labeled precision score.\n"}],"figure":[{"#tail":"\n","@confidence":"0.881401935897436","#text":"\nComputational Linguistics Volume 30, Number 3\nbook,\non, shelf\nbook,\non,\nput,\nbook,\non,\nshelf\nput,\non,\nshelf\nput,\non, on,\nbook,\nshelf\non,\nput,\nnoun\non,\non,\nshelf\nput,\non, shelf\non,\nput,\nidea\non,\nnoun\nput,\non,\nshelf\nput,\non, shelf\non,\non,\nidea\nidea\nidea\nbook,\nshelf\non,\non,\nwrote\non,\nnoun\nbook,\non,\nshelf\nbook,\non,\non,\nshelf shelf\nbook,\non,\non,\nwrote\nwrote wrote\nidea\non,\nnoun\non,\non,\non,\non,\non,\ntopic\nhad\nidea\non,\ntopictopic\nideahad\non,\nhad\nidea\nhad\ntopic\nu1, u2 u3 u4 u5\n"},{"#tail":"\n","@confidence":"0.549798666666667","#text":"\nComputational Linguistics Volume 30, Number 3\n0.4\nLikelihood of Attach NP\n"},{"#tail":"\n","@confidence":"0.974701027972028","#text":"\n76\n78\n80\n82\n84\n86\n0 5000 10000 15000 20000\nCl\nas\nsif\nica\ntio\nn\nac\ncu\nra\ncy\no\nn\nth\ne\nte\nst\ns\net\n(%\n)\nNumber of examples in the training set\nbaseline\nnovelty\nbackoff\n74\n76\n78\n80\n82\n84\n86\n0 5000 10000 15000 20000\nCl\nas\nsif\nica\ntio\nn\nac\ncu\nra\ncy\no\nn\nth\ne\nte\nst\ns\net\n(%\n)\nNumber of examples in the training set\nbaseline\nuncertainty\nconfidence\n(a) (b)\n74\n76\n78\n80\n82\n84\n86\n0 5000 10000 15000 20000\nCl\nas\nsif\nica\ntio\nn\nac\ncu\nra\ncy\no\nn\nth\ne\nte\nst\ns\net\n(%\n)\nNumber of examples in the training set\nbaseline\narea\n0\n5,000\n10,000\n15,000\n20,000\n25,000\nba\nse\nline\nno\nve\nlty\nba\ncko\nff\nun\nce\nrta\nint\ny\nco\nnfi\nde\nnc\ne\nare\na\nEvaluation Functions\nN\num\nbe\nr\nof\nL\nab\nel\ned\nT\nra\nin\nin\ng\nE\nxa\nm\npl\nes\n(c) (d)\n"},{"#tail":"\n","@confidence":"0.981324285714286","#text":"\nHwa Sample Selection for Statistical Parsing\n76\n77\n78\n79\n80\n81\n5000 10000 15000 20000 25000 30000 35000 40000 45000\nCl\nas\nsif\nica\ntio\nn\nac\ncu\nra\ncy\no\nn\nth\ne\nte\nst\ns\net\n(%\n)\nNumber of labeled brackets in the training set\nbaseline\nlength\nerror driven\ntree entropy\n(a)\n0\n5,000\n10,000\n15,000\n20,000\n25,000\n30,000\n35,000\n40,000\nba\nse\nline\nlen\ngth\nerr\nor\ndri\nve\nn\ntre\ne e\nntr\nop\ny\nEvaluation functions\nN\num\nbe\nr\nof\nL\nab\nel\ned\nB\nra\nck\net\ns\nin\nth\ne\nTr\nai\nni\nng\nD\nat\na\n(b)\n"},{"#tail":"\n","@confidence":"0.996165418604651","#text":"\n80\n82\n84\n86\n88\n100000 200000 300000 400000 500000 600000 700000 800000 900000\nCl\nas\nsif\nica\ntio\nn\nac\ncu\nra\ncy\no\nn\nth\ne\nte\nst\ns\net\n(%\n)\nNumber of labeled constituents in the training set\nbaseline\nlength\nnovel lex\nerror driven\ntree entropy\n(a)\n0\n100,000\n200,000\n300,000\n400,000\n500,000\n600,000\n700,000\n800,000\nba\nse\nline no\nve\nl\nlen\ngth\nerr\nor\ndri\nve\nn\ntre\ne e\nntr\nop\ny\nEvaluation Functions\nN\num\nbe\nr\nof\nL\nab\nel\ned\nC\non\nst\nitu\nen\nts\nin\nth\ne\nTr\nai\nni\nng\nD\nat\na\n(b)\n"}],"author":{"#tail":"\n","@confidence":"0.966579","#text":"\nRebecca Hwa?\n"},"equation":[{"#tail":"\n","@confidence":"0.722266166666667","#text":"\nC ? Train(L).\nRepeat\nN ? Select(n, U, C, f ).\nU ? U ? N.\nL ? L ? Label(N).\nC ? Train(L).\n"},{"#tail":"\n","@confidence":"0.79726525","#text":"\nCount(tuple) ? Count(tuple) + 1\nif a = noun then\nCountNP(tuple) ? CountNP(tuple) + 1\nsubroutine Test(U)\n"},{"#tail":"\n","@confidence":"0.873869571428571","#text":"\nprob ? CountNP(v,n,p,n2)Count(v,n,p,n2)\nelsif Count(v, p, n2) + Count(n, p, n2) + Count(v, n, p) > 0 then\nprob ? CountNP(v,p,n2)+CountNP(n,p,n2)+CountNP(v,n,p)Count(v,p,n2)+Count(n,p,n2)+Count(v,n,p)\nelsif Count(v, p) + Count(n, p) + Count(p, n2) > 0 then\nprob ? CountNP(v,p)+CountNP(n,p)+CountNP(p,n2)Count(v,p)+Count(n,p)+Count(p,n2)\nelsif Count(p) > 0 then\nprob ? CountNP(p)Count(p)\n"},{"#tail":"\n","@confidence":"0.995036833333333","#text":"\nfnovel(u, C) =\n?\nt?Tuples(u)\n{\n1 : Count(t) = 0\n0 : otherwise\n"},{"#tail":"\n","@confidence":"0.9989724","#text":"\nu1 = (put, book, on, shelf)\nu2 = (put, book, on, shelf)\nu3 = (put, idea, on, shelf)\nu4 = (wrote, book, on, shelf)\nu5 = (had, idea, on, topic)\n"},{"#tail":"\n","@confidence":"0.987263","#text":"\nfunc(u, C) = ferr(u, C)\n=\n{\n1 ? P(noun  |u, C) : P(noun  |u, C) ? 0.5\nP(noun  |u, C) : otherwise\n= 0.5 ? abs(0.5 ? P(noun  |u, C)) (1)\n"},{"#tail":"\n","@confidence":"0.982752882352941","#text":"\nconf int(p?, n) =\n1\n1 + t\n2\nn\n(\np? +\nt2\n2n\n? t\n?\np?(1 ? p?)\nn\n+\nt2\n4n2\n)\n"},{"#tail":"\n","@confidence":"0.6736218","#text":"\nfconf(u, C) =\n4\n?\nl=1\n|conf int(p?l(u, C), nl(u, C))|\n"},{"#tail":"\n","@confidence":"0.662361","#text":"\narea(p?, n) =\n? b\na\nN(x, 0.5, 0.1)dx\n"},{"#tail":"\n","@confidence":"0.92174175","#text":"\nflex(w, G) =\n?\nwi,wj?w new(wi, wj)? coocc(wi, wj)\nlength(w)\n"},{"#tail":"\n","@confidence":"0.969005","#text":"\n?\nv?V\nP(v  |G) = P(w  |G). The parse chosen for w is the most likely parse in V , denoted\nas vmax, where\nvmax = argmaxv?VP(v  |G)\n"},{"#tail":"\n","@confidence":"0.999780375","#text":"\nP(vmax  |w, G) =\nP(vmax  |G)\nP(w  |G)\n=\nP(vmax  |G)\n?\nv?V P(v  |G)\n. (2)\n"},{"#tail":"\n","@confidence":"0.9656765","#text":"\nH(V) = ?\n?\nv?V\np(v) lg(p(v)) (3)\nwhere V is a random variable that can take any possible outcome in set V , and p(v) =\nPr(V = v) is the density function. Further details about the properties of entropy can\n"},{"#tail":"\n","@confidence":"0.926807","#text":"\nfunc(w, G) =\nTE(w, G)\nlg(?V?)\n"},{"#tail":"\n","@confidence":"0.986964333333333","#text":"\n?\nv?V\nP(v  |w, G) = 1\n"},{"#tail":"\n","@confidence":"0.9997762","#text":"\nTE(w, G) = H(V)\n= ?\n?\nv?V\np(v) lg(p(v))\n= ?\n?\nv?V\nP(v  |G)\nP(w  |G) lg(\nP(v  |G)\nP(w  |G) )\n= ?\n?\nv?V\nP(v  |G)\nP(w  |G) lg(P(v  |G)) +\n?\nv?V\nP(v  |G)\nP(w  |G) lg(P(w  |G))\n= ? 1\nP(w  |G)\n?\nv?V\nP(v  |G) lg(P(v  |G)) + lg(P(w  |G))\nP(w  |G)\n?\nv?V\nP(v  |G)\n= ? 1\nP(w  |G)\n?\nv?V\nP(v  |G) lg(P(v  |G)) + lg(P(w  |G))\n"},{"#tail":"\n","@confidence":"0.973307666666667","#text":"\n?\nv?V\nP(v  |G) lg(P(v  |G)).\n"},{"#tail":"\n","@confidence":"0.991686","#text":"\nh(X, i, j) = ?\n?\nx?X ??wi...wj\nP(x  |G) lg(P(x  |G))\n"},{"#tail":"\n","@confidence":"0.806421333333333","#text":"\n?\nv?V\nP(v  |G) lg P(v  |G), is denoted as h(S, 1, n).\n"},{"#tail":"\n","@confidence":"0.944778666666667","#text":"\n?? wi . . .wk and\nZ ?? wk+1 . . .wj:\nh(X, i, j) =\nj?1\n?\nk=i\n?\n(X?YZ)\nhY,Z,k(X, i, j)\n"},{"#tail":"\n","@confidence":"0.99688225","#text":"\nhY,Z,k(X, i, j) = ?\n?\ny?Y ,z?Z\nP(x)P(y)P(z) lg(P(x)P(y)P(z))\n= ?\n?\ny?Y ,z?Z\nP(x)P(y)P(z)(lg P(x) + lg P(y) + lg P(z))\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.99884","#text":"\n3.1 A Summary of the Collins-Brooks Model\n"},{"#tail":"\n","@confidence":"0.997826","#text":"\n3.2 Evaluation Functions\n"},{"#tail":"\n","@confidence":"0.997679","#text":"\n3.3 Experimental Comparison\n"},{"#tail":"\n","@confidence":"0.991791","#text":"\n4.1 Evaluation Functions\n"},{"#tail":"\n","@confidence":"0.984008","#text":"\n4.2 Experiments and Results\n"}],"subsubsectionHeader":[{"#tail":"\n","@confidence":"0.646445","#text":"\n3.2.1 The Problem Space. One source of knowledge to exploit is our understanding of\n"},{"#tail":"\n","@confidence":"0.469923","#text":"\n3.2.2 The Performance of the Hypothesis. The evaluation functions discussed in the\n"},{"#tail":"\n","@confidence":"0.488162","#text":"\n3.2.3 The Parameters of the Hypothesis. The potential problems with performance-\n"},{"#tail":"\n","@confidence":"0.490075","#text":"\n3.3.1 Results and Discussion. This section presents the empirical measurements of\n"},{"#tail":"\n","@confidence":"0.891871","#text":"\n4.1.1 Problem Space. Similarly to scoring a PP candidate based on the novelty and\n"},{"#tail":"\n","@confidence":"0.955821","#text":"\n4.1.2 The Performance of the Hypothesis. We previously defined two performance-\n"},{"#tail":"\n","@confidence":"0.872335","#text":"\n4.1.3 The Parameters of the Hypothesis. Although the confidence-based function\n"},{"#tail":"\n","@confidence":"0.96114","#text":"\n4.2.1 An Expectation-Maximization-Based Learner. In the first experiment, we use\n"},{"#tail":"\n","@confidence":"0.810956","#text":"\n4.2.2 A History-Based Learner. In the second experiment, the basic learning model\n"}],"title":{"#tail":"\n","@confidence":"0.6283655","#text":"\nc? 2004 Association for Computational Linguistics\nSample Selection for Statistical Parsing\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.981548823529412","#text":"\nBanko, Michele and Eric Brill. 2001. Scaling\nto very very large corpora for natural\nlanguage disambiguation. In Proceedings of\nthe 39th Annual Meeting of the Association for\nComputational Linguistics, Toulouse,\nFrance, pages 26?33.\nBlum, Avrim and Tom Mitchell. 1998.\nCombining labeled and unlabeled data\nwith co-training. In Proceedings of the 1998\nConference on Computational Learning\nTheory, pages 92?100, Madison, WI.\nBrill, Eric and Philip S. Resnik. 1994. A rule\nbased approach to PP attachment\ndisambiguation. In Proceedings of the 15th\nInternational Conference on Computational\nLinguistics (COLING), Kyoto, Japan, pages\n1198?1204.\n"},{"#tail":"\n","@confidence":"0.999593768595041","#text":"\nHwa Sample Selection for Statistical Parsing\nCharniak, Eugene. 2000. A\nmaximum-entropy-inspired parser. In\nProceedings of the First Meeting of the North\nAmerican Association for Computational\nLinguistics, Seattle.\nCohn, David, Les Atlas, and Richard\nLadner. 1994. Improving generalization\nwith active learning. Machine Learning,\n15(2):201?221.\nCollins, Michael. 1997. Three generative,\nlexicalised models for statistical parsing.\nIn Proceedings of the 35th Annual Meeting of\nthe Association for Computational Linguistics,\npages 16?23, Madrid.\nCollins, Michael. 1999. Head-Driven Statistical\nModels for Natural Language Parsing. Ph.D.\nthesis, University of Pennsylvania,\nPhiladelphia.\nCollins, Michael and James Brooks. 1995.\nPrepositional phrase attachment through\na backed-off model. In Proceedings of the\nThird Workshop on Very Large Corpora,\nCambridge, MA, pages 27?38.\nCover, Thomas M. and Joy A. Thomas.\n1991. Elements of Information Theory. John\nWiley, New York.\nEngelson, Sean P. and Ido Dagan. 1996.\nMinimizing manual annotation cost in\nsupervised training from corpora. In\nProceedings of the 34th Annual Meeting of the\nAssociation for Computational Linguistics,\nSanta Cruz, CA, pages 319?326.\nFreund, Yoav, H. Sebastian Seung, Eli\nShamir, and Naftali Tishby. 1997. Selective\nsampling using the query by committee\nalgorithm. Machine Learning,\n28(2?3):133?168.\nFujii, Atsushi, Kentaro Inui, Takenobu\nTokunaga, and Hozumi Tanaka. 1998.\nSelective sampling for example-based\nword sense disambiguation. Computational\nLinguistics, 24(4):573?598.\nHenderson, John C. and Eric Brill. 2000.\nBagging and boosting a treebank parser.\nIn Proceedings of the First Meeting of the\nNorth American Association for Computational\nLinguistics, Seattle, pages 34?41.\nHwa, Rebecca. 1998. An empirical\nevaluation of probabilistic lexicalized tree\ninsertion grammars. In Proceedings of the\n36th Annual Meeting of the Association for\nComputational Linguistics and 17th\nInternational Conference on Computational\nLinguistics, Montreal, volume 1, pages\n557?563.\nHwa, Rebecca. 2000. Sample selection for\nstatistical grammar induction. In\nProceedings of 2000 Joint SIGDAT Conference\non Empirical Methods in Natural Language\nProcessing and Very Large Corpora, pages\n45?52, Hong Kong, October.\nHwa, Rebecca. 2001a. Learning Probabilistic\nLexicalized Grammars for Natural Language\nProcessing. Ph.D. thesis, Harvard\nUniversity, Cambridge, MA.\nHwa, Rebecca. 2001b. On minimizing\ntraining corpus for parser acquisition. In\nProceedings of the ACL 2001 Workshop on\nComputational Natural Language Learning\n(ConLL-2001), Toulouse, France, pages\n84?89.\nHwa, Rebecca, Miles Osborne, Anoop\nSarkar, and Mark Steedman. 2003.\nCorrected co-training for statistical\nparsers. In Proceedings of the ICML\nWorkshop on the Continuum from Labeled to\nUnlabeled Data in Machine Learning and Data\nMining at the 20th International Conference of\nMachine Learning (ICML-2003),\nWashington, DC, pages 95?102, August.\nJoshi, Aravind K., Leon S. Levy, and\nMasako Takahashi. 1975. Tree adjunction\ngrammars. Journal of Computer and System\nSciences, 10(1): 136?163.\nLari, Karim A. and Steve J. Young. 1990.\nThe estimation of stochastic context-free\ngrammars using the inside-outside\nalgorithm. Computer Speech and Language,\n4:35?56.\nLarsen, Richard J. and Morris L. Marx. 1986.\nAn Introduction to Mathematical Statistics\nand Its Applications. Prentice-Hall,\nEnglewood Cliffs, NJ.\nLewis, David D. and Jason Catlett. 1994.\nHeterogeneous uncertainty sampling for\nsupervised learning. In Proceedings of the\nEleventh International Conference on Machine\nLearning, San Francisco, pages 148?156.\nMagerman, David. 1994. Natural Language\nParsing as Statistical Pattern Recognition.\nPh.D. thesis, Stanford University,\nStanford, CA.\nMarcus, Mitchell, Beatrice Santorini, and\nMary Ann Marcinkiewicz. 1993. Building\na large annotated corpus of English: The\nPenn Treebank. Computational Linguistics,\n19(2):313?330.\nNgai, Grace and David Yarowsky. 2000.\nRule writing or annotation: Cost-efficient\nresource usage for base noun phrase\nchunking. In Proceedings of the 38th Annual\nMeeting of the Association for Computational\nLinguistics, pages 117?125, Hong Kong,\nOctober.\nPereira, Fernando C. N. and Yves Schabes.\n1992. Inside-outside reestimation from\npartially bracketed corpora. In Proceedings\nof the 30th Annual Meeting of the Association\nfor Computational Linguistics, pages\n128?135, Newark, DE.\n"},{"#tail":"\n","@confidence":"0.999746190476191","#text":"\nComputational Linguistics Volume 30, Number 3\nPierce, David and Claire Cardie. 2001.\nLimitations of co-training for natural\nlanguage learning from large datasets. In\nProceedings of the 2001 Conference on\nEmpirical Methods in Natural Language\nProcessing (EMNLP-2001), pages 1?9,\nPittsburgh, PA.\nRatnaparkhi, Adwait. 1998. Statistical\nmodels for unsupervised prepositional\nphrase attachment. In Proceedings of the\n36th Annual Meeting of the Association for\nComputational Linguistics and 17th\nInternational Conference on Computational\nLinguistics, Montreal, volume 2, pages\n1079?1085.\nSarkar, Anoop. 2001. Applying co-training\nmethods to statistical parsing. In\nProceedings of the Second Meeting of the\nNorth American Association for\nComputational Linguistics, Pittsburgh,\npages 175?182, June.\nSchabes, Yves and Richard Waters. 1993.\nStochastic lexicalized context-free\ngrammar. In Proceedings of the Third\nInternational Workshop on Parsing\nTechnologies, Tilburg, The Netherlands,\nand Durbuy, Belgium, pages 257?266.\nSteedman, Mark, Rebecca Hwa, Stephen\nClark, Miles Osborne, Anoop Sarkar, Julia\nHockenmaier, Paul Ruhlen, Steven Baker,\nand Jeremiah Crim. 2003. Example\nselection for bootstrapping statistical\nparsers. In Proceedings of the Joint\nConference of Human Language Technologies\nand the Annual Meeting of the North\nAmerican Chapter of the Association for\nComputational Linguistics, Edmonton,\nAlberta, Canada, pages 236?243.\nSteedman, Mark, Miles Osborne, Anoop\nSarkar, Stephen Clark, Rebecca Hwa, Julia\nHockenmaier, Paul Ruhlen, Steven Baker,\nand Jeremiah Crim. 2003. Bootstrapping\nstatistical parsers from small datasets. In\nProceedings of the Tenth Conference of the\nEuropean Chapter of the Association for\nComputational Linguistics, Budapest, pages\n331?338.\nTang, Min, Xiaoqiang Luo, and Salim\nRoukos. 2002. Active learning for\nstatistical natural language parsing. In\nProceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics,\nPhiladelphia, pages 120?127, July.\nThompson, Cynthia A., Mary Elaine Califf,\nand Raymond J. Mooney. 1999. Active\nlearning for natural language parsing and\ninformation extraction. In Proceedings of\nthe Sixteenth International Conference on\nMachine Learning (ICML-99), pages\n406?414, Bled, Slovenia.\nVan Rijsbergen, Cornelis J. 1979. Information\nRetrieval. Butterworth, London.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.996676","#text":"\nCorpus-based statistical parsing relies on using large quantities of annotated text as training\nexamples. Building this kind of resource is expensive and labor-intensive. This work proposes to\nuse sample selection to find helpful training examples and reduce human effort spent on annotating\nless informative ones. We consider several criteria for predicting whether unlabeled data might\nbe a helpful training example. Experiments are performed across two syntactic learning tasks\nand within the single task of parsing across two learning models to compare the effect of different\npredictive criteria. We find that sample selection can significantly reduce the size of annotated\ntraining corpora and that uncertainty is a robust predictive criterion that can be easily applied to\ndifferent learning models.\n"},{"#tail":"\n","@confidence":"0.963578592592593","#text":"\nMany learning tasks for natural language processing require supervised training; that\nis, the system successfully learns a concept only if it has been given annotated train-\ning data. For example, while it is difficult to induce a grammar with raw text alone,\nthe task is tractable when the syntactic analysis for each sentence is provided as a\npart of the training data (Pereira and Schabes 1992). Current state-of-the-art statisti-\ncal parsers (Collins 1999; Charniak 2000) are all trained on large annotated corpora\nsuch as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However,\nsupervised training data are difficult to obtain; existing corpora might not contain the\nrelevant type of annotation, and the data might not be in the domain of interest. For\nexample, one might need lexical-semantic analyses in addition to the syntactic anal-\nyses in the treebank, or one might be interested in processing languages, domains,\nor genres for which there are no annotated corpora. Because supervised training de-\nmands significant human involvement (e.g., annotating the syntactic structure of each\nsentence by hand), creating a new corpus is a labor-intensive and time-consuming en-\ndeavor. The goal of this work is to minimize a system?s reliance on annotated training\ndata.\nOne promising approach to mitigating the annotation bottleneck problem is to\nuse sample selection, a variant of active learning. Sample selection is an interactive\nlearning method in which the machine takes the initiative in selecting unlabeled data\nfor the human to annotate. Under this framework, the system has access to a large pool\nof unlabeled data, and it has to predict how much it can learn from each candidate in\nthe pool if that candidate is labeled. More quantitatively, we associate each candidate\nin the pool with a training utility value (TUV). If the system can accurately identify\nthe subset of examples with the highest TUV, it will have located the most beneficial\n? Computer Science Department, Pittsburgh, PA 15260. E-mail: hwa@cs.pitt.edu.\nSubmission received: 14 October 2002; Revised submission received: 30 September 2003; Accepted for\npublication: 22 December 2003\n"},{"#tail":"\n","@confidence":"0.989478058823529","#text":"\nComputational Linguistics Volume 30, Number 3\ntraining examples, thus freeing the annotators from having to label less informative\nexamples.\nIn this article, we apply sample selection to two syntactic learning tasks: training\na prepositional-phrase attachment (PP-attachment) model and training a statistical\nparsing model. We are interested in addressing two main questions. First, what are\ngood predictors of a candidate?s training utility? We propose several predictive criteria\nand define evaluation functions based on them to rank the candidates? utility. We\nhave performed experiments comparing the effect of these evaluation functions on\nthe size of the training corpus. We find that, with a judiciously chosen evaluation\nfunction, sample selection can significantly reduce the size of the training corpus. The\nsecond main question is: Are the predictors consistently effective for different types of\nlearners? We compare the predictive criteria both across tasks (between PP-attachment\nand parsing) and within a single task (applying the criteria to two parsing models:\nan expectation-maximization-trained parser and a count-based parser). We find that\nthe learner?s uncertainty is a robust predictive criterion that can be easily applied to\ndifferent learning models.\n"},{"#tail":"\n","@confidence":"0.983822153846154","#text":"\nUnlike traditional learning systems that receive training examples indiscriminately,\na sample selection learning system actively influences its own progress by choosing\nnew examples to incorporate into its training set. There are two types of selection algo-\nrithms: committee-based and single learner. A committee-based selection algorithm\nworks with multiple learners, each maintaining a different hypothesis (perhaps per-\ntaining to different aspects of the problem). The candidate examples that lead to the\nmost disagreements among the different learners are considered to have the highest\nTUV (Cohn, Atlas, and Ladner 1994; Freund et al 1997). For computationally intensive\nproblems, such as parsing, keeping multiple learners may be impractical.\nIn this work, we focus on sample selection using a single learner that keeps one\nworking hypothesis. Without access to multiple hypotheses, the selection algorithm\ncan nonetheless estimate the TUV of a candidate. We identify the following three\nclasses of predictive criteria:\n"},{"#tail":"\n","@confidence":"0.908111933333333","#text":"\ninformation about the type of candidates that are particularly plentiful or\ndifficult to learn. This criterion focuses on the general attributes of the\nlearning problem, such as the distribution of the input data and\nproperties of the learning algorithm, but it ignores the current state of\nthe hypothesis.\n2. Performance of the hypothesis: Testing the candidates on the current\nworking hypothesis shows the type of input data on which the\nhypothesis may perform weakly. That is, if the current hypothesis is\nunable to label a candidate or is uncertain about it, then the candidate\nmight be a good training example (Lewis and Catlett 1994). The\nunderlying assumption is that an uncertain output is likely to be wrong.\n3. Parameters of the hypothesis: Estimating the potential impact that the\ncandidates will have on the parameters of the current working\nhypothesis locates those examples that will change the current\nhypothesis the most.\n"},{"#tail":"\n","@confidence":"0.3664312","#text":"\nHwa Sample Selection for Statistical Parsing\nU is a set of unlabeled candidates.\nL is a set of labeled training examples.\nC is the current hypothesis.\nInitialize:\n"},{"#tail":"\n","@confidence":"0.967158944444445","#text":"\nUntil (C is good enough) or (U = ?) or (cutoff).\nFigure 1\nPseudo code for the sample selection learning algorithm.\nFigure 1 outlines the single-learner sample selection training loop in pseudocode.\nInitially, the training set, L, consists of a small number of labeled examples, based on\nwhich the learner proposes its first hypothesis of the target concept, C. Also available\nto the learner is a large pool of unlabeled training candidates, U. In each training\niteration, the selection algorithm, Select(n, U, C, f ), ranks the candidates of U according\nto their expected TUVs and returns the n candidates with the highest values. The\nalgorithm predicts the TUV of each candidate, u ? U, with an evaluation function,\nf (u, C). This function may rely on the hypothesis concept C to estimate the utility of a\ncandidate u. The n chosen candidates are then labeled by human experts and added\nto the existing training set. Running the learning algorithm, Train(L), on the updated\ntraining set, the system proposes a new hypothesis regarding the target concept that\nis the most compatible with the examples seen thus far. The loop continues until one\nof three stopping conditions is met: The hypothesis is considered to perform well\nenough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no\nmore resources).\n"},{"#tail":"\n","@confidence":"0.999442833333333","#text":"\nOne common source of structural ambiguities arises from syntactic constructs in which\na prepositional phrase might be equally likely to modify the verb or the noun pre-\nceding it. Researchers have proposed many computational models for resolving PP-\nattachment ambiguities. Some well-known approaches include rule-based models (Brill\nand Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximum-\nentropy model (Ratnaparkhi 1998). Following the tradition of using learning PP-\nattachment as a way to gain insight into the parsing problem, we first apply sample\nselection to reduce the amount of annotation used in training a PP-attachment model.\nWe use the Collins-Brooks model as the basic learning algorithm and experiment with\nseveral evaluation functions based on the types of predictive criteria described earlier.\nOur experiments show that the best evaluation function can reduce the number of\nlabeled examples by nearly half without loss of accuracy.\n"},{"#tail":"\n","@confidence":"0.99638","#text":"\nThe Collins-Brooks model takes prepositional phrases and their attachment classifica-\ntions as training examples: each is represented as a quintuple of the form (v, n, p, n2, a),\nwhere v, n, p, and n2 are the head words of the verb phrase, the object noun phrase, the\n"},{"#tail":"\n","@confidence":"0.760784","#text":"\nComputational Linguistics Volume 30, Number 3\nsubroutine Train(L)\nforeach ex ? L do\nextract (v, n, p, n2, a) from ex\nforeach tuple ? {(v, n, p, n2), (v, p, n2), (n, p, n2), (v, n, p), (v, p), (n, p), (p, n2), (p)} do\n"},{"#tail":"\n","@confidence":"0.557667666666667","#text":"\nforeach u ? U do\nextract (v, n, p, n2) from u\nif Count(v, n, p, n2) > 0 then\n"},{"#tail":"\n","@confidence":"0.975220346153846","#text":"\nelse prob ? 1\nif prob ? .5 then\noutput noun\nelse output verb\nFigure 2\nThe Collins-Brooks PP-attachment classification algorithm.\npreposition, and the prepositional noun phrase, respectively, and a specifies the attach-\nment classification. For example, (wrote a book in three days, attach-verb) would be anno-\ntated as (wrote, book, in, days, verb). The head words can be automatically extracted using\na heuristic table lookup in the manner described by Magerman (1994). For this learning\nproblem, the supervision is the one-bit information of whether p should attach to v or\nto n. In order to learn the attachment preferences of prepositional phrases, the system\nbuilds attachment statistics for each the characteristic tuple of all training examples. A\ncharacteristic tuple is some subset of the four head words in the example, with the con-\ndition that one of the elements must be the preposition. Each training example forms\neight characteristic tuples: (v, n, p, n2), (v, n, p), (v, p, n2), (n, p, n2), (v, p), (n, p), (p, n2), (p).\nThe attachment statistics are a collection of the occurrence frequencies for all the char-\nacteristic tuples in the training set and the occurrence frequencies for the characteristic\ntuples of those examples determined to attach to nouns. For some characteristic tuple\nt, Count(t) denotes the former and CountNP(t) denotes the latter. In terms of the sample\nselection algorithm, the collection of counts represents the learner?s current hypothesis\n(C in Figure 1). Figure 2 provides the pseudocode for the Train routine.\nOnce trained, the system can be used to classify test cases based on the statistics of\nthe most similar training examples and back off as necessary. For instance, to determine\nthe PP-attachment for a test case, the classifier would first consider the ratio of the\ntwo frequency counts for the four-word characteristic tuple of the test case. If the tuple\n"},{"#tail":"\n","@confidence":"0.946359538461538","#text":"\nHwa Sample Selection for Statistical Parsing\nFigure 3\nIn this example, the classification of the test case preposition is backed off to the\ntwo-word-tuple level. In the diagram, each circle represents a characteristic tuple. A filled\ncircle denotes that the tuple has occurred in the training set. The dashed rectangular box\nindicates the back-off level on which the classification is made.\nnever occurred in the training example, the classifier would then back off to look at the\ntest case?s three three-word characteristic tuples. It would continue to back off further,\nif necessary. In the case that the model has no information on any of the characteristic\ntuples of the test case, it would, by default, classify the test case as an instance of noun\nattachment. Figure 3 shows using the back-off scheme on a test case. We describe in\nthe Test pseudocode routine in Figure 2 the model?s classification procedure for each\nback-off level.\n"},{"#tail":"\n","@confidence":"0.9994145","#text":"\nBased on the three classes of predictive criteria discussed in Section 2, we propose\nseveral evaluation functions for the Collins-Brooks model.\n"},{"#tail":"\n","@confidence":"0.99738","#text":"\nthe PP-attachment model and properties of English prepositional phrases. For instance,\nwe know that the most problematic test cases for the PP-attachment model are those\nfor which it has no statistics at all. Therefore, those data that the system has not\nyet encountered might be good candidates. The first evaluation function we define,\nfnovel(u, C), equates the TUV of a candidate u with its degree of novelty, the number\nof its characteristic tuples that currently have zero counts:1\n"},{"#tail":"\n","@confidence":"0.889496142857143","#text":"\nThis evaluation function has some blatant defects. It may distort the data distribution\nso much that the system will not be able to build up a reliable collection of statistics.\nThe function does not take into account the intuition that those data that rarely occur,\nno matter how novel, probably have overall low training utility. Moreover, the scoring\nscheme does not make any distinction between the characteristic tuples of a candidate.\n1 Note that the current hypothesis C is ignored in evaluation functions of this class because they depend\nonly on the knowledge about the problem space.\n"},{"#tail":"\n","@confidence":"0.988192076923077","#text":"\nIf candidate u1 is selected, a total of 22 tuples can be ignored. The dashed rectangles show the\nclassification level before training, and the solid rectangles show the classification level after\nthe statistics of u1 have been taken. The obviated tuples are represented by the filled black\ncircles.\nWe know, however, that the PP-attachment classifier is a back-off model that makes\nits decision based first on statistics of the characteristic tuple with the most words. A\nmore sophisticated sampling of the data domain should consider not only the novelty\nof the data, but also the frequency of its occurrence, as well as the quality of its charac-\nteristic tuples. We define a back-off-model-based evaluation function, fbackoff(u, C), that\nscores a candidate u by counting the number of characteristic tuples that would be\nobviated in all candidates if u were included in the training set. For example, suppose\nwe have a small pool of five candidates, and we are about to pick the first training\nexample:\n"},{"#tail":"\n","@confidence":"0.986174588235294","#text":"\nAccording to fbackoff, either u1 or u2 would be the best choice. By selecting either as\nthe first training example, we could ignore all but the four-word characteristic tuple\nfor both u1 and u2 (a saving of seven tuples each); since u3 and u4 each have three\nwords in common with the first two candidates, they would no longer depend on\ntheir lower four tuples; and although we would also improve the statistics for one\nof u5?s tuples (on), nothing could be pruned from u5?s characteristic tuples. Thus,\nfbackoff(u1, C) = fbackoff(u2, C) = 7 + 7 + 4 + 4 = 22 (see Figure 4).\nUnder fbackoff, if u1 were chosen as the first example, u2 would lose all its utility,\nbecause we could not prune any extra characteristic tuples by using u2. That is, in\nthe next round of selection, fbackoff(u2, C) = 0. Candidate u5 would be the best second\nexample because it would now have the most tuples to prune (7 tuples).\nThe evaluation function fbackoff improves upon fnovel in two ways. First, novel can-\ndidates that occur frequently are favored over those that rarely come up. As we have\nseen in the above example, a candidate that is similar to other candidates can elimi-\nnate more characteristic tuples all at once. Second, the evaluation strategy follows the\nworking principle of the back-off model and discounts lower-level characteristic tuples\nthat do not affect the classification process, even if they were ?novel.? For instance,\n"},{"#tail":"\n","@confidence":"0.977062333333333","#text":"\nHwa Sample Selection for Statistical Parsing\nafter selecting u1 as the first training example, we would no longer care about the\ntwo-word tuples of u4 such as (wrote, on), even though we have no statistics for them.\nA potential problem with fbackoff is that after all the obvious candidates have been\nselected, the function is not very good at differentiating between the remaining can-\ndidates that have about the same level of novelty and occur infrequently.\n"},{"#tail":"\n","@confidence":"0.9987288","#text":"\nprevious section score candidates based on prior knowledge alone, independent of the\ncurrent state of the learner?s hypothesis and the annotation of the selected training\nexamples. To attune the selection of training examples to the learner?s progress, an\nevaluation function might factor in its current hypothesis in predicting a candidate?s\nTUV.\nOne way to incorporate the current hypothesis into the evaluation function is\nto score each candidate using the current model, assuming its hypothesis is right.\nAn error-driven evaluation function, ferr, equates the TUV of a candidate with the\nhypothesis? estimate of its likelihood to misclassify that candidate (i.e., one minus the\nprobability of the most-likely class). If the hypothesis predicts that the likelihood of a\nprepositional phrase to attach to the noun is 80%, and if the hypothesis is accurate,\nthen there is a 20% chance that it has misclassified.\nA related evaluation function is one that measures the hypothesis?s uncertainty\nacross all classes, rather than focusing on only the most likely class. Intuitively, if the\nhypothesis classifies a candidate as equally likely to attach to the verb as to the noun, it\nis the most uncertain of its answer. If the hypothesis assigns a candidate to a class with\na probability of one, then it is the most certain of its answer. For the binary-class case,\nthe uncertainty-based evaluation function, func, can be expressed in the same way as\nthe error-driven function, as a function that is symmetric about 0.5 and monotonically\ndecreases if the hypothesis prefers one class over another:2\n"},{"#tail":"\n","@confidence":"0.9565895","#text":"\nIn the general case of choosing between multiple classes, ferr and func are different from\none another. We shall return to this point in Section 4.1.2 when we consider training\nparsers.\nThe potential drawback of the performance-based evaluation functions is that they\nassume that the hypothesis is correct. Selecting training examples based on a poor hy-\npothesis is prone to pitfalls. On the one hand, the hypothesis may be overly confident\nabout the certainty of its decisions. For example, the hypothesis may assign noun to\na candidate with a probability of one based on parameter estimates computed from a\nsingle previous observation in which a similar example was labeled as noun. Despite\nthe unreliable statistics, this candidate would not be selected, since the hypothesis\nconsiders this a known case. Conversely, the hypothesis may also direct the selec-\ntion algorithm to chase after undecidable cases. For example, consider prepositional\nphrases (PPs) with in as the head. These PPs occur frequently, and about half of them\nshould attach to the object noun. Even though training on more labeled in examples\n2 As long as it adheres to these criteria, the specific form of the function is irrelevant, since the selection\nis not determined by the absolute scores of the candidates, but by their scores relative to each other.\n"},{"#tail":"\n","@confidence":"0.979023","#text":"\nComputational Linguistics Volume 30, Number 3\ndoes not improve the model?s performance on future in PPs, the selection algorithm\nwill keep on requesting more in training examples because the hypothesis remains un-\ncertain about this preposition.3 With an unlucky starting hypothesis, these evaluation\nfunctions may select uninformative candidates initially.\n"},{"#tail":"\n","@confidence":"0.999647","#text":"\nbased evaluation function stems from their trust in the model?s diagnosis of its own\nprogress. Another way to incorporate the current hypothesis is to determine how good\nit is and what type of examples will improve it the most. In this section we propose\nan evaluation function that scores candidates based on their utilities in increasing the\nconfidence about the parameters of the hypothesis (i.e., the collection of statistics over\nthe characteristic tuples of the training examples).\nTraining the parameters of the PP-attachment model is similar to empirically de-\ntermining the bias of a coin. We measure the coin?s bias by repeatedly tossing it and\nkeeping track of the percentage of times it lands on heads. The more trials we perform,\nthe more confident we become about our estimation of the bias. Similarly, in estimating\np, the likelihood of a PP?s attaching to its object noun, we are more confident about the\nclassification decision based on statistics with higher counts than based on statistics\nwith lower counts. A quantitative measurement of our confidence in a statistic is the\nconfidence interval. This is a region around the measured statistic, bounding the area\nwithin which the true statistic may lie. More specifically, the confidence interval for p,\na binomial parameter, is defined as\n"},{"#tail":"\n","@confidence":"0.995830142857143","#text":"\nwhere p? is the expected value of p based on n trials, and t is a threshold value that\ndepends on the number of trials and the level of confidence we desire. For instance,\nif we want to be 90% confident that the true statistic p lies within the interval, and p?\nis based on n = 30 trials, then we set t to be 1.697.4 Applying the confidence interval\nconcept to evaluating candidates for the back-off PP-attachment model, we define\na function fconf that scores a candidate by taking the average of the lengths of the\nconfidence interval of each back-off level. That is,\n"},{"#tail":"\n","@confidence":"0.943281181818182","#text":"\nwhere p?l(u, C) is the probability that model C will attach u to noun at back-off level l,\nand nl(u, C) is the number of training examples upon which this classification is based.\nThe confidence-based evaluation function has several potential problems. One of\nits flaws is similar to that of fnovel. In the early stage, fconf picks the same examples\n3 This phenomenon is particularly acute in the early stages of refining the hypothesis because most\ndecisions are based on statistics of the head preposition alone; in the later stages, the hypothesis can\nusually rely on higher-ordered characteristic tuples that tend to be better classifiers.\n4 For n ? 120, the values of t can be found in standard statistic textbooks; for n ? 120, t = 1.6576.\nBecause the derivation for the confidence interval equation makes a normality assumption, the\nequation does not hold for small values of n (cf Larsen and Marx [1986], pp. 277?278). When n is large,\nthe contributions from the terms in t\n"},{"#tail":"\n","@confidence":"0.80050075","#text":"\nn are negligible. Dropping these terms, we have the t statistic for\nlarge n, p? ? t\n?\np?(1 ? p?)/n.\n"},{"#tail":"\n","@confidence":"0.970530809523809","#text":"\nHwa Sample Selection for Statistical Parsing\nas fnovel, because we have no confidence in the statistics of novel examples. Therefore,\nfconf is also prone to chase after examples that rarely occur to build up the confidence\nof some unimportant parameters. A second problem is that fconf ignores the output of\nthe model. Thus, if candidate A has a confidence interval around [0.6, 1] and candidate\nB has a confidence interval around [0.4, 0.7], then fconf will prefer candidate A, even\nthough training on A will not change the hypothesis?s performance, since the entire\nconfidence interval is already in the noun zone.\n3.2.4 Hybrid Function. The three categories of predictive criteria discussed above are\ncomplementary, each focusing on a different aspect of the learner?s weakness. There-\nfore, it may be beneficial to combine these criteria into one evaluation function. For\ninstance, the deficiency of the confidence-based evaluation function described in the\nprevious section can be avoided if the confidence interval covering the region around\nthe uncertainty boundary (candidate B in the example just discussed) is weighed more\nheavily than one around the end points (candidate A).\nIn this section, we introduce a new function that tries to factor in both the uncer-\ntainty of the model performance and the confidence of the model parameters. First, we\ndefine a function, called area(p?, n), that computes the area under a Gaussian function\nN(x,?,?) with a mean of 0.5 and a standard deviation of 0.1 that is bounded by the\nconfidence interval as computed by conf int(p?, n) (see Figure 5).5 That is, suppose p?\nhas a confidence interval of [a, b]; then\n"},{"#tail":"\n","@confidence":"0.9979775","#text":"\nComputing area for each back-off level, we define an evaluation function, farea(u, C),\nas their average. This function can be viewed as a product of fconf and func.6\n"},{"#tail":"\n","@confidence":"0.999028705882353","#text":"\nTo determine the relative merits of the proposed evaluation functions, we compare the\nlearning curve of training with sample selection according to each function against a\nbaseline of random selection in an empirical study. The corpus for this comparison is\na collection of phrases extracted from the Wall Street Journal (WSJ) Treebank. We use\nSection 00 as the development set and Sections 2-23 as the training and test sets. We\nperform 10-fold cross-validation to ensure the statistical significance of the results. For\neach fold, the training candidate pool contains about 21,000 phrases, and the test set\ncontained about 2,000 phrases.\nAs shown in Figure 1, the learner generates an initial hypothesis based on a small\nset of training examples, L. These examples are randomly selected from the pool of\nunlabeled candidates and annotated by a human. Random sampling ensures that the\ninitial trained set reflects the distribution of the candidate pool and thus that the\ninitial hypothesis is unbiased. Starting with an unbiased hypothesis is important for\nthose evaluation functions whose scoring metrics are affected by the accuracy of the\nhypothesis. In these experiments, L initially contains 500 randomly selected examples.\nIn each selection iteration, all the candidates are scored by the evaluation function,\nand n examples with the highest TUVs are picked out from U to be labeled and added\n"},{"#tail":"\n","@confidence":"0.540329666666667","#text":"\ndistribution is between 0.25 and 0.75.\n6 Note that we can replace the function in equation (1) with the N(x, 0.5, ?) without affecting func,\nbecause it is also symmetric about 0.5 and monotonically decreasing as the input value moves further.\n"},{"#tail":"\n","@confidence":"0.991320173913044","#text":"\nFigure 5\nAn example: Suppose that the candidate has a likelihood of 0.4 for noun attachment and a\nconfidence interval of width 0.1. Then area computes the area bounded by the confidence\ninterval and the Gaussian curve.\nto L. Ideally, we would like to have n = 1 for each iteration. In practice, however, it is\noften more convenient for the human annotator to label data in larger batches rather\nthan one at a time. In these experiments, we use a batch size of n = 500 examples.\nWe make note of one caveat to this kind of n-best batch selection. Under a\nhypothesis-dependent evaluation function, identical examples will receive identical\nscores. Because identical (or very similar) examples tend to address the same defi-\nciency in the hypothesis, adding n very similar examples to the training set is unlikely\nto lead to big improvements in the hypothesis. To diversify the examples in each batch,\nwe simulate single-example selection (whenever possible) by reestimating the scores\nof the candidates after each selection. Suppose we have just chosen to add candidate\nx to the batch. Then, before selecting the next candidate, we estimate the potential\ndecrease in scores of candidates similar to x once it belongs to the annotated training\nset. The estimation is based entirely on the knowledge that x is chosen, but not on\nthe classification of x. Thus, only certain types of evaluation functions are amenable\nto the reestimation process. For example, if scores have been assigned by fconf, then\nwe know that the confidence intervals of the candidates similar to x must decrease\nslightly after learning x. On the other hand, if scores have been assigned by func, then\nwe cannot perceive any changes in the scores of similar candidates without knowing\nthe true classification of x.\n"},{"#tail":"\n","@confidence":"0.998958777777778","#text":"\nthe model?s performances using training examples selected by different evaluation\nfunctions. We compare each proposed function with the baseline of random selection\n(frand). The results are graphically depicted from two perspectives. One (e.g., Figure\n6(a)?6(c)) plots the learning curves of the functions, showing the relationship between\nthe number of training examples (x-axis) and the performance of the model on test\ndata (y-axis). We deem one evaluation function to be better than another if its learning\ncurve envelopes the other?s. An alternative way to interpret the results is to focus on\nthe reduction in training size offered by one evaluation function over another for some\nparticular performance level. Figure 6(d) is a bar graph comparing all the evaluation\n"},{"#tail":"\n","@confidence":"0.997706","#text":"\nA comparison of the performance of different evaluation functions: (a) compares the learning\ncurves of the functions that use knowledge about the problem space (fnovel and fbackoff) with\nthat of the baseline; (b) compares the learning curves of performance-based function (func and\nfconf ) with the baseline; (c) compares the learning curve of farea, which combines uncertainty\nand confidence, with func, fconf, and the baseline; (d) compares all the functions for the number\nof training examples selected at the final performance level (83.8%).\nfunctions at the highest performance level. The graph shows that in order to train a\nmodel that attaches PPs with an accuracy rate of 83.8%, sample selection with fnovel\nrequires 2,500 fewer examples than the baseline.\nCompared to fnovel, fbackoff selects more helpful training examples in the early stage.\nAs shown in Figure 6(a), the improvement rate of the model under fbackoff is always\nat least as fast that for as fnovel. However, the differences between these two functions\nbecome smaller for higher performance levels. This outcome validates our predictions.\nScoring candidates by a combination of their novelty, occurrence frequencies, and the\nqualities of their characteristic tuples, fbackoff selects helpful early (the first 4,000 or so)\ntraining examples. Then, just as in fnovel, the learning rate remains stagnant for the\nnext 2,000 poorly selected examples. Finally, when the remaining candidates all have\nsimilar novelty values and contain mostly characteristic tuples that occur infrequently,\nthe selection becomes random.\nFigure 6(b) compares the two evaluation functions that score candidates based on\nthe current state of the hypothesis. Although both functions suffer a slow start, they\nare more effective than fbackoff at reducing the training set when learning high-quality\nmodels. Initially, because all the unknown statistics are initialized to 0.5, selection based\non func is essentially random sampling. Only after the hypothesis becomes sufficiently\naccurate (after training on about 5,000 annotated examples) does it begin to make\n"},{"#tail":"\n","@confidence":"0.942277734693877","#text":"\nComputational Linguistics Volume 30, Number 3\ninformed selections. Following a similar but more exaggerated pattern, the confidence-\nbased function, fconf, also improves slowly at the beginning before finally overtaking\nthe baseline. As we noted earlier, because the hypothesis is not confident about novel\ncandidates, fconf and fnovel tend to select the same early examples. Therefore, the early\nlearning rate of fconf is as poor as that of fnovel. In the later stage, while fnovel continues\nto flounder, fconf can select better candidates based on a more reliable hypothesis.\nFinally, the best-performing evaluation function is the hybrid approach. Figure\n6(c) shows that the learning curve of farea combines the earlier success of func and the\nlater success of fconf to always outperform the other functions. As shown in Figure\n6(d), it requires the least number of examples to achieve the highest performance level\nof 83.8%. Compared to the baseline, farea requires 47% fewer examples to achieve this\nperformance level. From these comparison studies, we conclude that involving the\nhypothesis in the selection process is a key factor in reducing the size of the training\nset.\n4. Sample Selecting for Statistical Parsing\nIn applying sample selection to training a PP-attachment model, we have observed\nthat all effective evaluation functions make use of the model?s current hypothesis in\nestimating the training utility of the candidates. Although knowledge about the prob-\nlem space seems to help sharpening the learning curve initially, overall, it is not a good\npredictor. In this section, we investigate whether these observations hold true for train-\ning statistical parsing models as well. Moreover, in order to determine whether the\nperformances of the predictive criteria are consistent across different learning models\nwithin the same domain, we have performed the study on two parsing models: one\nbased on a context-free variant of tree-adjoining grammars (Joshi, Levy, and Taka-\nhashi 1975), the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism\n(Schabes and Waters 1993; Hwa 1998), and Collins?s Model 2 parser (1997). Although\nboth models are lexicalized, statistical parsers, their learning algorithms are different.\nThe Collins Parser is a fully supervised, history-based learner that models the pa-\nrameters of the parser by taking statistics directly from the training data. In contrast,\nPLTIG?s expectation-maximization-based induction algorithm is partially supervised;\nthe model?s parameters are estimated indirectly from the training data.\nAs a superset of the PP-attachment task, parsing is a more challenging learning\nproblem. Whereas a trained PP-attachment model is a binary classifier, a parser must\nidentify the correct syntactic analysis out of all possible parses for a sentence. This\nclassification task is more difficult than PP-attachment, since the number of possible\nparses for a sentence grows exponentially with respect to its length. Consequently,\nthe annotator?s task is more complex. Whereas the person labeling the training data\nfor PP-attachment reveals one unit of information (always choosing between noun or\nverb), the annotation needed for parser training is usually greater than one unit,7 and\nthe type of labels varies from sentence to sentence. Because the annotation complexity\ndiffers from sentence to sentence, the evaluation functions must strike a balance be-\ntween maximizing potential informational gain and minimizing the expected amount\n7 We consider each pair of brackets in the training sentence to be one unit of supervised information,\nassuming that the number of brackets correlates linearly with the amount of effort spent by the human\nannotator. This correlation is an approximation, however; in real life, adding one pair of brackets to a\nlonger sentence may require more effort than adding a pair of brackets to a shorter one. To capture\nbracketing interdependencies at this level, we would need to develop a model of the annotation\ndecision process and incorporate it as an additional factor in the evaluation functions.\n"},{"#tail":"\n","@confidence":"0.925647","#text":"\nHwa Sample Selection for Statistical Parsing\nof annotation exerted. We propose a set of evaluation functions similar in spirit to those\nfor the PP-attachment learner, but extended to accommodate the parsing domain.\n"},{"#tail":"\n","@confidence":"0.957026","#text":"\nfrequencies of its characteristic tuples, we define an evaluation function, flex(w, G) that\nscores a sentence candidate, w, based on the novelty and frequencies of word pair\nco-occurrences:\n"},{"#tail":"\n","@confidence":"0.983658190476191","#text":"\nwhere w is the unlabeled sentence candidate, G is the current parsing model (which\nis ignored by problem-space-based evaluation functions), new(wi, wj) is an indicator\nfunction that returns one if we have not yet selected any sentence in which wi and wj\nco-occurred, and coocc(wi, wj) is a function that returns the number of times that wi co-\noccurs8 with wj in the candidate pool. We expect these evaluation functions to be less\nrelevant for the parsing domain than for the PP-attachment domain for two reasons.\nFirst, because we do not have the actual parses, the extraction of lexical relationships\nis based on co-occurrence statistics, not syntactic relationships. Second, because the\ndistribution of words that form lexical relationships is wider and more uniform than\nthat of words that form PP characteristic tuples, most word pairs will be novel and\nappear only once.\nAnother simple evaluation function based on the problem space is one that esti-\nmates the TUV of a candidate from its sentence length:\nflen(w, G) = length(w)\nThe intuition behind this function is based on the general observation that longer\nsentences tend to have complex structures and introduce more opportunities for am-\nbiguous parses. Although these evaluation functions may seem simplistic, they have\none major advantage: They are easy to compute and require little processing time.\nBecause inducing parsing models demands significantly more time than inducing PP-\nattachment models, it becomes more important that the evaluation functions for pars-\ning models be as efficient as possible.\n"},{"#tail":"\n","@confidence":"0.9215475","#text":"\nbased evaluation functions: ferr, the model?s estimate of the likelihood that is has\nmade a classification error, and func, the model?s estimate of its uncertainty in making\nthe classification. We have shown the two functions to have similar performance for\nthe PP-attachment task. This is not the case for statistical parsing, however, because\nthe number of possible classes (parse trees) differs from sentence to sentence. For\nexample, suppose we wish to compare one candidate for which the current parsing\nmodel generated four equally likely parses with another candidate for which the model\ngenerated 1 parse with probability of 0.2 and 99 other parses with a probability of\n0.01 (such that they sum to 0.98). The error-driven function, ferr, would score the latter\ncandidate higher because its most likely parse has a lower probability than that of the\nmost likely parse of the former candidate; the uncertainty-based function, func, would\nscore the former candidate higher because the model does not have a strong preference\n8 We consider two words to be co-occuring if their log-likelihood ratio is greater than some threshold\nvalue determined with held-out data.\n"},{"#tail":"\n","@confidence":"0.968377166666667","#text":"\nComputational Linguistics Volume 30, Number 3\nfor one parse over any other. In this section, we provide a formal definition for both\nfunctions.\nSuppose that a parsing model G generates a candidate sentence w with probability\nP(w  |G), and that the set V contains all possible parses that G generated for w. Then,\nwe denote the probability of G?s generating a single parse, v ? V , as P(v  |G) such that\n"},{"#tail":"\n","@confidence":"0.9604128","#text":"\nNote that P(v  |G) reflects the probability of one particular parse tree, v, out of all\npossible parse trees for all possible sentences that G can generate. To compute the\nlikelihood of a parse?s being the correct parse out of the possible parses of w according\nto G, denoted as P(v  |w, G), we need to normalize the tree probability by the sentence\nprobability. So according to G, the likelihood that vmax is the correct parse for w is9\n"},{"#tail":"\n","@confidence":"0.989302166666667","#text":"\nTherefore, the error-driven evaluation function is defined as\nferr(w, G) = 1 ? P(vmax  |w, G)\nUnlike the error-driven function, which focuses on the most likely parse, the\nuncertainty-based function takes the probability distribution of all parses into account.\nTo quantitatively characterize its distribution, we compute the entropy of the distri-\nbution. That is,\n"},{"#tail":"\n","@confidence":"0.945429","#text":"\nbe found in textbooks on information theory (e.g., Cover and Thomas 1991).\nDetermining the parse tree for a sentence from a set of possible parses can be\nviewed as assigning a value to a random variable. Thus, a direct application of the\nentropy definition to the probability distribution of the parses for sentence w in G\ncomputes its tree entropy, TE(w, G), the expected number of bits needed to encode\nthe distribution of possible parses for w. However, we may not wish to compare\ntwo sentences with different numbers of parses by their entropy directly. If the parse\nprobability distributions for both sentences are uniform, the sentence with more parses\nwill have a higher entropy. Because longer sentences typically have more parses, using\nentropy directly would result in a bias toward selecting long sentences. To normalize\nfor the number of parses, the uncertainty-based evaluation function, func, is defined as\na measurement of similarity between the actual probability distribution of the parses\nand a hypothetical uniform distribution for that set of parses. In particular, we divide\n9 Note that P(w|v, G) = 1 for any v ? V , where V is the set of all possible parses for w, because v exists\nonly when w is observed.\n"},{"#tail":"\n","@confidence":"0.79143","#text":"\nHwa Sample Selection for Statistical Parsing\nthe tree entropy by the log of the number of parses:10\n"},{"#tail":"\n","@confidence":"0.920870666666667","#text":"\nWe now derive the expression for TE(w, G). Recall from equation (2) that if G\nproduces a set of parses, V , for sentence w, the set of probabilities P(v  |w, G) (for all\nv ? V) defines the distribution of parsing likelihoods for sentence w:\n"},{"#tail":"\n","@confidence":"0.933236333333333","#text":"\nNote that P(v  |w, G) can be viewed as a density function p(v) (i.e., the probability of\nassigning v to a random variable V). Mapping it back into the entropy definition from\nequation (3), we derive the tree entropy of w as follows:\n"},{"#tail":"\n","@confidence":"0.995722","#text":"\nUsing the bottom-up, dynamic programming technique (see the appendix for de-\ntails) of computing inside probabilities (Lari and Young 1990), we can efficiently com-\npute the probability of the sentence, P(w  |G). Similarly, the algorithm can be modified\nto compute the quantity\n"},{"#tail":"\n","@confidence":"0.980807846153846","#text":"\ngives good TUV estimates to candidates for training PP-attachment models, it is not\nclear how a similar technique can be applied to training parsers. Whereas binary\nclassification tasks can be described by binomial distributions, for which the confi-\ndence interval is well defined, a parsing model is made up of many multinomial\nclassification decisions. We therefore need a way to characterize the confidence for\neach decision as well as a way to combine them into an overall confidence. Another\ndifficulty is that the complexity of the induction algorithm deters us from reestimat-\ning the TUVs of the remaining candidates after selecting each new candidate. As we\n10 When func(w, G) = 1, the parser is considered to be the most uncertain about a particular sentence.\nInstead of dividing tree entropies, one could have computed the Kullback-Leibler distance between the\ntwo distributions (in which case a score of zero would indicate the highest level of uncertainty).\nBecause the selection is based on relative scores, as long as the function is monotonic, the exact form of\nthe function should not have much impact on the outcome.\n"},{"#tail":"\n","@confidence":"0.983615","#text":"\nComputational Linguistics Volume 30, Number 3\ndiscussed in Section 3.3, reestimation is important for batched annotation. Without\nsome means of updating the TUVs after each selection, the learner will not realize that\nit has already selected a candidate to train some parameter with low confidence until\nthe retraining phase, which occurs only at the end of the batch selection; therefore, it\nmay continue to select very similar candidates to train the same parameter. Even if we\nassume that the statistics can be updated, reestimating the TUVs is a computationally\nexpensive operation. Essentially, all the remaining candidates that share some param-\neters with the selected candidate will need to be re-parsed. For these practical rea-\nsons, we do not include an evaluation function measuring confidence for the parsing\nexperiment.\n"},{"#tail":"\n","@confidence":"0.943425","#text":"\nWe compare the effectiveness of sample selection using the proposed evaluation func-\ntions against a baseline of random selection (frand(w, G) = rand()). Similarly to previous\nexperimental designs, the learner is given a small set of annotated seed data from the\nWSJ Treebank and a large set of unlabeled data (also from the WSJ Treebank but with\nthe labels removed) from which to select new training examples. All training data are\nfrom Sections 2?21 of the treebank. We monitor the learning progress of the parser by\ntesting it on unseen test sentences. We use Section 00 for development and Section\n23 for testing. This study is repeated for two different models, the PLTIG parser and\nCollins?s Model 2 parser.\n"},{"#tail":"\n","@confidence":"0.9888995","#text":"\nan induction algorithm (Hwa 2001a) based on the expectation-maximization (EM)\nprinciple that induces parsers for PLTIGs. The algorithm performs heuristic search\nthrough an iterative reestimation procedure to find local optima: sets of values for\nthe grammar parameters that maximizes the grammar?s likelihood of generating the\ntraining data. In principle, the algorithm supports unsupervised learning; however,\nbecause the search space has too many local optima, the algorithm tends to converge\non a model that is unsuitable for parsing. Here, we consider a partially supervised\nvariant in which we assume that the learner is given the phrasal boundaries of the\ntraining sentences but not the label of the constituent units. For example, the sentence\nSeveral fund managers expect a rough market this morning before prices stabilize. would be\nlabeled as ?((Several fund managers) (expect ((a rough market) (this morning)) (before\n(prices stabilize))).)? Our algorithm is similar to the approach taken by Pereira and\nSchabes (1992) for inducing PCFG parsers.\nBecause the EM algorithm itself is an iterative procedure, performing sample se-\nlection on top of an EM-based learner is an extremely computational-intensive process.\nHere, we restrict the experiments for the PLTIG parsers to a smaller-scale study in the\nfollowing two aspects. First, the lexical anchors of the grammar rules are backed off to\npart-of-speech tags; this restricts the size of the grammar vocabulary to 48. Second, the\nunlabeled candidate pool is set to contain 3,600 sentences, which is sufficiently large\nfor inducing a grammar of this size. The initial model is trained on 500 labeled seed\nsentences. For each selection iteration, an additional 100 sentences are moved from\nthe unlabeled pool to be labeled and added to the training set. After training, the\nupdated parser is then tested on unseen sentences (backed off to their part-of-speech\ntags) and compared to the gold standard. Because the induced PLTIG produces binary-\nbranching parse trees, which have more layers than the gold standard, we measure\nparsing accuracy in terms of the crossing-bracket metric. The study is repeated for\n10 trials, each using a different portion of the full training set, to ensure statistical\nsignificance (using pairwise t-test at 95% confidence).\n"},{"#tail":"\n","@confidence":"0.947587666666667","#text":"\nPLTIG parser: (a) A comparison of the evaluation functions? learning curves. (b) A comparison\nof the evaluation functions for a test performance score of 80%.\nThe results of the experiment are graphically shown in Figure 7. As with the\nPP-attachment studies, Figure 7(a) compares the learning curves of the proposed eval-\nuation functions to that of the baseline. Note that even though these functions select\nexamples in terms of entire sentences, the amount of annotation is measured in the\ngraphs (x-axis) in terms of the number of brackets rather than sentences. Unlike in\nthe PP-attachment case, the amount of effort from the annotators varies significantly\nfrom example to example. A short and simple sentence takes much less time to an-\nnotate than a long and complex sentence. We address this effect by approximating\nthe amount of effort as the number of brackets the annotator needs to label. Thus,\nwe deem one evaluation function more effective than another if, for the desired level\nof performance, the smallest set of sentences selected by the function contains fewer\nbrackets than that of the other function. Figure 7(b) compares the evaluation functions\nat the final test performance level of 80%.\n"},{"#tail":"\n","@confidence":"0.982512318181818","#text":"\nComputational Linguistics Volume 30, Number 3\nQualitatively comparing the learning curves in the figure, we see that with the ap-\npropriate evaluation function, sample selection does reduce the amount of annotation.\nSimilarly to our findings in the PP-attachment study, the simple problem-space-based\nevaluation function, flen, offers only little savings; its performance is nearly indistin-\nguishable from that of the baseline, for the most part.11 The evaluation functions based\non hypothesis performances, on the other hand, do reduce the amount of annotation in\nthe training data. Of the two that we proposed for this category, the tree entropy eval-\nuation function, func, has a slight edge over the error-driven evaluation function, ferr.\nFor a quantitative comparison, let us consider the set of grammars that achieve\nan average parsing accuracy of 80% on the test sentences. We consider a grammar to\nbe comparable to that of the baseline if its mean test score is at least as high as that\nof the baseline and if the difference between the means is not statistically significant.\nThe baseline case requires an average of about 38,000 brackets in the training data. In\ncontrast, to induce a grammar that reaches the same 80% parsing accuracy with the\nexamples selected by func, the learner requires, on average, 19,000 training brackets.\nAlthough the learning rate of ferr is slower than that of func overall, it seems to have\ncaught up in the end; it needs 21,000 training brackets, slightly more than func. While\nthe simplistic sentence length evaluation function, flen, is less helpful, its learning rate\nstill improves slightly faster than that of the baseline. A grammar of comparable quality\ncan be induced from a set of training examples selected by flen containing an average\nof 28,000 brackets.12\n"},{"#tail":"\n","@confidence":"0.981747666666667","#text":"\nis Collins?s (1997) Model 2 parser, which uses a history-based learning algorithm that\ntakes statistics directly over the treebank. As a fully supervised algorithm, it does not\nhave to iteratively reestimate its parameters and is computationally efficient enough\nfor us to carry out a large-scale experiment. For this set of studies, the unlabeled\ncandidate pool consists of around 39,000 sentences. The initial model is trained on\n500 labeled seed sentences, and at each selection iteration, an additional 100 sentences\nare moved from the unlabeled pool into the training set. The parsing performance on\nthe test sentences is measured in terms of the parser?s F-score, the harmonic average\nof the labeled precision and labeled recall rates over the constituents (Van Rijsbergen\n"},{"#tail":"\n","@confidence":"0.998855666666667","#text":"\nWe plot the comparisons between different evaluation functions and the baseline\nfor the history-based parser in Figure 8. The examples selected by the problem-space-\nbased functions do not seem to be helpful. Their learning curves are, for the most part,\nslightly worse than the baseline. In contrast, the parsers trained on data selected by\nthe error-driven and uncertainty-based functions learn faster than the baseline; and as\nbefore, func performs slightly better than ferr.\nFor the final parsing performance of 88%, the parser requires a baseline training set\nof 30,500 sentences annotated with about 695,000 constituents. The same performance\ncan be achieved with a training set of 20,500 sentences selected by ferr, which contains\nabout 577,000 annotated constituents; or with a training set of 17,500 sentences selected\nby func, which contains about 505,000 annotated constituents, reducing the number of\nannotated constituents by 27%. Comparing the outcome of this experiment with that of\n"},{"#tail":"\n","@confidence":"0.98946825","#text":"\nModel 2 parser: (a) A comparison of the learning curves of the evaluation functions. (b) A\ncomparison of all the evaluation functions at the test performance level of 88%.\nthe experiment involving the EM-based learner, we see that the training data reduction\nrates are less dramatic than before. This may be because both func and ferr ignore lexical\nitems and chase after sentences containing words that rarely occur. Recent work by\nTang, Luo, and Roukos (2002) suggests that a hybrid approach that combines features\nof the problem space and the uncertainty of the parser may result in better performance\nfor lexicalized parsers.\n"},{"#tail":"\n","@confidence":"0.99915825","#text":"\nSample selection benefits problems in which the cost of acquiring raw data is cheap but\nthe cost of annotating them is high, as is certainly the case for many supervised learn-\ning tasks in natural language processing. In addition to PP-attachment, as discussed\nin this article, sample selection has been successfully applied to other classification\n"},{"#tail":"\n","@confidence":"0.9956275","#text":"\nComputational Linguistics Volume 30, Number 3\napplications. Some examples include text categorization (Lewis and Catlett 1994), base\nnoun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson\nDagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word\nsense disambiguation (Fujii et al 1998).\nMore challenging are learning problems whose objective is not classification, but\ngeneration of complex structures. One example in this direction is applying sample\nselection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentences\nare paired with their semantic representation using a deterministic shift-reduce parser.\nA recent effort that focuses on statistical syntactic parsing is the work by Tang, Lou,\nand Roukos (2002). Their results suggest that the number of training examples can be\nfurther reduced by using a hybrid evaluation function that combines a hypothesis-\nperformance-based metric such as tree entropy (?word entropy? in their terminology)\nwith a problem-space-based metric such as sentence clusters.\nAside from active learning, researchers have applied other learning techniques\nto combat the annotation bottleneck problem in parsing. For example, Henderson\nand Brill (2002) consider the case in which acquiring additional human-annotated\ntraining data is not possible. They show that parser performance can be improved by\nusing boosting and bagging techniques with multiple parsers. This approach assumes\nthat there are enough existing labeled data to train the individual parsers. Another\ntechnique for making better use of unlabeled data is cotraining (Blum and Mitchell\n1998), in which two sufficiently different learners help each other learn by labeling\ntraining data for one another. The work of Sarkar (2001) and Steedman, Osborne, et\nal. (2003) suggests that co-training can be helpful for statistical parsing. Pierce and\nCardie (2001) have shown, in the context of base noun identification, that combining\nsample selection and cotraining can be an effective learning framework for large-scale\ntraining. Similar approaches are being explored for parsing (Steedman, Hwa, et al\n2003; Hwa et al 2003).\n"},{"#tail":"\n","@confidence":"0.99973780952381","#text":"\nIn this article, we have argued that sample selection is a powerful learning technique\nfor reducing the amount of human-labeled training data. Our empirical studies suggest\nthat sample selection is helpful not only for binary classification tasks such as PP-\nattachment, but also for applications that generate complex outputs such as syntactic\nparsing.\nWe have proposed several criteria for predicting the training utility of the unla-\nbeled candidates and developed evaluation functions to rank them. We have conducted\nexperiments to compare the functions? ability to select the most helpful training exam-\nples. We have found that the uncertainty criterion is a good predictor that consistently\nfinds helpful examples. In our experiments, evaluation functions that factor in the\nuncertainty criterion consistently outperform the baseline of random selection across\ndifferent tasks and learning algorithms. For learning a PP-attachment model, the most\nhelpful evaluation function is a hybrid that factors in the prediction performance of the\nhypothesis and the confidence for the values of the parameters of the hypothesis. For\ntraining a parser, we found that uncertainty-based evaluation functions that use tree\nentropy were the most helpful for both the EM-based learner and the history-based\nlearner.\nThe current work points us in several future directions. First, we shall continue\nto develop alternative formulations of evaluation functions to improve the learn-\ning rates of parsers. Under the current framework, we did not experiment with any\nhypothesis-parameter-based evaluation functions for the parser induction task; how-\n"},{"#tail":"\n","@confidence":"0.996660390243902","#text":"\nHwa Sample Selection for Statistical Parsing\never, hypothesis-parameter-based functions may be feasible under a multilearner set-\nting, using parallel machines. Second, while in this work we focused on selecting\nentire sentences as training examples, we believe that further reduction in the amount\nof annotated training data might be possible if the system could ask the annotators\nmore-specific questions. For example, if the learner is unsure only of a local decision\nwithin a sentence (such as a PP-attachment ambiguity), the annotator should not have\nto label the entire sentence.\nIn order to allow for finer-grained interactions between the system and the an-\nnotators, we have to address some new challenges. To begin with, we must weigh\nin other factors in addition to the amount of annotations. For instance, the learner\nmay ask about multiple substrings in one sentence. Even if the total number of la-\nbels were fewer, the same sentence would still need to be mentally processed by the\nannotators multiple times. This situation is particularly problematic when there are\nvery few annotators, as it becomes much more likely that a person will encounter the\nsame sentence many times. Moreover, we must ensure that the questions asked by the\nlearner are well-formed. If the learner were simply to present the annotator with some\nsubstring that it could not process, the substring might not form a proper linguistic\nconstituent for the annotator to label. Additionally, we are interested in exploring the\ninteraction between sample selection and other semisupervised approaches such as\nboosting, reranking, and cotraining. Finally, based on our experience with parsing, we\nbelieve that active-learning techniques may be applicable to other tasks that produce\ncomplex outputs such as machine translation.\nAppendix: Efficient Computation of Tree Entropy\nAs discussed in Section 4.1.2, for learning tasks such as parsing, the number of possi-\nble classifications is so large that it may not be computationally efficient to calculate\nthe degree of uncertainty using the tree entropy definition. In the equation for the tree\nentropy of w (TE(w, G)) presented in Section 4.1.2, the computation requires summing\nover all possible parses, but the number of possible parses for a sentence grows ex-\nponentially with respect to the sentence length. In this appendix, we show that tree\nentropy can be efficiently computed using dynamic programming.\nFor illustrative purposes, we describe the computation process using a PCFG ex-\npressed in Chomsky normal form.14 The basic idea is to compose the tree entropy of\nthe entire sentence from the tree entropy of the subtrees. The process is similar to\nthat for computing the probability of the entire sentence from the probabilities of sub-\nstrings (called Inside Probabilities). We follow the notation convention of Lari and\nYoung (1990).\nThe inside probability of a nonterminal X generating the substring wi . . .wj is\ndenoted as e(X, i, j); it is the sum of the probabilities of all possible subtrees that have\nX as the root and wi . . .wj as the leaf nodes. We define a new function h(X, i, j) to\nrepresent the corresponding entropy for the substring:\n"},{"#tail":"\n","@confidence":"0.632402","#text":"\nwhere G is the current model. Under this notation, the tree entropy of a sentence,\n"},{"#tail":"\n","@confidence":"0.760667","#text":"\n14 That is, every production rule must be in one of two forms: a nonterminal expands into two more\nnonterminals, or a nonterminal expands into a terminal.\n"},{"#tail":"\n","@confidence":"0.989667444444444","#text":"\nComputational Linguistics Volume 30, Number 3\nAnalogously to the computation of inside probabilities, we compute h(X, i, j) re-\ncursively. The base case is when the nonterminal X generates a single token substring\nwi. The only possible tree has X at the root, immediately dominating the leaf node wi.\nTherefore, the tree entropy is\nh(X, i, i) = e(X, i, i) lg(e(X, i, i))\nFor the general case, h(X, i, j), we must find all rules of the form X ? YZ, where Y and\nZ are nonterminals, that have contributed toward X ?? wi . . .wj. To do so, we consider\nall possible ways dividing up wi . . .wj into two pieces such that Y\n"},{"#tail":"\n","@confidence":"0.997888625","#text":"\nThe function hY,Z,k(X, i, j) is a portion of h(X, i, j) that accounts for those parses in which\nthe rule X ? YZ is used and the division point is at word wk. The nonterminals Y and\nZ may, in turn, generate their substrings with multiple parses. Let Y represent the set\nof parses for Y ?? wi . . .wk; let Z represent the set of parses for Z\n?? wk+1 . . .wj; and\nlet x represent the parse step of X ? YZ. Then, there are a total of ?Y? ? ?Z? parses,\nand the probability of each parse is P(x)P(y)P(z), where y ? Y and z ? Z . To compute\nhY,Z,k, we need to sum over all possible parses:\n"},{"#tail":"\n","@confidence":"0.90903625","#text":"\n= ?P(x) lg(P(x))e(Y, i, k)e(Z, k + 1, j) + P(x)h(Y, i, k)e(Z, k + 1, j)\n+P(x)e(Y, i, k)h(Z, k + 1, j)\nThus, the tree entropy of the entire sentence can be recursively computed from the\nentropy values of the substrings.\n"},{"#tail":"\n","@confidence":"0.784874","#text":"\nWe thank Joshua Goodman, Lillian Lee,\nWheeler Ruml, and Stuart Shieber for\nhelpful discussions, and Ric Crabbe, Philip\nResnik, and the reviewers for their\nconstructive comments on this article.\nPortions of this work have appeared\npreviously (Hwa 2000, 2001b); we thank the\nreviewers of those papers for their helpful\ncomments. Parts of this work was carried\nout while the author was a graduate student\nat Harvard University, supported by the\nNational Science Foundation under Grant\nNo. IRI 9712068. The work is also supported\nby the Department of Defense contract\nRD-02-5700, and ONR MURI Contract\nFCPO.810548265.\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.970332","#text":"\nUniversity of Pittsburgh\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.992554","@genericHeader":"abstract","#text":"\n1. Introduction\n"},{"#tail":"\n","@confidence":"0.593427","@genericHeader":"method","#text":"\n2. Learning with Sample Selection\n"},{"#tail":"\n","@confidence":"0.52958","@genericHeader":"method","#text":"\n3. Sample Selection for Prepositional-Phrase Attachment\n"},{"#tail":"\n","@confidence":"0.378752","@genericHeader":"method","#text":"\n5 The standard deviation value for the Gaussian is chosen so that more than 98% of the mass of the\n"},{"#tail":"\n","@confidence":"0.998747","@genericHeader":"related work","#text":"\n5. Related Work\n"},{"#tail":"\n","@confidence":"0.992943","@genericHeader":"conclusions","#text":"\n6. Conclusion\n"},{"#tail":"\n","@confidence":"0.966936","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.783867","@genericHeader":"references","#text":"\nReferences\n"}],"page":[{"#tail":"\n","@confidence":"0.997688","#text":"\n254\n"},{"#tail":"\n","@confidence":"0.994141","#text":"\n255\n"},{"#tail":"\n","@confidence":"0.997355","#text":"\n256\n"},{"#tail":"\n","@confidence":"0.994716","#text":"\n257\n"},{"#tail":"\n","@confidence":"0.988116","#text":"\n258\n"},{"#tail":"\n","@confidence":"0.996281","#text":"\n259\n"},{"#tail":"\n","@confidence":"0.989926","#text":"\n260\n"},{"#tail":"\n","@confidence":"0.954266","#text":"\n4\n"},{"#tail":"\n","@confidence":"0.968893","#text":"\n2\n"},{"#tail":"\n","@confidence":"0.987309","#text":"\n261\n"},{"#tail":"\n","@confidence":"0.973983","#text":"\n262\n"},{"#tail":"\n","@confidence":"0.871391","#text":"\n1.00\n"},{"#tail":"\n","@confidence":"0.997703","#text":"\n263\n"},{"#tail":"\n","@confidence":"0.851552","#text":"\n74\n"},{"#tail":"\n","@confidence":"0.993332","#text":"\n264\n"},{"#tail":"\n","@confidence":"0.98569","#text":"\n265\n"},{"#tail":"\n","@confidence":"0.994998","#text":"\n266\n"},{"#tail":"\n","@confidence":"0.986732","#text":"\n267\n"},{"#tail":"\n","@confidence":"0.988412","#text":"\n268\n"},{"#tail":"\n","@confidence":"0.937107","#text":"\n269\n"},{"#tail":"\n","@confidence":"0.962432","#text":"\n270\n"},{"#tail":"\n","@confidence":"0.822853","#text":"\n1979).13\n"},{"#tail":"\n","@confidence":"0.946508","#text":"\n271\n"},{"#tail":"\n","@confidence":"0.733337","#text":"\n78\n"},{"#tail":"\n","@confidence":"0.970901","#text":"\n272\n"},{"#tail":"\n","@confidence":"0.979223","#text":"\n273\n"},{"#tail":"\n","@confidence":"0.988475","#text":"\n274\n"},{"#tail":"\n","@confidence":"0.938924","#text":"\n275\n"},{"#tail":"\n","@confidence":"0.965272","#text":"\n276\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.517932","#text":"\nFigure 4\n"},{"#tail":"\n","@confidence":"0.648609","#text":"\nFigure 6\n"},{"#tail":"\n","@confidence":"0.997149","#text":"\nFigure 7\n"},{"#tail":"\n","@confidence":"0.911744","#text":"\nFigure 8\n"}],"table":[{"#tail":"\n","@confidence":"0.491771","#text":"\nHwa Sample Selection for Statistical Parsing\n"},{"#tail":"\n","@confidence":"0.542845","#text":"\nHwa Sample Selection for Statistical Parsing\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.558214","#tail":"\n","@no":"0","#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.998649","#text":"University of Pittsburgh"},"author":{"#tail":"\n","@confidence":"0.996413","#text":"Rebecca Hwa"},"abstract":{"#tail":"\n","@confidence":"0.984256111111111","#text":"Corpus-based statistical parsing relies on using large quantities of annotated text as training examples. Building this kind of resource is expensive and labor-intensive. This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones. We consider several criteria for predicting whether unlabeled data might be a helpful training example. Experiments are performed across two syntactic learning tasks and within the single task of parsing across two learning models to compare the effect of different predictive criteria. We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models."},"title":{"#tail":"\n","@confidence":"0.81967","#text":"c? 2004 Association for Computational Linguistics Sample Selection for Statistical Parsing"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Banko, Michele and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, France, pages 26?33."},"#text":"\n","pages":{"#tail":"\n","#text":"26--33"},"marker":{"#tail":"\n","#text":"Banko, Brill, 2001"},"location":{"#tail":"\n","#text":"Toulouse, France,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification 272 Computational Linguistics Volume 30, Number 3 applications. Some examples include text categorization (Lewis and Catlett 1994), base noun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson Dagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word sense disambiguation (Fujii et al 1998). More challenging are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. A recent effort that focuses on statistical syntactic parsing is the work by Tang, Lou, and Roukos (2002). Their results suggest that the number of training examples can be further reduced by using a hybrid e","@endWordPosition":"9552","@position":"58155","annotationId":"T1","@startWordPosition":"9549","@citStr":"Banko and Brill 2001"}},"title":{"#tail":"\n","#text":"Scaling to very very large corpora for natural language disambiguation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michele Banko"},{"#tail":"\n","#text":"Eric Brill"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Blum, Avrim and Tom Mitchell. 1998."},"#text":"\n","marker":{"#tail":"\n","#text":"Blum, Mitchell, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ace-based metric such as sentence clusters. Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing. For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of Sarkar (2001) and Steedman, Osborne, et al. (2003) suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing (Steedman, Hwa, et al 2003; Hwa et al 2003). 6. Conclusion In this article, we have argued that sample selection is ","@endWordPosition":"9750","@position":"59524","annotationId":"T2","@startWordPosition":"9747","@citStr":"Blum and Mitchell 1998"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Avrim Blum"},{"#tail":"\n","#text":"Tom Mitchell"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"Combining labeled and unlabeled data with co-training. In Proceedings of the 1998 Conference on Computational Learning Theory, pages 92?100, Madison, WI."},"#text":"\n","pages":{"#tail":"\n","#text":"92--100"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Madison, WI."},"title":{"#tail":"\n","#text":"Combining labeled and unlabeled data with co-training."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 1998 Conference on Computational Learning Theory,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Brill, Eric and Philip S. Resnik. 1994. A rule based approach to PP attachment disambiguation. In Proceedings of the 15th International Conference on Computational Linguistics (COLING), Kyoto, Japan, pages 1198?1204."},"#text":"\n","pages":{"#tail":"\n","#text":"1198--1204"},"marker":{"#tail":"\n","#text":"Brill, Resnik, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s far. The loop continues until one of three stopping conditions is met: The hypothesis is considered to perform well enough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no more resources). 3. Sample Selection for Prepositional-Phrase Attachment One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models (Brill and Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximumentropy model (Ratnaparkhi 1998). Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier. Our experiments show that the best evaluation function can reduce the number of labeled examples by nearly half without","@endWordPosition":"1297","@position":"8452","annotationId":"T3","@startWordPosition":"1294","@citStr":"Brill and Resnik 1994"}},"title":{"#tail":"\n","#text":"A rule based approach to PP attachment disambiguation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 15th International Conference on Computational Linguistics (COLING), Kyoto, Japan,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Eric Brill"},{"#tail":"\n","#text":"Philip S Resnik"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Hwa Sample Selection for Statistical Parsing Charniak, Eugene. 2000. A maximum-entropy-inspired parser. In Proceedings of the First Meeting of the North American Association for Computational Linguistics, Seattle."},"#text":"\n","marker":{"#tail":"\n","#text":"Sample, 2000"},"location":{"#tail":"\n","#text":"Eugene."},"title":{"#tail":"\n","#text":"Selection for Statistical Parsing Charniak,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the First Meeting of the North American Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Hwa Sample"}}},{"volume":{"#tail":"\n","#text":"15"},"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Cohn, David, Les Atlas, and Richard Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201?221."},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Cohn, Atlas, Ladner, 1994"},"title":{"#tail":"\n","#text":"Improving generalization with active learning."},"booktitle":{"#tail":"\n","#text":"Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Cohn"},{"#tail":"\n","#text":"Les Atlas"},{"#tail":"\n","#text":"Richard Ladner"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Collins, Michael. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16?23, Madrid."},"#text":"\n","pages":{"#tail":"\n","#text":"16--23"},"marker":{"#tail":"\n","#text":"Collins, 1997"},"location":{"#tail":"\n","#text":"Madrid."},"title":{"#tail":"\n","#text":"Three generative, lexicalised models for statistical parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Collins"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"University of Pennsylvania,"},"rawString":{"#tail":"\n","#text":"Collins, Michael. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1999"},"location":{"#tail":"\n","#text":"Philadelphia."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models. 1. Introduction Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers (Collins 1999; Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands significant human involvement (","@endWordPosition":"208","@position":"1407","annotationId":"T4","@startWordPosition":"207","@citStr":"Collins 1999"}},"title":{"#tail":"\n","#text":"Head-Driven Statistical Models for Natural Language Parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Collins, Michael and James Brooks. 1995. Prepositional phrase attachment through a backed-off model. In Proceedings of the Third Workshop on Very Large Corpora, Cambridge, MA, pages 27?38."},"#text":"\n","pages":{"#tail":"\n","#text":"27--38"},"marker":{"#tail":"\n","#text":"Collins, Brooks, 1995"},"location":{"#tail":"\n","#text":"Cambridge, MA,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e stopping conditions is met: The hypothesis is considered to perform well enough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no more resources). 3. Sample Selection for Prepositional-Phrase Attachment One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models (Brill and Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximumentropy model (Ratnaparkhi 1998). Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier. Our experiments show that the best evaluation function can reduce the number of labeled examples by nearly half without loss of accuracy. 3.1 A Summary of the Colli","@endWordPosition":"1303","@position":"8497","annotationId":"T5","@startWordPosition":"1300","@citStr":"Collins and Brooks 1995"}},"title":{"#tail":"\n","#text":"Prepositional phrase attachment through a backed-off model."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Third Workshop on Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michael Collins"},{"#tail":"\n","#text":"James Brooks"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Cover, Thomas M. and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Cover, Thomas, 1991"},"publisher":{"#tail":"\n","#text":"John Wiley,"},"location":{"#tail":"\n","#text":"New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"driven evaluation function is defined as ferr(w, G) = 1 ? P(vmax | w, G) Unlike the error-driven function, which focuses on the most likely parse, the uncertainty-based function takes the probability distribution of all parses into account. To quantitatively characterize its distribution, we compute the entropy of the distribution. That is, H(V) = ? ? v?V p(v) lg(p(v)) (3) where V is a random variable that can take any possible outcome in set V , and p(v) = Pr(V = v) is the density function. Further details about the properties of entropy can be found in textbooks on information theory (e.g., Cover and Thomas 1991). Determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable. Thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in G computes its tree entropy, TE(w, G), the expected number of bits needed to encode the distribution of possible parses for w. However, we may not wish to compare two sentences with different numbers of parses by their entropy directly. If the parse probability distributions for both sentences are uniform, the sentence with more parses will have a hi","@endWordPosition":"7008","@position":"42980","annotationId":"T6","@startWordPosition":"7005","@citStr":"Cover and Thomas 1991"}},"title":{"#tail":"\n","#text":"Elements of Information Theory."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Thomas M Cover"},{"#tail":"\n","#text":"Joy A Thomas"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Engelson, Sean P. and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, CA, pages 319?326."},"#text":"\n","pages":{"#tail":"\n","#text":"319--326"},"marker":{"#tail":"\n","#text":"Engelson, Dagan, 1996"},"location":{"#tail":"\n","#text":"Santa Cruz, CA,"},"title":{"#tail":"\n","#text":"Minimizing manual annotation cost in supervised training from corpora."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sean P Engelson"},{"#tail":"\n","#text":"Ido Dagan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Freund, Yoav, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2?3):133?168."},"#text":"\n","pages":{"#tail":"\n","#text":"28--2"},"marker":{"#tail":"\n","#text":"Freund, Seung, Shamir, Tishby, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ng systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set. There are two types of selection algorithms: committee-based and single learner. A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV (Cohn, Atlas, and Ladner 1994; Freund et al 1997). For computationally intensive problems, such as parsing, keeping multiple learners may be impractical. In this work, we focus on sample selection using a single learner that keeps one working hypothesis. Without access to multiple hypotheses, the selection algorithm can nonetheless estimate the TUV of a candidate. We identify the following three classes of predictive criteria: 1. Problem-space: Knowledge about the problem space may provide information about the type of candidates that are particularly plentiful or difficult to learn. This criterion focuses on the general attributes of the le","@endWordPosition":"754","@position":"5081","annotationId":"T7","@startWordPosition":"751","@citStr":"Freund et al 1997"}},"title":{"#tail":"\n","#text":"Selective sampling using the query by committee algorithm."},"booktitle":{"#tail":"\n","#text":"Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yoav Freund"},{"#tail":"\n","#text":"H Sebastian Seung"},{"#tail":"\n","#text":"Eli Shamir"},{"#tail":"\n","#text":"Naftali Tishby"}]}},{"volume":{"#tail":"\n","#text":"24"},"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Fujii, Atsushi, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998. Selective sampling for example-based word sense disambiguation. Computational Linguistics, 24(4):573?598."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Fujii, 1998"},"title":{"#tail":"\n","#text":"Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Atsushi Fujii"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Henderson, John C. and Eric Brill. 2000. Bagging and boosting a treebank parser. In Proceedings of the First Meeting of the North American Association for Computational Linguistics, Seattle, pages 34?41."},"#text":"\n","pages":{"#tail":"\n","#text":"34--41"},"marker":{"#tail":"\n","#text":"Henderson, Brill, 2000"},"location":{"#tail":"\n","#text":"Seattle,"},"title":{"#tail":"\n","#text":"Bagging and boosting a treebank parser."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the First Meeting of the North American Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"John C Henderson"},{"#tail":"\n","#text":"Eric Brill"}]}},{"volume":{"#tail":"\n","#text":"1"},"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Hwa, Rebecca. 1998. An empirical evaluation of probabilistic lexicalized tree insertion grammars. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Montreal, volume 1, pages 557?563."},"#text":"\n","pages":{"#tail":"\n","#text":"557--563"},"marker":{"#tail":"\n","#text":"Hwa, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"p sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover, in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain, we have performed the study on two parsing models: one based on a context-free variant of tree-adjoining grammars (Joshi, Levy, and Takahashi 1975), the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters 1993; Hwa 1998), and Collins?s Model 2 parser (1997). Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG?s expectation-maximization-based induction algorithm is partially supervised; the model?s parameters are estimated indirectly from the training data. As a superset of the PP-attachment task, parsing is a more challenging learning problem. Whereas a trained PP-attachment model is a b","@endWordPosition":"5859","@position":"35909","annotationId":"T8","@startWordPosition":"5858","@citStr":"Hwa 1998"}},"title":{"#tail":"\n","#text":"An empirical evaluation of probabilistic lexicalized tree insertion grammars."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Montreal,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Rebecca Hwa"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Hwa, Rebecca. 2000. Sample selection for statistical grammar induction. In Proceedings of 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 45?52, Hong Kong, October."},"#text":"\n","pages":{"#tail":"\n","#text":"45--52"},"marker":{"#tail":"\n","#text":"Hwa, 2000"},"location":{"#tail":"\n","#text":"Hong Kong,"},"title":{"#tail":"\n","#text":"Sample selection for statistical grammar induction."},"booktitle":{"#tail":"\n","#text":"In Proceedings of 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Rebecca Hwa"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"2001"},"institution":{"#tail":"\n","#text":"Harvard University,"},"rawString":{"#tail":"\n","#text":"Hwa, Rebecca. 2001a. Learning Probabilistic Lexicalized Grammars for Natural Language Processing. Ph.D. thesis, Harvard University, Cambridge, MA."},"#text":"\n","marker":{"#tail":"\n","#text":"Hwa, 2001"},"location":{"#tail":"\n","#text":"Cambridge, MA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"a small set of annotated seed data from the WSJ Treebank and a large set of unlabeled data (also from the WSJ Treebank but with the labels removed) from which to select new training examples. All training data are from Sections 2?21 of the treebank. We monitor the learning progress of the parser by testing it on unseen test sentences. We use Section 00 for development and Section 23 for testing. This study is repeated for two different models, the PLTIG parser and Collins?s Model 2 parser. 4.2.1 An Expectation-Maximization-Based Learner. In the first experiment, we use an induction algorithm (Hwa 2001a) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs. The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar?s likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing. Here, we consider a partially supervised variant in which we assume that the learner is given ","@endWordPosition":"7935","@position":"48279","annotationId":"T9","@startWordPosition":"7934","@citStr":"Hwa 2001"}},"title":{"#tail":"\n","#text":"Learning Probabilistic Lexicalized Grammars for Natural Language Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Rebecca Hwa"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Hwa, Rebecca. 2001b. On minimizing training corpus for parser acquisition. In Proceedings of the ACL 2001 Workshop on Computational Natural Language Learning (ConLL-2001), Toulouse, France, pages 84?89."},"#text":"\n","pages":{"#tail":"\n","#text":"84--89"},"marker":{"#tail":"\n","#text":"Hwa, 2001"},"location":{"#tail":"\n","#text":"Toulouse, France,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"a small set of annotated seed data from the WSJ Treebank and a large set of unlabeled data (also from the WSJ Treebank but with the labels removed) from which to select new training examples. All training data are from Sections 2?21 of the treebank. We monitor the learning progress of the parser by testing it on unseen test sentences. We use Section 00 for development and Section 23 for testing. This study is repeated for two different models, the PLTIG parser and Collins?s Model 2 parser. 4.2.1 An Expectation-Maximization-Based Learner. In the first experiment, we use an induction algorithm (Hwa 2001a) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs. The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar?s likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing. Here, we consider a partially supervised variant in which we assume that the learner is given ","@endWordPosition":"7935","@position":"48279","annotationId":"T10","@startWordPosition":"7934","@citStr":"Hwa 2001"}},"title":{"#tail":"\n","#text":"On minimizing training corpus for parser acquisition."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL 2001 Workshop on Computational Natural Language Learning (ConLL-2001),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Rebecca Hwa"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Hwa, Rebecca, Miles Osborne, Anoop Sarkar, and Mark Steedman. 2003."},"#text":"\n","marker":{"#tail":"\n","#text":"Hwa, Osborne, Sarkar, Steedman, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"parsers. Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of Sarkar (2001) and Steedman, Osborne, et al. (2003) suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing (Steedman, Hwa, et al 2003; Hwa et al 2003). 6. Conclusion In this article, we have argued that sample selection is a powerful learning technique for reducing the amount of human-labeled training data. Our empirical studies suggest that sample selection is helpful not only for binary classification tasks such as PPattachment, but also for applications that generate complex outputs such as syntactic parsing. We have proposed several criteria for predicting the training utility of the unlabeled candidates and developed evaluation functions to rank them. We have conducted experiments to compare the functions? ability to s","@endWordPosition":"9827","@position":"60034","annotationId":"T11","@startWordPosition":"9824","@citStr":"Hwa, et al 2003"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Rebecca Hwa"},{"#tail":"\n","#text":"Miles Osborne"},{"#tail":"\n","#text":"Anoop Sarkar"},{"#tail":"\n","#text":"Mark Steedman"}]}},{"#tail":"\n","date":{"#tail":"\n"},"rawString":{"#tail":"\n","#text":"Corrected co-training for statistical parsers. In Proceedings of the ICML Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining at the 20th International Conference of Machine Learning (ICML-2003), Washington, DC, pages 95?102, August."},"#text":"\n","pages":{"#tail":"\n","#text":"95--102"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Washington, DC,"},"title":{"#tail":"\n","#text":"Corrected co-training for statistical parsers."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ICML Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining at the 20th International Conference of Machine Learning (ICML-2003),"},"@valid":"true"},{"volume":{"#tail":"\n","#text":"10"},"#tail":"\n","date":{"#tail":"\n","#text":"1975"},"rawString":{"#tail":"\n","#text":"Joshi, Aravind K., Leon S. Levy, and Masako Takahashi. 1975. Tree adjunction grammars. Journal of Computer and System Sciences, 10(1): 136?163."},"journal":{"#tail":"\n","#text":"Journal of Computer and System Sciences,"},"#text":"\n","pages":{"#tail":"\n","#text":"136--163"},"issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Joshi, Levy, Takahashi, 1975"},"title":{"#tail":"\n","#text":"Tree adjunction grammars."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Aravind K Joshi"},{"#tail":"\n","#text":"Leon S Levy"},{"#tail":"\n","#text":"Masako Takahashi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"Lari, Karim A. and Steve J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35?56."},"#text":"\n","pages":{"#tail":"\n","#text":"4--35"},"marker":{"#tail":"\n","#text":"Lari, Young, 1990"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" (i.e., the probability of assigning v to a random variable V). Mapping it back into the entropy definition from equation (3), we derive the tree entropy of w as follows: TE(w, G) = H(V) = ? ? v?V p(v) lg(p(v)) = ? ? v?V P(v | G) P(w | G) lg( P(v | G) P(w | G) ) = ? ? v?V P(v | G) P(w | G) lg(P(v | G)) + ? v?V P(v | G) P(w | G) lg(P(w | G)) = ? 1 P(w | G) ? v?V P(v | G) lg(P(v | G)) + lg(P(w | G)) P(w | G) ? v?V P(v | G) = ? 1 P(w | G) ? v?V P(v | G) lg(P(v | G)) + lg(P(w | G)) Using the bottom-up, dynamic programming technique (see the appendix for details) of computing inside probabilities (Lari and Young 1990), we can efficiently compute the probability of the sentence, P(w | G). Similarly, the algorithm can be modified to compute the quantity ? v?V P(v | G) lg(P(v | G)). 4.1.3 The Parameters of the Hypothesis. Although the confidence-based function gives good TUV estimates to candidates for training PP-attachment models, it is not clear how a similar technique can be applied to training parsers. Whereas binary classification tasks can be described by binomial distributions, for which the confidence interval is well defined, a parsing model is made up of many multinomial classification decisions. W","@endWordPosition":"7443","@position":"45215","annotationId":"T12","@startWordPosition":"7440","@citStr":"Lari and Young 1990"},{"#tail":"\n","#text":" of possible parses for a sentence grows exponentially with respect to the sentence length. In this appendix, we show that tree entropy can be efficiently computed using dynamic programming. For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form.14 The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees. The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities). We follow the notation convention of Lari and Young (1990). The inside probability of a nonterminal X generating the substring wi . . .wj is denoted as e(X, i, j); it is the sum of the probabilities of all possible subtrees that have X as the root and wi . . .wj as the leaf nodes. We define a new function h(X, i, j) to represent the corresponding entropy for the substring: h(X, i, j) = ? ? x?X ??wi...wj P(x | G) lg(P(x | G)) where G is the current model. Under this notation, the tree entropy of a sentence, ? v?V P(v | G) lg P(v | G), is denoted as h(S, 1, n). 14 That is, every production rule must be in one of two forms: a nonterminal expands into tw","@endWordPosition":"10524","@position":"64563","annotationId":"T13","@startWordPosition":"10521","@citStr":"Lari and Young (1990)"}]},"title":{"#tail":"\n","#text":"The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Karim A Lari"},{"#tail":"\n","#text":"Steve J Young"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1986"},"rawString":{"#tail":"\n","#text":"Larsen, Richard J. and Morris L. Marx. 1986. An Introduction to Mathematical Statistics and Its Applications. Prentice-Hall, Englewood Cliffs, NJ."},"#text":"\n","marker":{"#tail":"\n","#text":"Larsen, Marx, 1986"},"location":{"#tail":"\n","#text":"Englewood Cliffs, NJ."},"title":{"#tail":"\n","#text":"An Introduction to Mathematical Statistics and Its Applications. Prentice-Hall,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Richard J Larsen"},{"#tail":"\n","#text":"Morris L Marx"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Lewis, David D. and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the Eleventh International Conference on Machine Learning, San Francisco, pages 148?156."},"#text":"\n","pages":{"#tail":"\n","#text":"148--156"},"marker":{"#tail":"\n","#text":"Lewis, Catlett, 1994"},"location":{"#tail":"\n","#text":"San Francisco,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"t the type of candidates that are particularly plentiful or difficult to learn. This criterion focuses on the general attributes of the learning problem, such as the distribution of the input data and properties of the learning algorithm, but it ignores the current state of the hypothesis. 2. Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly. That is, if the current hypothesis is unable to label a candidate or is uncertain about it, then the candidate might be a good training example (Lewis and Catlett 1994). The underlying assumption is that an uncertain output is likely to be wrong. 3. Parameters of the hypothesis: Estimating the potential impact that the candidates will have on the parameters of the current working hypothesis locates those examples that will change the current hypothesis the most. 255 Hwa Sample Selection for Statistical Parsing U is a set of unlabeled candidates. L is a set of labeled training examples. C is the current hypothesis. Initialize: C ? Train(L). Repeat N ? Select(n, U, C, f ). U ? U ? N. L ? L ? Label(N). C ? Train(L). Until (C is good enough) or (U = ?) or (cutof","@endWordPosition":"922","@position":"6166","annotationId":"T14","@startWordPosition":"919","@citStr":"Lewis and Catlett 1994"},{"#tail":"\n","#text":"mbines features of the problem space and the uncertainty of the parser may result in better performance for lexicalized parsers. 5. Related Work Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification 272 Computational Linguistics Volume 30, Number 3 applications. Some examples include text categorization (Lewis and Catlett 1994), base noun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson Dagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word sense disambiguation (Fujii et al 1998). More challenging are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. A recent effort that focuses on statistical synt","@endWordPosition":"9531","@position":"57995","annotationId":"T15","@startWordPosition":"9528","@citStr":"Lewis and Catlett 1994"}]},"title":{"#tail":"\n","#text":"Heterogeneous uncertainty sampling for supervised learning."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Eleventh International Conference on Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David D Lewis"},{"#tail":"\n","#text":"Jason Catlett"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1994"},"institution":{"#tail":"\n","#text":"Stanford University,"},"rawString":{"#tail":"\n","#text":"Magerman, David. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. thesis, Stanford University, Stanford, CA."},"#text":"\n","marker":{"#tail":"\n","#text":"Magerman, 1994"},"location":{"#tail":"\n","#text":"Stanford, CA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"unt(p, n2) > 0 then prob ? CountNP(v,p)+CountNP(n,p)+CountNP(p,n2)Count(v,p)+Count(n,p)+Count(p,n2) elsif Count(p) > 0 then prob ? CountNP(p)Count(p) else prob ? 1 if prob ? .5 then output noun else output verb Figure 2 The Collins-Brooks PP-attachment classification algorithm. preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by Magerman (1994). For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition. Each training example forms eight characteristic tuples: (v, n, p, n2), (v, n, p), (v, p, n2), (n, p, n2), (v, p), (n, p), (p, n2), (p). The attachment statistics are a c","@endWordPosition":"1646","@position":"10632","annotationId":"T16","@startWordPosition":"1645","@citStr":"Magerman (1994)"}},"title":{"#tail":"\n","#text":"Natural Language Parsing as Statistical Pattern Recognition."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David Magerman"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Marcus, Mitchell, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313?330."},"#text":"\n","marker":{"#tail":"\n","#text":"Marcus, Santorini, Marcinkiewicz, 1993"},"title":{"#tail":"\n","#text":"Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mitchell Marcus"},{"#tail":"\n","#text":"Beatrice Santorini"},{"#tail":"\n","#text":"Mary Ann Marcinkiewicz"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Ngai, Grace and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 117?125, Hong Kong, October."},"#text":"\n","pages":{"#tail":"\n","#text":"117--125"},"marker":{"#tail":"\n","#text":"Ngai, Yarowsky, 2000"},"location":{"#tail":"\n","#text":"Hong Kong,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"inty of the parser may result in better performance for lexicalized parsers. 5. Related Work Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification 272 Computational Linguistics Volume 30, Number 3 applications. Some examples include text categorization (Lewis and Catlett 1994), base noun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson Dagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word sense disambiguation (Fujii et al 1998). More challenging are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. A recent effort that focuses on statistical syntactic parsing is the work by Tang, Lou, and Roukos (","@endWordPosition":"9539","@position":"58047","annotationId":"T17","@startWordPosition":"9536","@citStr":"Ngai and Yarowsky 2000"}},"title":{"#tail":"\n","#text":"Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Grace Ngai"},{"#tail":"\n","#text":"David Yarowsky"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1992"},"rawString":{"#tail":"\n","#text":"Pereira, Fernando C. N. and Yves Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 128?135, Newark, DE."},"#text":"\n","pages":{"#tail":"\n","#text":"128--135"},"marker":{"#tail":"\n","#text":"Pereira, Schabes, 1992"},"location":{"#tail":"\n","#text":"Newark, DE."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"riteria. We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models. 1. Introduction Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers (Collins 1999; Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because","@endWordPosition":"201","@position":"1347","annotationId":"T18","@startWordPosition":"198","@citStr":"Pereira and Schabes 1992"},{"#tail":"\n","#text":"owever, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing. Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units. For example, the sentence Several fund managers expect a rough market this morning before prices stabilize. would be labeled as ?((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)? Our algorithm is similar to the approach taken by Pereira and Schabes (1992) for inducing PCFG parsers. Because the EM algorithm itself is an iterative procedure, performing sample selection on top of an EM-based learner is an extremely computational-intensive process. Here, we restrict the experiments for the PLTIG parsers to a smaller-scale study in the following two aspects. First, the lexical anchors of the grammar rules are backed off to part-of-speech tags; this restricts the size of the grammar vocabulary to 48. Second, the unlabeled candidate pool is set to contain 3,600 sentences, which is sufficiently large for inducing a grammar of this size. The initial mo","@endWordPosition":"8083","@position":"49276","annotationId":"T19","@startWordPosition":"8080","@citStr":"Pereira and Schabes (1992)"}]},"title":{"#tail":"\n","#text":"Inside-outside reestimation from partially bracketed corpora."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Fernando C N Pereira"},{"#tail":"\n","#text":"Yves Schabes"}]}},{"volume":{"#tail":"\n","#text":"30"},"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"institution":{"#tail":"\n","#text":"Computational Linguistics"},"rawString":{"#tail":"\n","#text":"Computational Linguistics Volume 30, Number 3 Pierce, David and Claire Cardie. 2001."},"#text":"\n","marker":{"#tail":"\n","#text":"Pierce, Cardie, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"al human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of Sarkar (2001) and Steedman, Osborne, et al. (2003) suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing (Steedman, Hwa, et al 2003; Hwa et al 2003). 6. Conclusion In this article, we have argued that sample selection is a powerful learning technique for reducing the amount of human-labeled training data. Our empirical studies suggest that sample selection is helpful not only for binary classification tasks such as PPattachment, but also for applications that generate complex outpu","@endWordPosition":"9791","@position":"59789","annotationId":"T20","@startWordPosition":"9788","@citStr":"Pierce and Cardie (2001)"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David Pierce"},{"#tail":"\n","#text":"Claire Cardie"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"Limitations of co-training for natural language learning from large datasets. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-2001), pages 1?9, Pittsburgh, PA."},"#text":"\n","pages":{"#tail":"\n","#text":"1--9"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Pittsburgh, PA."},"title":{"#tail":"\n","#text":"Limitations of co-training for natural language learning from large datasets."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-2001),"},"@valid":"false"},{"volume":{"#tail":"\n","#text":"2"},"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Ratnaparkhi, Adwait. 1998. Statistical models for unsupervised prepositional phrase attachment. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Montreal, volume 2, pages 1079?1085."},"#text":"\n","pages":{"#tail":"\n","#text":"1079--1085"},"marker":{"#tail":"\n","#text":"Ratnaparkhi, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ered to perform well enough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no more resources). 3. Sample Selection for Prepositional-Phrase Attachment One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models (Brill and Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximumentropy model (Ratnaparkhi 1998). Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier. Our experiments show that the best evaluation function can reduce the number of labeled examples by nearly half without loss of accuracy. 3.1 A Summary of the Collins-Brooks Model The Collins-Brooks model takes ","@endWordPosition":"1310","@position":"8544","annotationId":"T21","@startWordPosition":"1309","@citStr":"Ratnaparkhi 1998"}},"title":{"#tail":"\n","#text":"Statistical models for unsupervised prepositional phrase attachment."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Montreal,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Adwait Ratnaparkhi"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Sarkar, Anoop. 2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting of the North American Association for Computational Linguistics, Pittsburgh, pages 175?182, June."},"#text":"\n","pages":{"#tail":"\n","#text":"175--182"},"marker":{"#tail":"\n","#text":"Sarkar, 2001"},"location":{"#tail":"\n","#text":"Pittsburgh,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" bottleneck problem in parsing. For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of Sarkar (2001) and Steedman, Osborne, et al. (2003) suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing (Steedman, Hwa, et al 2003; Hwa et al 2003). 6. Conclusion In this article, we have argued that sample selection is a powerful learning technique for reducing the amount of human-labeled training data. Our empirical studies suggest that sample selection","@endWordPosition":"9772","@position":"59661","annotationId":"T22","@startWordPosition":"9771","@citStr":"Sarkar (2001)"}},"title":{"#tail":"\n","#text":"Applying co-training methods to statistical parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Second Meeting of the North American Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Anoop Sarkar"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Schabes, Yves and Richard Waters. 1993. Stochastic lexicalized context-free grammar. In Proceedings of the Third International Workshop on Parsing Technologies, Tilburg, The Netherlands, and Durbuy, Belgium, pages 257?266."},"#text":"\n","pages":{"#tail":"\n","#text":"257--266"},"marker":{"#tail":"\n","#text":"Schabes, Waters, 1993"},"location":{"#tail":"\n","#text":"Tilburg, The Netherlands, and Durbuy, Belgium,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"roblem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover, in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain, we have performed the study on two parsing models: one based on a context-free variant of tree-adjoining grammars (Joshi, Levy, and Takahashi 1975), the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters 1993; Hwa 1998), and Collins?s Model 2 parser (1997). Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG?s expectation-maximization-based induction algorithm is partially supervised; the model?s parameters are estimated indirectly from the training data. As a superset of the PP-attachment task, parsing is a more challenging learning problem. Whereas a trained PP-attachment m","@endWordPosition":"5857","@position":"35898","annotationId":"T23","@startWordPosition":"5854","@citStr":"Schabes and Waters 1993"}},"title":{"#tail":"\n","#text":"Stochastic lexicalized context-free grammar."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Third International Workshop on Parsing Technologies,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yves Schabes"},{"#tail":"\n","#text":"Richard Waters"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Steedman, Mark, Rebecca Hwa, Stephen Clark, Miles Osborne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Example selection for bootstrapping statistical parsers. In Proceedings of the Joint Conference of Human Language Technologies and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics, Edmonton, Alberta, Canada, pages 236?243."},"#text":"\n","pages":{"#tail":"\n","#text":"236--243"},"marker":{"#tail":"\n","#text":"Steedman, Hwa, Clark, Osborne, Sarkar, Hockenmaier, Ruhlen, Baker, Crim, 2003"},"location":{"#tail":"\n","#text":"Edmonton, Alberta, Canada,"},"title":{"#tail":"\n","#text":"Example selection for bootstrapping statistical parsers."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Joint Conference of Human Language Technologies and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics,"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mark Steedman"},{"#tail":"\n","#text":"Rebecca Hwa"},{"#tail":"\n","#text":"Stephen Clark"},{"#tail":"\n","#text":"Miles Osborne"},{"#tail":"\n","#text":"Anoop Sarkar"},{"#tail":"\n","#text":"Julia Hockenmaier"},{"#tail":"\n","#text":"Paul Ruhlen"},{"#tail":"\n","#text":"Steven Baker"},{"#tail":"\n","#text":"Jeremiah Crim"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Steedman, Mark, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of the Tenth Conference of the European Chapter of the Association for Computational Linguistics, Budapest, pages 331?338."},"#text":"\n","pages":{"#tail":"\n","#text":"331--338"},"marker":{"#tail":"\n","#text":"Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003"},"location":{"#tail":"\n","#text":"Budapest,"},"title":{"#tail":"\n","#text":"Bootstrapping statistical parsers from small datasets."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Tenth Conference of the European Chapter of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mark Steedman"},{"#tail":"\n","#text":"Miles Osborne"},{"#tail":"\n","#text":"Anoop Sarkar"},{"#tail":"\n","#text":"Stephen Clark"},{"#tail":"\n","#text":"Rebecca Hwa"},{"#tail":"\n","#text":"Julia Hockenmaier"},{"#tail":"\n","#text":"Paul Ruhlen"},{"#tail":"\n","#text":"Steven Baker"},{"#tail":"\n","#text":"Jeremiah Crim"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Tang, Min, Xiaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, pages 120?127, July."},"#text":"\n","pages":{"#tail":"\n","#text":"120--127"},"marker":{"#tail":"\n","#text":"Tang, Luo, Roukos, 2002"},"location":{"#tail":"\n","#text":"Philadelphia,"},"title":{"#tail":"\n","#text":"Active learning for statistical natural language parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Min Tang"},{"#tail":"\n","#text":"Xiaoqiang Luo"},{"#tail":"\n","#text":"Salim Roukos"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Thompson, Cynthia A., Mary Elaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proceedings of the Sixteenth International Conference on Machine Learning (ICML-99), pages 406?414, Bled, Slovenia."},"#text":"\n","pages":{"#tail":"\n","#text":"406--414"},"marker":{"#tail":"\n","#text":"Thompson, Califf, Mooney, 1999"},"location":{"#tail":"\n","#text":"Bled, Slovenia."},"title":{"#tail":"\n","#text":"Active learning for natural language parsing and information extraction."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixteenth International Conference on Machine Learning (ICML-99),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Cynthia A Thompson"},{"#tail":"\n","#text":"Mary Elaine Califf"},{"#tail":"\n","#text":"Raymond J Mooney"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1979"},"rawString":{"#tail":"\n","#text":"Van Rijsbergen, Cornelis J. 1979. Information Retrieval. Butterworth, London."},"#text":"\n","marker":{"#tail":"\n","#text":"Van Rijsbergen, 1979"},"location":{"#tail":"\n","#text":"Butterworth, London."},"title":{"#tail":"\n","#text":"Information Retrieval."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Cornelis J Van Rijsbergen"}}}]}}]}}
