{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"Church, Kenneth W. and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22-29."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"16--1"},"marker":{"#tail":"\n","#text":"Church, Hanks, 1990"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"al expert word types. Panel (a) shows the number of side-effect-related word types as judged by a medical expert (Nexpert) as a function of the first 23 frequency classes. Panel (b) shows the proportion of expert types/total corpus types (Ntotal) for the first 23 frequency classes. The horizontal dashed line indicates the mean proportion of 0.0619. It is common practice in information retrieval to discard the lowest-frequency words a priori as nonsignificant (Rijsbergen 1979). In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993). Church and Hanks (1990) use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five. A frequency threshold of five seems quite low. Unfortunately, even this lower frequency threshold of five is too high for the extraction of side-effect-related terms from our medical abstracts. To see this, consider the left panel of Figure 1, which plots the number of side-effect-related words in our corpus of abstracts as judged by a medical expert, as a function of word-frequency class. The side-effect-related words with a frequency of less than fiv","@endWordPosition":"532","@position":"3657","annotationId":"T1","@startWordPosition":"529","@citStr":"Church and Hanks (1990)"},{"#tail":"\n","#text":"using Fisher's exact test. Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G2 and Fisher's exact test for these words. For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests). While we have observed reasonable results with both G2 and Fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information (MI) measure (Church and Hanks 1990): P(x, y) (4) 1(x, y) =-- log2 p(x)p(y) In (4), y is the seed term and x a potential target word. A high MI score for a given target word suggests an association between this target and the seed term. Or perhaps more precisely, a low MI score suggests a dissociation between target and seed word (Manning and Schiltze 1999). To compute recall, precision, and F, we require a cut-off value. As there is no theoretically motivated cut-off value, we vary it systematically. Panel (a) of Figure 7 plots the results for the of corpus. The x-axis represents the MI 312 Weeber, Vos, and Baayen Extracting th","@endWordPosition":"5885","@position":"34865","annotationId":"T2","@startWordPosition":"5882","@citStr":"Church and Hanks 1990"},{"#tail":"\n","#text":"r's Exact Test. We used a network algorithm to compute Fisher's exact test (Mehta and Patel 1986; Clarkson, Fan, and Joe 1993). This algorithm is computationally intensive, but since many words have the same table, only a few tables have to be computed and their results can be cached. It takes an average of 50 seconds to compute one window size in a 100,000 word corpus on a Pentium 133MHz, 48MB Linux machine. Source code for the algorithm can be found at: http: //www. acm. org/pubs /cit at ions/ journals/toms/1986-12-2/p154-mehta/ Mutual Information Given the definition of Mutual Information (Church and Hanks 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4. P(x) is the relative frequency of the target word, P(y) is the relative frequency of the seed term, and P(x, y) is the frequency of the target word in the window. In terms of the contingency table, we have: nu n++ I(x, y) = log2 n1+ s where S n++ n++ \u2014u, we find that is the frequency of the seed. Substituting nn = ni+ \u2014 nu n++ I(x, y) = log2 n1+ S ' n++ n++ 1 = log2 n++ ni+ s n++(ni+ \u2014 n12) n++ 316 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words = lo","@endWordPosition":"7386","@position":"43118","annotationId":"T3","@startWordPosition":"7383","@citStr":"Church and Hanks 1990"}]},"title":{"#tail":"\n","#text":"Word association norms, mutual information, and lexicography."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kenneth W Church"},{"#tail":"\n","#text":"Patrick Hanks"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"Church, Kenneth W. and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22-29."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"16--1"},"marker":{"#tail":"\n","#text":"Church, Hanks, 1990"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"al expert word types. Panel (a) shows the number of side-effect-related word types as judged by a medical expert (Nexpert) as a function of the first 23 frequency classes. Panel (b) shows the proportion of expert types/total corpus types (Ntotal) for the first 23 frequency classes. The horizontal dashed line indicates the mean proportion of 0.0619. It is common practice in information retrieval to discard the lowest-frequency words a priori as nonsignificant (Rijsbergen 1979). In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993). Church and Hanks (1990) use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five. A frequency threshold of five seems quite low. Unfortunately, even this lower frequency threshold of five is too high for the extraction of side-effect-related terms from our medical abstracts. To see this, consider the left panel of Figure 1, which plots the number of side-effect-related words in our corpus of abstracts as judged by a medical expert, as a function of word-frequency class. The side-effect-related words with a frequency of less than fiv","@endWordPosition":"532","@position":"3657","annotationId":"T4","@startWordPosition":"529","@citStr":"Church and Hanks (1990)"},{"#tail":"\n","#text":"using Fisher's exact test. Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G2 and Fisher's exact test for these words. For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests). While we have observed reasonable results with both G2 and Fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information (MI) measure (Church and Hanks 1990): P(x, y) (4) 1(x, y) =-- log2 p(x)p(y) In (4), y is the seed term and x a potential target word. A high MI score for a given target word suggests an association between this target and the seed term. Or perhaps more precisely, a low MI score suggests a dissociation between target and seed word (Manning and Schiltze 1999). To compute recall, precision, and F, we require a cut-off value. As there is no theoretically motivated cut-off value, we vary it systematically. Panel (a) of Figure 7 plots the results for the of corpus. The x-axis represents the MI 312 Weeber, Vos, and Baayen Extracting th","@endWordPosition":"5885","@position":"34865","annotationId":"T5","@startWordPosition":"5882","@citStr":"Church and Hanks 1990"},{"#tail":"\n","#text":"r's Exact Test. We used a network algorithm to compute Fisher's exact test (Mehta and Patel 1986; Clarkson, Fan, and Joe 1993). This algorithm is computationally intensive, but since many words have the same table, only a few tables have to be computed and their results can be cached. It takes an average of 50 seconds to compute one window size in a 100,000 word corpus on a Pentium 133MHz, 48MB Linux machine. Source code for the algorithm can be found at: http: //www. acm. org/pubs /cit at ions/ journals/toms/1986-12-2/p154-mehta/ Mutual Information Given the definition of Mutual Information (Church and Hanks 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4. P(x) is the relative frequency of the target word, P(y) is the relative frequency of the seed term, and P(x, y) is the frequency of the target word in the window. In terms of the contingency table, we have: nu n++ I(x, y) = log2 n1+ s where S n++ n++ \u2014u, we find that is the frequency of the seed. Substituting nn = ni+ \u2014 nu n++ I(x, y) = log2 n1+ S ' n++ n++ 1 = log2 n++ ni+ s n++(ni+ \u2014 n12) n++ 316 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words = lo","@endWordPosition":"7386","@position":"43118","annotationId":"T6","@startWordPosition":"7383","@citStr":"Church and Hanks 1990"}]},"title":{"#tail":"\n","#text":"Word association norms, mutual information, and lexicography."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kenneth W Church"},{"#tail":"\n","#text":"Patrick Hanks"}]}}]}}}}
