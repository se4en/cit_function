{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cascaded grammatical relation assignment. In Proeedings of EMNLP/VL C-99, pages 239-246, University of Maryland, USA, June."},"#text":"\n","pages":{"#tail":"\n","#text":"239--246"},"marker":{"#tail":"\n","#text":"Buchholz, Veenstra, Daelemans, 1999"},"location":{"#tail":"\n","#text":"University of Maryland, USA,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ctic patterns, and may therefore require less training information. That would not be the case when evaluating a full parser on selected target patterns, because its training material would still include full parse-trees labeled with other patterns as well. The approach presented here, of trainable partial parsing, attempts to reduce the gap between shallow and full parsing. It is an extension of shallow parsing towards handling composite and multiple patterns, while maintaining the local nature of the task, and simplicity of training material. One approach to partial parsing was presented by Buchholz et al. (1999), who extended a shallow-parsing technique to partial parsing. The output of NP and VP chunking was used as an input to grammatical relation inference. The inferences process is cascaded, and a clear improvement was obtained by passing results across cascades. Another approach for partial parsing was presented by Skut and Brants (1998). Their method is an extension of that of Church (1988) for finding NP's, achieved by extending the feature space to include structural information. Processing goes simultaneously for structures at all levels, from left to right. Since there are no cascades, the ","@endWordPosition":"449","@position":"2875","annotationId":"T1","@startWordPosition":"446","@citStr":"Buchholz et al. (1999)"},{"#tail":"\n","#text":"993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buchholz et al. (1999), the method extends an existing flat shallow-parsing method to handle composite structures. It yields a significant improvement over the flat method, especially for long and more complex structures. As can be expected, the performance of the partial method is still lower than that of full parsers, which exploit (and require) much richer information. The results of this line of research enrich the space of alternative parsing approaches, aiming to reduce the gap between shallow and full parsing. Acknowledgements Y. K. thanks Jorn Veenstra, Sabine Buchholz, and Khalil Sima'an for thorough and h","@endWordPosition":"3317","@position":"18917","annotationId":"T2","@startWordPosition":"3314","@citStr":"Buchholz et al. (1999)"}]},"title":{"#tail":"\n","#text":"Cascaded grammatical relation assignment."},"booktitle":{"#tail":"\n","#text":"In Proeedings of EMNLP/VL C-99,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Buchholz"},{"#tail":"\n","#text":"J Veenstra"},{"#tail":"\n","#text":"W Daelemans"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cascaded grammatical relation assignment. In Proeedings of EMNLP/VL C-99, pages 239-246, University of Maryland, USA, June."},"#text":"\n","pages":{"#tail":"\n","#text":"239--246"},"marker":{"#tail":"\n","#text":"Buchholz, Veenstra, Daelemans, 1999"},"location":{"#tail":"\n","#text":"University of Maryland, USA,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ctic patterns, and may therefore require less training information. That would not be the case when evaluating a full parser on selected target patterns, because its training material would still include full parse-trees labeled with other patterns as well. The approach presented here, of trainable partial parsing, attempts to reduce the gap between shallow and full parsing. It is an extension of shallow parsing towards handling composite and multiple patterns, while maintaining the local nature of the task, and simplicity of training material. One approach to partial parsing was presented by Buchholz et al. (1999), who extended a shallow-parsing technique to partial parsing. The output of NP and VP chunking was used as an input to grammatical relation inference. The inferences process is cascaded, and a clear improvement was obtained by passing results across cascades. Another approach for partial parsing was presented by Skut and Brants (1998). Their method is an extension of that of Church (1988) for finding NP's, achieved by extending the feature space to include structural information. Processing goes simultaneously for structures at all levels, from left to right. Since there are no cascades, the ","@endWordPosition":"449","@position":"2875","annotationId":"T3","@startWordPosition":"446","@citStr":"Buchholz et al. (1999)"},{"#tail":"\n","#text":"993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buchholz et al. (1999), the method extends an existing flat shallow-parsing method to handle composite structures. It yields a significant improvement over the flat method, especially for long and more complex structures. As can be expected, the performance of the partial method is still lower than that of full parsers, which exploit (and require) much richer information. The results of this line of research enrich the space of alternative parsing approaches, aiming to reduce the gap between shallow and full parsing. Acknowledgements Y. K. thanks Jorn Veenstra, Sabine Buchholz, and Khalil Sima'an for thorough and h","@endWordPosition":"3317","@position":"18917","annotationId":"T4","@startWordPosition":"3314","@citStr":"Buchholz et al. (1999)"}]},"title":{"#tail":"\n","#text":"Cascaded grammatical relation assignment."},"booktitle":{"#tail":"\n","#text":"In Proeedings of EMNLP/VL C-99,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Buchholz"},{"#tail":"\n","#text":"J Veenstra"},{"#tail":"\n","#text":"W Daelemans"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. of the ACL/EACL Annual Meeting, pages 16-23, Madrid, Spain, July."},"#text":"\n","pages":{"#tail":"\n","#text":"16--23"},"marker":{"#tail":"\n","#text":"Collins, 1997"},"location":{"#tail":"\n","#text":"Madrid, Spain,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ract In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recognizing subjectverb ","@endWordPosition":"127","@position":"838","annotationId":"T5","@startWordPosition":"126","@citStr":"Collins (1997)"},{"#tail":"\n","#text":"ining fewer tiles, with larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedded instances, ne","@endWordPosition":"1987","@position":"11378","annotationId":"T6","@startWordPosition":"1986","@citStr":"Collins (1997)"},{"#tail":"\n","#text":"67.1 51.7 all NP 79.3 88.5 83.7 base NP 1 93.2 93.5 93.3 composite 71.4 49.0 58.1 all NP 87.2 77.7 82.2 NP (TKS99) 76.1 91.3 83.0 Table 3: NP Results, OT = 0.6, tile length< 5. Rows 5-10 refer to experiments where baseNPs were distinguished from composite ones. There are currently no other partial parsers on these tasks to compare the combined VP and NP results to. Tjong Kim Sang (1999) presented result for composite NP, obtained by repeated cascading, similar to our results with seperate base and composite NPs and no internal structure. Our results are lower than those of full parsers, e.g., Collins (1997) - as might be expected since much less structural data, and no lexical data are being used. 4 Discussion We have presented a memory-based learning method for partial parsing which can handle and exploit compositional information. Like other shallow-parsing systems, it is most useful when the number of target patterns is small. In particular, the method does not require fully-parsed sentences as training, unlike trainable full parsing methods. The training material has to contain only bracketing of the target patterns, implying much simpler training material when the parsing task is limited. T","@endWordPosition":"2581","@position":"14689","annotationId":"T7","@startWordPosition":"2580","@citStr":"Collins (1997)"}]},"title":{"#tail":"\n","#text":"Three generative, lexicalised models for statistical parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of the ACL/EACL Annual Meeting,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. of the ACL/EACL Annual Meeting, pages 16-23, Madrid, Spain, July."},"#text":"\n","pages":{"#tail":"\n","#text":"16--23"},"marker":{"#tail":"\n","#text":"Collins, 1997"},"location":{"#tail":"\n","#text":"Madrid, Spain,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ract In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recognizing subjectverb ","@endWordPosition":"127","@position":"838","annotationId":"T8","@startWordPosition":"126","@citStr":"Collins (1997)"},{"#tail":"\n","#text":"ining fewer tiles, with larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedded instances, ne","@endWordPosition":"1987","@position":"11378","annotationId":"T9","@startWordPosition":"1986","@citStr":"Collins (1997)"},{"#tail":"\n","#text":"67.1 51.7 all NP 79.3 88.5 83.7 base NP 1 93.2 93.5 93.3 composite 71.4 49.0 58.1 all NP 87.2 77.7 82.2 NP (TKS99) 76.1 91.3 83.0 Table 3: NP Results, OT = 0.6, tile length< 5. Rows 5-10 refer to experiments where baseNPs were distinguished from composite ones. There are currently no other partial parsers on these tasks to compare the combined VP and NP results to. Tjong Kim Sang (1999) presented result for composite NP, obtained by repeated cascading, similar to our results with seperate base and composite NPs and no internal structure. Our results are lower than those of full parsers, e.g., Collins (1997) - as might be expected since much less structural data, and no lexical data are being used. 4 Discussion We have presented a memory-based learning method for partial parsing which can handle and exploit compositional information. Like other shallow-parsing systems, it is most useful when the number of target patterns is small. In particular, the method does not require fully-parsed sentences as training, unlike trainable full parsing methods. The training material has to contain only bracketing of the target patterns, implying much simpler training material when the parsing task is limited. T","@endWordPosition":"2581","@position":"14689","annotationId":"T10","@startWordPosition":"2580","@citStr":"Collins (1997)"}]},"title":{"#tail":"\n","#text":"Three generative, lexicalised models for statistical parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of the ACL/EACL Annual Meeting,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. of the ACL/EACL Annual Meeting, pages 16-23, Madrid, Spain, July."},"#text":"\n","pages":{"#tail":"\n","#text":"16--23"},"marker":{"#tail":"\n","#text":"Collins, 1997"},"location":{"#tail":"\n","#text":"Madrid, Spain,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ract In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recognizing subjectverb ","@endWordPosition":"127","@position":"838","annotationId":"T11","@startWordPosition":"126","@citStr":"Collins (1997)"},{"#tail":"\n","#text":"ining fewer tiles, with larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedded instances, ne","@endWordPosition":"1987","@position":"11378","annotationId":"T12","@startWordPosition":"1986","@citStr":"Collins (1997)"},{"#tail":"\n","#text":"67.1 51.7 all NP 79.3 88.5 83.7 base NP 1 93.2 93.5 93.3 composite 71.4 49.0 58.1 all NP 87.2 77.7 82.2 NP (TKS99) 76.1 91.3 83.0 Table 3: NP Results, OT = 0.6, tile length< 5. Rows 5-10 refer to experiments where baseNPs were distinguished from composite ones. There are currently no other partial parsers on these tasks to compare the combined VP and NP results to. Tjong Kim Sang (1999) presented result for composite NP, obtained by repeated cascading, similar to our results with seperate base and composite NPs and no internal structure. Our results are lower than those of full parsers, e.g., Collins (1997) - as might be expected since much less structural data, and no lexical data are being used. 4 Discussion We have presented a memory-based learning method for partial parsing which can handle and exploit compositional information. Like other shallow-parsing systems, it is most useful when the number of target patterns is small. In particular, the method does not require fully-parsed sentences as training, unlike trainable full parsing methods. The training material has to contain only bracketing of the target patterns, implying much simpler training material when the parsing task is limited. T","@endWordPosition":"2581","@position":"14689","annotationId":"T13","@startWordPosition":"2580","@citStr":"Collins (1997)"}]},"title":{"#tail":"\n","#text":"Three generative, lexicalised models for statistical parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of the ACL/EACL Annual Meeting,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Walter Daelemans, Sabine Buchholz, and Jorn Veenstra. 1999. Memory-based shallow parsing. In Proeedings of CoNLL-99, Bergen, Norway, June."},"#text":"\n","marker":{"#tail":"\n","#text":"Daelemans, Buchholz, Veenstra, 1999"},"location":{"#tail":"\n","#text":"Bergen, Norway,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rom complete instances. \u2022 The grammar rules of APP do not include context, which is taken into account when generating the non-terminal S. In MBSL, the context is consulted for each instance candidate. \u2022 APP, like DOP, uses a probabilistic model. The probability of a grammar rule X Y is Freq(X y)/Freq(X). Analogously, the denominator in MBSL would be Freq(Y). The presented method concerns primarily with phrases, which can be represented by a tree structure. It is not aimed at handling dependencies, which require heavy use of lexical information (Hindle and Rooth, 1993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buchholz et al. (1999), the method extends an existi","@endWordPosition":"3218","@position":"18347","annotationId":"T14","@startWordPosition":"3215","@citStr":"Daelemans et al., 1999"}},"title":{"#tail":"\n","#text":"Memory-based shallow parsing."},"booktitle":{"#tail":"\n","#text":"In Proeedings of CoNLL-99,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Walter Daelemans"},{"#tail":"\n","#text":"Sabine Buchholz"},{"#tail":"\n","#text":"Jorn Veenstra"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"D. Hindle and M. Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103-120."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"19--1"},"marker":{"#tail":"\n","#text":"Hindle, Rooth, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" generalizations, while APP uses rules derived from complete instances. \u2022 The grammar rules of APP do not include context, which is taken into account when generating the non-terminal S. In MBSL, the context is consulted for each instance candidate. \u2022 APP, like DOP, uses a probabilistic model. The probability of a grammar rule X Y is Freq(X y)/Freq(X). Analogously, the denominator in MBSL would be Freq(Y). The presented method concerns primarily with phrases, which can be represented by a tree structure. It is not aimed at handling dependencies, which require heavy use of lexical information (Hindle and Rooth, 1993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buc","@endWordPosition":"3210","@position":"18298","annotationId":"T15","@startWordPosition":"3207","@citStr":"Hindle and Rooth, 1993"}},"title":{"#tail":"\n","#text":"Structural ambiguity and lexical relations."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Hindle"},{"#tail":"\n","#text":"M Rooth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"David M. Magerman. 1995. Statistical decisiontree models for parsing. In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics. Cambridge. MA, 26-30."},"#text":"\n","pages":{"#tail":"\n","#text":"26--30"},"marker":{"#tail":"\n","#text":"Magerman, 1995"},"location":{"#tail":"\n","#text":"Cambridge. MA,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"cs.biu.ac.il Abstract In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recogniz","@endWordPosition":"125","@position":"822","annotationId":"T16","@startWordPosition":"124","@citStr":"Magerman (1995)"},{"#tail":"\n","#text":"with covers containing fewer tiles, with larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedd","@endWordPosition":"1985","@position":"11362","annotationId":"T17","@startWordPosition":"1984","@citStr":"Magerman (1995)"}]},"title":{"#tail":"\n","#text":"Statistical decisiontree models for parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David M Magerman"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"David M. Magerman. 1995. Statistical decisiontree models for parsing. In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics. Cambridge. MA, 26-30."},"#text":"\n","pages":{"#tail":"\n","#text":"26--30"},"marker":{"#tail":"\n","#text":"Magerman, 1995"},"location":{"#tail":"\n","#text":"Cambridge. MA,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"cs.biu.ac.il Abstract In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recogniz","@endWordPosition":"125","@position":"822","annotationId":"T18","@startWordPosition":"124","@citStr":"Magerman (1995)"},{"#tail":"\n","#text":"with covers containing fewer tiles, with larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedd","@endWordPosition":"1985","@position":"11362","annotationId":"T19","@startWordPosition":"1984","@citStr":"Magerman (1995)"}]},"title":{"#tail":"\n","#text":"Statistical decisiontree models for parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"David M Magerman"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In EMNLP2, Providence, RI, March."},"#text":"\n","marker":{"#tail":"\n","#text":"Ratnaparkhi, 1997"},"location":{"#tail":"\n","#text":"Providence, RI,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"er, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recognizing subjectverb (SV) and verb-object","@endWordPosition":"129","@position":"858","annotationId":"T20","@startWordPosition":"128","@citStr":"Ratnaparkhi (1997)"},{"#tail":"\n","#text":"ith larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedded instances, nemb in a tile. A limit of","@endWordPosition":"1990","@position":"11402","annotationId":"T21","@startWordPosition":"1989","@citStr":"Ratnaparkhi (1997)"}]},"title":{"#tail":"\n","#text":"A linear observed time statistical parser based on maximum entropy models."},"booktitle":{"#tail":"\n","#text":"In EMNLP2,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Ratnaparkhi"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In EMNLP2, Providence, RI, March."},"#text":"\n","marker":{"#tail":"\n","#text":"Ratnaparkhi, 1997"},"location":{"#tail":"\n","#text":"Providence, RI,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"er, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked. 1 Introduction A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences (e.g., Bod (1992), Magerman (1995), Collins (1997), Ratnaparkhi (1997), and Sekine (1998)). In parallel, a lot of work is being done on shallow parsing (Abney, 1991; Greffenstette, 1993), focusing on partial analysis of sentences at the level of local phrases and the relations between them. Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping sequences of syntactic structures, a task called chunking. Most of the chunking works have concentrated on noun-phrases (NPs, e.g. Church (1988), Ramshaw and Marcus (1995), Cardie and Pierce (1998), Veenstra (1998)). Other chunking tasks involve recognizing subjectverb (SV) and verb-object","@endWordPosition":"129","@position":"858","annotationId":"T22","@startWordPosition":"128","@citStr":"Ratnaparkhi (1997)"},{"#tail":"\n","#text":"ith larger covered contexts, and when all else is equal, candidates whose covers have more overlap between connecting tiles. The flat version used a similar function without using totalratio, hence num was the most important quantity. In the composite case, inner instances increase the number of possible covers to the extent that it no longer becomes a good measure of reliability (at least not at face value). 3 Evaluation The system was trained on the Penn Treebank (Marcus et al., 1993) WSJ Sections 2- 21 and tested on Section 23 (Table 1), same as used by Magerman (1995), Collins (1997), and Ratnaparkhi (1997), and became a common testbed. The tasks were selected so as to demonstrate the benefit of using internal structure Train Data, WSJ 02-21, 28884 sentences base composite base:all NP 166242 61384 73% VP 43377 28017 61% Test Data, WSJ 23, 2416 sentences base composite base:all NP 13524 5106 73% VP 3496 2267 61% Table 1: Sizes of training and test data, note the similar proportions of base instances data for learning composite structures. We have studied the effect of noun-phrase information on learning verb phrases by setting limits on the number of embedded instances, nemb in a tile. A limit of","@endWordPosition":"1990","@position":"11402","annotationId":"T23","@startWordPosition":"1989","@citStr":"Ratnaparkhi (1997)"}]},"title":{"#tail":"\n","#text":"A linear observed time statistical parser based on maximum entropy models."},"booktitle":{"#tail":"\n","#text":"In EMNLP2,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Ratnaparkhi"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"W. Skut and T. Brants. 1998. A maximumentropy partial parser for unrestricted text. In Proc. of the sixth Workshop on Very Large Corpora, Montreal, Canada."},"#text":"\n","marker":{"#tail":"\n","#text":"Skut, Brants, 1998"},"location":{"#tail":"\n","#text":"Montreal, Canada."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"he gap between shallow and full parsing. It is an extension of shallow parsing towards handling composite and multiple patterns, while maintaining the local nature of the task, and simplicity of training material. One approach to partial parsing was presented by Buchholz et al. (1999), who extended a shallow-parsing technique to partial parsing. The output of NP and VP chunking was used as an input to grammatical relation inference. The inferences process is cascaded, and a clear improvement was obtained by passing results across cascades. Another approach for partial parsing was presented by Skut and Brants (1998). Their method is an extension of that of Church (1988) for finding NP's, achieved by extending the feature space to include structural information. Processing goes simultaneously for structures at all levels, from left to right. Since there are no cascades, the structural level of the output is limited by that of the feature set. This paper presents an extension of the algorithm of Argamon et al. (1998, 1999, hereafter MBSL), which handles and exploits compositional structures. MBSL is a memory-based algorithm that uses raw-data segments for learning chunks. It works with POS tags, and combin","@endWordPosition":"505","@position":"3212","annotationId":"T24","@startWordPosition":"502","@citStr":"Skut and Brants (1998)"},{"#tail":"\n","#text":"mation (Hindle and Rooth, 1993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buchholz et al. (1999), the method extends an existing flat shallow-parsing method to handle composite structures. It yields a significant improvement over the flat method, especially for long and more complex structures. As can be expected, the performance of the partial method is still lower than that of full parsers, which exploit (and require) much richer information. The results of this line of research enrich the space of alternative parsing approaches, aiming to reduce the gap between shallow and full parsing. Acknowledgements Y. K. thanks Jorn Veenstra, Sabine Buchholz, and Khalil","@endWordPosition":"3312","@position":"18890","annotationId":"T25","@startWordPosition":"3309","@citStr":"Skut and Brants (1998)"}]},"title":{"#tail":"\n","#text":"A maximumentropy partial parser for unrestricted text."},"booktitle":{"#tail":"\n","#text":"In Proc. of the sixth Workshop on Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W Skut"},{"#tail":"\n","#text":"T Brants"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"W. Skut and T. Brants. 1998. A maximumentropy partial parser for unrestricted text. In Proc. of the sixth Workshop on Very Large Corpora, Montreal, Canada."},"#text":"\n","marker":{"#tail":"\n","#text":"Skut, Brants, 1998"},"location":{"#tail":"\n","#text":"Montreal, Canada."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"he gap between shallow and full parsing. It is an extension of shallow parsing towards handling composite and multiple patterns, while maintaining the local nature of the task, and simplicity of training material. One approach to partial parsing was presented by Buchholz et al. (1999), who extended a shallow-parsing technique to partial parsing. The output of NP and VP chunking was used as an input to grammatical relation inference. The inferences process is cascaded, and a clear improvement was obtained by passing results across cascades. Another approach for partial parsing was presented by Skut and Brants (1998). Their method is an extension of that of Church (1988) for finding NP's, achieved by extending the feature space to include structural information. Processing goes simultaneously for structures at all levels, from left to right. Since there are no cascades, the structural level of the output is limited by that of the feature set. This paper presents an extension of the algorithm of Argamon et al. (1998, 1999, hereafter MBSL), which handles and exploits compositional structures. MBSL is a memory-based algorithm that uses raw-data segments for learning chunks. It works with POS tags, and combin","@endWordPosition":"505","@position":"3212","annotationId":"T26","@startWordPosition":"502","@citStr":"Skut and Brants (1998)"},{"#tail":"\n","#text":"mation (Hindle and Rooth, 1993, for PP attachment). As (Daelemans et al., 1999) show, lexical information improves on NP and VP chunking as well. Since our method uses raw data, representing lexical entries will require a lot of memory. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use word-types, such as a special tag for be-verbs, or for prepositions like 'of' which attaches mainly to nouns (Sekine and Grishman, 1995). In a similar vain to Skut and Brants (1998) and Buchholz et al. (1999), the method extends an existing flat shallow-parsing method to handle composite structures. It yields a significant improvement over the flat method, especially for long and more complex structures. As can be expected, the performance of the partial method is still lower than that of full parsers, which exploit (and require) much richer information. The results of this line of research enrich the space of alternative parsing approaches, aiming to reduce the gap between shallow and full parsing. Acknowledgements Y. K. thanks Jorn Veenstra, Sabine Buchholz, and Khalil","@endWordPosition":"3312","@position":"18890","annotationId":"T27","@startWordPosition":"3309","@citStr":"Skut and Brants (1998)"}]},"title":{"#tail":"\n","#text":"A maximumentropy partial parser for unrestricted text."},"booktitle":{"#tail":"\n","#text":"In Proc. of the sixth Workshop on Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W Skut"},{"#tail":"\n","#text":"T Brants"}]}}]}}}}
