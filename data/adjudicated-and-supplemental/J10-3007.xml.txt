minal work of Brown et al (1993b) introduced a series of probabilistic models (IBM Models 1?5) for statistical machine translation and the concept of ?word-byword? alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al 2004; Chiang et al 2005]) as well as for ? INESC-ID Lisboa, Spoken Language Systems Lab, R. Alves Redol 9, 1000-029 LISBOA, Portugal. E-mail: joao.graca@l2f.inesc-id.pt. ?? University of Pennsylvania, Department of Computer and Information Science, Levine Hall, 3330 Walnut Street, Philadelphia, PA 19104-6309. E-mail: kuzman@cis.upenn.edu. ? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu. Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for publicatio
 et al (1993b) introduced a series of probabilistic models (IBM Models 1?5) for statistical machine translation and the concept of ?word-byword? alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al 2004; Chiang et al 2005]) as well as for ? INESC-ID Lisboa, Spoken Language Systems Lab, R. Alves Redol 9, 1000-029 LISBOA, Portugal. E-mail: joao.graca@l2f.inesc-id.pt. ?? University of Pennsylvania, Department of Computer and Information Science, Levine Hall, 3330 Walnut Street, Philadelphia, PA 19104-6309. E-mail: kuzman@cis.upenn.edu. ? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu. Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for publication: 10 March 2010. ?
, PA 19104-6309. E-mail: kuzman@cis.upenn.edu. ? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu. Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for publication: 10 March 2010. ? 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008). IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that requir
 kuzman@cis.upenn.edu. ? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu. Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for publication: 10 March 2010. ? 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008). IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristi
formation Science, 3330 Walnut Street, Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu. Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for publication: 10 March 2010. ? 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008). IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA++ software package
bmission received: 1 August 2009; revised submission received: 24 December 2009; accepted for publication: 10 March 2010. ? 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008). IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA++ software package (Och and Ney 2003) as a black box, selecting IBM Model 4 as a compromise between alignment q
and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008). IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA++ software package (Och and Ney 2003) as a black box, selecting IBM Model 4 as a compromise between alignment quality and efficiency. All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word). Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86?98%). This leads to the common practice of post
nglish He walked and French Il est alle?), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity, manual annotations usually distinguish between sure correspondences for unambiguous translations, and possible, for ambiguous translations (Och and Ney 2003). The top row of Figure 1 shows two word alignments between an English?French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source?target (target?source) and contains source (target) words as rows and target (source) words as columns. Each entry in the matrix corresponds to a source?target word pair, and is the candidate for an alignment link. Sure links are represented as squares with borders, and possible links Figure 1 Posterior marginal distributions for different models for an English to French sentence translation. Left: EN?FR model
lish?French, English?Spanish, English?Portuguese, Portuguese?Spanish, Portuguese?French, and Spanish?French. Corpus Sentence Pairs Ave Length Max Length % Sure % 1-1 En/Fr 447 16/17 30/30 21 98 En/Es 400 29/31 90/99 67 86 En/Pt 60 11/11 20/20 54 91 Pt/Es 60 11/11 20/20 69 92 Pt/Fr 60 11/12 20/20 77 88 Es/Fr 60 11/12 20/20 79 87 are represented as squares without borders. Circles indicate the posterior probability associated with a given link and will be explained latter. We use six manually annotated corpora whose characteristics are summarized in Table 1. The corpora are: the Hansard corpus (Och and Ney 2000) of English/French Canadian Parliamentary proceedings (En-Fr), and the English/Spanish portion of the Europarl corpus (Koehn 2005) where the annotation is from EPPS (Lambert et al 2005) (En-Es) using standard test and development set split. We also used the English/ Portuguese (En-Pt), Portuguese/Spanish (Pt-Es), Portuguese/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%. Table 1 shows some of the variety of challenges presented by each cor
ength Max Length % Sure % 1-1 En/Fr 447 16/17 30/30 21 98 En/Es 400 29/31 90/99 67 86 En/Pt 60 11/11 20/20 54 91 Pt/Es 60 11/11 20/20 69 92 Pt/Fr 60 11/12 20/20 77 88 Es/Fr 60 11/12 20/20 79 87 are represented as squares without borders. Circles indicate the posterior probability associated with a given link and will be explained latter. We use six manually annotated corpora whose characteristics are summarized in Table 1. The corpora are: the Hansard corpus (Och and Ney 2000) of English/French Canadian Parliamentary proceedings (En-Fr), and the English/Spanish portion of the Europarl corpus (Koehn 2005) where the annotation is from EPPS (Lambert et al 2005) (En-Es) using standard test and development set split. We also used the English/ Portuguese (En-Pt), Portuguese/Spanish (Pt-Es), Portuguese/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%. Table 1 shows some of the variety of challenges presented by each corpus. For example, En-Es has longer sentences and hence more ambiguity for alignment. Furthermore, it has a smaller percentage of b
hood : L(?) = ?E[log p?(x | y)] = ?E[log ? z p?(x, z | y)] (2) where ?E[ f (x, y)] = 1N ?N n=1 f (x n, yn) denotes the empirical average of a function f (xn, yn) over the N pairs of sentences {(x1, y1) . . . , (xN, yN )} in the training corpus. Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977). EM maximizes L(?) via block-coordinate ascent on a lower bound F(q, ?) using an auxiliary distribution over the latent variables q(z | x, y) (Neal and Hinton 1998): EM Lower Bound : L(?) ? F(q, ?) = ?E [ ? z q(z | x, y) log p?(x, z | y) q(z | x, y) ] (3) 2 For both of these points, see the experimental setup in Section 4.1. 485 Computational Linguistics Volume 36, Number 3 To simplify notation, we will drop the dependence on y and will write p?(x, z | y) as p?(x, z), p?(z | x, y) as p?(z | x) and q(z | x, y) as q(z | x). The alternating E and M steps at iteration t + 1 are given by: E : qt+1(z | x) = arg max q(z|x) F(q, ?t) = arg min q(z|x) KL(q(z | x) || p?t (z | x)) = p?t (z | x) (4) M : ?t+1 = arg max ? F(qt+1, ?) = arg max ? ?E [ ? z qt+1(z | x) log
 sentences it needs to spread its probability mass over fewer competing translations. In this case, choosing to align the rare word to all of these words leads to a higher likelihood than correctly linking them or linking them to the special null word, because it increases the likelihood of this sentence without lowering the likelihood of many other sentences. 2.3 Decoding Alignments are normally predicted using the Viterbi algorithm (which selects the single most probable path through the HMM?s lattice). Another possibility that often works better is to use Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne 2002; Liang, Taskar, and Klein 2006; Grac?a, Ganchev, and Taskar 2007). Using this decoding we include an alignment link i ? j if the posterior probability that word i aligns to word j is above some threshold. This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link. The threshold is tuned on some small amount of labeled data?in our case the development set?to minimize some loss. Kumar and Byrne (2002) study different loss functions that incorporate linguistic knowledge, and show significant 486 Grac?a, Ganchev, and Taskar Learning Alignment 
on vs recall curves, by generating alignments for different thresholds (0..1). Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated. 3. Posterior Regularization Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4+ (Brown et al 1993b; Och and Ney 2003), and more recently by the LEAF model (Fraser and Marcu 2007). Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Grac?a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model?s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint
ent thresholds (0..1). Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated. 3. Posterior Regularization Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4+ (Brown et al 1993b; Och and Ney 2003), and more recently by the LEAF model (Fraser and Marcu 2007). Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Grac?a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model?s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the mode
rojection step uses the same inference algorithm (forward?backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of ?. ?i ? 0;1 while ||?(?)||2 > ? do2 ??(x, zc) ? ?(x, zc) exp?? f(x,zc );3 q(z | x) ? forwardBackward(??(x, zc));4 ? ? ? + ???(?);5 end6 Algorithm 1: Computing KL(Qx ? p?(z|x)) = min q?Qx KL(q(z|x) ? p?(z|x)) 489 Computational Linguistics Volume 36, Number 3 We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here ? is an optimization precision, ? is a step size chosen with the strong Wolfe?s rule (Nocedal and Wright 1999). Here, ??(?) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when ? = 0, the objective is not differentiable. In practice this only happens at the start of optimization and we use a sub-gradient for the first direction. Computing the projection requires an algorithm for inference in the original model, and uses that inference as a subroutine. For HMM word alignments, we need to make several calls to forward?backward 
urrent setting of ?. ?i ? 0;1 while ||?(?)||2 > ? do2 ??(x, zc) ? ?(x, zc) exp?? f(x,zc );3 q(z | x) ? forwardBackward(??(x, zc));4 ? ? ? + ???(?);5 end6 Algorithm 1: Computing KL(Qx ? p?(z|x)) = min q?Qx KL(q(z|x) ? p?(z|x)) 489 Computational Linguistics Volume 36, Number 3 We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here ? is an optimization precision, ? is a step size chosen with the strong Wolfe?s rule (Nocedal and Wright 1999). Here, ??(?) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when ? = 0, the objective is not differentiable. In practice this only happens at the start of optimization and we use a sub-gradient for the first direction. Computing the projection requires an algorithm for inference in the original model, and uses that inference as a subroutine. For HMM word alignments, we need to make several calls to forward?backward in order to choose ?. Setting the optimization precision ? more loosely allows the optimization to terminate more quickly but at a less a
-1 alignment links from 78% to 97.3% for En-Fr (manual annotations have 98.1%); for En-Pt the increase is from 84.7% to 95.8% (manual annotations have 90.8%) (see Section 4.1). 3.4 Symmetry Constraints The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations. In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions (Och and Ney 2003). However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: ??p (??z ) (source?target) and ??p (??z ) (target?source). We suppress dependence on x and y for brevity. Define z to range over the union of all possible 491 Computational Linguistics Volume 36, Number 3 directional alignments ?? Z ???Z . We define a mixture model p(z) = 12 ??p (z) + 12 ??p (z) where ??p (??z ) = 0 and vice versa (i.e., the alignment of one directional model has probabil
ed with normal EM vs. trained with PR plus constraints. We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference. However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference. Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements. We used a standard implementation of IBM Model 4 (Och and Ney 2003) and because changing the existing code is not trivial, we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves. We trained IBM Model 4 using the default configuration of the Figure 4 Precision/Recall curves for different models using 1,000k sentences. Precision on the horizontal axis. Left: Hansard EN-FR direction. Right: EN-PT Portuguese-English direction. 493 Computational Linguistics Volume 36, Number 3 Figure 5 Word alignment precision when the threshold is chosen to achieve IBM Model 4 recall with a difference of ? 0.005. T
rds to null. 4.4 Symmetrization As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair. Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single 495 Computational Linguistics Volume 36, Number 3 Figure 7 Precision and Recall as a function of training data size for En-Fr by common and rare words. Top Left: Common Precision, Top Right: Rare Precision, Bottom: Rare Recall. alignment. For MT the most commonly used heuristic is called grow diagonal final (Och and Ney 2003). This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union. In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages. One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more di
 will help and when they will hinder system performance. In this work we followed a more principled approach that uses Figure 8 Precision/recall curves for the different models after soft union symmetrization. Precision is on the horizontal axis. Left EN-FR, Right PT-ES. 496 Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union (DeNero and Klein 2007). Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization?trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time. Applying the symmetrization to the model with symmetry constraints does not affect performance. 4.5 Analysis In this section we discuss some scenarios in which the constraints make the alignments better, and some sce
 word alignments; (5) build phrase table; (6) tune weights for the phrase table. For more details consult the shared task description.6 To evaluate the quality of the produced alignments, we keep the pipeline unchanged, and use the models described earlier to generate the word alignments in Step 3. For Step 4, we use the soft union symmetrization heuristic. Symmetrization has almost no effect on alignments produced by S-HMM, but we use it for uniformity in the experiments. We tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs 5 The open source Moses (Hoang et al 2007) toolkit from www.statmt.org/moses/. 6 www.statmt.org/wmt08/baseline.html. 498 Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints Table 2 BLEU scores for all language pairs. The best threshold was selected according to the development set after the last MERT iteration. Bold denotes the best score. Fr ? En En ? Fr Es ? En En ? Es Pt ? En En ? Pt IBM M4 GDF 35.7 31.2 32.4 31.6 31.4 28.9 HMM SU 35.9 28.9 32.3 31.6 30.9 31.6 B-HMM SU 36.0 31.5 32.6 31.7 31.0 32.2 S-HMM SU 35.5 31.2 31.9 32.5 31.4 32.3 of precision vs. recall, and pick the best according to the translati
for MERT optimization to converge varied from 2 to 28; and the best choice of threshold on the development set did not always correspond to the best on the test set. Contrary to conventional wisdom in the MT community, bigger phrase tables did not always perform better. In 14 out of 18 cases, the threshold picked was 0.4 (medium size phrase tables) and the other four times 0.2 was picked (smaller phrase tables). When we include only high confidence alignments, more phrases are extracted but many of these are erroneous. Potentially this leads to a poor estimate of the phrase probabilities. See Lopez and Resnik (2006) for further discussion. 5.2 Syntax Transfer In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages. We used the system proposed by Ganchev, Gillenwater, and Taskar (2009). This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language). We consider a parse tree on the source language as a set of dependency edges to be transferred. For each such edge, if both end points a
anchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En?Bg) and from English to Spanish (En?Es). The En?Bg 499 Computational Linguistics Volume 36, Number 3 Figure 10 Edge conservation for cross-lingual grammar induction. Left: En?Bg subtitle corpus; Right: En?Es parliamentary proceedings. Vertical axis: percentage of transferred edges that are correct. Horizontal axis: average number of transferred edges per sentence. results are based on a corpus of movie subtitles (Tiedemann 2007), and are consequently shorter sentences, whereas the En?Es results are based on a corpus of parliamentary proceedings (Koehn 2005). We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM. 6. Related Work The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations. The fertility of each source word is implicitly encoded in the dur
 for transferring from English to Bulgarian (En?Bg) and from English to Spanish (En?Es). The En?Bg 499 Computational Linguistics Volume 36, Number 3 Figure 10 Edge conservation for cross-lingual grammar induction. Left: En?Bg subtitle corpus; Right: En?Es parliamentary proceedings. Vertical axis: percentage of transferred edges that are correct. Horizontal axis: average number of transferred edges per sentence. results are based on a corpus of movie subtitles (Tiedemann 2007), and are consequently shorter sentences, whereas the En?Es results are based on a corpus of parliamentary proceedings (Koehn 2005). We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM. 6. Related Work The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations. The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control 
red edges that are correct. Horizontal axis: average number of transferred edges per sentence. results are based on a corpus of movie subtitles (Tiedemann 2007), and are consequently shorter sentences, whereas the En?Es results are based on a corpus of parliamentary proceedings (Koehn 2005). We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM. 6. Related Work The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations. The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant ? > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form ?the average length of dependencies should be X? to capture the locality of syntax (at le
ing constraints over a model to better guide the learning process has appeared before. In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations. The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant ? > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form ?the average length of dependencies should be X? to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing. They modify the model?s distribution over trees p?(y) by a penalty term as: p ? ?(y) ? p?(y)e (? ? e?y length(e)), where length(e) is the surface length of edge e. The factor ? changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training. These two approaches also have the goal of controlling unsu
 of ?, for instance if ? ? 0, even if the data is such that the model already uses too many short edges on average, this value of ? will push for more short edges. By contrast the statements we can make in PR are of the form ?there should be more short edges than long edges?. Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes. 500 Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints PR is closely related to the work of Mann and McCallum (2007, 2008), who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning. They call their method generalized expectation (GE) constraints or alternatively expectation regularization. In the original GE framework, the posteriors of the model on unlabeled data are regularized directly. They train a discriminative model, using conditional likelihood on labeled data and an ?expectation regularization? penalty term on the unlabeled data: arg max ? Llabeled(?) ? ??E[||Ep? [f(x, z) ? b|| 2 2]. (16) Notice that there is no intermedia
