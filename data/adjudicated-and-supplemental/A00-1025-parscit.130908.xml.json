{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"institution":{"#tail":"\n","#text":"University of Montreal, Montreal, Canada. Association for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"C. Cardie and D. Pierce. 1998. Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and COLING-98, pages 218-224, University of Montreal, Montreal, Canada. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"218--224"},"marker":{"#tail":"\n","#text":"Cardie, Pierce, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ses as answers. In the TREC8 development corpus, for example, 36 of 38 questions have noun phrase answers. As a result, we next investigate the use of a very simple linguistic filter that considers only noun phrases as answer hypotheses. The filter operates on the ordered list of summary extracts for a particular question and produces a list of answer hypotheses, one for each noun phrase (NP) in the extracts in the left-to-right order in which they appeared. The NP-based QA System. Our implementation of the NP-based QA system uses the Empire noun phrase finder, which is described in detail in Cardie and Pierce (1998). Empire identifies base NPs \u2014 non-recursive noun phrases \u2014 using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar. The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal (Marcus et al., 1993). In the experiments below, the NP filter follows the application of the document retrieval and text summarization components. Pronoun answer hypotheses are discarded, and the NPs are assembled into 50-byte chunks. Evaluation. Results for the NP-based QA system are shown in the third r","@endWordPosition":"2784","@position":"17427","annotationId":"T1","@startWordPosition":"2781","@citStr":"Cardie and Pierce (1998)"}},"title":{"#tail":"\n","#text":"Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and COLING-98,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"C Cardie"},{"#tail":"\n","#text":"D Pierce"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"University of Bergen,"},"rawString":{"#tail":"\n","#text":"I. Mani, T. Firmin, D. House, G. Klein, B. Sundheim, and L. Hirschman. 1999. The TIPSTER SUMMAC Text Summarization Evaluation. In Ninth Annual Meeting of the EACL, University of Bergen, Bergen, Norway."},"#text":"\n","marker":{"#tail":"\n","#text":"Mani, Firmin, House, Klein, Sundheim, Hirschman, 1999"},"location":{"#tail":"\n","#text":"Bergen,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"y 19 questions were not answered in the top 20 documents. 4 Query-Dependent Text Summarization for Question Answering We next hypothesize that query-dependent text summarization algorithms will improve the perfor- \u2022 mance of the QA system by focusing the system on the most relevant portions of the retrieved documents. The goal for query-dependent summarization algorithms is to provide a short summary of a document with respect to a specific query. Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings (Mani et al., 1999), we again propose the use of vector space methods from IR, which can be easily extended to the summarization task (Salton et al., 1994): 1. Given a question and a document, divide the document into chunks (e.g. sentences, paragraphs, 200-word passages). 2. Generate the vector representation for the question and for each document chunk. 3. Use the cosine similarity measure to determine the similarity of each chunk to the question. 4. Return as the query-dependent summary the most similar chunks up to a predetermined summary length (e.g. 10% or 20% of the original document). This approach to te","@endWordPosition":"1947","@position":"12319","annotationId":"T2","@startWordPosition":"1944","@citStr":"Mani et al., 1999"}},"title":{"#tail":"\n","#text":"The TIPSTER SUMMAC Text Summarization Evaluation."},"booktitle":{"#tail":"\n","#text":"In Ninth Annual Meeting of the EACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"I Mani"},{"#tail":"\n","#text":"T Firmin"},{"#tail":"\n","#text":"D House"},{"#tail":"\n","#text":"G Klein"},{"#tail":"\n","#text":"B Sundheim"},{"#tail":"\n","#text":"L Hirschman"}]}}]}}}}
