{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.9899855","#text":"\n1. Find a set of &quot;links&quot; among word tokens in the\nbitext, using the likelihood ratios and the com-\npetitive linking algorithm.\n2. Use the links to re-estimate A+, A-, and the\nlikelihood ratios.\n3. Repeat from Step 1 until the model converges\n"},{"#tail":"\n","@confidence":"0.499464166666667","#text":"\n2We could just as easily use other symmetric &quot;asso-\nciation&quot; measures, uch as ?2 (Gale & Church, 1991) or\nthe Dice coefficient (Smadja, 1992).\n? ? ? Uk . 1 tJk ~ = Uk+l ? ? ?\nt\n? , ? Vk . 1 Vk Vk+l ? . .\n"},{"#tail":"\n","@confidence":"0.98236025","#text":"\n2. Sort all remaining likelihood estimates L(u, v)\nfrom highest to lowest.\n3. Find u and v such that the likelihood ratio\nL(u,v) is highest. Token pairs of these types\n"},{"#tail":"\n","@confidence":"0.820274166666667","#text":"\n4. Link all token pairs (u, v) in the bitext.\n5. The one-to-one assumption means that linked\nwords cannot be linked again. Therefore, re-\nmove all linked word tokens from their respec-\ntive texts.\n6. If there is another co-occurring word token pair\n"},{"#tail":"\n","@confidence":"0.850244","#text":"\nber k(u,v ) of links connecting word types u and v\nhas a binomial distribution with parameters n(u.,,l\nand P(u.,,)- If u and v are mutual translations, then\nP(u,,,) tends to a relatively high probability, which we\nwill call A +. If u and v are not mutual translations,\nthen P(u,v) tends to a very low probability, which\n"},{"#tail":"\n","@confidence":"0.69363","#text":"\n(4) and rearranging the terms, we get:\n"}],"figure":[{"#tail":"\n","@confidence":"0.983177","#text":"\nc\n-1.2\n-1.4\nE -1.6\n&quot;~ -1.8 )\n0\n"},{"#tail":"\n","@confidence":"0.517876166666667","#text":"\nB(ku,vln<,,,,, ),+)\nL(u,v) = B(ku,vln~,v, A_ ) . (61\n4 C lass -Based Word- to -Word\nMode ls\nIn the basic word-to-word model, the hidden param-\neters A + and A- depend only on the distributions of\n"},{"#tail":"\n","@confidence":"0.9197195","#text":"\n100\n98\n96\n~) 94\nu 92 t_\n9O\n88\n86\n84\n(99.2%) ~\n(9~ .6%) t'&quot;,,,\n&quot;&quot;-,}(89.2%)\n. . . . . . . . &quot; . . . . . . . . . . . . . . . . . . . . . . x\nincomplete =incorrect ......... -~(86.8%)\n3'6 4'6 9'0\n% recall\n"},{"#tail":"\n","@confidence":"0.828372","#text":"\nIBM Model 2 by our model\nwrong link\nmissing link\npartial link\nclass conflict\ntokenization\nparaphrase\n32\n12\n7\n3\n39\n7\n36\n10\n5\n2\n"},{"#tail":"\n","@confidence":"0.9551152","#text":"\ndes screaming\nvents . winds\n,A ,\ndechames ---&quot; and\net dangerous\nune ~ sea\nmer . conditions\ns\np\nde'montee.-&quot;&quot;\n"}],"equation":[{"#tail":"\n","@confidence":"0.99502775","#text":"\nn(u,v)\nN\nk(u.v)\nK\nT\nk+\nk-\nB(k{n,p)\n"},{"#tail":"\n","@confidence":"0.917595666666667","#text":"\n= ~&quot;\\].(u,v) k(u.,,) = total number of links in the bitext\n= Pr( mutual translations I co-occurrence )\n= Pr( link I co-occurrence )\n= Pr( link \\[ co-occurrence of mutual translations )\n= Pr( link I co-occurrence of not mutual translations )\n= Pr (k in ,p) , where k has a binomial distribution with parameters n and p\n"},{"#tail":"\n","@confidence":"0.78849","#text":"\nI0(0)\n,oo\nLI,. {0\n} 0 ~\n(u V)/n(u v) o~ ,\n"},{"#tail":"\n","@confidence":"0.999352333333333","#text":"\nPr(linkslm?del) = H Vr(k(u,v)\\[n(u,v), A +, A-).\nR~V\n(1)\n"},{"#tail":"\n","@confidence":"0.771690333333333","#text":"\nPr(k(u,v) ln(u,v), A +, A-) = (2)\n= rB(k(u,v)ln(u,v), A +)\n? (1 - r )B(k (u ,v ) ln (u ,v ) ,A - )\n"},{"#tail":"\n","@confidence":"0.855029","#text":"\nA = rA + + (1 - r)A-. (3)\n"},{"#tail":"\n","@confidence":"0.914257","#text":"\n~(u,v) k(u,v/, N = ~(~,v) n(u,v). By definition,\nA = KIN. (4)\n"},{"#tail":"\n","@confidence":"0.698507666666667","#text":"\nKIN - ,X-\n- (5 )\nA+ _ )~-\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.945299","#text":"\n3.1 Compet i t i ve L inking A lgor i thm\n"},{"#tail":"\n","@confidence":"0.994042","#text":"\n3.2 Parameter Es t imat ion\n"},{"#tail":"\n","@confidence":"0.975911","#text":"\n5.1 L ink Types\n"},{"#tail":"\n","@confidence":"0.969377","#text":"\n5.2 Link Tokens\n"}],"footnote":{"#tail":"\n","@confidence":"0.9579555","#text":"\n5The exact number depends on the tokenization\nmethod.\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.999265217391304","#text":"\nP. F. Brown, J. Cocke, S. Della Pietra, V. Della\nPietra, F. Jelinek, R. Mercer, & P. Roossin, &quot;A\nStatistical Approach to Language Translation,&quot;\nProceedings of the 12th International Conference\non Computational Linguistics, Budapest, Hun-\ngary, 1988.\nP. F. Brown, J. Cocke, S. Della Pietra, V. Della\nPietra, F. Jelinek, R. Mercer, & P. Roossin,\n&quot;A Statistical Approach to Machine Translation,&quot;\nComputational Linguistics 16(2), 1990.\nP. F. Brown, V. J. Della Pietra, S. A. Della Pietra\n& R. L. Mercer, &quot;The Mathematics of Statisti-\ncal Machine Translation: Parameter Estimation,&quot;\nComputational Linguistics 19(2), 1993.\nP. F. Brown, S. A. Della Pietra, V. J. Della Pietra,\nM. J. Goldsmith, J. Hajic, R. L. Mercer & S. Mo-\nhanty, &quot;But Dictionaries are Data Too,&quot; Proceed-\nings of the ARPA HLT Workshop, Princeton, N J,\n1993.\nR. Catizone, G. Russell & S. Warwick &quot;Deriving\nTranslation Data from Bilingual Texts,&quot; Proceed-\nings of the First International Lexical Acquisition\nWorkshop, Detroit, MI, 1993.\n"},{"#tail":"\n","@confidence":"0.999713063829787","#text":"\nS. Chen, Building Probabilistic Models for Natu-\nral Language, Ph.D. Thesis, Harvard University,\n1996.\nK. W. Church & E. H. Hovy, &quot;Good Applications for\nCrummy Machine Translation,&quot; Machine Transla-\ntion 8, 1993.\nI. Dagan, K. Church, & W. Gale, &quot;Robust Word\nAlignment for Machine Aided Translation,&quot; Pro-\nceedings of the Workshop on Very Large Corpora:\nAcademic and Industrial Perspectives, Columbus,\nOH, 1993.\nA. P. Dempster, N. M. Laird & D. B. Rubin, &quot;Maxi-\nmum likelihood from incomplete data via the EM\nalgorithm,&quot; Journal of the Royal Statistical Soci-\nety 34(B), 1977.\nT. Dunning, &quot;Accurate Methods for the Statistics\nof Surprise and Coincidence,&quot; Computational Lin-\nguistics 19(1), 1993.\nP. Fung, &quot;Compiling Bilingual Lexicon Entries from\na Non-Parallel English-Chinese Corpus,&quot; Proceed-\nings of the Third Workshop on Very Large Cor-\npora, Boston, MA, 1995a.\nP. Fung, &quot;A Pattern Matching Method for Find-\ning Noun and Proper Noun Translations from\nNoisy Parallel Corpora,&quot; Proceedings of the 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics, Boston, MA, 1995b.\nW. Gale & K. W. Church, &quot;A Program for Align-\ning Sentences in Bilingual Corpora&quot; Proceedings\nof the 29th Annual Meeting of the Association for\nComputational Linguistics, Berkeley, CA, 1991.\nW. Gale & K. W. Church, &quot;Identifying Word Corre-\nspondences in Parallel Texts,&quot; Proceedings of the\nDARPA SNL Workshop, 1991.\nA. Kumano & H. Hirakawa, &quot;Building an MT Dic-\ntionary from Parallel Texts Based on Linguistic\nand Statistical Information,&quot; Proceedings of the\n15th International Conference on Computational\nLinguistics, Kyoto, Japan, 1994.\nE. Macklovitch :'Using Bi-textual Alignment for\nTranslation Validation: The TransCheck Sys-\ntem,&quot; Proceedings of the 1st Conference of the As-\nsociation for Machine Translation in the Ameri-\ncas, Columbia, MD, 1994.\nE. Macklovitch & M.-L. Hannan, &quot;Line 'Em Up: Ad-\nvances in Alignment Technology and their Impact\non Translation Support Tools,&quot; 2nd Conference\nof the Association for Machine Translation in the\nAmericas, Montreal, Canada, 1996.\nI. D. Melamed &quot;Automatic Evaluation and Uniform\nFilter Cascades for Inducing N-best Translation\nLexicons,&quot; Proceedings of the Third Workshop on\nVery Large Corpora, Boston, MA, 1995.\nI. D. Melamed, &quot;A Geometric Approach to Mapping\nBitext Correspondence,&quot; Proceedings of the First\nConference on Empirical Methods in Natural Lan-\nguage Processing, Philadelphia, PA, 1996a.\nI. D. Melamed &quot;Automatic Detection of Omissions\nin Translations,&quot; Proceedings of the 16th Interna-\ntional Conference on Computational Linguistics,\nCopenhagen, Denmark, 1996b.\nI. D Melamed, &quot;Automatic Construction of Clean\nBroad-Coverage Translation Lexicons,&quot; 2nd Con-\nference of the Association for Machine Transla-\ntion in the Americas, Montreal, Canada, 1996c.\nI. D. Melamed, &quot;Measuring Semantic Entropy,&quot; Pro-\nceedings of the SIGLEX Workshop on Tagging\nText with Lexical Semantics, Washington, DC,\n1997.\nI. D. Melamed, &quot;A Portable Algorithm for Mapping\nBitext Correspondence,&quot; Proceedings of the 35th\nConference of the Association for Computational\nLinguistics, Madrid, Spain, 1997. (in this volume)\nA. Melby, &quot;A Bilingual Concordance System and its\nUse in Linguistic Studies,&quot; Proceedings of the En-\nglish LACUS Forum, Columbia, SC, 1981.\nA. Nasr, personal communication, 1997.\nP. Resnik & I. D. Melamed, &quot;Semi-Automatic A qui-\nsition of Domain-Specific Translation Lexicons,&quot;\nProceedings of the 7th ACL Conference on Ap-\nplied Natural Language Processing, Washington,\nDC, 1997.\nD. W. Oard & B. J. Dorr, &quot;A Survey of Multilingual\nText Retrieval, UMIACS TR-96-19, University of\nMaryland, College Park, MD, 1996.\nF. Smadja, &quot;How to Compile a Bilingual Collo-\ncational Lexicon Automatically,&quot; Proceedings of\nthe AAAI Workshop on Statistically-Based NLP\nTechniques, 1992.\nD. Wu & X. Xia, &quot;Learning an English-Chinese\nLexicon from a Parallel Corpus,&quot; Proceedings of\nthe First Conference of the Association for Ma-\nchine Translation in the Americas, Columbia,\nMD, 1994.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.967454970588235","#text":"\nA Word-to-Word Model of Translational Equivalence\nI. Dan Me lamed\nDept . of Computer and Information Science\nUnivers i ty of Pennsy lvan ia\nPh i lade lph ia , PA, 19104, U.S.A.\nraelamed~unagi, c is. upenn, edu\nAbst ract\nMany multilingual NLP applications need\nto translate words between different lan-\nguages, but cannot afford the computa-\ntional expense of inducing or applying a full\ntranslation model. For these applications,\nwe have designed a fast algorithm for esti-\nmating a partial translation model, which\naccounts for translational equivalence only\nat the word level . The model's preci-\nsion/recall trade-off can be directly con-\ntrolled via one threshold parameter. This\nfeature makes the model more suitable for\napplications that are not fully statistical.\nThe model's hidden parameters can be eas-\nily conditioned on information extrinsic to\nthe model, providing an easy way to inte-\ngrate pre-existing knowledge such as part-\nof-speech, dictionaries, word order, etc..\nOur model can link word tokens in paral-\nlel texts as well as other translation mod-\nels in the literature. Unlike other transla-\ntion models, it can automatically produce\ndictionary-sized translation lexicons, and it\ncan do so with over 99% accuracy.\n1 In t roduct ion\nOver the past decade, researchers at IBM have devel-\noped a series of increasingly sophisticated statistical\nmodels for machine translation (Brown et al, 1988;\nBrown et al, 1990; Brown et al, 1993a). However,\nthe IBM models, which attempt o capture a broad\nrange of translation phenomena, are computation-\nally expensive to apply. Table look-up using an ex-\nplicit translation lexicon is sufficient and preferable\nfor many multilingual NLP applications, including\n&quot;crummy&quot; MT on the World Wide Web (Church\n& I-Iovy, 1993), certain machine-assisted translation\ntools (e.g. (Macklovitch, 1994; Melamed, 1996b)),\nconcordancing for bilingual lexicography (Catizone\net al, 1993; Gale & Church, 1991), computer-\nassisted language learning, corpus linguistics (Melby.\n1981), and cross-lingual information retrieval (Oard\n&Dorr, 1996).\nIn this paper, we present a fast method for in-\nducing accurate translation lexicons. The method\nassumes that words are translated one-to-one. This\nassumption reduces the explanatory power of our\nmodel in comparison to the IBM models, but, as\nshown in Section 3.1, it helps us to avoid what we\ncall indirect associations, a major source of errors in\nother models. Section 3.1 also shows how the one-\nto-one assumption enables us to use a new greedy\ncompetitive linking algorithm for re-estimating the\nmodel's parameters, instead of more expensive algo-\nrithms that consider a much larger set of word cor-\nrespondence possibilities. The model uses two hid-\nden parameters to estimate the confidence of its own\npredictions. The confidence stimates enable direct\ncontrol of the balance between the model's preci-\nsion and recall via a simple threshold. The hidden\nparameters can be conditioned on prior knowledge\nabout the bitext to improve the model's accuracy.\n"},{"#tail":"\n","@confidence":"0.955555285714286","#text":"\nWith the exception of (Fung, 1998b), previous\nmethods for automatically constructing statistical\ntranslation models begin by looking at word co-\noccurrence frequencies in bitexts (Gale & Church,\n1991; Kumano & Hirakawa, 1994; Fung, 1998a;\nMelamed, 1995). A bitext comprises a pair of texts\nin two languages, where each text is a translation\nof the other. Word co-occurrence an be defined in\nvarious ways. The most common way is to divide\neach half of the bitext into an equal number of seg-\nments and to align the segments so that each pair of\nsegments Si and Ti are translations of each other\n(Gale & Church, 1991; Melamed, 1996a). Then,\ntwo word tokens (u, v) are said to co-occur in the\n"},{"#tail":"\n","@confidence":"0.98277875","#text":"\naligned segment pair i if u E Si and v E Ti. The\nco-occurrence r lation can also be based on distance\nin a bitext space, which is a more general represen-\ntations of bitext correspondence (Dagan et al, 1993;\nResnik & Melamed, 1997), or it can be restricted to\nwords pairs that satisfy some matching predicate,\nwhich can be extrinsic to the model (Melamed, 1995;\nMelamed, 1997).\n"},{"#tail":"\n","@confidence":"0.972544285714285","#text":"\nOur translation model consists of the hidden param-\neters A + and A-, and likelihood ratios L(u, v). The\ntwo hidden parameters are the probabilities of the\nmodel generating true and false positives in the data.\nL(u,v) represents the likelihood that u and v can\nbe mutual translations. For each co-occurring pair of\nword types u and v, these likelihoods are initially set\nproportional to their co-occurrence frequency n(u,v)\nand inversely proportional to their marginal frequen-\ncies n(u) and n(v) z, following (Dunning, 1993) 2.\nWhen the L(u, v) are re-estimated, the model's hid-\nden parameters come into play.\nAfter initialization, the model induction algorithm\niterates:\n"},{"#tail":"\n","@confidence":"0.98920775","#text":"\nto the desired degree.\nThe competitive linking algorithm and its one-to-one\nassumption are detailed in Section 3.1. Section 3.1\nexplains how to re-estimate the model parameters.\n"},{"#tail":"\n","@confidence":"0.982430153846154","#text":"\nThe competitive linking algorithm is designed to\novercome the problem of indirect associations, illus-\ntrated in Figure 1. The sequences of u's and v's\nrepresent corresponding regions of a bitext. If uk\nand vk co-occur much more often than expected by\nchance, then any reasonable model will deem them\nlikely to be mutual translations. If uk and Vk are\nindeed mutual translations, then their tendency to\nZThe co-occurrence frequency of a word type pair is\nsimply the number of times the pair co-occurs in the\ncorpus. However, n(u) = ~-~v n(u.v), which is not the\nsame as the frequency of u, because ach token of u can\nco-occur with several differentv's.\n"},{"#tail":"\n","@confidence":"0.9648456","#text":"\nuk+z. The direct association between uk and vk, and\nthe direct association between uk and Uk+l give rise\nto an indirect association between v~ and uk+l.\nco-occur is called a direct associat ion. Now, sup-\npose that uk and Uk+z often co-occur within their\nlanguage. Then vk and uk+l will also co-occur more\noften than expected by chance. The arrow connect-\ning vk and u~+l in Figure 1 represents an indi rect\nassociat ion, since the association between vk and\nUk+z arises only by virtue of the association between\neach of them and uk. Models of translational equiv-\nalence that are ignorant of indirect associations have\n&quot;a tendency ... to be confused by collocates&quot; (Dagan\net al, 1993).\nFortunately, indirect associations are usually not\ndifficult to identify, because they tend to be weaker\nthan the direct associations on which they are based\n(Melamed, 1996c). The majority of indirect associ-\nations can be filtered out by a simple competition\nheuristic: Whenever several word tokens ui in one\nhalf of the bitext co-occur with a particular word to-\nken v in the other half of the bitext, the word that is\nmost likely to be v's translation is the one for which\nthe likelihood L(u, v) of translational equivalence is\nhighest. The competitive linking algorithm imple-\nments this heuristic:\n1. Discard all likelihood scores for word types\ndeemed unlikely to be mutual translations, i.e.\nall L(u,v) < 1. This step significantly reduces\nthe computational burden of the algorithm. It\nis analogous to the step in other translation\nmodel induction algorithms that sets all prob-\nabilities below a certain threshold to negligible\nvalues (Brown et al, 1990; Dagan et al, 1993;\nChen, 1996). To retain word type pairs that\nare at least twice as likely to be mutual transla-\ntions than not, the threshold can be raised to 2.\nConversely, the threshold can be lowered to buy\nmore coverage at the cost of a larger model that\nwill converge more slowly.\n"},{"#tail":"\n","@confidence":"0.686209333333333","#text":"\n= frequency of co-occurrence between word types u and v\n= ~&quot;\\].(u.,,) n(u.v) = total number of co-occurrences in the bitext\n= frequency of links between word types u and v\n"},{"#tail":"\n","@confidence":"0.676309","#text":"\nwould be the winners in any competitions in-\nvolving u or v.\n"},{"#tail":"\n","@confidence":"0.9329366875","#text":"\n(u, v) such that L(u, v) exists, then repeat from\nStep 3.\nThe competitive linking algorithm is more greedy\nthan algorithms that try to find a set of link types\nthat are jointly most probable over some segment of\nthe bitext. In practice, our linking algorithm can be\nimplemented so that its worst-case running time is\nO(lm), where l and m are the lengths of the aligned\nsegments.\nThe simplicity of the competitive linking algo-\nrithm depends on the one- to -one assumpt ion :\nEach word translates to at most one other word.\nCertainly, there are cases where this assumption is\nfalse. We prefer not to model those cases, in order to\nachieve higher accuracy with less effort on the cases\nwhere the assumption is true.\n"},{"#tail":"\n","@confidence":"0.987904105263158","#text":"\nThe purpose of the competitive linking algorithm is\nto help us re-estimate the model parameters. The\nvariables that we use in our estimation are summa-\nrized in Figure 2. The linking algorithm produces a\nset of links between word tokens in the bitext. We\ndefine a l ink token to be an ordered pair of word\ntokens, one from each half of the bitext. A l ink\ntype is an ordered pair of word types. Let n(u.,,) be\nthe co-occurrence frequency of u and v and k(~,,,) be\nthe number of links between tokens of u and v 3. An\n3Note that k(u,v) depends on the linking algorithm,\nbut n(u.v) is a constant property of the bitext.\nimportant property of the competitive linking algo-\nrithm is that the ratio kiu.,,)/n(u,v ) tends to be very\nhigh if u and v are mutual translations, and quite\nlow if they are not. The bimodality of this ratio\nfor several values of n(u.,,i is illustrated in Figure 3.\nThis figure was plotted after the model's first iter-\nation over 300000 aligned sentence pairs from the\n"},{"#tail":"\n","@confidence":"0.987103833333333","#text":"\nplotted on a log scale -- the bimodality is quite sharp.\nCanad ian Hansard bitext. Note that the frequencies\nare plotted on a log scale -- the b imodal i ty is quite\nsharp.\nThe linking algorithm creates all the links of a\ngiven type independently of each other, so the num-\n"},{"#tail":"\n","@confidence":"0.9987052","#text":"\nwe will call A-. A + and A- correspond to the two\npeaks in the frequency distribution of k(u.,,)/niu.v~\nin Figure 2. The two parameters can also be inter-\npreted as the percentage of true and false positives.\nIf the translation in the bitext is consistent and the\n"},{"#tail":"\n","@confidence":"0.965118888888889","#text":"\nmodel is accurate, then A + should be near 1 and A-\nshould be near 0.\nTo find the most probable values of the hidden\nmodel parameters A + and A-, we adopt he standard\nmethod of maximum likelihood estimation, and find\nthe values that maximize the probability of the link\nfrequency distributions. The one-to-one assumption\nimplies independence between different link types,\nso that\n"},{"#tail":"\n","@confidence":"0.9898886","#text":"\nThe factors on the right-hand side of Equation 1 can\nbe written explicitly with the help of a mixture co-\nefficient. Let r be the probability that an arbitrary\nco-occurring pair of word types are mutual transla-\ntions. Let B(kln,p ) denote the probability that k\nlinks are observed out of n co-occurrences, where k\nhas a binomial distribution with parameters n and p.\nThen the probability that u and v are linked k(u,v)\ntimes out of n(u,v) co-occurrences is a mixture of two\nbinomials:\n"},{"#tail":"\n","@confidence":"0.88354125","#text":"\nOne more variable allows us to express r in terms\nof A + and A- : Let A be the probability that an arbi-\ntrary co-occuring pair of word tokens will be linked,\nregardless of whether they are mutual translations.\nSince r is constant over all word types, it also repre-\nsents the probability that an arbitrary co-occurring\npair of word tokens are mutual translations. There-\nfore,\n"},{"#tail":"\n","@confidence":"0.951304666666667","#text":"\nA can also be estimated empirically. Let K be the\ntotal number of links in the bitext and let N be the\ntotal number of co-occuring word token pairs: K =\n"},{"#tail":"\n","@confidence":"0.752574","#text":"\nEquating the right-hand sides of Equations (3) and\n"},{"#tail":"\n","@confidence":"0.833604","#text":"\nSince r is now a function of A + and A-, only the\nlatter two variables represent degrees of freedom in\nthe model.\nThe probability function expressed by Equations 1\nand 2 has many local maxima. In practice, these\n"},{"#tail":"\n","@confidence":"0.99488475","#text":"\nimum in the region of interest.\nlocal maxima are like pebbles on a mountain, in-\nvisible at low resolution. We computed Equation 1\nover various combinations of A + and A- after the\nmodel's first iteration over 300000 aligned sentence\npairs from the Canadian Hansard bitext. Figure 4\nshows that the region of interest in the parameter\nspace, where 1 > A + > A > A- > 0, has only one\nclearly visible global maximum. This global maxi-\nmum can be found by standard hill-climbing meth-\nods, as long as the step size is large enough to avoid\ngetting stuck on the pebbles.\nGiven estimates for A + and A-, we can compute\nB(ku,,,\\[nu,v, A +) and B(ku,v\\[nu,v, A-). These are\nprobabilities that k(u,v) links were generated by an\nalgorithm that generates correct links and by an al-\ngorithm that generates incorrect links, respectively,\nout ofn(u,v) co-occurrences. The ratio of these prob-\nabilities is the likelihood ratio in favor of u and v\nbeing mutual translations, for all u and v:\n"},{"#tail":"\n","@confidence":"0.977225785714286","#text":"\nlink frequencies generated by the competitive link-\ning algorithm. More accurate models can be induced\nby taking into account various features of the linked\ntokens. For example, frequent words are translated\nless consistently than rare words (Melamed, 1997).\nTo account for this difference, we can estimate sep-\narate values of X + and A- for different ranges of\nn(u,v). Similarly, the hidden parameters can be con-\nditioned on the linked parts of speech. Word order\ncan be taken into account by conditioning the hid-\nden parameters on the relative positions of linked\nword tokens in their respective sentences. Just as\neasily, we can model links that coincide with en-\ntries in a pre-existing translation lexicon separately\n"},{"#tail":"\n","@confidence":"0.999183428571429","#text":"\nfrom those that do not. This method of incorporat-\ning dictionary information seems simpler than the\nmethod proposed by Brown et ai. for their models\n(Brown et al, 1993b). When the hidden parameters\nare conditioned on different link classes, the estima-\ntion method does not change; it is just repeated for\neach link class.\n"},{"#tail":"\n","@confidence":"0.995774857142857","#text":"\nA word-to-word model of translational equivalence\ncan be evaluated either over types or over tokens.\nIt is impossible to replicate the experiments used to\nevaluate other translation models in the literature,\nbecause neither the models nor the programs that\ninduce them are generally available. For each kind\nof evaluation, we have found one case where we can\ncome close.\nWe induced a two-class word-to-word model of\ntranslational equivalence from 13 million words of\nthe Canadian Hansards, aligned using the method\nin (Gale & Church, 1991). One class repre-\nsented content-word links and the other represented\nfunction-word links 4. Link types with negative\nlog-likelihood were discarded after each iteration.\nBoth classes' parameters converged after six it-\nerations. The value of class-based models was\ndemonstrated by the differences between the hid-\nden parameters for the two classes. (A +,A-) con-\nverged at (.78,00016) for content-class links and at\n(.43,.000094) for function-class links.\n"},{"#tail":"\n","@confidence":"0.99489525","#text":"\nThe most direct way to evaluate the link types in\na word-level model of translational equivalence is to\ntreat each link type as a candidate translation lexi-\ncon entry, and to measure precision and recall. This\nevaluation criterion carries much practical import,\nbecause many of the applications mentioned in Sec-\ntion 1 depend on accurate broad-coverage transla-\ntion lexicons. Machine readable bilingual dictionar-\nies, even when they are available, have only limited\ncoverage and rarely include domain-specific terms\n(Resnik & Melamed, 1997).\nWe define the recall of a word-to-word translation\nmodel as the fraction of the bitext vocabulary repre-\nsented in the model. Translation model precision is\na more thorny issue, because people disagree about\nthe degree to which context should play a role in\njudgements of translational equivalence. We hand-\nevaluated the precision of the link types in our model\nin the context of the bitext from which the model\n4Since function words can be identified by table look-\nup, no POS-tagger was involved.\nwas induced, using a simple bilingual concordancer.\nA link type (u, v) was considered correct if u and v\never co-occurred as direct translations of each other.\nWhere the one-to-one assumption failed, but a link\ntype captured part of a correct translation, it was\njudged &quot;incomplete.&quot; Whether incomplete links are\ncorrect or incorrect depends on the application.\n"},{"#tail":"\n","@confidence":"0.957126483870968","#text":"\nintervals at varying levels of recall.\nWe evaluated five random samples of 100 link\ntypes each at three levels of recall. For our bitext,\nrecall of 36%, 46% and 90% corresponded to trans-\nlation lexicons containing 32274, 43075 and 88633\nwords, respectively. Figure 5 shows the precision of\nthe model with 95% confidence intervals. The upper\ncurve represents precision when incomplete links are\nconsidered correct, and the lower when they are con-\nsidered incorrect. On the former metric, our model\ncan generate translation lexicons with precision and\nrecall both exceeding 90%, as well as dictionary-\nsized translation lexicons that are over 99% correct.\nThough some have tried, it is not clear how to\nextract such accurate lexicons from other published\ntranslation models. Part of the difficulty stems from\nthe implicit assumption in other models that each\nword has only one sense. Each word is assigned the\nsame unit of probability mass, which the model dis-\ntributes over all candidate translations. The correct\ntranslations of a word that has several correct rans-\nlations will be assigned a lower probability than the\ncorrect translation of a word that has only one cor-\nrect translation. This imbalance foils thresholding\nstrategies, clever as they might be (Gale & Church,\n1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods\nin the word-to-word model remain unnormalized, so\nthey do not compete.\nThe word-to-word model maintains high preci-\nsion even given much less training data. Resnik\n& Melamed (1997) report that the model produced\n"},{"#tail":"\n","@confidence":"0.999803916666667","#text":"\ntranslation lexicons with 94% precision and 30% re-\ncall, when trained on French/English software man-\nuals totaling about 400,000 words. The model\nwas also used to induce a translation lexicon from\na 6200-word corpus of French/English weather e-\nports. Nasr (1997) reported that the translation\nlexicon that our model induced from this tiny bitext\naccounted for 30% of the word types with precision\nbetween 84% and 90%. Recall drops when there is\ntess training data, because the model refuses to make\npredictions that it cannot make with confidence. For\nmany applications, this is the desired behavior.\n"},{"#tail":"\n","@confidence":"0.893531","#text":"\ntype of error errors made by errors made\n"},{"#tail":"\n","@confidence":"0.997828396226415","#text":"\nThe most detailed evaluation of link tokens to\ndate was performed by (Macklovitch & Hannan,\n1996), who trained Brown et al's Model 2 on 74\nmillion words of the Canadian Hansards. These au-\nthors kindly provided us with the links generated\nby that model in 51 aligned sentences from a held-\nout test set. We generated links in the same 51\nsentences using our two-class word-to-word model,\nand manually evaluated the content-word links from\nboth models. The IBM models are directional; i.e.\nthey posit the English words that gave rise to each\nFrench word, but ignore the distribution of the En-\nglish words. Therefore, we ignored English words\nthat were linked to nothing.\nThe errors are classified in Table 1. The &quot;wrong\nlink&quot; and &quot;missing link&quot; error categories should be\nself-explanatory. &quot;Partial inks&quot; are those where one\nFrench word resulted from multiple English words,\nbut the model only links the French word to one of\nits English sources. &quot;Class conflict&quot; errors resulted\nfrom our model's refusal to link content words with\nfunction words. Usually, this is the desired behavior,\nbut words like English auxiliary verbs are sometimes\nused as content words, giving rise to content words\nin French. Such errors could be overcome by a model\nthat classifies each word token, for example using a\npart-of-speech tagger, instead of assigning the same\nclass to all tokens of a given type. The bitext pre-\nprocessor for our word-to-word model split hyphen-\nated words, but Macklovitch &Hannan's preproces-\nsor did not. In some cases, hyphenated words were\neasier to link correctly; in other cases they were more\ndifficult. Both models made some errors because of\nthis tokenization problem, albeit in different places.\nThe &quot;paraphrase&quot; category covers all link errors that\nresulted from paraphrases in the translation. Nei-\nther IBM's Model 2 nor our model is capable of link-\ning multi-word sequences to multi-word sequences,\nand this was the biggest source of error for both\nmodels.\nThe test sample contained only about 400 content\nwords 5, and the links for both models were evaluated\npost-hoc by only one evaluator. Nevertheless, it ap-\npears that our word-to-word model with only two\nlink classes does not perform any worse than IBM's\nModel 2, even though the word-to-word model was\ntrained on less than one fifth the amount of data that\nwas used to train the IBM model. Since it doesn't\nstore indirect associations, our word-to-word model\ncontained an average of 4.5 French words for every\nEnglish word. Such a compact model requires rel-\natively little computational effort to induce and to\napply.\n"},{"#tail":"\n","@confidence":"0.713647833333333","#text":"\nrors made by the word-to-word model and the IBM\nModel 2. Solid lines are links made by both mod-\nels; dashes lines are links made by the IBM model\nonly. Only content-class links are shown. Neither\nmodel makes the correct links (ddcha?nds,screaming)\nand (ddmontde, dangerous).\n"},{"#tail":"\n","@confidence":"0.9989529","#text":"\nIn addition to the quantitative differences between\nthe word-to-word model and the IBM model, there\nis an important qualitative difference, illustrated in\nFigure 6. As shown in Table 1, the most common\nkind of error for the word-to-word model was a miss-\ning link, whereas the most common error for IBM's\nModel 2 was a wrong link. Missing links are more in-\nformative: they indicate where the model has failed.\nThe level at which the model trusts its own judge-\nment can be varied directly by changing the likeli-\nhood cutoff in Step 1 of the competitive linking algo-\nrithm. Each application of the word-to-word model\ncan choose its own balance between link token pre-\ncision and recall. An application that calls on the\nword-to-word model to link words in a bitext could\ntreat unlinked words differently from linked words,\nand avoid basing subsequent decisions on uncertain\ninputs. It is not clear how the precision/recall trade-\noff can be controlled in the IBM models.\nOne advantage that Brown et al's Model i has\nover our word-to-word model is that their objec-\ntive function has no local maxima. By using the\nEM algorithm (Dempster et al, 1977), they can\nguarantee convergence towards the globally opti-\nmum parameter set. In contrast, the dynamic na-\nture of the competitive linking algorithm changes\nthe Pr(datalmodel ) in a non-monotonic fashion. We\nhave adopted the simple heuristic that the model\n&quot;has converged&quot; when this probability stops increas-\ning.\n"},{"#tail":"\n","@confidence":"0.999695823529412","#text":"\nMany multilingual NLP applications need to trans-\nlate words between different languages, but cannot\nafford the computational expense of modeling the\nfull range of translation phenomena. For these ap-\nplications, we have designed afast algorithm for esti-\nmating word-to-word models of translational equiv-\nalence. The estimation method uses a pair of hid-\nden parameters to measure the model's uncertainty,\nand avoids making decisions that it's not likely to\nmake correctly. The hidden parameters can be con-\nditioned on information extrinsic to the model, pro-\nviding an easy way to integrate pre-existing knowl-\nedge.\nSo far we have only implemented a two-class\nmodel, to exploit the differences in translation con-\nsistency between content words and function words.\nThis relatively simple two-class model linked word\ntokens in parallel texts as accurately as other trans-\nlation models in the literature, despite being trained\non only one fifth as much data. Unlike other transla-\ntion models, the word-to-word model can automat-\nically produce dictionary-sized translation lexicons,\nand it can do so with over 99% accuracy.\nEven better accuracy can be achieved with a more\nfine-grained link class structure. Promising features\nfor classification include part of speech, frequency\nof co-occurrence, relative word position, and trans-\nlational entropy (Melamed, 1997). Another inter-\nesting extension is to broaden the definition of a\n&quot;word&quot; to include multi-word lexical units (Smadja,\n1992). If such units can be identified a priori, their\ntranslations can be estimated without modifying the\nword-to-word model. In this manner, the model can\naccount for a wider range of translation phenomena.\n"},{"#tail":"\n","@confidence":"0.965693545454546","#text":"\nThe French/English software manuals were provided\nby Gary Adams of Sun MicroSystems Laboratories.\nThe weather bitext was prepared at the University\nof Montreal, under the direction Of Richard Kit-\ntredge. Thanks to Alexis Nasr for hand-evaluating\nthe weather translation lexicon. Thanks also to Mike\nCollins, George Foster, Mitch Marcus, Lyle Ungar,\nand three anonymous reviewers for helpful com-\nments. This research was supported by at. equip-\nment grant from Sun MicroSystems and by ARPA\nContract #N66001-94C-6043.\n"}],"#text":"\n","sectionHeader":[{"#tail":"\n","@confidence":"0.996418","@genericHeader":"method","#text":"\n2 Co-occur rence\n"},{"#tail":"\n","@confidence":"0.794005","@genericHeader":"method","#text":"\n3 The Bas ic Word- to -Word Mode l\n"},{"#tail":"\n","@confidence":"0.660107","@genericHeader":"method","#text":"\n5 Eva luat ion\n"},{"#tail":"\n","@confidence":"0.909817","@genericHeader":"method","#text":"\n6 Conc lus ion\n"},{"#tail":"\n","@confidence":"0.943919","@genericHeader":"acknowledgments","#text":"\nAcknowledgements\n"},{"#tail":"\n","@confidence":"0.979111","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":{"#tail":"\n","@confidence":"0.7601235","#text":"\nTable 1: Erroneous link tokens generated by two\ntranslation models.\n"},"page":[{"#tail":"\n","@confidence":"0.991652","#text":"\n490\n"},{"#tail":"\n","@confidence":"0.978806","#text":"\n491\n"},{"#tail":"\n","@confidence":"0.996712","#text":"\n492\n"},{"#tail":"\n","@confidence":"0.995172","#text":"\n493\n"},{"#tail":"\n","@confidence":"0.998355","#text":"\n494\n"},{"#tail":"\n","@confidence":"0.75793","#text":"\n36\n"},{"#tail":"\n","@confidence":"0.999553","#text":"\n495\n"},{"#tail":"\n","@confidence":"0.982592","#text":"\n496\n"},{"#tail":"\n","@confidence":"0.999056","#text":"\n497\n"}],"figureCaption":[{"#tail":"\n","@confidence":"0.982386","#text":"\nFigure 1: Uk and vk often co-occur, as do uk and\n"},{"#tail":"\n","@confidence":"0.7766095","#text":"\nN.B.: k + and )~- need not sum to 1, because they are conditioned on different events.\nFigure 2: Variables used to estimate the model parameters.\n"},{"#tail":"\n","@confidence":"0.8312355","#text":"\nFigure 3: A fragment of the joint frequency\n(k(u.v)/n(u.v), n(u.v)). Note that the frequencies are\n"},{"#tail":"\n","@confidence":"0.99989","#text":"\nFigure 4: Pr(links\\[model) has only one global max-\n"},{"#tail":"\n","@confidence":"0.999611","#text":"\nFigure 5: Link type precision with 95~ confidence\n"},{"#tail":"\n","@confidence":"0.999747","#text":"\nFigure 6: An example of the different sorts of er-\n"}],"table":{"#tail":"\n","@confidence":"0.469131","#text":"\nTOTAL 93 96\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.000001","#tail":"\n","@no":"0","note":[{"#tail":"\n","@confidence":"0.796858","#text":"Statistical Approach to Language Translation,&quot; Proceedings of the 12th International Conference on Computational Linguistics, Budapest, Hungary, 1988."},{"#tail":"\n","@confidence":"0.9373816","#text":"Computational Linguistics 16(2), 1990. P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & R. L. Mercer, &quot;The Mathematics of Statistical Machine Translation: Parameter Estimation,&quot; Computational Linguistics 19(2), 1993."},{"#tail":"\n","@confidence":"0.871046766990291","#text":"hanty, &quot;But Dictionaries are Data Too,&quot; Proceedings of the ARPA HLT Workshop, Princeton, N J, 1993. R. Catizone, G. Russell & S. Warwick &quot;Deriving Translation Data from Bilingual Texts,&quot; Proceedings of the First International Lexical Acquisition Workshop, Detroit, MI, 1993. 496 S. Chen, Building Probabilistic Models for Natural Language, Ph.D. Thesis, Harvard University, 1996. K. W. Church & E. H. Hovy, &quot;Good Applications for Crummy Machine Translation,&quot; Machine Translation 8, 1993. I. Dagan, K. Church, & W. Gale, &quot;Robust Word Alignment for Machine Aided Translation,&quot; Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, Columbus, OH, 1993. A. P. Dempster, N. M. Laird & D. B. Rubin, &quot;Maximum likelihood from incomplete data via the EM algorithm,&quot; Journal of the Royal Statistical Society 34(B), 1977. T. Dunning, &quot;Accurate Methods for the Statistics of Surprise and Coincidence,&quot; Computational Linguistics 19(1), 1993. P. Fung, &quot;Compiling Bilingual Lexicon Entries from a Non-Parallel English-Chinese Corpus,&quot; Proceedings of the Third Workshop on Very Large Corpora, Boston, MA, 1995a. P. Fung, &quot;A Pattern Matching Method for Finding Noun and Proper Noun Translations from Noisy Parallel Corpora,&quot; Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Boston, MA, 1995b. W. Gale & K. W. Church, &quot;A Program for Aligning Sentences in Bilingual Corpora&quot; Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, CA, 1991. W. Gale & K. W. Church, &quot;Identifying Word Correspondences in Parallel Texts,&quot; Proceedings of the DARPA SNL Workshop, 1991. A. Kumano & H. Hirakawa, &quot;Building an MT Dictionary from Parallel Texts Based on Linguistic and Statistical Information,&quot; Proceedings of the 15th International Conference on Computational Linguistics, Kyoto, Japan, 1994. E. Macklovitch :'Using Bi-textual Alignment for Translation Validation: The TransCheck System,&quot; Proceedings of the 1st Conference of the Association for Machine Translation in the Americas, Columbia, MD, 1994. E. Macklovitch & M.-L. Hannan, &quot;Line 'Em Up: Advances in Alignment Technology and their Impact on Translation Support Tools,&quot; 2nd Conference of the Association for Machine Translation in the Americas, Montreal, Canada, 1996. I. D. Melamed &quot;Automatic Evaluation and Uniform Filter Cascades for Inducing N-best Translation Lexicons,&quot; Proceedings of the Third Workshop on Very Large Corpora, Boston, MA, 1995. I. D. Melamed, &quot;A Geometric Approach to Mapping Bitext Correspondence,&quot; Proceedings of the First Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, 1996a. I. D. Melamed &quot;Automatic Detection of Omissions in Translations,&quot; Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, Denmark, 1996b. I. D Melamed, &quot;Automatic Construction of Clean Broad-Coverage Translation Lexicons,&quot; 2nd Conference of the Association for Machine Translation in the Americas, Montreal, Canada, 1996c. I. D. Melamed, &quot;Measuring Semantic Entropy,&quot; Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics, Washington, DC, 1997. I. D. Melamed, &quot;A Portable Algorithm for Mapping Bitext Correspondence,&quot; Proceedings of the 35th Conference of the Association for Computational Linguistics, Madrid, Spain, 1997. (in this volume) A. Melby, &quot;A Bilingual Concordance System and its Use in Linguistic Studies,&quot; Proceedings of the English LACUS Forum, Columbia, SC, 1981. A. Nasr, personal communication, 1997. P. Resnik & I. D. Melamed, &quot;Semi-Automatic A quisition of Domain-Specific Translation Lexicons,&quot; Proceedings of the 7th ACL Conference on Applied Natural Language Processing, Washington, DC, 1997. D. W. Oard & B. J. Dorr, &quot;A Survey of Multilingual Text Retrieval, UMIACS TR-96-19, University of Maryland, College Park, MD, 1996. F. Smadja, &quot;How to Compile a Bilingual Collocational Lexicon Automatically,&quot; Proceedings of the AAAI Workshop on Statistically-Based NLP Techniques, 1992. D. Wu & X. Xia, &quot;Learning an English-Chinese Lexicon from a Parallel Corpus,&quot; Proceedings of the First Conference of the Association for Machine Translation in the Americas, Columbia, MD, 1994. 497"}],"address":{"#tail":"\n","@confidence":"0.937119","#text":"Ph i lade lph ia , PA, 19104, U.S.A."},"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.9800375","#text":"Dept . of Computer and Information Science Univers i ty of Pennsy lvan ia"},{"#tail":"\n","@confidence":"0.433731","#text":"amp;quot;A Statistical Approach to Machine Translation,&quot;"}],"author":[{"#tail":"\n","@confidence":"0.997217","#text":"I Dan Me lamed"},{"#tail":"\n","@confidence":"0.694201","#text":"P F Brown"},{"#tail":"\n","@confidence":"0.694201","#text":"J Cocke"},{"#tail":"\n","@confidence":"0.694201","#text":"S Della Pietra"},{"#tail":"\n","@confidence":"0.694201","#text":"V Della Pietra"},{"#tail":"\n","@confidence":"0.694201","#text":"F Jelinek"},{"#tail":"\n","@confidence":"0.694201","#text":"R Mercer"},{"#tail":"\n","@confidence":"0.694201","#text":"P Roossin"},{"#tail":"\n","@confidence":"0.694201","#text":"A"},{"#tail":"\n","@confidence":"0.88799","#text":"P F Brown"},{"#tail":"\n","@confidence":"0.88799","#text":"J Cocke"},{"#tail":"\n","@confidence":"0.88799","#text":"S Della Pietra"},{"#tail":"\n","@confidence":"0.88799","#text":"V Della Pietra"},{"#tail":"\n","@confidence":"0.88799","#text":"F Jelinek"},{"#tail":"\n","@confidence":"0.88799","#text":"R Mercer"},{"#tail":"\n","@confidence":"0.88799","#text":"P Roossin"},{"#tail":"\n","@confidence":"0.761233","#text":"P F Brown"},{"#tail":"\n","@confidence":"0.761233","#text":"S A Della Pietra"},{"#tail":"\n","@confidence":"0.761233","#text":"V J Della Pietra"},{"#tail":"\n","@confidence":"0.761233","#text":"M J Goldsmith"},{"#tail":"\n","@confidence":"0.761233","#text":"J Hajic"},{"#tail":"\n","@confidence":"0.761233","#text":"R L Mercer"},{"#tail":"\n","@confidence":"0.761233","#text":"S Mo-"}],"abstract":{"#tail":"\n","@confidence":"0.98271352905199","#text":"Abst ract Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model. For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level . The model's precision/recall trade-off can be directly controlled via one threshold parameter. This feature makes the model more suitable for applications that are not fully statistical. The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as partof-speech, dictionaries, word order, etc.. Our model can link word tokens in parallel texts as well as other translation models in the literature. Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. 1 In t roduct ion Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for inducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 also shows how the oneto-one assumption enables us to use a new greedy competitive linking algorithm for re-estimating the model's parameters, instead of more expensive algorithms that consider a much larger set of word correspondence possibilities. The model uses two hidden parameters to estimate the confidence of its own predictions. The confidence stimates enable direct control of the balance between the model's precision and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. 2 Co-occur rence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Wordto -Word Mode l Our translation model consists of the hidden parameters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n(u,v) and inversely proportional to their marginal frequencies n(u) and n(v) z, following (Dunning, 1993) 2. When the L(u, v) are re-estimated, the model's hidden parameters come into play. After initialization, the model induction algorithm iterates: 1. Find a set of &quot;links&quot; among word tokens in the bitext, using the likelihood ratios and the competitive linking algorithm. 2. Use the links to re-estimate A+, A-, and the likelihood ratios. 3. Repeat from Step 1 until the model converges to the desired degree. The competitive linking algorithm and its one-to-one assumption are detailed in Section 3.1. Section 3.1 explains how to re-estimate the model parameters. 3.1 Compet i t i ve L inking A lgor i thm The competitive linking algorithm is designed to overcome the problem of indirect associations, illustrated in Figure 1. The sequences of u's and v's represent corresponding regions of a bitext. If uk and vk co-occur much more often than expected by chance, then any reasonable model will deem them likely to be mutual translations. If uk and Vk are indeed mutual translations, then their tendency to ZThe co-occurrence frequency of a word type pair is simply the number of times the pair co-occurs in the corpus. However, n(u) = ~-~v n(u.v), which is not the same as the frequency of u, because ach token of u can co-occur with several differentv's. 2We could just as easily use other symmetric &quot;association&quot; measures, uch as ?2 (Gale & Church, 1991) or the Dice coefficient (Smadja, 1992). ? ? ? Uk . 1 tJk ~ = Uk+l ? ? ? t ? , ? Vk . 1 Vk Vk+l ? . . Figure 1: Uk and vk often co-occur, as do uk and uk+z. The direct association between uk and vk, and the direct association between uk and Uk+l give rise to an indirect association between v~ and uk+l. co-occur is called a direct associat ion. Now, suppose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connecting vk and u~+l in Figure 1 represents an indi rect associat ion, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have &quot;a tendency ... to be confused by collocates&quot; (Dagan et al, 1993). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic: 1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values (Brown et al, 1990; Dagan et al, 1993; Chen, 1996). To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly. 2. Sort all remaining likelihood estimates L(u, v) from highest to lowest. 3. Find u and v such that the likelihood ratio L(u,v) is highest. Token pairs of these types 491 n(u,v) N k(u.v) K T k+ k- B(k{n,p) = frequency of co-occurrence between word types u and v = ~&quot;\\].(u.,,) n(u.v) = total number of co-occurrences in the bitext = frequency of links between word types u and v = ~&quot;\\].(u,v) k(u.,,) = total number of links in the bitext = Pr( mutual translations I co-occurrence ) = Pr( link I co-occurrence ) = Pr( link \\[ co-occurrence of mutual translations ) = Pr( link I co-occurrence of not mutual translations ) = Pr (k in ,p) , where k has a binomial distribution with parameters n and p N.B.: k + and )~need not sum to 1, because they are conditioned on different events. Figure 2: Variables used to estimate the model parameters. would be the winners in any competitions involving u or v. 4. Link all token pairs (u, v) in the bitext. 5. The one-to-one assumption means that linked words cannot be linked again. Therefore, remove all linked word tokens from their respective texts. 6. If there is another co-occurring word token pair (u, v) such that L(u, v) exists, then repeat from Step 3. The competitive linking algorithm is more greedy than algorithms that try to find a set of link types that are jointly most probable over some segment of the bitext. In practice, our linking algorithm can be implemented so that its worst-case running time is O(lm), where l and m are the lengths of the aligned segments. The simplicity of the competitive linking algorithm depends on the oneto -one assumpt ion : Each word translates to at most one other word. Certainly, there are cases where this assumption is false. We prefer not to model those cases, in order to achieve higher accuracy with less effort on the cases where the assumption is true. 3.2 Parameter Es t imat ion The purpose of the competitive linking algorithm is to help us re-estimate the model parameters. The variables that we use in our estimation are summarized in Figure 2. The linking algorithm produces a set of links between word tokens in the bitext. We define a l ink token to be an ordered pair of word tokens, one from each half of the bitext. A l ink type is an ordered pair of word types. Let n(u.,,) be the co-occurrence frequency of u and v and k(~,,,) be the number of links between tokens of u and v 3. An 3Note that k(u,v) depends on the linking algorithm, but n(u.v) is a constant property of the bitext. important property of the competitive linking algorithm is that the ratio kiu.,,)/n(u,v ) tends to be very high if u and v are mutual translations, and quite low if they are not. The bimodality of this ratio for several values of n(u.,,i is illustrated in Figure 3. This figure was plotted after the model's first iteration over 300000 aligned sentence pairs from the I0(0) ,oo LI,. {0 } 0 ~ (u V)/n(u v) o~ , Figure 3: A fragment of the joint frequency (k(u.v)/n(u.v), n(u.v)). Note that the frequencies are plotted on a log scale -the bimodality is quite sharp. Canad ian Hansard bitext. Note that the frequencies are plotted on a log scale -the b imodal i ty is quite sharp. The linking algorithm creates all the links of a given type independently of each other, so the number k(u,v ) of links connecting word types u and v has a binomial distribution with parameters n(u.,,l and P(u.,,)- If u and v are mutual translations, then P(u,,,) tends to a relatively high probability, which we will call A +. If u and v are not mutual translations, then P(u,v) tends to a very low probability, which we will call A-. A + and Acorrespond to the two peaks in the frequency distribution of k(u.,,)/niu.v~ in Figure 2. The two parameters can also be interpreted as the percentage of true and false positives. If the translation in the bitext is consistent and the 492 model is accurate, then A + should be near 1 and Ashould be near 0. To find the most probable values of the hidden model parameters A + and A-, we adopt he standard method of maximum likelihood estimation, and find the values that maximize the probability of the link frequency distributions. The one-to-one assumption implies independence between different link types, so that Pr(linkslm?del) = H Vr(k(u,v)\\[n(u,v), A +, A-). R~V (1) The factors on the right-hand side of Equation 1 can be written explicitly with the help of a mixture coefficient. Let r be the probability that an arbitrary co-occurring pair of word types are mutual translations. Let B(kln,p ) denote the probability that k links are observed out of n co-occurrences, where k has a binomial distribution with parameters n and p. Then the probability that u and v are linked k(u,v) times out of n(u,v) co-occurrences is a mixture of two binomials: Pr(k(u,v) ln(u,v), A +, A-) = (2) = rB(k(u,v)ln(u,v), A +) ? (1 r )B(k (u ,v ) ln (u ,v ) ,A - ) One more variable allows us to express r in terms of A + and A- : Let A be the probability that an arbitrary co-occuring pair of word tokens will be linked, regardless of whether they are mutual translations. Since r is constant over all word types, it also represents the probability that an arbitrary co-occurring pair of word tokens are mutual translations. Therefore, A = rA + + (1 r)A-. (3) A can also be estimated empirically. Let K be the total number of links in the bitext and let N be the total number of co-occuring word token pairs: K = ~(u,v) k(u,v/, N = ~(~,v) n(u,v). By definition, A = KIN. (4) Equating the right-hand sides of Equations (3) and (4) and rearranging the terms, we get: KIN - ,X- - (5 ) A+ _ )~- Since r is now a function of A + and A-, only the latter two variables represent degrees of freedom in the model. The probability function expressed by Equations 1 and 2 has many local maxima. In practice, these c -1.2 -1.4 E -1.6 &quot;~ -1.8 ) 0 Figure 4: Pr(links\\[model) has only one global maximum in the region of interest. local maxima are like pebbles on a mountain, invisible at low resolution. We computed Equation 1 over various combinations of A + and Aafter the model's first iteration over 300000 aligned sentence pairs from the Canadian Hansard bitext. Figure 4 shows that the region of interest in the parameter space, where 1 > A + > A > A- > 0, has only one clearly visible global maximum. This global maximum can be found by standard hill-climbing methods, as long as the step size is large enough to avoid getting stuck on the pebbles. Given estimates for A + and A-, we can compute B(ku,,,\\[nu,v, A +) and B(ku,v\\[nu,v, A-). These are probabilities that k(u,v) links were generated by an algorithm that generates correct links and by an algorithm that generates incorrect links, respectively, out ofn(u,v) co-occurrences. The ratio of these probabilities is the likelihood ratio in favor of u and v being mutual translations, for all u and v: B(ku,vln<,,,,, ),+) L(u,v) = B(ku,vln~,v, A_ ) . (61 4 C lass -Based Wordto -Word Mode ls In the basic word-to-word model, the hidden parameters A + and Adepend only on the distributions of link frequencies generated by the competitive linking algorithm. More accurate models can be induced by taking into account various features of the linked tokens. For example, frequent words are translated less consistently than rare words (Melamed, 1997). To account for this difference, we can estimate separate values of X + and Afor different ranges of n(u,v). Similarly, the hidden parameters can be conditioned on the linked parts of speech. Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately 493 from those that do not. This method of incorporating dictionary information seems simpler than the method proposed by Brown et ai. for their models (Brown et al, 1993b). When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class. 5 Eva luat ion A word-to-word model of translational equivalence can be evaluated either over types or over tokens. It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available. For each kind of evaluation, we have found one case where we can come close. We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards, aligned using the method in (Gale & Church, 1991). One class represented content-word links and the other represented function-word links 4. Link types with negative log-likelihood were discarded after each iteration. Both classes' parameters converged after six iterations. The value of class-based models was demonstrated by the differences between the hidden parameters for the two classes. (A +,A-) converged at (.78,00016) for content-class links and at (.43,.000094) for function-class links. 5.1 L ink Types The most direct way to evaluate the link types in a word-level model of translational equivalence is to treat each link type as a candidate translation lexicon entry, and to measure precision and recall. This evaluation criterion carries much practical import, because many of the applications mentioned in Section 1 depend on accurate broad-coverage translation lexicons. Machine readable bilingual dictionaries, even when they are available, have only limited coverage and rarely include domain-specific terms (Resnik & Melamed, 1997). We define the recall of a word-to-word translation model as the fraction of the bitext vocabulary represented in the model. Translation model precision is a more thorny issue, because people disagree about the degree to which context should play a role in judgements of translational equivalence. We handevaluated the precision of the link types in our model in the context of the bitext from which the model 4Since function words can be identified by table lookup, no POS-tagger was involved. was induced, using a simple bilingual concordancer. A link type (u, v) was considered correct if u and v ever co-occurred as direct translations of each other. Where the one-to-one assumption failed, but a link type captured part of a correct translation, it was judged &quot;incomplete.&quot; Whether incomplete links are correct or incorrect depends on the application. 100 98 96 ~) 94 u 92 t_ 9O 88 86 84 (99.2%) ~ (9~ .6%) t'&quot;,,, &quot;&quot;-,}(89.2%) . . . . . . . . &quot; . . . . . . . . . . . . . . . . . . . . . . x incomplete =incorrect ......... -~(86.8%) 3'6 4'6 9'0 % recall Figure 5: Link type precision with 95~ confidence intervals at varying levels of recall. We evaluated five random samples of 100 link types each at three levels of recall. For our bitext, recall of 36%, 46% and 90% corresponded to translation lexicons containing 32274, 43075 and 88633 words, respectively. Figure 5 shows the precision of the model with 95% confidence intervals. The upper curve represents precision when incomplete links are considered correct, and the lower when they are considered incorrect. On the former metric, our model can generate translation lexicons with precision and recall both exceeding 90%, as well as dictionarysized translation lexicons that are over 99% correct. Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct ranslations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high precision even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% recall, when trained on French/English software manuals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather eports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. 5.2 Link Tokens type of error errors made by errors made IBM Model 2 by our model wrong link missing link partial link class conflict tokenization paraphrase 32 12 7 3 39 7 36 10 5 2 36 TOTAL 93 96 Table 1: Erroneous link tokens generated by two translation models. The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al's Model 2 on 74 million words of the Canadian Hansards. These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words. Therefore, we ignored English words that were linked to nothing. The errors are classified in Table 1. The &quot;wrong link&quot; and &quot;missing link&quot; error categories should be self-explanatory. &quot;Partial inks&quot; are those where one French word resulted from multiple English words, but the model only links the French word to one of its English sources. &quot;Class conflict&quot; errors resulted from our model's refusal to link content words with function words. Usually, this is the desired behavior, but words like English auxiliary verbs are sometimes used as content words, giving rise to content words in French. Such errors could be overcome by a model that classifies each word token, for example using a part-of-speech tagger, instead of assigning the same class to all tokens of a given type. The bitext preprocessor for our word-to-word model split hyphenated words, but Macklovitch &Hannan's preprocessor did not. In some cases, hyphenated words were easier to link correctly; in other cases they were more difficult. Both models made some errors because of this tokenization problem, albeit in different places. The &quot;paraphrase&quot; category covers all link errors that resulted from paraphrases in the translation. Neither IBM's Model 2 nor our model is capable of linking multi-word sequences to multi-word sequences, and this was the biggest source of error for both models. The test sample contained only about 400 content words 5, and the links for both models were evaluated post-hoc by only one evaluator. Nevertheless, it appears that our word-to-word model with only two link classes does not perform any worse than IBM's Model 2, even though the word-to-word model was trained on less than one fifth the amount of data that was used to train the IBM model. Since it doesn't store indirect associations, our word-to-word model contained an average of 4.5 French words for every English word. Such a compact model requires relatively little computational effort to induce and to apply. des screaming vents . winds ,A , dechames ---&quot; and et dangerous une ~ sea mer . conditions s p de'montee.-&quot;&quot; Figure 6: An example of the different sorts of errors made by the word-to-word model and the IBM Model 2. Solid lines are links made by both models; dashes lines are links made by the IBM model only. Only content-class links are shown. Neither model makes the correct links (ddcha?nds,screaming) and (ddmontde, dangerous). 5The exact number depends on the tokenization method. 495 In addition to the quantitative differences between the word-to-word model and the IBM model, there is an important qualitative difference, illustrated in Figure 6. As shown in Table 1, the most common kind of error for the word-to-word model was a missing link, whereas the most common error for IBM's Model 2 was a wrong link. Missing links are more informative: they indicate where the model has failed. The level at which the model trusts its own judgement can be varied directly by changing the likelihood cutoff in Step 1 of the competitive linking algorithm. Each application of the word-to-word model can choose its own balance between link token precision and recall. An application that calls on the word-to-word model to link words in a bitext could treat unlinked words differently from linked words, and avoid basing subsequent decisions on uncertain inputs. It is not clear how the precision/recall tradeoff can be controlled in the IBM models. One advantage that Brown et al's Model i has over our word-to-word model is that their objective function has no local maxima. By using the EM algorithm (Dempster et al, 1977), they can guarantee convergence towards the globally optimum parameter set. In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion. We have adopted the simple heuristic that the model &quot;has converged&quot; when this probability stops increasing. 6 Conc lus ion Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of modeling the full range of translation phenomena. For these applications, we have designed afast algorithm for estimating word-to-word models of translational equivalence. The estimation method uses a pair of hidden parameters to measure the model's uncertainty, and avoids making decisions that it's not likely to make correctly. The hidden parameters can be conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge. So far we have only implemented a two-class model, to exploit the differences in translation consistency between content words and function words. This relatively simple two-class model linked word tokens in parallel texts as accurately as other translation models in the literature, despite being trained on only one fifth as much data. Unlike other translation models, the word-to-word model can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy (Melamed, 1997). Another interesting extension is to broaden the definition of a &quot;word&quot; to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena. Acknowledgements The French/English software manuals were provided by Gary Adams of Sun MicroSystems Laboratories. The weather bitext was prepared at the University of Montreal, under the direction Of Richard Kittredge. Thanks to Alexis Nasr for hand-evaluating the weather translation lexicon. Thanks also to Mike Collins, George Foster, Mitch Marcus, Lyle Ungar, and three anonymous reviewers for helpful comments. This research was supported by at. equipment grant from Sun MicroSystems and by ARPA Contract #N66001-94C-6043."},"title":[{"#tail":"\n","@confidence":"0.999216","#text":"A Word-to-Word Model of Translational Equivalence"},{"#tail":"\n","@confidence":"0.894099","#text":"References"}],"email":{"#tail":"\n","@confidence":"0.973504","#text":"raelamed~unagi,cis.upenn,edu"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1988"},"rawString":{"#tail":"\n","#text":"P. F. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, R. Mercer, & P. Roossin, &quot;A Statistical Approach to Language Translation,&quot; Proceedings of the 12th International Conference on Computational Linguistics, Budapest, Hungary, 1988."},"#text":"\n","marker":{"#tail":"\n","#text":"Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Roossin, 1988"},"location":{"#tail":"\n","#text":"Budapest, Hungary,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ters can be eas- ily conditioned on information extrinsic to the model, providing an easy way to inte- grate pre-existing knowledge such as part- of-speech, dictionaries, word order, etc.. Our model can link word tokens in paral- lel texts as well as other translation mod- els in the literature. Unlike other transla- tion models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melb","@endWordPosition":"223","@position":"1413","annotationId":"T1","@startWordPosition":"220","@citStr":"Brown et al, 1988"}},"title":{"#tail":"\n","#text":"A Statistical Approach to Language Translation,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 12th International Conference on Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P F Brown"},{"#tail":"\n","#text":"J Cocke"},{"#tail":"\n","#text":"S Della Pietra"},{"#tail":"\n","#text":"V Della Pietra"},{"#tail":"\n","#text":"F Jelinek"},{"#tail":"\n","#text":"R Mercer"},{"#tail":"\n","#text":"P Roossin"}]}},{"volume":{"#tail":"\n","#text":"16"},"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"P. F. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, R. Mercer, & P. Roossin, &quot;A Statistical Approach to Machine Translation,&quot; Computational Linguistics 16(2), 1990."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Roossin, 1990"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"y conditioned on information extrinsic to the model, providing an easy way to inte- grate pre-existing knowledge such as part- of-speech, dictionaries, word order, etc.. Our model can link word tokens in paral- lel texts as well as other translation mod- els in the literature. Unlike other transla- tion models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross","@endWordPosition":"227","@position":"1432","annotationId":"T2","@startWordPosition":"224","@citStr":"Brown et al, 1990"},{"#tail":"\n","#text":" particular word to- ken v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuristic: 1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all prob- abilities below a certain threshold to negligible values (Brown et al, 1990; Dagan et al, 1993; Chen, 1996). To retain word type pairs that are at least twice as likely to be mutual transla- tions than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly. 2. Sort all remaining likelihood estimates L(u, v) from highest to lowest. 3. Find u and v such that the likelihood ratio L(u,v) is highest. Token pairs of these types 491 n(u,v) N k(u.v) K T k+ k- B(k{n,p) = frequency of co-occurrence between word types u and v = ~&quot;\\].(u.,,) n(u.v) = total number of co-occur","@endWordPosition":"1302","@position":"7857","annotationId":"T3","@startWordPosition":"1299","@citStr":"Brown et al, 1990"}]},"title":{"#tail":"\n","#text":"A Statistical Approach to Machine Translation,&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P F Brown"},{"#tail":"\n","#text":"J Cocke"},{"#tail":"\n","#text":"S Della Pietra"},{"#tail":"\n","#text":"V Della Pietra"},{"#tail":"\n","#text":"F Jelinek"},{"#tail":"\n","#text":"R Mercer"},{"#tail":"\n","#text":"P Roossin"}]}},{"volume":{"#tail":"\n","#text":"19"},"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & R. L. Mercer, &quot;The Mathematics of Statistical Machine Translation: Parameter Estimation,&quot; Computational Linguistics 19(2), 1993."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Mercer, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"formation extrinsic to the model, providing an easy way to inte- grate pre-existing knowledge such as part- of-speech, dictionaries, word order, etc.. Our model can link word tokens in paral- lel texts as well as other translation mod- els in the literature. Unlike other transla- tion models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual informatio","@endWordPosition":"231","@position":"1451","annotationId":"T4","@startWordPosition":"228","@citStr":"Brown et al, 1993"},{"#tail":"\n","#text":"ifference, we can estimate sep- arate values of X + and A- for different ranges of n(u,v). Similarly, the hidden parameters can be con- ditioned on the linked parts of speech. Word order can be taken into account by conditioning the hid- den parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with en- tries in a pre-existing translation lexicon separately 493 from those that do not. This method of incorporat- ing dictionary information seems simpler than the method proposed by Brown et ai. for their models (Brown et al, 1993b). When the hidden parameters are conditioned on different link classes, the estima- tion method does not change; it is just repeated for each link class. 5 Eva luat ion A word-to-word model of translational equivalence can be evaluated either over types or over tokens. It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available. For each kind of evaluation, we have found one case where we can come close. We induced a two-class word-to-word model of translational","@endWordPosition":"2806","@position":"16157","annotationId":"T5","@startWordPosition":"2803","@citStr":"Brown et al, 1993"}]},"title":{"#tail":"\n","#text":"The Mathematics of Statistical Machine Translation: Parameter Estimation,&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P F Brown"},{"#tail":"\n","#text":"V J Della Pietra"},{"#tail":"\n","#text":"S A Della Pietra"},{"#tail":"\n","#text":"R L Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, M. J. Goldsmith, J. Hajic, R. L. Mercer & S. Mohanty, &quot;But Dictionaries are Data Too,&quot; Proceedings of the ARPA HLT Workshop, Princeton, N J, 1993."},"#text":"\n","marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Goldsmith, Hajic, Mercer, Mohanty, 1993"},"location":{"#tail":"\n","#text":"Princeton, N J,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"formation extrinsic to the model, providing an easy way to inte- grate pre-existing knowledge such as part- of-speech, dictionaries, word order, etc.. Our model can link word tokens in paral- lel texts as well as other translation mod- els in the literature. Unlike other transla- tion models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual informatio","@endWordPosition":"231","@position":"1451","annotationId":"T6","@startWordPosition":"228","@citStr":"Brown et al, 1993"},{"#tail":"\n","#text":"ifference, we can estimate sep- arate values of X + and A- for different ranges of n(u,v). Similarly, the hidden parameters can be con- ditioned on the linked parts of speech. Word order can be taken into account by conditioning the hid- den parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with en- tries in a pre-existing translation lexicon separately 493 from those that do not. This method of incorporat- ing dictionary information seems simpler than the method proposed by Brown et ai. for their models (Brown et al, 1993b). When the hidden parameters are conditioned on different link classes, the estima- tion method does not change; it is just repeated for each link class. 5 Eva luat ion A word-to-word model of translational equivalence can be evaluated either over types or over tokens. It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available. For each kind of evaluation, we have found one case where we can come close. We induced a two-class word-to-word model of translational","@endWordPosition":"2806","@position":"16157","annotationId":"T7","@startWordPosition":"2803","@citStr":"Brown et al, 1993"}]},"title":{"#tail":"\n","#text":"But Dictionaries are Data Too,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the ARPA HLT Workshop,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P F Brown"},{"#tail":"\n","#text":"S A Della Pietra"},{"#tail":"\n","#text":"V J Della Pietra"},{"#tail":"\n","#text":"M J Goldsmith"},{"#tail":"\n","#text":"J Hajic"},{"#tail":"\n","#text":"R L Mercer"},{"#tail":"\n","#text":"S Mohanty"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"R. Catizone, G. Russell & S. Warwick &quot;Deriving Translation Data from Bilingual Texts,&quot; Proceedings of the First International Lexical Acquisition Workshop, Detroit, MI, 1993."},"#text":"\n","marker":{"#tail":"\n","#text":"Catizone, Russell, Warwick, 1993"},"location":{"#tail":"\n","#text":"Detroit, MI,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 also shows how the one- to-one assumption enables us to use a new gr","@endWordPosition":"297","@position":"1927","annotationId":"T8","@startWordPosition":"294","@citStr":"Catizone et al, 1993"}},"title":{"#tail":"\n","#text":"Deriving Translation Data from Bilingual Texts,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the First International Lexical Acquisition Workshop,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Catizone"},{"#tail":"\n","#text":"G Russell"},{"#tail":"\n","#text":"S Warwick"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. Thesis,"},"date":{"#tail":"\n","#text":"1996"},"institution":{"#tail":"\n","#text":"Harvard University,"},"rawString":{"#tail":"\n","#text":"S. Chen, Building Probabilistic Models for Natural Language, Ph.D. Thesis, Harvard University, 1996."},"#text":"\n","marker":{"#tail":"\n","#text":"Chen, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"r half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuristic: 1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all prob- abilities below a certain threshold to negligible values (Brown et al, 1990; Dagan et al, 1993; Chen, 1996). To retain word type pairs that are at least twice as likely to be mutual transla- tions than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly. 2. Sort all remaining likelihood estimates L(u, v) from highest to lowest. 3. Find u and v such that the likelihood ratio L(u,v) is highest. Token pairs of these types 491 n(u,v) N k(u.v) K T k+ k- B(k{n,p) = frequency of co-occurrence between word types u and v = ~&quot;\\].(u.,,) n(u.v) = total number of co-occurrences in the bitext = frequency","@endWordPosition":"1308","@position":"7889","annotationId":"T9","@startWordPosition":"1307","@citStr":"Chen, 1996"},{"#tail":"\n","#text":"xtract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30%","@endWordPosition":"3490","@position":"20316","annotationId":"T10","@startWordPosition":"3489","@citStr":"Chen, 1996"}]},"title":{"#tail":"\n","#text":"Building Probabilistic Models for Natural Language,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"S Chen"}}},{"volume":{"#tail":"\n","#text":"8"},"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"K. W. Church & E. H. Hovy, &quot;Good Applications for Crummy Machine Translation,&quot; Machine Translation 8, 1993."},"journal":{"#tail":"\n","#text":"Machine Translation"},"#text":"\n","marker":{"#tail":"\n","#text":"Church, Hovy, 1993"},"title":{"#tail":"\n","#text":"Good Applications for Crummy Machine Translation,&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K W Church"},{"#tail":"\n","#text":"E H Hovy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"I. Dagan, K. Church, & W. Gale, &quot;Robust Word Alignment for Machine Aided Translation,&quot; Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, Columbus, OH, 1993."},"#text":"\n","marker":{"#tail":"\n","#text":"Dagan, Church, Gale, 1993"},"location":{"#tail":"\n","#text":"Columbus, OH,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their ","@endWordPosition":"633","@position":"3960","annotationId":"T11","@startWordPosition":"630","@citStr":"Dagan et al, 1993"},{"#tail":"\n","#text":"ciation between uk and Uk+l give rise to an indirect association between v~ and uk+l. co-occur is called a direct associat ion. Now, sup- pose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equiv- alence that are ignorant of indirect associations have &quot;a tendency ... to be confused by collocates&quot; (Dagan et al, 1993). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associ- ations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word to- ken v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuris","@endWordPosition":"1146","@position":"6894","annotationId":"T12","@startWordPosition":"1143","@citStr":"Dagan et al, 1993"}]},"title":{"#tail":"\n","#text":"Robust Word Alignment for Machine Aided Translation,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"I Dagan"},{"#tail":"\n","#text":"K Church"},{"#tail":"\n","#text":"W Gale"}]}},{"volume":{"#tail":"\n","#text":"34"},"#tail":"\n","date":{"#tail":"\n","#text":"1977"},"rawString":{"#tail":"\n","#text":"A. P. Dempster, N. M. Laird & D. B. Rubin, &quot;Maximum likelihood from incomplete data via the EM algorithm,&quot; Journal of the Royal Statistical Society 34(B), 1977."},"journal":{"#tail":"\n","#text":"Journal of the Royal Statistical Society"},"#text":"\n","marker":{"#tail":"\n","#text":"Dempster, Laird, Rubin, 1977"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Step 1 of the competitive linking algo- rithm. Each application of the word-to-word model can choose its own balance between link token pre- cision and recall. An application that calls on the word-to-word model to link words in a bitext could treat unlinked words differently from linked words, and avoid basing subsequent decisions on uncertain inputs. It is not clear how the precision/recall trade- off can be controlled in the IBM models. One advantage that Brown et al's Model i has over our word-to-word model is that their objec- tive function has no local maxima. By using the EM algorithm (Dempster et al, 1977), they can guarantee convergence towards the globally opti- mum parameter set. In contrast, the dynamic na- ture of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion. We have adopted the simple heuristic that the model &quot;has converged&quot; when this probability stops increas- ing. 6 Conc lus ion Many multilingual NLP applications need to trans- late words between different languages, but cannot afford the computational expense of modeling the full range of translation phenomena. For these ap- plications, we have designed afast algorithm for esti- mating word-t","@endWordPosition":"4381","@position":"25665","annotationId":"T13","@startWordPosition":"4378","@citStr":"Dempster et al, 1977"}},"title":{"#tail":"\n","#text":"Maximum likelihood from incomplete data via the EM algorithm,&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A P Dempster"},{"#tail":"\n","#text":"N M Laird"},{"#tail":"\n","#text":"D B Rubin"}]}},{"volume":{"#tail":"\n","#text":"19"},"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"T. Dunning, &quot;Accurate Methods for the Statistics of Surprise and Coincidence,&quot; Computational Linguistics 19(1), 1993."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Dunning, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"xtrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n(u,v) and inversely proportional to their marginal frequen- cies n(u) and n(v) z, following (Dunning, 1993) 2. When the L(u, v) are re-estimated, the model's hid- den parameters come into play. After initialization, the model induction algorithm iterates: 1. Find a set of &quot;links&quot; among word tokens in the bitext, using the likelihood ratios and the com- petitive linking algorithm. 2. Use the links to re-estimate A+, A-, and the likelihood ratios. 3. Repeat from Step 1 until the model converges to the desired degree. The competitive linking algorithm and its one-to-one assumption are detailed in Section 3.1. Section 3.1 explains how to re-estimate the model parameters. 3.1 Compet i t i ve L inking A ","@endWordPosition":"754","@position":"4692","annotationId":"T14","@startWordPosition":"753","@citStr":"Dunning, 1993"}},"title":{"#tail":"\n","#text":"Accurate Methods for the Statistics of Surprise and Coincidence,&quot;"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"T Dunning"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"P. Fung, &quot;Compiling Bilingual Lexicon Entries from a Non-Parallel English-Chinese Corpus,&quot; Proceedings of the Third Workshop on Very Large Corpora, Boston, MA, 1995a."},"#text":"\n","marker":{"#tail":"\n","#text":"Fung, 1995"},"location":{"#tail":"\n","#text":"Boston, MA,"},"title":{"#tail":"\n","#text":"Compiling Bilingual Lexicon Entries from a Non-Parallel English-Chinese Corpus,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the Third Workshop on Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"P Fung"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"P. Fung, &quot;A Pattern Matching Method for Finding Noun and Proper Noun Translations from Noisy Parallel Corpora,&quot; Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Boston, MA, 1995b."},"#text":"\n","marker":{"#tail":"\n","#text":"Fung, 1995"},"location":{"#tail":"\n","#text":"Boston, MA,"},"title":{"#tail":"\n","#text":"A Pattern Matching Method for Finding Noun and Proper Noun Translations from Noisy Parallel Corpora,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"P Fung"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"W. Gale & K. W. Church, &quot;A Program for Aligning Sentences in Bilingual Corpora&quot; Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, CA, 1991."},"#text":"\n","marker":{"#tail":"\n","#text":"Gale, Church, 1991"},"location":{"#tail":"\n","#text":"Berkeley, CA,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"easingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 also shows how the one- to-one assumption enables us to use a new greedy competitive linki","@endWordPosition":"301","@position":"1949","annotationId":"T15","@startWordPosition":"298","@citStr":"Gale & Church, 1991"},{"#tail":"\n","#text":"ms that consider a much larger set of word cor- respondence possibilities. The model uses two hid- den parameters to estimate the confidence of its own predictions. The confidence stimates enable direct control of the balance between the model's preci- sion and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. 2 Co-occur rence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in ","@endWordPosition":"501","@position":"3259","annotationId":"T16","@startWordPosition":"498","@citStr":"Gale & Church, 1991"},{"#tail":"\n","#text":"s of u's and v's represent corresponding regions of a bitext. If uk and vk co-occur much more often than expected by chance, then any reasonable model will deem them likely to be mutual translations. If uk and Vk are indeed mutual translations, then their tendency to ZThe co-occurrence frequency of a word type pair is simply the number of times the pair co-occurs in the corpus. However, n(u) = ~-~v n(u.v), which is not the same as the frequency of u, because ach token of u can co-occur with several differentv's. 2We could just as easily use other symmetric &quot;asso- ciation&quot; measures, uch as ?2 (Gale & Church, 1991) or the Dice coefficient (Smadja, 1992). ? ? ? Uk . 1 tJk ~ = Uk+l ? ? ? t ? , ? Vk . 1 Vk Vk+l ? . . Figure 1: Uk and vk often co-occur, as do uk and uk+z. The direct association between uk and vk, and the direct association between uk and Uk+l give rise to an indirect association between v~ and uk+l. co-occur is called a direct associat ion. Now, sup- pose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association betwe","@endWordPosition":"986","@position":"6058","annotationId":"T17","@startWordPosition":"983","@citStr":"Gale & Church, 1991"},{"#tail":"\n","#text":" method does not change; it is just repeated for each link class. 5 Eva luat ion A word-to-word model of translational equivalence can be evaluated either over types or over tokens. It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available. For each kind of evaluation, we have found one case where we can come close. We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards, aligned using the method in (Gale & Church, 1991). One class repre- sented content-word links and the other represented function-word links 4. Link types with negative log-likelihood were discarded after each iteration. Both classes' parameters converged after six it- erations. The value of class-based models was demonstrated by the differences between the hid- den parameters for the two classes. (A +,A-) con- verged at (.78,00016) for content-class links and at (.43,.000094) for function-class links. 5.1 L ink Types The most direct way to evaluate the link types in a word-level model of translational equivalence is to treat each link type a","@endWordPosition":"2920","@position":"16867","annotationId":"T18","@startWordPosition":"2917","@citStr":"Gale & Church, 1991"},{"#tail":"\n","#text":"e have tried, it is not clear how to extract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this","@endWordPosition":"3484","@position":"20286","annotationId":"T19","@startWordPosition":"3481","@citStr":"Gale & Church, 1991"}]},"title":{"#tail":"\n","#text":"A Program for Aligning Sentences in Bilingual Corpora&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W Gale"},{"#tail":"\n","#text":"K W Church"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"W. Gale & K. W. Church, &quot;Identifying Word Correspondences in Parallel Texts,&quot; Proceedings of the DARPA SNL Workshop, 1991."},"#text":"\n","marker":{"#tail":"\n","#text":"Gale, Church, 1991"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"easingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 also shows how the one- to-one assumption enables us to use a new greedy competitive linki","@endWordPosition":"301","@position":"1949","annotationId":"T20","@startWordPosition":"298","@citStr":"Gale & Church, 1991"},{"#tail":"\n","#text":"ms that consider a much larger set of word cor- respondence possibilities. The model uses two hid- den parameters to estimate the confidence of its own predictions. The confidence stimates enable direct control of the balance between the model's preci- sion and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. 2 Co-occur rence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in ","@endWordPosition":"501","@position":"3259","annotationId":"T21","@startWordPosition":"498","@citStr":"Gale & Church, 1991"},{"#tail":"\n","#text":"s of u's and v's represent corresponding regions of a bitext. If uk and vk co-occur much more often than expected by chance, then any reasonable model will deem them likely to be mutual translations. If uk and Vk are indeed mutual translations, then their tendency to ZThe co-occurrence frequency of a word type pair is simply the number of times the pair co-occurs in the corpus. However, n(u) = ~-~v n(u.v), which is not the same as the frequency of u, because ach token of u can co-occur with several differentv's. 2We could just as easily use other symmetric &quot;asso- ciation&quot; measures, uch as ?2 (Gale & Church, 1991) or the Dice coefficient (Smadja, 1992). ? ? ? Uk . 1 tJk ~ = Uk+l ? ? ? t ? , ? Vk . 1 Vk Vk+l ? . . Figure 1: Uk and vk often co-occur, as do uk and uk+z. The direct association between uk and vk, and the direct association between uk and Uk+l give rise to an indirect association between v~ and uk+l. co-occur is called a direct associat ion. Now, sup- pose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association betwe","@endWordPosition":"986","@position":"6058","annotationId":"T22","@startWordPosition":"983","@citStr":"Gale & Church, 1991"},{"#tail":"\n","#text":" method does not change; it is just repeated for each link class. 5 Eva luat ion A word-to-word model of translational equivalence can be evaluated either over types or over tokens. It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available. For each kind of evaluation, we have found one case where we can come close. We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards, aligned using the method in (Gale & Church, 1991). One class repre- sented content-word links and the other represented function-word links 4. Link types with negative log-likelihood were discarded after each iteration. Both classes' parameters converged after six it- erations. The value of class-based models was demonstrated by the differences between the hid- den parameters for the two classes. (A +,A-) con- verged at (.78,00016) for content-class links and at (.43,.000094) for function-class links. 5.1 L ink Types The most direct way to evaluate the link types in a word-level model of translational equivalence is to treat each link type a","@endWordPosition":"2920","@position":"16867","annotationId":"T23","@startWordPosition":"2917","@citStr":"Gale & Church, 1991"},{"#tail":"\n","#text":"e have tried, it is not clear how to extract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this","@endWordPosition":"3484","@position":"20286","annotationId":"T24","@startWordPosition":"3481","@citStr":"Gale & Church, 1991"}]},"title":{"#tail":"\n","#text":"Identifying Word Correspondences in Parallel Texts,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the DARPA SNL Workshop,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W Gale"},{"#tail":"\n","#text":"K W Church"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"A. Kumano & H. Hirakawa, &quot;Building an MT Dictionary from Parallel Texts Based on Linguistic and Statistical Information,&quot; Proceedings of the 15th International Conference on Computational Linguistics, Kyoto, Japan, 1994."},"#text":"\n","marker":{"#tail":"\n","#text":"Kumano, Hirakawa, 1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ch larger set of word cor- respondence possibilities. The model uses two hid- den parameters to estimate the confidence of its own predictions. The confidence stimates enable direct control of the balance between the model's preci- sion and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. 2 Co-occur rence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is ","@endWordPosition":"505","@position":"3284","annotationId":"T25","@startWordPosition":"502","@citStr":"Kumano & Hirakawa, 1994"}},"title":{"#tail":"\n","#text":"Building an MT Dictionary from Parallel Texts Based on Linguistic and Statistical Information,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 15th International Conference on Computational Linguistics, Kyoto,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Kumano"},{"#tail":"\n","#text":"H Hirakawa"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"E. Macklovitch :'Using Bi-textual Alignment for Translation Validation: The TransCheck System,&quot; Proceedings of the 1st Conference of the Association for Machine Translation in the Americas, Columbia, MD, 1994."},"#text":"\n","marker":{"#tail":"\n","#text":"Macklovitch, 1994"},"location":{"#tail":"\n","#text":"Columbia, MD,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ccuracy. 1 In t roduct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models","@endWordPosition":"287","@position":"1845","annotationId":"T26","@startWordPosition":"286","@citStr":"Macklovitch, 1994"}},"title":{"#tail":"\n","#text":"Using Bi-textual Alignment for Translation Validation: The TransCheck System,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 1st Conference of the Association for Machine Translation in the Americas,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E Macklovitch"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"E. Macklovitch & M.-L. Hannan, &quot;Line 'Em Up: Advances in Alignment Technology and their Impact on Translation Support Tools,&quot; 2nd Conference of the Association for Machine Translation in the Americas, Montreal, Canada, 1996."},"#text":"\n","marker":{"#tail":"\n","#text":"Macklovitch, Hannan, 1996"},"location":{"#tail":"\n","#text":"Montreal, Canada,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"y bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. 5.2 Link Tokens type of error errors made by errors made IBM Model 2 by our model wrong link missing link partial link class conflict tokenization paraphrase 32 12 7 3 39 7 36 10 5 2 36 TOTAL 93 96 Table 1: Erroneous link tokens generated by two translation models. The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al's Model 2 on 74 million words of the Canadian Hansards. These au- thors kindly provided us with the links generated by that model in 51 aligned sentences from a held- out test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the En- glish words. Therefore, we ignored English words that were linked to nothing. The errors are classified ","@endWordPosition":"3689","@position":"21517","annotationId":"T27","@startWordPosition":"3686","@citStr":"Macklovitch & Hannan, 1996"}},"title":{"#tail":"\n","#text":"Line 'Em Up: Advances in Alignment Technology and their Impact on Translation Support Tools,&quot;"},"booktitle":{"#tail":"\n","#text":"2nd Conference of the Association for Machine Translation in the Americas,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E Macklovitch"},{"#tail":"\n","#text":"M-L Hannan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"I. D. Melamed &quot;Automatic Evaluation and Uniform Filter Cascades for Inducing N-best Translation Lexicons,&quot; Proceedings of the Third Workshop on Very Large Corpora, Boston, MA, 1995."},"#text":"\n","marker":{"#tail":"\n","#text":"Melamed, 1995"},"location":{"#tail":"\n","#text":"Boston, MA,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" possibilities. The model uses two hid- den parameters to estimate the confidence of its own predictions. The confidence stimates enable direct control of the balance between the model's preci- sion and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. 2 Co-occur rence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tati","@endWordPosition":"509","@position":"3313","annotationId":"T28","@startWordPosition":"508","@citStr":"Melamed, 1995"}},"title":{"#tail":"\n","#text":"Automatic Evaluation and Uniform Filter Cascades for Inducing N-best Translation Lexicons,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the Third Workshop on Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"I D Melamed"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"I. D. Melamed, &quot;A Geometric Approach to Mapping Bitext Correspondence,&quot; Proceedings of the First Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, 1996a."},"#text":"\n","marker":{"#tail":"\n","#text":"Melamed, 1996"},"location":{"#tail":"\n","#text":"Philadelphia, PA,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"uct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 a","@endWordPosition":"289","@position":"1860","annotationId":"T29","@startWordPosition":"288","@citStr":"Melamed, 1996"},{"#tail":"\n","#text":"on of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parame","@endWordPosition":"580","@position":"3689","annotationId":"T30","@startWordPosition":"579","@citStr":"Melamed, 1996"},{"#tail":"\n","#text":"in their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equiv- alence that are ignorant of indirect associations have &quot;a tendency ... to be confused by collocates&quot; (Dagan et al, 1993). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associ- ations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word to- ken v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuristic: 1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden ","@endWordPosition":"1172","@position":"7068","annotationId":"T31","@startWordPosition":"1171","@citStr":"Melamed, 1996"}]},"title":{"#tail":"\n","#text":"A Geometric Approach to Mapping Bitext Correspondence,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the First Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"I D Melamed"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"I. D. Melamed &quot;Automatic Detection of Omissions in Translations,&quot; Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, Denmark, 1996b."},"#text":"\n","marker":{"#tail":"\n","#text":"D, 1996"},"location":{"#tail":"\n","#text":"Copenhagen, Denmark,"},"title":{"#tail":"\n","#text":"Melamed &quot;Automatic Detection of Omissions in Translations,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 16th International Conference on Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"I D"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"I. D Melamed, &quot;Automatic Construction of Clean Broad-Coverage Translation Lexicons,&quot; 2nd Conference of the Association for Machine Translation in the Americas, Montreal, Canada, 1996c."},"#text":"\n","marker":{"#tail":"\n","#text":"Melamed, 1996"},"location":{"#tail":"\n","#text":"Montreal, Canada,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"uct ion Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al, 1988; Brown et al, 1990; Brown et al, 1993a). However, the IBM models, which attempt o capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al, 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996). In this paper, we present a fast method for in- ducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 a","@endWordPosition":"289","@position":"1860","annotationId":"T32","@startWordPosition":"288","@citStr":"Melamed, 1996"},{"#tail":"\n","#text":"on of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word co- occurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1998a; Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parame","@endWordPosition":"580","@position":"3689","annotationId":"T33","@startWordPosition":"579","@citStr":"Melamed, 1996"},{"#tail":"\n","#text":"in their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equiv- alence that are ignorant of indirect associations have &quot;a tendency ... to be confused by collocates&quot; (Dagan et al, 1993). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associ- ations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word to- ken v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm imple- ments this heuristic: 1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden ","@endWordPosition":"1172","@position":"7068","annotationId":"T34","@startWordPosition":"1171","@citStr":"Melamed, 1996"}]},"title":{"#tail":"\n","#text":"Automatic Construction of Clean Broad-Coverage Translation Lexicons,&quot;"},"booktitle":{"#tail":"\n","#text":"2nd Conference of the Association for Machine Translation in the Americas,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"I D Melamed"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"I. D. Melamed, &quot;Measuring Semantic Entropy,&quot; Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics, Washington, DC, 1997."},"#text":"\n","marker":{"#tail":"\n","#text":"Melamed, 1997"},"location":{"#tail":"\n","#text":"Washington, DC,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"re each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n","@endWordPosition":"637","@position":"3985","annotationId":"T35","@startWordPosition":"636","@citStr":"Melamed, 1997"},{"#tail":"\n","#text":"ctively, out ofn(u,v) co-occurrences. The ratio of these prob- abilities is the likelihood ratio in favor of u and v being mutual translations, for all u and v: B(ku,vln<,,,,, ),+) L(u,v) = B(ku,vln~,v, A_ ) . (61 4 C lass -Based Word- to -Word Mode ls In the basic word-to-word model, the hidden param- eters A + and A- depend only on the distributions of link frequencies generated by the competitive link- ing algorithm. More accurate models can be induced by taking into account various features of the linked tokens. For example, frequent words are translated less consistently than rare words (Melamed, 1997). To account for this difference, we can estimate sep- arate values of X + and A- for different ranges of n(u,v). Similarly, the hidden parameters can be con- ditioned on the linked parts of speech. Word order can be taken into account by conditioning the hid- den parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with en- tries in a pre-existing translation lexicon separately 493 from those that do not. This method of incorporat- ing dictionary information seems simpler than the method proposed by Brown et","@endWordPosition":"2698","@position":"15517","annotationId":"T36","@startWordPosition":"2697","@citStr":"Melamed, 1997"},{"#tail":"\n","#text":" links and at (.43,.000094) for function-class links. 5.1 L ink Types The most direct way to evaluate the link types in a word-level model of translational equivalence is to treat each link type as a candidate translation lexi- con entry, and to measure precision and recall. This evaluation criterion carries much practical import, because many of the applications mentioned in Sec- tion 1 depend on accurate broad-coverage transla- tion lexicons. Machine readable bilingual dictionar- ies, even when they are available, have only limited coverage and rarely include domain-specific terms (Resnik & Melamed, 1997). We define the recall of a word-to-word translation model as the fraction of the bitext vocabulary repre- sented in the model. Translation model precision is a more thorny issue, because people disagree about the degree to which context should play a role in judgements of translational equivalence. We hand- evaluated the precision of the link types in our model in the context of the bitext from which the model 4Since function words can be identified by table look- up, no POS-tagger was involved. was induced, using a simple bilingual concordancer. A link type (u, v) was considered correct if u","@endWordPosition":"3071","@position":"17885","annotationId":"T37","@startWordPosition":"3070","@citStr":"Melamed, 1997"},{"#tail":"\n","#text":"signed the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many appl","@endWordPosition":"3520","@position":"20514","annotationId":"T38","@startWordPosition":"3519","@citStr":"Melamed (1997)"},{"#tail":"\n","#text":"nction words. This relatively simple two-class model linked word tokens in parallel texts as accurately as other trans- lation models in the literature, despite being trained on only one fifth as much data. Unlike other transla- tion models, the word-to-word model can automat- ically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and trans- lational entropy (Melamed, 1997). Another inter- esting extension is to broaden the definition of a &quot;word&quot; to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena. Acknowledgements The French/English software manuals were provided by Gary Adams of Sun MicroSystems Laboratories. The weather bitext was prepared at the University of Montreal, under the direction Of Richard Kit- tredge. Thanks to Alexis Nasr for hand-evaluating the wea","@endWordPosition":"4636","@position":"27366","annotationId":"T39","@startWordPosition":"4635","@citStr":"Melamed, 1997"}]},"title":{"#tail":"\n","#text":"Measuring Semantic Entropy,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"I D Melamed"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"note":{"#tail":"\n","#text":"(in this volume)"},"rawString":{"#tail":"\n","#text":"I. D. Melamed, &quot;A Portable Algorithm for Mapping Bitext Correspondence,&quot; Proceedings of the 35th Conference of the Association for Computational Linguistics, Madrid, Spain, 1997. (in this volume) A. Melby, &quot;A Bilingual Concordance System and its Use in Linguistic Studies,&quot; Proceedings of the English LACUS Forum, Columbia, SC, 1981."},"#text":"\n","marker":{"#tail":"\n","#text":"Melamed, 1997"},"location":{"#tail":"\n","#text":"Madrid,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"re each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n","@endWordPosition":"637","@position":"3985","annotationId":"T40","@startWordPosition":"636","@citStr":"Melamed, 1997"},{"#tail":"\n","#text":"ctively, out ofn(u,v) co-occurrences. The ratio of these prob- abilities is the likelihood ratio in favor of u and v being mutual translations, for all u and v: B(ku,vln<,,,,, ),+) L(u,v) = B(ku,vln~,v, A_ ) . (61 4 C lass -Based Word- to -Word Mode ls In the basic word-to-word model, the hidden param- eters A + and A- depend only on the distributions of link frequencies generated by the competitive link- ing algorithm. More accurate models can be induced by taking into account various features of the linked tokens. For example, frequent words are translated less consistently than rare words (Melamed, 1997). To account for this difference, we can estimate sep- arate values of X + and A- for different ranges of n(u,v). Similarly, the hidden parameters can be con- ditioned on the linked parts of speech. Word order can be taken into account by conditioning the hid- den parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with en- tries in a pre-existing translation lexicon separately 493 from those that do not. This method of incorporat- ing dictionary information seems simpler than the method proposed by Brown et","@endWordPosition":"2698","@position":"15517","annotationId":"T41","@startWordPosition":"2697","@citStr":"Melamed, 1997"},{"#tail":"\n","#text":" links and at (.43,.000094) for function-class links. 5.1 L ink Types The most direct way to evaluate the link types in a word-level model of translational equivalence is to treat each link type as a candidate translation lexi- con entry, and to measure precision and recall. This evaluation criterion carries much practical import, because many of the applications mentioned in Sec- tion 1 depend on accurate broad-coverage transla- tion lexicons. Machine readable bilingual dictionar- ies, even when they are available, have only limited coverage and rarely include domain-specific terms (Resnik & Melamed, 1997). We define the recall of a word-to-word translation model as the fraction of the bitext vocabulary repre- sented in the model. Translation model precision is a more thorny issue, because people disagree about the degree to which context should play a role in judgements of translational equivalence. We hand- evaluated the precision of the link types in our model in the context of the bitext from which the model 4Since function words can be identified by table look- up, no POS-tagger was involved. was induced, using a simple bilingual concordancer. A link type (u, v) was considered correct if u","@endWordPosition":"3071","@position":"17885","annotationId":"T42","@startWordPosition":"3070","@citStr":"Melamed, 1997"},{"#tail":"\n","#text":"signed the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many appl","@endWordPosition":"3520","@position":"20514","annotationId":"T43","@startWordPosition":"3519","@citStr":"Melamed (1997)"},{"#tail":"\n","#text":"nction words. This relatively simple two-class model linked word tokens in parallel texts as accurately as other trans- lation models in the literature, despite being trained on only one fifth as much data. Unlike other transla- tion models, the word-to-word model can automat- ically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and trans- lational entropy (Melamed, 1997). Another inter- esting extension is to broaden the definition of a &quot;word&quot; to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena. Acknowledgements The French/English software manuals were provided by Gary Adams of Sun MicroSystems Laboratories. The weather bitext was prepared at the University of Montreal, under the direction Of Richard Kit- tredge. Thanks to Alexis Nasr for hand-evaluating the wea","@endWordPosition":"4636","@position":"27366","annotationId":"T44","@startWordPosition":"4635","@citStr":"Melamed, 1997"}]},"title":{"#tail":"\n","#text":"A Portable Algorithm for Mapping Bitext Correspondence,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 35th Conference of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"I D Melamed"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"A. Nasr, personal communication, 1997."},"#text":"\n","marker":{"#tail":"\n","#text":"Nasr, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"alance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. 5.2 Link Tokens type of error errors made by errors made IBM Model 2 by our model wrong link missing link partial link class conflict tokenization paraphrase 32 12 7 3 39 7 36 10 5 2 36 TOTAL 93 96 Table 1: Erroneous link tokens generated by two translation mod","@endWordPosition":"3567","@position":"20815","annotationId":"T45","@startWordPosition":"3566","@citStr":"Nasr (1997)"}},"title":{"#tail":"\n","#text":"personal communication,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Nasr"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"P. Resnik & I. D. Melamed, &quot;Semi-Automatic A quisition of Domain-Specific Translation Lexicons,&quot; Proceedings of the 7th ACL Conference on Applied Natural Language Processing, Washington, DC, 1997."},"#text":"\n","marker":{"#tail":"\n","#text":"Resnik, Melamed, 1997"},"location":{"#tail":"\n","#text":"Washington, DC,"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ages, where each text is a translation of the other. Word co-occurrence an be defined in various ways. The most common way is to divide each half of the bitext into an equal number of seg- ments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a). Then, two word tokens (u, v) are said to co-occur in the 490 aligned segment pair i if u E Si and v E Ti. The co-occurrence r lation can also be based on distance in a bitext space, which is a more general represen- tations of bitext correspondence (Dagan et al, 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997). 3 The Bas ic Word- to -Word Mode l Our translation model consists of the hidden param- eters A + and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n","@endWordPosition":"637","@position":"3985","annotationId":"T46","@startWordPosition":"634","@citStr":"Resnik & Melamed, 1997"},{"#tail":"\n","#text":"ent-class links and at (.43,.000094) for function-class links. 5.1 L ink Types The most direct way to evaluate the link types in a word-level model of translational equivalence is to treat each link type as a candidate translation lexi- con entry, and to measure precision and recall. This evaluation criterion carries much practical import, because many of the applications mentioned in Sec- tion 1 depend on accurate broad-coverage transla- tion lexicons. Machine readable bilingual dictionar- ies, even when they are available, have only limited coverage and rarely include domain-specific terms (Resnik & Melamed, 1997). We define the recall of a word-to-word translation model as the fraction of the bitext vocabulary repre- sented in the model. Translation model precision is a more thorny issue, because people disagree about the degree to which context should play a role in judgements of translational equivalence. We hand- evaluated the precision of the link types in our model in the context of the bitext from which the model 4Since function words can be identified by table look- up, no POS-tagger was involved. was induced, using a simple bilingual concordancer. A link type (u, v) was considered correct if u","@endWordPosition":"3071","@position":"17885","annotationId":"T47","@startWordPosition":"3068","@citStr":"Resnik & Melamed, 1997"},{"#tail":"\n","#text":"ord is assigned the same unit of probability mass, which the model dis- tributes over all candidate translations. The correct translations of a word that has several correct rans- lations will be assigned a lower probability than the correct translation of a word that has only one cor- rect translation. This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu ~z Xia, 1994; Chen, 1996). The likelihoods in the word-to-word model remain unnormalized, so they do not compete. The word-to-word model maintains high preci- sion even given much less training data. Resnik & Melamed (1997) report that the model produced 494 translation lexicons with 94% precision and 30% re- call, when trained on French/English software man- uals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather e- ports. Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many appl","@endWordPosition":"3520","@position":"20514","annotationId":"T48","@startWordPosition":"3517","@citStr":"Resnik & Melamed (1997)"}]},"title":{"#tail":"\n","#text":"Semi-Automatic A quisition of Domain-Specific Translation Lexicons,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 7th ACL Conference on Applied Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P Resnik"},{"#tail":"\n","#text":"I D Melamed"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"UMIACS TR-96-19,"},"date":{"#tail":"\n","#text":"1996"},"institution":{"#tail":"\n","#text":"University of Maryland,"},"rawString":{"#tail":"\n","#text":"D. W. Oard & B. J. Dorr, &quot;A Survey of Multilingual Text Retrieval, UMIACS TR-96-19, University of Maryland, College Park, MD, 1996."},"#text":"\n","marker":{"#tail":"\n","#text":"Oard, Dorr, 1996"},"location":{"#tail":"\n","#text":"College Park, MD,"},"title":{"#tail":"\n","#text":"A Survey of Multilingual Text Retrieval,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D W Oard"},{"#tail":"\n","#text":"B J Dorr"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1992"},"rawString":{"#tail":"\n","#text":"F. Smadja, &quot;How to Compile a Bilingual Collocational Lexicon Automatically,&quot; Proceedings of the AAAI Workshop on Statistically-Based NLP Techniques, 1992."},"#text":"\n","marker":{"#tail":"\n","#text":"Smadja, 1992"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ns of a bitext. If uk and vk co-occur much more often than expected by chance, then any reasonable model will deem them likely to be mutual translations. If uk and Vk are indeed mutual translations, then their tendency to ZThe co-occurrence frequency of a word type pair is simply the number of times the pair co-occurs in the corpus. However, n(u) = ~-~v n(u.v), which is not the same as the frequency of u, because ach token of u can co-occur with several differentv's. 2We could just as easily use other symmetric &quot;asso- ciation&quot; measures, uch as ?2 (Gale & Church, 1991) or the Dice coefficient (Smadja, 1992). ? ? ? Uk . 1 tJk ~ = Uk+l ? ? ? t ? , ? Vk . 1 Vk Vk+l ? . . Figure 1: Uk and vk often co-occur, as do uk and uk+z. The direct association between uk and vk, and the direct association between uk and Uk+l give rise to an indirect association between v~ and uk+l. co-occur is called a direct associat ion. Now, sup- pose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connect- ing vk and u~+l in Figure 1 represents an indi rect associat ion, since the association between vk and Uk+z arises only by virtue of","@endWordPosition":"992","@position":"6097","annotationId":"T49","@startWordPosition":"991","@citStr":"Smadja, 1992"},{"#tail":"\n","#text":" models in the literature, despite being trained on only one fifth as much data. Unlike other transla- tion models, the word-to-word model can automat- ically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and trans- lational entropy (Melamed, 1997). Another inter- esting extension is to broaden the definition of a &quot;word&quot; to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena. Acknowledgements The French/English software manuals were provided by Gary Adams of Sun MicroSystems Laboratories. The weather bitext was prepared at the University of Montreal, under the direction Of Richard Kit- tredge. Thanks to Alexis Nasr for hand-evaluating the weather translation lexicon. Thanks also to Mike Collins, George Foster, Mitch Marcus, Lyle Ungar, and three anonymous reviewers","@endWordPosition":"4655","@position":"27491","annotationId":"T50","@startWordPosition":"4654","@citStr":"Smadja, 1992"}]},"title":{"#tail":"\n","#text":"How to Compile a Bilingual Collocational Lexicon Automatically,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the AAAI Workshop on Statistically-Based NLP Techniques,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"F Smadja"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"D. Wu & X. Xia, &quot;Learning an English-Chinese Lexicon from a Parallel Corpus,&quot; Proceedings of the First Conference of the Association for Machine Translation in the Americas, Columbia, MD, 1994."},"#text":"\n","marker":{"#tail":"\n","#text":"Wu, Xia, 1994"},"location":{"#tail":"\n","#text":"Columbia, MD,"},"title":{"#tail":"\n","#text":"Learning an English-Chinese Lexicon from a Parallel Corpus,&quot;"},"booktitle":{"#tail":"\n","#text":"Proceedings of the First Conference of the Association for Machine Translation in the Americas,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Wu"},{"#tail":"\n","#text":"X Xia"}]}}]}}]}}
