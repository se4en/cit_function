{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Martin Kay. 1996. Chart Generation. In Proceedings of ACL-96, pages 200-204."},"#text":"\n","pages":{"#tail":"\n","#text":"200--204"},"marker":{"#tail":"\n","#text":"Kay, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cles. The system consists of a base generator using a rule-based grammar to produce candidates, potential outputs of the system. The grammar is automatically derived from the semantically markedup training set in combination with the corresponding Penn treebank structures. The ranker scores these candidates according to a similarity metric which measures their distance to the elements in the instance base. The ranks are determined by the similarity to the closest instances and the highest ranked sentence is chosen as the final generation output. IGEN uses standard chart generation techniques (Kay, 1996) in its base generator to efficiently produce generation candidates. The set of candidates forms a subset of the chart edges in that they comprise all edges of syntactic category S. The particular advantage of a bottom-up generation algorithm is that it allows our surface-based ranker to score edges as soon as they are built. 3 Semantic Annotation The MUC-6 IE task consisted of extracting information about incoming and outgoing person and the post name. Generating the actual texts requires considerably more input than what was extracted in the MUC-6 task. We collected the first sentences of 14","@endWordPosition":"1130","@position":"7248","annotationId":"T1","@startWordPosition":"1129","@citStr":"Kay, 1996"}},"title":{"#tail":"\n","#text":"Chart Generation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL-96,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Martin Kay"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Irene Langkilde. 2000. Forest-based Statistical Sentence Generation. In Proceedings of NAACL-00, pages 170-177."},"#text":"\n","pages":{"#tail":"\n","#text":"170--177"},"marker":{"#tail":"\n","#text":"Langkilde, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"mputational cost of computing the similarity between every new edge and the entire instance base, we have to consider two important characteristics of the search problem. First, edges have a number of cosine scores from different instances. If we want to use a Viterbi-style algorithm which defines equivalence classes for chart edges and only keeps the current best one for each class in a table, we need to maintain a separate table for each instance. This is because we cannot combine scores from different instances. In contrast, a single statistical model allows one to maintain a single table (Langkilde, 2000). Second, the cosine score of an edge can increase as well as decrease when it is combined with other edges, depending on whether or not the newly added terms match the instance terms. Thus, the cosine score does not exhibit a monotonic behaviour that could be straightforwardly exploited. We can, however, define the notion of the expectation of an edge that is monotonically decreasing. This is the potentially best cosine score an edge would have with respect to a specific instance if all missing matching words are added to it. To see its monotonic behaviour consider the following: The expectat","@endWordPosition":"2875","@position":"18170","annotationId":"T2","@startWordPosition":"2874","@citStr":"Langkilde, 2000"}},"title":{"#tail":"\n","#text":"Forest-based Statistical Sentence Generation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of NAACL-00,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Irene Langkilde"}}}]}}}}
