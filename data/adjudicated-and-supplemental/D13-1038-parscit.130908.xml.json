{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Kotaro Funakoshi, Satoru Watanabe, and Takenobu Tokunaga. 2006. Group-based generation of referring expressions. In INLG, pages 73\u201380."},"#text":"\n","pages":{"#tail":"\n","#text":"73--80"},"marker":{"#tail":"\n","#text":"Funakoshi, Watanabe, Tokunaga, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automat","@endWordPosition":"724","@position":"4856","annotationId":"T1","@startWordPosition":"721","@citStr":"Funakoshi et al., 2006"},{"#tail":"\n","#text":"ognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs to describe the target object 5 in Figure 1(b), he/she may have to start by indicating the group of three objects at the bottom and then specify the relationship (i.e., top) of the target object within this group. The importance of group descriptions has been shown not only here, but also in previous works on REG (Funakoshi et al., 2004; Funakoshi et al., 2006; Weijers, 2011). While the original graphbased approach can effectively represent attributes and binary relations between objects (Krahmer et al., 2003), it is insufficient to capture within-group or between-group relations. Therefore, to address the low perceptual capabilities of artificial agents, we introduce hypergraphs to represent the shared environment. Our approach has two unique characteristics compared to previous graph-based approaches: (1) A hypergraph representation is more general than a regular graph. Besides attributes and binary relations, it can also represent group-based re","@endWordPosition":"1641","@position":"10713","annotationId":"T2","@startWordPosition":"1638","@citStr":"Funakoshi et al., 2006"}]},"title":{"#tail":"\n","#text":"Group-based generation of referring expressions."},"booktitle":{"#tail":"\n","#text":"In INLG,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kotaro Funakoshi"},{"#tail":"\n","#text":"Satoru Watanabe"},{"#tail":"\n","#text":"Takenobu Tokunaga"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Kotaro Funakoshi, Satoru Watanabe, and Takenobu Tokunaga. 2006. Group-based generation of referring expressions. In INLG, pages 73\u201380."},"#text":"\n","pages":{"#tail":"\n","#text":"73--80"},"marker":{"#tail":"\n","#text":"Funakoshi, Watanabe, Tokunaga, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automat","@endWordPosition":"724","@position":"4856","annotationId":"T3","@startWordPosition":"721","@citStr":"Funakoshi et al., 2006"},{"#tail":"\n","#text":"ognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs to describe the target object 5 in Figure 1(b), he/she may have to start by indicating the group of three objects at the bottom and then specify the relationship (i.e., top) of the target object within this group. The importance of group descriptions has been shown not only here, but also in previous works on REG (Funakoshi et al., 2004; Funakoshi et al., 2006; Weijers, 2011). While the original graphbased approach can effectively represent attributes and binary relations between objects (Krahmer et al., 2003), it is insufficient to capture within-group or between-group relations. Therefore, to address the low perceptual capabilities of artificial agents, we introduce hypergraphs to represent the shared environment. Our approach has two unique characteristics compared to previous graph-based approaches: (1) A hypergraph representation is more general than a regular graph. Besides attributes and binary relations, it can also represent group-based re","@endWordPosition":"1641","@position":"10713","annotationId":"T4","@startWordPosition":"1638","@citStr":"Funakoshi et al., 2006"}]},"title":{"#tail":"\n","#text":"Group-based generation of referring expressions."},"booktitle":{"#tail":"\n","#text":"In INLG,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kotaro Funakoshi"},{"#tail":"\n","#text":"Satoru Watanabe"},{"#tail":"\n","#text":"Takenobu Tokunaga"}]}},{"date":{"#tail":"\n","#text":"2008"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"es. Second, in situated dialogue the agent and the human have mismatched representations of the environment. The agent needs to take this difference into consideration to identify the most reliable features for REG. Given these two distinctions, it is not clear whether state-of-the-art REG approaches are applicable under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.","@endWordPosition":"677","@position":"4537","annotationId":"T5","@startWordPosition":"674","@citStr":"Gatt and Belz, 2008"},{"#tail":"\n","#text":"tly outperforms the regular graphs by taking advantage of spatial grouping information (p = 0.002). It is worthwhile to mention that currently we use spatial proximity to identify groups. However, the hypergraph based approach is not restricted to spatial grouping. In theory, it can represent any type of group based on different similarity criteria. Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets, this result is consistent with results from previous work (Gatt and Belz, 2008; Gatt et al., 2009). However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to Environment Regular Graph Hypergraph Pefect Perception 80.4% 84.2% Imperfect Perception 36.7% 45.2% Table 2: Results of comparing perfect perception and imperfect perception of the shared world. the situation where the agent\u2019s representation of the shared world is problematic and full of mistakes. These results i","@endWordPosition":"4722","@position":"29928","annotationId":"T6","@startWordPosition":"4719","@citStr":"Gatt and Belz, 2008"}]},"title":{"#tail":"\n","#text":"Attribute selection for referring expression generation: new algorithms and evaluation methods."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Albert Gatt and Anja Belz. 2008. Attribute selection for referring expression generation: new algorithms and evaluation methods. In Proceedings of the Fifth International Natural Language Generation Conference, INLG \u201908, pages 50\u201358, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"50--58"},"marker":{"#tail":"\n","#text":"Gatt, Belz, 2008"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth International Natural Language Generation Conference, INLG \u201908,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Albert Gatt"},{"#tail":"\n","#text":"Anja Belz"}]}},{"date":{"#tail":"\n","#text":"2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ed dialogue the agent and the human have mismatched representations of the environment. The agent needs to take this difference into consideration to identify the most reliable features for REG. Given these two distinctions, it is not clear whether state-of-the-art REG approaches are applicable under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it perform","@endWordPosition":"681","@position":"4557","annotationId":"T7","@startWordPosition":"678","@citStr":"Gatt et al., 2009"},{"#tail":"\n","#text":"egular graphs by taking advantage of spatial grouping information (p = 0.002). It is worthwhile to mention that currently we use spatial proximity to identify groups. However, the hypergraph based approach is not restricted to spatial grouping. In theory, it can represent any type of group based on different similarity criteria. Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets, this result is consistent with results from previous work (Gatt and Belz, 2008; Gatt et al., 2009). However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to Environment Regular Graph Hypergraph Pefect Perception 80.4% 84.2% Imperfect Perception 36.7% 45.2% Table 2: Results of comparing perfect perception and imperfect perception of the shared world. the situation where the agent\u2019s representation of the shared world is problematic and full of mistakes. These results indicate that REG for","@endWordPosition":"4726","@position":"29948","annotationId":"T8","@startWordPosition":"4723","@citStr":"Gatt et al., 2009"}]},"title":{"#tail":"\n","#text":"The tunareg challenge 2009: overview and evaluation results."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Albert Gatt, Anja Belz, and Eric Kow. 2009. The tunareg challenge 2009: overview and evaluation results. In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG \u201909, pages 174\u2013 182, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"174--182"},"marker":{"#tail":"\n","#text":"Gatt, Belz, Kow, 2009"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG \u201909,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Albert Gatt"},{"#tail":"\n","#text":"Anja Belz"},{"#tail":"\n","#text":"Eric Kow"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Albert Gatt. 2006. Structuring knowledge for reference generation: A clustering algorithm. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics, pages 321\u2013328."},"#text":"\n","pages":{"#tail":"\n","#text":"321--328"},"marker":{"#tail":"\n","#text":"Gatt, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"he first heuristic is based on perceptual principles, also called the Gestalt Laws of perception (Sternberg, 2003), which describe how people group visually similar objects into entities or groups. Two well known principles of perceptual grouping are proximity and similarity (Wertheimer, 1938): objects that lie close together are often perceived as groups; objects of similar shape, size or color are more likely to form groups than objects differing along these dimensions. Based on these two principles, previous works have developed different algorithms for perceptual grouping (Thrisson, 1994; Gatt, 2006). In our investigation, we adopted Gatt\u2019s algorithm (Gatt, 2006), which has shown to be more accurate for spatial grouping. Given the results from spatial grouping, we only retain hyperarcs that represent spatial relations between two objects, between two perceived groups, between one object and a perceived group, or between one object and the group it belongs to. The second heuristic is based on the observation that, given a certain orientation, people tend to use a relatum that is closer to the referent than more distant relata. In other words, it is less likely to refer to an object relativ","@endWordPosition":"2340","@position":"14978","annotationId":"T9","@startWordPosition":"2339","@citStr":"Gatt, 2006"}},"title":{"#tail":"\n","#text":"Structuring knowledge for reference generation: A clustering algorithm."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Albert Gatt"}}},{"date":{"#tail":"\n","#text":"2012"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"and make inference of the shared environment. Due to its limited perceptual and reasoning capabilities, the robot\u2019s representation of the shared world is often incomplete, error-prone, and significantly mismatched from that of its human partner\u2019s. Although physically co-present, a joint perceptual basis between the human and the robot cannot be established (Clark and Brennan, 1991). Thus, referential communication between the human and the robot becomes difficult. How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (Liu et al., 2012). In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human pa","@endWordPosition":"290","@position":"2053","annotationId":"T10","@startWordPosition":"287","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"tual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can b","@endWordPosition":"728","@position":"4875","annotationId":"T11","@startWordPosition":"725","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"certainties associated with automated perception of the environment. The reason we chose a graph-based approach is that graph representations are widely used in the fields of computer vision (CV) and pattern recognition to represent spatially rich scenes. Nevertheless, the findings from this investigation provide insight to other approaches. 393 (a) an original scene (b) the corresponding impoverished scene Figure 1: An original scene and its impoverished scene processed by CV algorithm 3 Hypergraph-based REG Towards mediating a shared perceptual basis in situated dialogue, our previous work (Liu et al., 2012) has conducted experiments to study referential communication between partners with mismatched perceptual capabilities. We simulated mismatched capabilities by making an original scene (Figure 1(a)) available to a director (simulating higher perceptual calibre) and a corresponding impoverished scene (Figure 1(b)) available to a matcher (simulating lowered perceptual calibre). The impoverished scene is created by re-rendering automated recognition results of the original scene by a CV algorithm. An example of the original scene and an impoverished scene is shown in Figure 1. Using this setup, t","@endWordPosition":"1292","@position":"8504","annotationId":"T12","@startWordPosition":"1289","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"e matcher were instructed to collaborate with each other on some naming games. Through these games, they collected data on how partners with mismatched perceptual capabilities collaborate to ground their referential communication. The setup in (Liu et al., 2012) is intended to simulate situated dialogue between a human (like the director) and a robot (like the matcher). The robot has a significantly lowered ability in perception and reasoning. The robot\u2019s internal representation of the shared world will be much like the impoverished scene which contains many recognition errors. The data from (Liu et al., 2012; Liu et al., 2013) shows that different strategies were used by conversation partners to produce referential descriptions. Besides directly describing attributes or binary relations with a relatum, they often use group-based descriptions (e.g., a cluster offour objects on the right). This is mainly due to the fact that some objects are simply not recognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the m","@endWordPosition":"1480","@position":"9738","annotationId":"T13","@startWordPosition":"1477","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"o assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or blue have different means and co-variances as the following: color : red = fr(vcolor) = N(vcolor |A1, E1) color : green = fg(vcolor) = N(vcolor |A2, E2) color : blue = fb(vcolor) = N(vcolor","@endWordPosition":"2684","@position":"17060","annotationId":"T14","@startWordPosition":"2681","@citStr":"Liu et al., 2012"}]},"title":{"#tail":"\n","#text":"Towards mediating shared perceptual basis in situated dialogue."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Changsong Liu, Rui Fang, and Joyce Y. Chai. 2012. Towards mediating shared perceptual basis in situated dialogue. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL \u201912, pages 140\u2013149, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"140--149"},"marker":{"#tail":"\n","#text":"Liu, Fang, Chai, 2012"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL \u201912,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Changsong Liu"},{"#tail":"\n","#text":"Rui Fang"},{"#tail":"\n","#text":"Joyce Y Chai"}]}},{"date":{"#tail":"\n","#text":"2012"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"and make inference of the shared environment. Due to its limited perceptual and reasoning capabilities, the robot\u2019s representation of the shared world is often incomplete, error-prone, and significantly mismatched from that of its human partner\u2019s. Although physically co-present, a joint perceptual basis between the human and the robot cannot be established (Clark and Brennan, 1991). Thus, referential communication between the human and the robot becomes difficult. How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (Liu et al., 2012). In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human pa","@endWordPosition":"290","@position":"2053","annotationId":"T15","@startWordPosition":"287","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"tual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can b","@endWordPosition":"728","@position":"4875","annotationId":"T16","@startWordPosition":"725","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"certainties associated with automated perception of the environment. The reason we chose a graph-based approach is that graph representations are widely used in the fields of computer vision (CV) and pattern recognition to represent spatially rich scenes. Nevertheless, the findings from this investigation provide insight to other approaches. 393 (a) an original scene (b) the corresponding impoverished scene Figure 1: An original scene and its impoverished scene processed by CV algorithm 3 Hypergraph-based REG Towards mediating a shared perceptual basis in situated dialogue, our previous work (Liu et al., 2012) has conducted experiments to study referential communication between partners with mismatched perceptual capabilities. We simulated mismatched capabilities by making an original scene (Figure 1(a)) available to a director (simulating higher perceptual calibre) and a corresponding impoverished scene (Figure 1(b)) available to a matcher (simulating lowered perceptual calibre). The impoverished scene is created by re-rendering automated recognition results of the original scene by a CV algorithm. An example of the original scene and an impoverished scene is shown in Figure 1. Using this setup, t","@endWordPosition":"1292","@position":"8504","annotationId":"T17","@startWordPosition":"1289","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"e matcher were instructed to collaborate with each other on some naming games. Through these games, they collected data on how partners with mismatched perceptual capabilities collaborate to ground their referential communication. The setup in (Liu et al., 2012) is intended to simulate situated dialogue between a human (like the director) and a robot (like the matcher). The robot has a significantly lowered ability in perception and reasoning. The robot\u2019s internal representation of the shared world will be much like the impoverished scene which contains many recognition errors. The data from (Liu et al., 2012; Liu et al., 2013) shows that different strategies were used by conversation partners to produce referential descriptions. Besides directly describing attributes or binary relations with a relatum, they often use group-based descriptions (e.g., a cluster offour objects on the right). This is mainly due to the fact that some objects are simply not recognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the m","@endWordPosition":"1480","@position":"9738","annotationId":"T18","@startWordPosition":"1477","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"o assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or blue have different means and co-variances as the following: color : red = fr(vcolor) = N(vcolor |A1, E1) color : green = fg(vcolor) = N(vcolor |A2, E2) color : blue = fb(vcolor) = N(vcolor","@endWordPosition":"2684","@position":"17060","annotationId":"T19","@startWordPosition":"2681","@citStr":"Liu et al., 2012"}]},"title":{"#tail":"\n","#text":"Towards mediating shared perceptual basis in situated dialogue."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Changsong Liu, Rui Fang, and Joyce Y. Chai. 2012. Towards mediating shared perceptual basis in situated dialogue. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL \u201912, pages 140\u2013149, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"140--149"},"marker":{"#tail":"\n","#text":"Liu, Fang, Chai, 2012"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL \u201912,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Changsong Liu"},{"#tail":"\n","#text":"Rui Fang"},{"#tail":"\n","#text":"Joyce Y Chai"}]}},{"date":{"#tail":"\n","#text":"2012"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"and make inference of the shared environment. Due to its limited perceptual and reasoning capabilities, the robot\u2019s representation of the shared world is often incomplete, error-prone, and significantly mismatched from that of its human partner\u2019s. Although physically co-present, a joint perceptual basis between the human and the robot cannot be established (Clark and Brennan, 1991). Thus, referential communication between the human and the robot becomes difficult. How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (Liu et al., 2012). In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human pa","@endWordPosition":"290","@position":"2053","annotationId":"T20","@startWordPosition":"287","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"tual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can b","@endWordPosition":"728","@position":"4875","annotationId":"T21","@startWordPosition":"725","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"certainties associated with automated perception of the environment. The reason we chose a graph-based approach is that graph representations are widely used in the fields of computer vision (CV) and pattern recognition to represent spatially rich scenes. Nevertheless, the findings from this investigation provide insight to other approaches. 393 (a) an original scene (b) the corresponding impoverished scene Figure 1: An original scene and its impoverished scene processed by CV algorithm 3 Hypergraph-based REG Towards mediating a shared perceptual basis in situated dialogue, our previous work (Liu et al., 2012) has conducted experiments to study referential communication between partners with mismatched perceptual capabilities. We simulated mismatched capabilities by making an original scene (Figure 1(a)) available to a director (simulating higher perceptual calibre) and a corresponding impoverished scene (Figure 1(b)) available to a matcher (simulating lowered perceptual calibre). The impoverished scene is created by re-rendering automated recognition results of the original scene by a CV algorithm. An example of the original scene and an impoverished scene is shown in Figure 1. Using this setup, t","@endWordPosition":"1292","@position":"8504","annotationId":"T22","@startWordPosition":"1289","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"e matcher were instructed to collaborate with each other on some naming games. Through these games, they collected data on how partners with mismatched perceptual capabilities collaborate to ground their referential communication. The setup in (Liu et al., 2012) is intended to simulate situated dialogue between a human (like the director) and a robot (like the matcher). The robot has a significantly lowered ability in perception and reasoning. The robot\u2019s internal representation of the shared world will be much like the impoverished scene which contains many recognition errors. The data from (Liu et al., 2012; Liu et al., 2013) shows that different strategies were used by conversation partners to produce referential descriptions. Besides directly describing attributes or binary relations with a relatum, they often use group-based descriptions (e.g., a cluster offour objects on the right). This is mainly due to the fact that some objects are simply not recognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the m","@endWordPosition":"1480","@position":"9738","annotationId":"T23","@startWordPosition":"1477","@citStr":"Liu et al., 2012"},{"#tail":"\n","#text":"o assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or blue have different means and co-variances as the following: color : red = fr(vcolor) = N(vcolor |A1, E1) color : green = fg(vcolor) = N(vcolor |A2, E2) color : blue = fb(vcolor) = N(vcolor","@endWordPosition":"2684","@position":"17060","annotationId":"T24","@startWordPosition":"2681","@citStr":"Liu et al., 2012"}]},"title":{"#tail":"\n","#text":"Towards mediating shared perceptual basis in situated dialogue."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Changsong Liu, Rui Fang, and Joyce Y. Chai. 2012. Towards mediating shared perceptual basis in situated dialogue. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL \u201912, pages 140\u2013149, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"140--149"},"marker":{"#tail":"\n","#text":"Liu, Fang, Chai, 2012"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL \u201912,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Changsong Liu"},{"#tail":"\n","#text":"Rui Fang"},{"#tail":"\n","#text":"Joyce Y Chai"}]}}]}}}}
