{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","note":{"#tail":"\n","@confidence":"0.583509","#text":"\nProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 536?543,\n"},"listItem":{"#tail":"\n","@confidence":"0.9370175","#text":"\n(1) WORD: For each word w in NPi, we create a\nWORD feature whose value is equal to w. No fea-\ntures are created from stopwords, however.\n(2) SUBJ VERB: If NPi is involved in a subject-verb relation, we create a SUBJ VERB feature whose\nvalue is the verb participating in the relation. We\nuse Lin?s (1998b) MINIPAR dependency parser to\nextract grammatical relations. Our motivation here\nis to coarsely model subcategorization.\n(3) VERB OBJ: A VERB OBJ feature is created in\na similar fashion as SUBJ VERB if NPi participatesin a verb-object relation. Again, this represents our\nattempt to coarsely model subcategorization.\n(4) NE: We use BBN?s IdentiFinder (Bikel et al,\n1999), a MUC-style NE recognizer to determine the\nNE type of NPi. If NPi is determined to be a PERSONor ORGANIZATION, we create an NE feature whose\nvalue is simply its MUC NE type. However, if NPiis determined to be a LOCATION, we create a feature\nwith value GPE (because most of the MUC LOCA-\n"},"figure":{"#tail":"\n","@confidence":"0.96068156","#text":"\nSystem Variation R P F R P F R P F R P F\n1 Baseline system 60.9 53.6 57.0 ? ? ? ? ? ? ? ? ?\n2 Duplicated Soon et al 56.1 54.4 55.3 ? ? ? ? ? ? ? ? ?\nAdd to the Baseline Soon?s SC Method Decision List SVM Perfect Information\n3 Mention(C) only 56.9 69.7 62.6 59.5 70.6 64.6 59.5 70.7 64.6 61.2 83.1 70.5\n4 Mention(F) only 60.9 54.0 57.2 61.2 52.9 56.7 60.9 53.6 57.0 62.3 33.7 43.8\n5 SCA(C) only 56.4 70.0 62.5 57.7 71.2 63.7 58.9 70.7 64.3 61.3 86.1 71.6\n6 SCA(F) only 62.0 52.8 57.0 62.5 53.5 57.6 63.0 53.3 57.7 71.1 33.0 45.1\n7 Mention(C) + SCA(C) 56.4 70.0 62.5 57.7 71.2 63.7 58.9 70.8 64.3 61.3 86.1 71.6\n8 Mention(C) + SCA(F) 58.2 66.4 62.0 60.9 66.8 63.7 61.4 66.5 63.8 71.1 76.7 73.8\n9 Mention(F) + SCA(C) 56.4 69.8 62.4 57.7 71.3 63.8 58.9 70.6 64.3 62.7 85.3 72.3\n10 Mention(F) + SCA(F) 62.0 52.7 57.0 62.6 52.8 57.3 63.2 52.6 57.4 71.8 30.3 42.6\nTable 6: Coreference results obtained via the MUC scoring program for the ACE test set.\nSystem Variation PRO PN CN All PRO PN CN All PRO PN CN All\n1 Baseline system 59.2 54.8 22.5 48.4 ? ? ? ? ? ? ? ?\n2 Duplicated Soon et al 53.4 45.7 16.9 41.4 ? ? ? ? ? ? ? ?\nAdd to the Baseline Soon?s SC Method Decision List SVM\n3 Mention(C) only 58.5 51.3 16.5 45.3 59.1 54.1 20.2 47.5 59.1 53.9 20.6 47.5\n4 Mention(F) only 59.2 55.0 22.5 48.5 59.2 56.1 22.4 48.8 59.4 55.2 22.6 48.6\n5 SCA(C) only 58.1 50.1 16.4 44.7 58.1 51.8 17.1 45.5 58.5 52.0 19.6 46.3\n6 SCA(F) only 59.2 54.9 27.8 49.7 60.4 56.7 30.1 51.5 60.8 56.4 29.4 51.3\n7 Mention(C) + SCA(C) 58.1 50.1 16.4 44.7 58.1 51.8 17.1 45.5 58.5 51.9 19.5 46.3\n8 Mention(C) + SCA(F) 58.9 52.0 22.3 47.2 60.2 55.9 28.1 50.6 60.7 55.3 27.4 50.4\n9 Mention(F) + SCA(C) 58.1 50.3 16.3 44.8 58.1 52.4 16.7 45.6 58.6 52.4 19.7 46.6\n10 Mention(F) + SCA(F) 59.2 55.0 27.6 49.7 60.4 56.8 30.1 51.5 60.8 56.5 29.5 51.4\n"},"address":{"#tail":"\n","@confidence":"0.962364","#text":"\nRichardson, TX 75083-0688\n"},"author":{"#tail":"\n","@confidence":"0.851704","#text":"\nVincent Ng\n"},"subsectionHeader":[{"#tail":"\n","@confidence":"0.995096","#text":"\n3.1 Training the Classifier\n"},{"#tail":"\n","@confidence":"0.997496","#text":"\n3.2 Evaluating the Classifiers\n"},{"#tail":"\n","@confidence":"0.997943","#text":"\n4.1 Experimental Setup\n"},{"#tail":"\n","@confidence":"0.998136","#text":"\n4.2 The Baseline Coreference System\n"},{"#tail":"\n","@confidence":"0.995896","#text":"\n4.3 Coreference with Induced SC Knowledge\n"}],"footnote":[{"#tail":"\n","@confidence":"0.980824454545455","#text":"\n1This is motivated by Lin?s (1998c) observation that a coref-\nerence resolver that employs only the first WordNet sense per-\nforms slightly better than one that employs more than one sense.\n2The keywords are obtained via our experimentation with\nWordNet and the ACE SCs of the NPs in the ACE training data.\n3We used (1) the BLLIP corpus (30M words), which con-\nsists of WSJ articles from 1987 to 1989, and (2) the Reuters\nCorpus (3.7GB data), which has 806,791 Reuters articles.\n4Person, organization, location, date, time, money, percent.\n5This indicates the proper noun is not a MUC NE.\n6For simplicity, OTHERS is viewed as an NE type here.\n"},{"#tail":"\n","@confidence":"0.91135625","#text":"\nSoon et al?s method to independently make predic-\n7See http://www.cs.ualberta.ca/?lindek/downloads.htm\n8In our implementation of Soon?s method, we label an in-\nstance as OTHERS if no NE or WN CLASS feature is generated;\n"}],"construct":{"#tail":"\n","@confidence":"0.6826985","#text":"\nFACILITY establishment, construction, building, facil-\nity, workplace\nGPE country, province, government, town, city,\nadministration, society, island, community\nLOCATION dry land, region, landmass, body of water,\ngeographical area, geological formation\n"},"title":{"#tail":"\n","@confidence":"0.2729695","#text":"\nPrague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics\nSemantic Class Induction and Coreference Resolution\n"},"@confidence":"0.000003","reference":{"#tail":"\n","@confidence":"0.9982561","#text":"\nD. Bean and E. Riloff. 2004. Unsupervised learning of contex-\ntual role knowledge for coreference resolution. In Proc. of\nHLT/NAACL, pages 297?304.\nD. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999. An\nalgorithm that learns what?s in a name. Machine Learning\n34(1?3):211?231.\nC.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library\nfor support vector machines. Software available at\nhttp://www.csie.ntu.edu.tw/?cjlin/libsvm.\nM. Collins and Y. Singer. 1999. Unsupervised models for\nnamed entity classification. In Proc. of EMNLP/VLC.\nW. Daelemans, J. Zavrel, K. van der Sloot, and A. van den\nBosch. 2004. TiMBL: Tilburg Memory Based Learner, ver-\nsion 5.1, Reference Guide. ILK Technical Report.\nH. Daume? III and D. Marcu. 2005. A large-scale exploration\nof effective global features for a joint entity detection and\ntracking model. In Proc. of HLT/EMNLP, pages 97?104.\nR. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006. Fac-\ntorizing complex models: A case study in mention detection.\nIn Proc. of COLING/ACL, pages 473?480.\nM. Hearst. 1992. Automatic acquisition of hyponyms from\nlarge text corpora. In Proc. of COLING.\nH. Ji, D. Westbrook, and R. Grishman. 2005. Using seman-\ntic relations to refine coreference decisions. In Proc. of\nHLT/EMNLP, pages 17?24.\nA. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The\n(non)utility of predicate-argument frequencies for pronoun\ninterpretation. In Proc. of NAACL, pages 289?296.\nD. Lin. 1998a. Automatic retrieval and clustering of similar\nwords. In Proc. of COLING/ACL, pages 768?774.\nD. Lin. 1998b. Dependency-based evaluation of MINIPAR. In\nProc. of the LREC Workshop on the Evaluation of Parsing\nSystems, pages 48?56.\nD. Lin. 1998c. Using collocation statistics in information ex-\ntraction. In Proc. of MUC-7.\nX. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.\n2004. A mention-synchronous coreference resolution algo-\nrithm based on the Bell tree. In Proc. of the ACL.\nK. Markert and M. Nissim. 2005. Comparing knowledge\nsources for nominal anaphora resolution. Computational\nLinguistics, 31(3):367?402.\nR. Mitkov. 2002. Anaphora Resolution. Longman.\nR. Mitkov. 1998. Robust pronoun resolution with limited\nknowledge. In Proc. of COLING/ACL, pages 869?875.\nV. Ng and C. Cardie. 2002. Improving machine learning ap-\nproaches to coreference resolution. In Proc. of the ACL.\nE. W. Noreen. 1989. Computer Intensive Methods for Testing\nHypothesis: An Introduction. John Wiley & Sons.\nM. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.\nLearning to resolve bridging references. In Proc. of the ACL.\nS. P. Ponzetto and M. Strube. 2006. Exploiting semantic role\nlabeling, WordNet and Wikipedia for coreference resolution.\nIn Proc. of HLT/NAACL, pages 192?199.\nJ. R. Quinlan. 1993. C4.5: Programs for Machine Learning.\nMorgan Kaufmann, San Mateo, CA.\nW. M. Soon, H. T. Ng, and D. Lim. 2001. A machine learning\napproach to coreference resolution of noun phrases. Compu-\ntational Linguistics, 27(4):521?544.\nJ. Tetreault. 2001. A corpus-based evaluation of centering and\npronoun resolution. Computational Linguistics, 27(4).\nM. Vilain, J. Burger, J. Aberdeen, D. Connolly, and\nL. Hirschman. 1995. A model-theoretic coreference scor-\ning scheme. In Proc. of MUC-6, pages 45?52.\nR. Weischedel and A. Brunstein. 2005. BBN pronoun corefer-\nence and entity type corpus. Linguistica Data Consortium.\nX. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference\nresolution using competitive learning approach. In Proc. of\nthe ACL, pages 176?183.\nD. Yarowsky. 1995. Unsupervised word sense disambiguation\nrivaling supervised methods. In Proc. of the ACL.\n"},"#tail":"\n","bodyText":[{"#tail":"\n","@confidence":"0.998747928571429","#text":"\nThis paper examines whether a learning-\nbased coreference resolver can be improved\nusing semantic class knowledge that is au-\ntomatically acquired from a version of the\nPenn Treebank in which the noun phrases\nare labeled with their semantic classes. Ex-\nperiments on the ACE test data show that a\nresolver that employs such induced semantic\nclass knowledge yields a statistically signif-\nicant improvement of 2% in F-measure over\none that exploits heuristically computed se-\nmantic class knowledge. In addition, the in-\nduced knowledge improves the accuracy of\ncommon noun resolution by 2-6%.\n"},{"#tail":"\n","@confidence":"0.99950742","#text":"\nIn the past decade, knowledge-lean approaches have\nsignificantly influenced research in noun phrase\n(NP) coreference resolution ? the problem of deter-\nmining which NPs refer to the same real-world en-\ntity in a document. In knowledge-lean approaches,\ncoreference resolvers employ only morpho-syntactic\ncues as knowledge sources in the resolution process\n(e.g., Mitkov (1998), Tetreault (2001)). While these\napproaches have been reasonably successful (see\nMitkov (2002)), Kehler et al (2004) speculate that\ndeeper linguistic knowledge needs to be made avail-\nable to resolvers in order to reach the next level of\nperformance. In fact, semantics plays a crucially im-\nportant role in the resolution of common NPs, allow-\ning us to identify the coreference relation between\ntwo lexically dissimilar common nouns (e.g., talks\nand negotiations) and to eliminate George W. Bush\nfrom the list of candidate antecedents of the city, for\ninstance. As a result, researchers have re-adopted\nthe once-popular knowledge-rich approach, investi-\ngating a variety of semantic knowledge sources for\ncommon noun resolution, such as the semantic rela-\ntions between two NPs (e.g., Ji et al (2005)), their\nsemantic similarity as computed using WordNet\n(e.g., Poesio et al (2004)) or Wikipedia (Ponzetto\nand Strube, 2006), and the contextual role played by\nan NP (see Bean and Riloff (2004)).\nAnother type of semantic knowledge that has\nbeen employed by coreference resolvers is the se-\nmantic class (SC) of an NP, which can be used to dis-\nallow coreference between semantically incompat-\nible NPs. However, learning-based resolvers have\nnot been able to benefit from having an SC agree-\nment feature, presumably because the method used\nto compute the SC of an NP is too simplistic: while\nthe SC of a proper name is computed fairly accu-\nrately using a named entity (NE) recognizer, many\nresolvers simply assign to a common noun the first\n(i.e., most frequent) WordNet sense as its SC (e.g.,\nSoon et al (2001), Markert and Nissim (2005)). It\nis not easy to measure the accuracy of this heuristic,\nbut the fact that the SC agreement feature is not used\nby Soon et al?s decision tree coreference classifier\nseems to suggest that the SC values of the NPs are\nnot computed accurately by this first-sense heuristic.\nMotivated in part by this observation, we exam-\nine whether automatically induced semantic class\nknowledge can improve the performance of a\nlearning-based coreference resolver, reporting eval-\nuation results on the commonly-used ACE corefer-\n"},{"#tail":"\n","@confidence":"0.99532184","#text":"\nence corpus. Our investigation proceeds as follows.\nTrain a classifier for labeling the SC of an NP.\nIn ACE, we are primarily concerned with classify-\ning an NP as belonging to one of the ACE seman-\ntic classes. For instance, part of the ACE Phase 2\nevaluation involves classifying an NP as PERSON,\nORGANIZATION, GPE (a geographical-political re-\ngion), FACILITY, LOCATION, or OTHERS. We adopt\na corpus-based approach to SC determination, re-\ncasting the problem as a six-class classification task.\nDerive two knowledge sources for coreference\nresolution from the induced SCs. The first\nknowledge source (KS) is semantic class agreement\n(SCA). Following Soon et al (2001), we represent\nSCA as a binary value that indicates whether the in-\nduced SCs of the two NPs involved are the same or\nnot. The second KS is mention, which is represented\nas a binary value that indicates whether an NP be-\nlongs to one of the five ACE SCs mentioned above.\nHence, the mention value of an NP can be readily\nderived from its induced SC: the value is NO if its\nSC is OTHERS, and YES otherwise. This KS could\nbe useful for ACE coreference, since ACE is con-\ncerned with resolving only NPs that are mentions.\nIncorporate the two knowledge sources in a\ncoreference resolver. Next, we investigate whether\nthese two KSs can improve a learning-based base-\nline resolver that employs a fairly standard feature\nset. Since (1) the two KSs can each be repre-\nsented in the resolver as a constraint (for filtering\nnon-mentions or disallowing coreference between\nsemantically incompatible NPs) or as a feature, and\n(2) they can be applied to the resolver in isolation or\nin combination, we have eight ways of incorporating\nthese KSs into the baseline resolver.\nIn our experiments on the ACE Phase 2 coref-\nerence corpus, we found that (1) our SC induc-\ntion method yields a significant improvement of 2%\nin accuracy over Soon et al?s first-sense heuristic\nmethod as described above; (2) the coreference re-\nsolver that incorporates our induced SC knowledge\nby means of the two KSs mentioned above yields\na significant improvement of 2% in F-measure over\nthe resolver that exploits the SC knowledge com-\nputed by Soon et al?s method; (3) the mention KS,\nwhen used in the baseline resolver as a constraint,\nimproves the resolver by approximately 5-7% in F-\nmeasure; and (4) SCA, when employed as a feature\nby the baseline resolver, improves the accuracy of\ncommon noun resolution by about 5-8%.\n"},{"#tail":"\n","@confidence":"0.999772564102564","#text":"\nMention detection. Many ACE participants have\nalso adopted a corpus-based approach to SC deter-\nmination that is investigated as part of the mention\ndetection (MD) task (e.g., Florian et al (2006)).\nBriefly, the goal of MD is to identify the boundary\nof a mention, its mention type (e.g., pronoun, name),\nand its semantic type (e.g., person, location). Un-\nlike them, (1) we do not perform the full MD task,\nas our goal is to investigate the role of SC knowl-\nedge in coreference resolution; and (2) we do not\nuse the ACE training data for acquiring our SC clas-\nsifier; instead, we use the BBN Entity Type Corpus\n(Weischedel and Brunstein, 2005), which consists of\nall the Penn Treebank Wall Street Journal articles\nwith the ACE mentions manually identified and an-\nnotated with their SCs. This provides us with a train-\ning set that is approximately five times bigger than\nthat of ACE. More importantly, the ACE participants\ndo not evaluate the role of induced SC knowledge\nin coreference resolution: many of them evaluate\ncoreference performance on perfect mentions (e.g.,\nLuo et al (2004)); and for those that do report per-\nformance on automatically extracted mentions, they\ndo not explain whether or how the induced SC infor-\nmation is used in their coreference algorithms.\nJoint probabilistic models of coreference. Re-\ncently, there has been a surge of interest in im-\nproving coreference resolution by jointly modeling\ncoreference with a related task such as MD (e.g.,\nDaume? and Marcu (2005)). However, joint models\ntypically need to be trained on data that is simulta-\nneously annotated with information required by all\nof the underlying models. For instance, Daume? and\nMarcu?s model assumes as input a corpus annotated\nwith both MD and coreference information. On the\nother hand, we tackle coreference and SC induction\nseparately (rather than jointly), since we train our SC\ndetermination model on the BBN Entity Type Cor-\npus, where coreference information is absent.\n"},{"#tail":"\n","@confidence":"0.997479","#text":"\nThis section describes how we train and evaluate a\nclassifier for determining the SC of an NP.\n"},{"#tail":"\n","@confidence":"0.979619391304348","#text":"\nTraining corpus. As mentioned before, we use\nthe BBN Entity Type Corpus for training the SC\nclassifier. This corpus was originally developed to\nsupport the ACE and AQUAINT programs and con-\nsists of annotations of 12 named entity types and\nnine nominal entity types. Nevertheless, we will\nonly make use of the annotations of the five ACE\nsemantic types that are present in our ACE Phase 2\ncoreference corpus, namely, PERSON, ORGANIZA-\nTION, GPE, FACILITY, and LOCATION.\nTraining instance creation. We create one train-\ning instance for each proper or common NP (ex-\ntracted using an NP chunker and an NE recognizer)\nin each training text. Each instance is represented\nby a set of lexical, syntactic, and semantic features,\nas described below. If the NP under consideration is\nannotated as one of the five ACE SCs in the corpus,\nthen the classification of the associated training in-\nstance is simply the ACE SC value of the NP. Other-\nwise, the instance is labeled as OTHERS. This results\nin 310063 instances in the training set.\nFeatures. We represent the training instance for a\nnoun phrase, NPi, using seven types of features:\n"},{"#tail":"\n","@confidence":"0.921831666666667","#text":"\nTION NEs are ACE GPE NEs). Otherwise, no NE\nfeature will be created (because we are not interested\nin the other MUC NE types).\n"},{"#tail":"\n","@confidence":"0.923469555555555","#text":"\nfor generating WN CLASS features.\n(5) WN CLASS: For each keyword w shown in the\nright column of Table 1, we determine whether the\nhead noun of NPi is a hyponym of w in WordNet,using only the first WordNet sense of NPi.1 If so,we create a WN CLASS feature with w as its value.\nThese keywords are potentially useful features be-\ncause some of them are subclasses of the ACE SCs\nshown in the left column of Table 1, while others\nappear to be correlated with these ACE SCs.2\n(6) INDUCED CLASS: Since the first-sense heuris-\ntic used in the previous feature may not be accurate\nin capturing the SC of an NP, we employ a corpus-\nbased method for inducing SCs that is motivated by\nresearch in lexical semantics (e.g., Hearst (1992)).\nGiven a large, unannotated corpus3 , we use Identi-\nFinder to label each NE with its NE type and MINI-\nPAR to extract all the appositive relations. An ex-\nample extraction would be <Eastern Airlines, the\ncarrier>, where the first entry is a proper noun la-\nbeled with either one of the seven MUC-style NE\ntypes4 or OTHERS5 and the second entry is a com-\nmon noun. We then infer the SC of a common\nnoun as follows: (1) we compute the probability\nthat the common noun co-occurs with each of the\neight NE types6 based on the extracted appositive\nrelations, and (2) if the most likely NE type has a\nco-occurrence probability above a certain threshold\n(we set it to 0.7), we create a INDUCED CLASS fea-\n"},{"#tail":"\n","@confidence":"0.995154738095238","#text":"\nture for NPi whose value is the most likely NE type.\n(7) NEIGHBOR: Research in lexical semantics sug-\ngests that the SC of an NP can be inferred from its\ndistributionally similar NPs (see Lin (1998a)). Mo-\ntivated by this observation, we create for each of\nNPi?s ten most semantically similar NPs a NEIGH-\nBOR feature whose value is the surface string of\nthe NP. To determine the ten nearest neighbors, we\nuse the semantic similarity values provided by Lin?s\ndependency-based thesaurus, which is constructed\nusing a distributional approach combined with an\ninformation-theoretic definition of similarity.\nLearning algorithms. We experiment with four\nlearners commonly employed in language learning:\nDecision List (DL): We use the DL learner as de-\nscribed in Collins and Singer (1999), motivated by\nits success in the related tasks of word sense dis-\nambiguation (Yarowsky, 1995) and NE classifica-\ntion (Collins and Singer, 1999). We apply add-one\nsmoothing to smooth the class posteriors.\n1-Nearest Neighbor (1-NN): We use the 1-NN clas-\nsifier as implemented in TiMBL (Daelemans et al,\n2004), employing dot product as the similarity func-\ntion (which defines similarity as the number of com-\nmon feature-value pairs between two instances). All\nother parameters are set to their default values.\nMaximum Entropy (ME): We employ Lin?s ME\nimplementation7 , using a Gaussian prior for smooth-\ning and running the algorithm until convergence.\nNaive Bayes (NB): We use an in-house implementa-\ntion of NB, using add-one smoothing to smooth the\nclass priors and the class-conditional probabilities.\nIn addition, we train an SVM classifier for SC\ndetermination by combining the output of five clas-\nsification methods: DL, 1-NN, ME, NB, and Soon\net al?s method as described in the introduction,8\nwith the goal of examining whether SC classifica-\ntion accuracy can be improved by combining the\noutput of individual classifiers in a supervised man-\nner. Specifically, we (1) use 80% of the instances\ngenerated from the BBN Entity Type Corpus to train\nthe four classifiers; (2) apply the four classifiers and\n"},{"#tail":"\n","@confidence":"0.6712895","#text":"\notherwise its label is the value of the NE feature or the ACE SC\nthat has the WN CLASS features as its keywords (see Table 1).\n"},{"#tail":"\n","@confidence":"0.973122857142857","#text":"\ntions for the remaining 20% of the instances; and (3)\ntrain an SVM classifier (using the LIBSVM pack-\nage (Chang and Lin, 2001)) on these 20% of the in-\nstances, where each instance, i, is represented by a\nset of 31 binary features. More specifically, let Li =\n{li1, li2, li3, li4, li5} be the set of predictions that weobtained for i in step (2). To represent i, we generate\none feature from each non-empty subset of Li.\n"},{"#tail":"\n","@confidence":"0.978149454545455","#text":"\nFor evaluation, we use the ACE Phase 2 coreference\ncorpus, which comprises 422 training texts and 97\ntest texts. Each text has its mentions annotated with\ntheir ACE SCs. We create our test instances from\nthe ACE texts in the same way as the training in-\nstances described in Section 3.1. Table 2 shows the\npercentages of instances corresponding to each SC.\nTable 3 shows the accuracy of each classifier (see\nrow 1) for the ACE training set (54641 NPs, with\n16414 proper NPs and 38227 common NPs) and the\nACE test set (13444 NPs, with 3713 proper NPs and\n9731 common NPs), as well as their performance on\nthe proper NPs (row 2) and the common NPs (row\n3). We employ as our baseline system the Soon et al\nmethod (see Footnote 8), whose accuracy is shown\nunder the Soon column. As we can see, DL, 1-NN,\nand SVM show a statistically significant improve-\nment over the baseline for both data sets, whereas\nME and NB perform significantly worse.9 Addi-\ntional experiments are needed to determine the rea-\nson for ME and NB?s poor performance.\nIn an attempt to gain additional insight into the\nperformance contribution of each type of features,\nwe conduct feature ablation experiments using the\nDL classifier (DL is chosen simply because it is the\nbest performer on the ACE training set). Results are\nshown in Table 4, where each row shows the accu-\nracy of the DL trained on all types of features except\nfor the one shown in that row (All), as well as accu-\nracies on the proper NPs (PN) and the common NPs\n(CN). For easy reference, the accuracy of the DL\n9We use Noreen?s (1989) Approximate Randomization test\nfor significance testing, with p set to .05 unless otherwise stated.\n"},{"#tail":"\n","@confidence":"0.98265052","#text":"\nclassifier trained on all types of features is shown\nin row 1 of the table. As we can see, accuracy drops\nsignificantly with the removal of NE and NEIGHBOR.\nAs expected, removing NE precipitates a large drop\nin proper NP accuracy; somewhat surprisingly, re-\nmoving NEIGHBOR also causes proper NP accuracy\nto drop significantly. To our knowledge, there are no\nprior results on using distributionally similar neigh-\nbors as features for supervised SC induction.\nNote, however, that these results do not imply\nthat the remaining feature types are not useful for\nSC classification; they simply suggest, for instance,\nthat WORD is not important in the presence of other\nfeature types. To get a better idea of the utility of\neach feature type, we conduct another experiment in\nwhich we train seven classifiers, each of which em-\nploys exactly one type of features. The accuracies\nof these classifiers are shown in Table 5. As we can\nsee, NEIGHBOR has the largest contribution. This\nagain demonstrates the effectiveness of a distribu-\ntional approach to semantic similarity. Its superior\nperformance to WORD, the second largest contribu-\ntor, could be attributed to its ability to combat data\nsparseness. The NE feature, as expected, is crucial\nto the classification of proper NPs.\n"},{"#tail":"\n","@confidence":"0.999711","#text":"\nWe can now derive from the induced SC informa-\ntion two KSs ? semantic class agreement and men-\ntion ? and incorporate them into our learning-based\ncoreference resolver in eight different ways, as de-\nscribed in the introduction. This section examines\nwhether our coreference resolver can benefit from\nany of the eight ways of incorporating these KSs.\n"},{"#tail":"\n","@confidence":"0.998401785714286","#text":"\nAs in SC induction, we use the ACE Phase 2 coref-\nerence corpus for evaluation purposes, acquiring the\ncoreference classifiers on the 422 training texts and\nevaluating their output on the 97 test texts. We re-\nport performance in terms of two metrics: (1) the F-\nmeasure score as computed by the commonly-used\nMUC scorer (Vilain et al, 1995), and (2) the accu-\nracy on the anaphoric references, computed as the\nfraction of anaphoric references correctly resolved.\nFollowing Ponzetto and Strube (2006), we consider\nan anaphoric reference, NPi, correctly resolved if NPiand its closest antecedent are in the same corefer-\nence chain in the resulting partition. In all of our\nexperiments, we use NPs automatically extracted by\nan in-house NP chunker and IdentiFinder.\n"},{"#tail":"\n","@confidence":"0.996278285714286","#text":"\nOur baseline coreference system uses the C4.5 deci-\nsion tree learner (Quinlan, 1993) to acquire a classi-\nfier on the training texts for determining whether two\nNPs are coreferent. Following previous work (e.g.,\nSoon et al (2001) and Ponzetto and Strube (2006)),\nwe generate training instances as follows: a positive\ninstance is created for each anaphoric NP, NPj , andits closest antecedent, NPi; and a negative instance iscreated for NPj paired with each of the interveningNPs, NPi+1, NPi+2, . . ., NPj?1. Each instance is rep-resented by 33 lexical, grammatical, semantic, and\n"},{"#tail":"\n","@confidence":"0.996324280701754","#text":"\npositional features that have been employed by high-\nperforming resolvers such as Ng and Cardie (2002)\nand Yang et al (2003), as described below.\nLexical features. Nine features allow different\ntypes of string matching operations to be performed\non the given pair of NPs, NPx and NPy10, including(1) exact string match for pronouns, proper nouns,\nand non-pronominal NPs (both before and after de-\nterminers are removed); (2) substring match for\nproper nouns and non-pronominal NPs; and (3) head\nnoun match. In addition, one feature tests whether\nall the words that appear in one NP also appear in\nthe other NP. Finally, a nationality matching feature\nis used to match, for instance, British with Britain.\nGrammatical features. 22 features test the gram-\nmatical properties of one or both of the NPs. These\ninclude ten features that test whether each of the two\nNPs is a pronoun, a definite NP, an indefinite NP, a\nnested NP, and a clausal subject. A similar set of\nfive features is used to test whether both NPs are\npronouns, definite NPs, nested NPs, proper nouns,\nand clausal subjects. In addition, five features deter-\nmine whether the two NPs are compatible with re-\nspect to gender, number, animacy, and grammatical\nrole. Furthermore, two features test whether the two\nNPs are in apposition or participate in a predicate\nnominal construction (i.e., the IS-A relation).\nSemantic features. Motivated by Soon et al\n(2001), we have a semantic feature that tests whether\none NP is a name alias or acronym of the other.\nPositional feature. We have a feature that com-\nputes the distance between the two NPs in sentences.\nAfter training, the decision tree classifier is used\nto select an antecedent for each NP in a test text.\nFollowing Soon et al (2001), we select as the an-\ntecedent of each NP, NPj , the closest preceding NPthat is classified as coreferent with NPj . If no suchNP exists, no antecedent is selected for NPj .\nRow 1 of Table 6 and Table 7 shows the results\nof the baseline system in terms of F-measure (F)\nand accuracy in resolving 4599 anaphoric references\n(All), respectively. For further analysis, we also re-\nport the corresponding recall (R) and precision (P)\nin Table 6, as well as the accuracies of the system in\nresolving 1769 pronouns (PRO), 1675 proper NPs\n(PN), and 1155 common NPs (CN) in Table 7. As\n10We assume that NPx precedes NPy in the associated text.\nwe can see, the baseline achieves an F-measure of\n57.0 and a resolution accuracy of 48.4.\nTo get a better sense of how strong our baseline\nis, we re-implement the Soon et al (2001) corefer-\nence resolver. This simply amounts to replacing the\n33 features in the baseline resolver with the 12 fea-\ntures employed by Soon et al?s system. Results of\nour Duplicated Soon et al system are shown in row\n2 of Tables 6 and 7. In comparison to our baseline,\nthe Duplicated Soon et al system performs worse\naccording to both metrics, and although the drop in\nF-measure seems moderate, the performance differ-\nence is in fact highly significant (p=0.002).11\n"},{"#tail":"\n","@confidence":"0.995410161290322","#text":"\nRecall from the introduction that our investigation of\nthe role of induced SC knowledge in learning-based\ncoreference resolution proceeds in three steps:\nLabel the SC of each NP in each ACE document.\nIf a noun phrase, NPi, is a proper or common NP,then its SC value is determined using an SC classi-\nfier that we acquired in Section 3. On the other hand,\nif NPi is a pronoun, then we will be conservative andposit its SC value as UNCONSTRAINED (i.e., it is se-\nmantically compatible with all other NPs).12\nDerive two KSs from the induced SCs. Recall that\nour first KS, Mention, is defined on an NP; its value\nis YES if the induced SC of the NP is not OTHERS,\nand NO otherwise. On the other hand, our second\nKS, SCA, is defined on a pair of NPs; its value is\nYES if the two NPs have the same induced SC that\nis not OTHERS, and NO otherwise.\nIncorporate the two KSs into the baseline re-\nsolver. Recall that there are eight ways of incor-\nporating these two KSs into our resolver: they can\neach be represented as a constraint or as a feature,\nand they can be applied to the resolver in isolation\nand in combination. Constraints are applied dur-\ning the antecedent selection step. Specifically, when\nemployed as a constraint, the Mention KS disallows\ncoreference between two NPs if at least one of them\nhas a Mention value of NO, whereas the SCA KS dis-\nallows coreference if the SCA value of the two NPs\ninvolved is NO. When encoded as a feature for the\nresolver, the Mention feature for an NP pair has the\n11Again, we use Approximate Randomization with p=.05.\n12The only exception is pronouns whose SC value can be eas-\nily determined to be PERSON (e.g., he, him, his, himself).\n"},{"#tail":"\n","@confidence":"0.99476476","#text":"\nvalue YES if and only if the Mention value for both\nNPs is YES, whereas the SCA feature for an NP pair\nhas its value taken from the SCA KS.\nNow, we can evaluate the impact of the two KSs\non the performance of our baseline resolver. Specifi-\ncally, rows 3-6 of Tables 6 and 7 show the F-measure\nand the resolution accuracy, respectively, when ex-\nactly one of the two KSs is employed by the baseline\nas either a constraint (C) or a feature (F), and rows\n7-10 of the two tables show the results when both\nKSs are applied to the baseline. Furthermore, each\nrow of Table 6 contains four sets of results, each of\nwhich corresponds to a different method for deter-\nmining the SC value of an NP. For instance, the first\nset is obtained by using Soon et al?s method as de-\nscribed in Footnote 8 to compute SC values, serving\nas sort of a baseline for our results using induced SC\nvalues. The second and third sets are obtained based\non the SC values computed by the DL and the SVM\nclassifier, respectively.13 The last set corresponds to\nan oracle experiment in which the resolver has ac-\ncess to perfect SC information. Rows 3-10 of Table\n13Results using other learners are not shown due to space lim-\nitations. DL and SVM are chosen simply because they achieve\nthe highest SC classification accuracies on the ACE training set.\n7 can be interpreted in a similar manner.\nFrom Table 6, we can see that (1) in comparison to\nthe baseline, F-measure increases significantly in the\nfive cases where at least one of the KSs is employed\nas a constraint by the resolver, and such improve-\nments stem mainly from significant gains in preci-\nsion; (2) in these five cases, the resolvers that use\nSCs induced by DL and SVM achieve significantly\nhigher F-measure scores than their counterparts that\nrely on Soon?s method for SC determination; and (3)\nnone of the resolvers appears to benefit from SCA in-\nformation whenever mention is used as a constraint.\nMoreover, note that even with perfectly computed\nSC information, the performance of the baseline sys-\ntem does not improve when neither MD nor SCA is\nemployed as a constraint. These results provide fur-\nther evidence that the decision tree learner is not ex-\nploiting these two semantic KSs in an optimal man-\nner, whether they are computed automatically or per-\nfectly. Hence, in machine learning for coreference\nresolution, it is important to determine not only what\nlinguistic KSs to use, but also how to use them.\nWhile the coreference results in Table 6 seem to\nsuggest that SCA and mention should be employed\nas constraints, the resolution results in Table 7 sug-\n"},{"#tail":"\n","@confidence":"0.999595222222222","#text":"\ngest that SCA is better encoded as a feature. Specifi-\ncally, (1) in comparison to the baseline, the accuracy\nof common NP resolution improves by about 5-8%\nwhen SCA is encoded as a feature; and (2) whenever\nSCA is employed as a feature, the overall resolution\naccuracy is significantly higher for resolvers that use\nSCs induced by DL and SVM than those that rely on\nSoon?s method for SC determination, with improve-\nments in resolution observed on all three NP types.\nOverall, these results provide suggestive evidence\nthat both KSs are useful for learning-based corefer-\nence resolution. In particular, mention should be em-\nployed as a constraint, whereas SCA should be used\nas a feature. Interestingly, this is consistent with the\nresults that we obtained when the resolver has access\nto perfect SC information (see Table 6), where the\nhighest F-measure is achieved by employing men-\ntion as a constraint and SCA as a feature.\n"},{"#tail":"\n","@confidence":"0.999790214285714","#text":"\nWe have shown that (1) both mention and SCA can\nbe usefully employed to improve the performance\nof a learning-based coreference system, and (2) em-\nploying SC knowledge induced in a supervised man-\nner enables a resolver to achieve better performance\nthan employing SC knowledge computed by Soon\net al?s simple method. In addition, we found that\nthe MUC scoring program is unable to reveal the\nusefulness of the SCA KS, which, when encoded\nas a feature, substantially improves the accuracy of\ncommon NP resolution. This underscores the im-\nportance of reporting both resolution accuracy and\nclustering-level accuracy when analyzing the perfor-\nmance of a coreference resolver.\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.863894","#text":"\nHuman Language Technology Research Institute\nUniversity of Texas at Dallas\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.990779","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998263","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.999611","@genericHeader":"related work","#text":"\n2 Related Work\n"},{"#tail":"\n","@confidence":"0.996198","@genericHeader":"method","#text":"\n3 Semantic Class Induction\n"},{"#tail":"\n","@confidence":"0.921702333333333","@genericHeader":"method","#text":"\nACE SC Keywords\nPERSON person\nORGANIZATION social group\n"},{"#tail":"\n","@confidence":"0.737976","@genericHeader":"method","#text":"\n4 Application to Coreference Resolution\n"},{"#tail":"\n","@confidence":"0.999536","@genericHeader":"conclusions","#text":"\n5 Conclusions\n"},{"#tail":"\n","@confidence":"0.990639","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.849777","#text":"\nTable 1: List of keywords used in WordNet search\n"},{"#tail":"\n","@confidence":"0.998637","#text":"\nTable 2: Distribution of SCs in the ACE corpus.\n"},{"#tail":"\n","@confidence":"0.801344","#text":"\nTable 3: SC classification accuracies of different methods for the ACE training set and test set.\n"},{"#tail":"\n","@confidence":"0.516612","#text":"\nTable 4: Results for feature ablation experiments.\n"},{"#tail":"\n","@confidence":"0.985108","#text":"\nTable 5: Accuracies of single-feature classifiers.\n"},{"#tail":"\n","@confidence":"0.992598","#text":"\nTable 7: Resolution accuracies for the ACE test set.\n"}],"page":[{"#tail":"\n","@confidence":"0.995917","#text":"\n536\n"},{"#tail":"\n","@confidence":"0.992828","#text":"\n537\n"},{"#tail":"\n","@confidence":"0.999398","#text":"\n538\n"},{"#tail":"\n","@confidence":"0.989085","#text":"\n539\n"},{"#tail":"\n","@confidence":"0.962586","#text":"\n540\n"},{"#tail":"\n","@confidence":"0.954563","#text":"\n541\n"},{"#tail":"\n","@confidence":"0.964462","#text":"\n542\n"},{"#tail":"\n","@confidence":"0.998952","#text":"\n543\n"}],"table":[{"#tail":"\n","@confidence":"0.980717666666667","#text":"\nPER ORG GPE FAC LOC OTH\nTraining 19.8 9.6 11.4 1.6 1.2 56.3\nTest 19.5 9.0 9.6 1.8 1.1 59.0\n"},{"#tail":"\n","@confidence":"0.997265","#text":"\nTraining Set Test Set\nSoon DL 1-NN ME NB SVM Soon DL 1-NN ME NB SVM\n1 Overall 83.1 85.0 84.0 54.5 71.3 84.2 81.1 82.9 83.1 53.0 70.3 83.3\n2 Proper NPs 83.1 84.1 81.0 54.2 65.5 82.2 79.6 82.0 79.8 55.8 64.4 80.4\n3 Common NPs 83.1 85.4 85.2 54.6 73.8 85.1 81.6 83.3 84.3 51.9 72.6 84.4\n"},{"#tail":"\n","@confidence":"0.9996999","#text":"\nTraining Set Test Set\nFeature Type PN CN All PN CN All\nAll features 84.1 85.4 85.0 82.0 83.3 82.9\n- WORD 84.2 85.4 85.0 82.0 83.1 82.8\n- SUBJ VERB 84.1 85.4 85.0 82.0 83.3 82.9\n- VERB OBJ 84.1 85.4 85.0 82.0 83.3 82.9\n- NE 72.9 85.3 81.6 74.1 83.2 80.7\n- WN CLASS 84.1 85.9 85.3 81.9 84.1 83.5\n- INDUCED C 84.0 85.6 85.1 82.0 83.6 83.2\n- NEIGHBOR 82.8 84.9 84.3 80.2 82.9 82.1\n"},{"#tail":"\n","@confidence":"0.999729","#text":"\nTraining Set Test Set\nFeature Type PN CN All PN CN All\nWORD 64.0 83.9 77.9 66.5 82.4 78.0\nSUBJ VERB 24.0 70.2 56.3 28.8 70.5 59.0\nVERB OBJ 24.0 70.2 56.3 28.8 70.5 59.0\nNE 81.1 72.1 74.8 78.4 71.4 73.3\nWN CLASS 25.6 78.8 62.8 30.4 78.9 65.5\nINDUCED C 25.8 81.1 64.5 30.0 80.3 66.3\nNEIGHBOR 67.7 85.8 80.4 68.0 84.4 79.8\n"}],"email":{"#tail":"\n","@confidence":"0.99672","#text":"\nvince@hlt.utdallas.edu\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.677467","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.9410965","#text":"Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 536?543, Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics"},"address":{"#tail":"\n","@confidence":"0.948131","#text":"Richardson, TX 75083-0688"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.999653","#text":"Human Language Technology Research Institute University of Texas at Dallas"},"author":{"#tail":"\n","@confidence":"0.998797","#text":"Vincent Ng"},"abstract":{"#tail":"\n","@confidence":"0.987093533333333","#text":"This paper examines whether a learningbased coreference resolver can be improved using semantic class knowledge that is automatically acquired from a version of the Penn Treebank in which the noun phrases are labeled with their semantic classes. Experiments on the ACE test data show that a resolver that employs such induced semantic class knowledge yields a statistically significant improvement of 2% in F-measure over one that exploits heuristically computed semantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%."},"title":{"#tail":"\n","@confidence":"0.950635","#text":"Semantic Class Induction and Coreference Resolution"},"email":{"#tail":"\n","@confidence":"0.999669","#text":"vince@hlt.utdallas.edu"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"D. Bean and E. Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. In Proc. of HLT/NAACL, pages 297?304."},"#text":"\n","pages":{"#tail":"\n","#text":"297--304"},"marker":{"#tail":"\n","#text":"Bean, Riloff, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et","@endWordPosition":"348","@position":"2334","annotationId":"T1","@startWordPosition":"345","@citStr":"Bean and Riloff (2004)"}},"title":{"#tail":"\n","#text":"Unsupervised learning of contextual role knowledge for coreference resolution."},"booktitle":{"#tail":"\n","#text":"In Proc. of HLT/NAACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Bean"},{"#tail":"\n","#text":"E Riloff"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999. An algorithm that learns what?s in a name. Machine Learning 34(1?3):211?231."},"journal":{"#tail":"\n","#text":"Machine Learning"},"#text":"\n","pages":{"#tail":"\n","#text":"34--1"},"marker":{"#tail":"\n","#text":"Bikel, Schwartz, Weischedel, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ture whose value is equal to w. No features are created from stopwords, however. (2) SUBJ VERB: If NPi is involved in a subject-verb relation, we create a SUBJ VERB feature whose value is the verb participating in the relation. We use Lin?s (1998b) MINIPAR dependency parser to extract grammatical relations. Our motivation here is to coarsely model subcategorization. (3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NPi participatesin a verb-object relation. Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN?s IdentiFinder (Bikel et al, 1999), a MUC-style NE recognizer to determine the NE type of NPi. If NPi is determined to be a PERSONor ORGANIZATION, we create an NE feature whose value is simply its MUC NE type. However, if NPiis determined to be a LOCATION, we create a feature with value GPE (because most of the MUC LOCATION NEs are ACE GPE NEs). Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types). ACE SC Keywords PERSON person ORGANIZATION social group FACILITY establishment, construction, building, facility, workplace GPE country, province, government, town, city, administration,","@endWordPosition":"1611","@position":"9838","annotationId":"T2","@startWordPosition":"1608","@citStr":"Bikel et al, 1999"}},"title":{"#tail":"\n","#text":"An algorithm that learns what?s in a name."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D M Bikel"},{"#tail":"\n","#text":"R Schwartz"},{"#tail":"\n","#text":"R M Weischedel"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm."},"#text":"\n","marker":{"#tail":"\n","#text":"Chang, Lin, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ssifiers and Soon et al?s method to independently make predic7See http://www.cs.ualberta.ca/?lindek/downloads.htm 8In our implementation of Soon?s method, we label an instance as OTHERS if no NE or WN CLASS feature is generated; otherwise its label is the value of the NE feature or the ACE SC that has the WN CLASS features as its keywords (see Table 1). PER ORG GPE FAC LOC OTH Training 19.8 9.6 11.4 1.6 1.2 56.3 Test 19.5 9.0 9.6 1.8 1.1 59.0 Table 2: Distribution of SCs in the ACE corpus. tions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package (Chang and Lin, 2001)) on these 20% of the instances, where each instance, i, is represented by a set of 31 binary features. More specifically, let Li = {li1, li2, li3, li4, li5} be the set of predictions that weobtained for i in step (2). To represent i, we generate one feature from each non-empty subset of Li. 3.2 Evaluating the Classifiers For evaluation, we use the ACE Phase 2 coreference corpus, which comprises 422 training texts and 97 test texts. Each text has its mentions annotated with their ACE SCs. We create our test instances from the ACE texts in the same way as the training instances described in Sec","@endWordPosition":"2549","@position":"15333","annotationId":"T3","@startWordPosition":"2546","@citStr":"Chang and Lin, 2001"}},"title":{"#tail":"\n","#text":"LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"C-C Chang"},{"#tail":"\n","#text":"C-J Lin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proc. of EMNLP/VLC."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, Singer, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" distributionally similar NPs (see Lin (1998a)). Motivated by this observation, we create for each of NPi?s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin?s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its success in the related tasks of word sense disambiguation (Yarowsky, 1995) and NE classification (Collins and Singer, 1999). We apply add-one smoothing to smooth the class posteriors. 1-Nearest Neighbor (1-NN): We use the 1-NN classifier as implemented in TiMBL (Daelemans et al, 2004), employing dot product as the similarity function (which defines similarity as the number of common feature-value pairs between two instances). All other parameters are set to their default values. Maximum Entropy (ME): We employ Lin?s ME implementation7 , using a Gaussian prior for smoothing a","@endWordPosition":"2230","@position":"13431","annotationId":"T4","@startWordPosition":"2227","@citStr":"Collins and Singer (1999)"}},"title":{"#tail":"\n","#text":"Unsupervised models for named entity classification."},"booktitle":{"#tail":"\n","#text":"In Proc. of EMNLP/VLC."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Collins"},{"#tail":"\n","#text":"Y Singer"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"ILK Technical Report."},"date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2004. TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide. ILK Technical Report."},"#text":"\n","marker":{"#tail":"\n","#text":"Daelemans, Zavrel, van der Sloot, van den Bosch, 2004"},"title":{"#tail":"\n","#text":"TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W Daelemans"},{"#tail":"\n","#text":"J Zavrel"},{"#tail":"\n","#text":"K van der Sloot"},{"#tail":"\n","#text":"A van den Bosch"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"H. Daume? III and D. Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In Proc. of HLT/EMNLP, pages 97?104."},"#text":"\n","pages":{"#tail":"\n","#text":"97--104"},"marker":{"#tail":"\n","#text":"Daume, Marcu, 2005"},"title":{"#tail":"\n","#text":"A large-scale exploration of effective global features for a joint entity detection and tracking model."},"booktitle":{"#tail":"\n","#text":"In Proc. of HLT/EMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"H Daume"},{"#tail":"\n","#text":"D Marcu"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006. Factorizing complex models: A case study in mention detection. In Proc. of COLING/ACL, pages 473?480."},"#text":"\n","pages":{"#tail":"\n","#text":"473--480"},"marker":{"#tail":"\n","#text":"Florian, Jing, Kambhatla, Zitouni, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Ss mentioned above yields a significant improvement of 2% in F-measure over the resolver that exploits the SC knowledge computed by Soon et al?s method; (3) the mention KS, when used in the baseline resolver as a constraint, improves the resolver by approximately 5-7% in Fmeasure; and (4) SCA, when employed as a feature by the baseline resolver, improves the accuracy of common noun resolution by about 5-8%. 2 Related Work Mention detection. Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al (2006)). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides","@endWordPosition":"989","@position":"6130","annotationId":"T5","@startWordPosition":"986","@citStr":"Florian et al (2006)"}},"title":{"#tail":"\n","#text":"Factorizing complex models: A case study in mention detection."},"booktitle":{"#tail":"\n","#text":"In Proc. of COLING/ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Florian"},{"#tail":"\n","#text":"H Jing"},{"#tail":"\n","#text":"N Kambhatla"},{"#tail":"\n","#text":"I Zitouni"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1992"},"rawString":{"#tail":"\n","#text":"M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of COLING."},"#text":"\n","marker":{"#tail":"\n","#text":"Hearst, 1992"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"we determine whether the head noun of NPi is a hyponym of w in WordNet,using only the first WordNet sense of NPi.1 If so,we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs.2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)). Given a large, unannotated corpus3 , we use IdentiFinder to label each NE with its NE type and MINIPAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types4 or OTHERS5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrenc","@endWordPosition":"1868","@position":"11325","annotationId":"T6","@startWordPosition":"1867","@citStr":"Hearst (1992)"}},"title":{"#tail":"\n","#text":"Automatic acquisition of hyponyms from large text corpora."},"booktitle":{"#tail":"\n","#text":"In Proc. of COLING."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Hearst"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"H. Ji, D. Westbrook, and R. Grishman. 2005. Using semantic relations to refine coreference decisions. In Proc. of HLT/EMNLP, pages 17?24."},"#text":"\n","pages":{"#tail":"\n","#text":"17--24"},"marker":{"#tail":"\n","#text":"Ji, Westbrook, Grishman, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a prop","@endWordPosition":"317","@position":"2142","annotationId":"T7","@startWordPosition":"314","@citStr":"Ji et al (2005)"}},"title":{"#tail":"\n","#text":"Using semantic relations to refine coreference decisions."},"booktitle":{"#tail":"\n","#text":"In Proc. of HLT/EMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"H Ji"},{"#tail":"\n","#text":"D Westbrook"},{"#tail":"\n","#text":"R Grishman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proc. of NAACL, pages 289?296."},"#text":"\n","pages":{"#tail":"\n","#text":"289--296"},"marker":{"#tail":"\n","#text":"Kehler, Appelt, Taylor, Simma, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"emantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%. 1 Introduction In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution ? the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resol","@endWordPosition":"209","@position":"1466","annotationId":"T8","@startWordPosition":"206","@citStr":"Kehler et al (2004)"}},"title":{"#tail":"\n","#text":"The (non)utility of predicate-argument frequencies for pronoun interpretation."},"booktitle":{"#tail":"\n","#text":"In Proc. of NAACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Kehler"},{"#tail":"\n","#text":"D Appelt"},{"#tail":"\n","#text":"L Taylor"},{"#tail":"\n","#text":"A Simma"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"D. Lin. 1998a. Automatic retrieval and clustering of similar words. In Proc. of COLING/ACL, pages 768?774."},"#text":"\n","pages":{"#tail":"\n","#text":"768--774"},"marker":{"#tail":"\n","#text":"Lin, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"mentation with WordNet and the ACE SCs of the NPs in the ACE training data. 3We used (1) the BLLIP corpus (30M words), which consists of WSJ articles from 1987 to 1989, and (2) the Reuters Corpus (3.7GB data), which has 806,791 Reuters articles. 4Person, organization, location, date, time, money, percent. 5This indicates the proper noun is not a MUC NE. 6For simplicity, OTHERS is viewed as an NE type here. 538 ture for NPi whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)). Motivated by this observation, we create for each of NPi?s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin?s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its ","@endWordPosition":"2142","@position":"12850","annotationId":"T9","@startWordPosition":"2141","@citStr":"Lin (1998"}},"title":{"#tail":"\n","#text":"Automatic retrieval and clustering of similar words."},"booktitle":{"#tail":"\n","#text":"In Proc. of COLING/ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Lin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"D. Lin. 1998b. Dependency-based evaluation of MINIPAR. In Proc. of the LREC Workshop on the Evaluation of Parsing Systems, pages 48?56."},"#text":"\n","pages":{"#tail":"\n","#text":"48--56"},"marker":{"#tail":"\n","#text":"Lin, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"mentation with WordNet and the ACE SCs of the NPs in the ACE training data. 3We used (1) the BLLIP corpus (30M words), which consists of WSJ articles from 1987 to 1989, and (2) the Reuters Corpus (3.7GB data), which has 806,791 Reuters articles. 4Person, organization, location, date, time, money, percent. 5This indicates the proper noun is not a MUC NE. 6For simplicity, OTHERS is viewed as an NE type here. 538 ture for NPi whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)). Motivated by this observation, we create for each of NPi?s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin?s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its ","@endWordPosition":"2142","@position":"12850","annotationId":"T10","@startWordPosition":"2141","@citStr":"Lin (1998"}},"title":{"#tail":"\n","#text":"Dependency-based evaluation of MINIPAR."},"booktitle":{"#tail":"\n","#text":"In Proc. of the LREC Workshop on the Evaluation of Parsing Systems,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Lin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"D. Lin. 1998c. Using collocation statistics in information extraction. In Proc. of MUC-7."},"#text":"\n","marker":{"#tail":"\n","#text":"Lin, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"mentation with WordNet and the ACE SCs of the NPs in the ACE training data. 3We used (1) the BLLIP corpus (30M words), which consists of WSJ articles from 1987 to 1989, and (2) the Reuters Corpus (3.7GB data), which has 806,791 Reuters articles. 4Person, organization, location, date, time, money, percent. 5This indicates the proper noun is not a MUC NE. 6For simplicity, OTHERS is viewed as an NE type here. 538 ture for NPi whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)). Motivated by this observation, we create for each of NPi?s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin?s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its ","@endWordPosition":"2142","@position":"12850","annotationId":"T11","@startWordPosition":"2141","@citStr":"Lin (1998"}},"title":{"#tail":"\n","#text":"Using collocation statistics in information extraction."},"booktitle":{"#tail":"\n","#text":"In Proc. of MUC-7."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Lin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the Bell tree. In Proc. of the ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"reference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al (2004)); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms. Joint probabilistic models of coreference. Recently, there has been a surge of interest in improving coreference resolution by jointly modeling coreference with a related task such as MD (e.g., Daume? and Marcu (2005)). However, joint models typically need to be trained on data that is simultaneously annotated with information required by all of the underlying models. For instance, Daume? and Marcu?s model assume","@endWordPosition":"1142","@position":"7016","annotationId":"T12","@startWordPosition":"1139","@citStr":"Luo et al (2004)"}},"title":{"#tail":"\n","#text":"A mention-synchronous coreference resolution algorithm based on the Bell tree."},"booktitle":{"#tail":"\n","#text":"In Proc. of the ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"X Luo"},{"#tail":"\n","#text":"A Ittycheriah"},{"#tail":"\n","#text":"H Jing"},{"#tail":"\n","#text":"N Kambhatla"},{"#tail":"\n","#text":"S Roukos"}]}},{"volume":{"#tail":"\n","#text":"31"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"K. Markert and M. Nissim. 2005. Comparing knowledge sources for nominal anaphora resolution. Computational Linguistics, 31(3):367?402."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Markert, Nissim, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ype of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et al (2001), Markert and Nissim (2005)). It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al?s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic. Motivated in part by this observation, we examine whether automatically induced semantic class knowledge can improve the performance of a learning-based coreference resolver, reporting evaluation results on the commonly-used ACE corefer536 ence corpus. Our investigation proceeds as follows. Train a classifier for labeling t","@endWordPosition":"458","@position":"2971","annotationId":"T13","@startWordPosition":"455","@citStr":"Markert and Nissim (2005)"}},"title":{"#tail":"\n","#text":"Comparing knowledge sources for nominal anaphora resolution."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"K Markert"},{"#tail":"\n","#text":"M Nissim"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"R. Mitkov. 2002. Anaphora Resolution. Longman."},"#text":"\n","marker":{"#tail":"\n","#text":"Mitkov, 2002"},"publisher":{"#tail":"\n","#text":"Longman."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cally computed semantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%. 1 Introduction In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution ? the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources","@endWordPosition":"205","@position":"1444","annotationId":"T14","@startWordPosition":"204","@citStr":"Mitkov (2002)"}},"title":{"#tail":"\n","#text":"Anaphora Resolution."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R Mitkov"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"R. Mitkov. 1998. Robust pronoun resolution with limited knowledge. In Proc. of COLING/ACL, pages 869?875."},"#text":"\n","pages":{"#tail":"\n","#text":"869--875"},"marker":{"#tail":"\n","#text":"Mitkov, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"lds a statistically significant improvement of 2% in F-measure over one that exploits heuristically computed semantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%. 1 Introduction In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution ? the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted t","@endWordPosition":"193","@position":"1350","annotationId":"T15","@startWordPosition":"192","@citStr":"Mitkov (1998)"}},"title":{"#tail":"\n","#text":"Robust pronoun resolution with limited knowledge."},"booktitle":{"#tail":"\n","#text":"In Proc. of COLING/ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R Mitkov"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proc. of the ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Ng, Cardie, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"earner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj , andits closest antecedent, NPi; and a negative instance iscreated for NPj paired with each of the interveningNPs, NPi+1, NPi+2, . . ., NPj?1. Each instance is rep-resented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. Lexical features. Nine features allow different types of string matching operations to be performed on the given pair of NPs, NPx and NPy10, including(1) exact string match for pronouns, proper nouns, and non-pronominal NPs (both before and after determiners are removed); (2) substring match for proper nouns and non-pronominal NPs; and (3) head noun match. In addition, one feature tests whether all the words that appear in one NP also appear in the other NP. Finally, a nationality matching feature is used to match, for instance, British with Britain.","@endWordPosition":"3650","@position":"21650","annotationId":"T16","@startWordPosition":"3647","@citStr":"Ng and Cardie (2002)"}},"title":{"#tail":"\n","#text":"Improving machine learning approaches to coreference resolution."},"booktitle":{"#tail":"\n","#text":"In Proc. of the ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"V Ng"},{"#tail":"\n","#text":"C Cardie"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"E. W. Noreen. 1989. Computer Intensive Methods for Testing Hypothesis: An Introduction. John Wiley & Sons."},"#text":"\n","marker":{"#tail":"\n","#text":"Noreen, 1989"},"publisher":{"#tail":"\n","#text":"John Wiley & Sons."},"title":{"#tail":"\n","#text":"Computer Intensive Methods for Testing Hypothesis: An Introduction."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E W Noreen"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004. Learning to resolve bridging references. In Proc. of the ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Poesio, Mehta, Maroudas, Hitzeman, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ct, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many ","@endWordPosition":"329","@position":"2223","annotationId":"T17","@startWordPosition":"326","@citStr":"Poesio et al (2004)"}},"title":{"#tail":"\n","#text":"Learning to resolve bridging references."},"booktitle":{"#tail":"\n","#text":"In Proc. of the ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Poesio"},{"#tail":"\n","#text":"R Mehta"},{"#tail":"\n","#text":"A Maroudas"},{"#tail":"\n","#text":"J Hitzeman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"S. P. Ponzetto and M. Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proc. of HLT/NAACL, pages 192?199."},"#text":"\n","pages":{"#tail":"\n","#text":"192--199"},"marker":{"#tail":"\n","#text":"Ponzetto, Strube, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ortant role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun t","@endWordPosition":"335","@position":"2265","annotationId":"T18","@startWordPosition":"332","@citStr":"Ponzetto and Strube, 2006"},{"#tail":"\n","#text":"n examines whether our coreference resolver can benefit from any of the eight ways of incorporating these KSs. 4.1 Experimental Setup As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al, 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPiand its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows","@endWordPosition":"3495","@position":"20670","annotationId":"T19","@startWordPosition":"3492","@citStr":"Ponzetto and Strube (2006)"}]},"title":{"#tail":"\n","#text":"Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution."},"booktitle":{"#tail":"\n","#text":"In Proc. of HLT/NAACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S P Ponzetto"},{"#tail":"\n","#text":"M Strube"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA."},"#text":"\n","marker":{"#tail":"\n","#text":"Quinlan, 1993"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann,"},"location":{"#tail":"\n","#text":"San Mateo, CA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sure score as computed by the commonly-used MUC scorer (Vilain et al, 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPiand its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj , andits closest antecedent, NPi; and a negative instance iscreated for NPj paired with each of the interveningNPs, NPi+1, NPi+2, . . ., NPj?1. Each instance is rep-resented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) a","@endWordPosition":"3554","@position":"21052","annotationId":"T20","@startWordPosition":"3553","@citStr":"Quinlan, 1993"}},"title":{"#tail":"\n","#text":"C4.5: Programs for Machine Learning."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J R Quinlan"}}},{"volume":{"#tail":"\n","#text":"27"},"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521?544."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Soon, Ng, Lim, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et al (2001), Markert and Nissim (2005)). It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al?s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic. Motivated in part by this observation, we examine whether automatically induced semantic class knowledge can improve the performance of a learning-based coreference resolver, reporting evaluation results on the commonly-used ACE corefer536 ence corpus. Our investigation proceeds as follows. Train ","@endWordPosition":"454","@position":"2944","annotationId":"T21","@startWordPosition":"451","@citStr":"Soon et al (2001)"},{"#tail":"\n","#text":"he fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPiand its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj , andits closest antecedent, NPi; and a negative instance iscreated for NPj paired with each of the interveningNPs, NPi+1, NPi+2, . . ., NPj?1. Each instance is rep-resented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. Lexical features. Nine features allow different types of string matching operations to be performed o","@endWordPosition":"3578","@position":"21195","annotationId":"T22","@startWordPosition":"3575","@citStr":"Soon et al (2001)"},{"#tail":"\n","#text":"ese include ten features that test whether each of the two NPs is a pronoun, a definite NP, an indefinite NP, a nested NP, and a clausal subject. A similar set of five features is used to test whether both NPs are pronouns, definite NPs, nested NPs, proper nouns, and clausal subjects. In addition, five features determine whether the two NPs are compatible with respect to gender, number, animacy, and grammatical role. Furthermore, two features test whether the two NPs are in apposition or participate in a predicate nominal construction (i.e., the IS-A relation). Semantic features. Motivated by Soon et al (2001), we have a semantic feature that tests whether one NP is a name alias or acronym of the other. Positional feature. We have a feature that computes the distance between the two NPs in sentences. After training, the decision tree classifier is used to select an antecedent for each NP in a test text. Following Soon et al (2001), we select as the antecedent of each NP, NPj , the closest preceding NPthat is classified as coreferent with NPj . If no suchNP exists, no antecedent is selected for NPj . Row 1 of Table 6 and Table 7 shows the results of the baseline system in terms of F-measure (F) and ","@endWordPosition":"3868","@position":"22963","annotationId":"T23","@startWordPosition":"3865","@citStr":"Soon et al (2001)"}]},"title":{"#tail":"\n","#text":"A machine learning approach to coreference resolution of noun phrases."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W M Soon"},{"#tail":"\n","#text":"H T Ng"},{"#tail":"\n","#text":"D Lim"}]}},{"volume":{"#tail":"\n","#text":"27"},"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"J. Tetreault. 2001. A corpus-based evaluation of centering and pronoun resolution. Computational Linguistics, 27(4)."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Tetreault, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ally significant improvement of 2% in F-measure over one that exploits heuristically computed semantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%. 1 Introduction In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution ? the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular kn","@endWordPosition":"195","@position":"1368","annotationId":"T24","@startWordPosition":"194","@citStr":"Tetreault (2001)"}},"title":{"#tail":"\n","#text":"A corpus-based evaluation of centering and pronoun resolution."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Tetreault"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proc. of MUC-6, pages 45?52."},"#text":"\n","pages":{"#tail":"\n","#text":"45--52"},"marker":{"#tail":"\n","#text":"Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"and mention ? and incorporate them into our learning-based coreference resolver in eight different ways, as described in the introduction. This section examines whether our coreference resolver can benefit from any of the eight ways of incorporating these KSs. 4.1 Experimental Setup As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al, 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPiand its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determinin","@endWordPosition":"3472","@position":"20513","annotationId":"T25","@startWordPosition":"3469","@citStr":"Vilain et al, 1995"}},"title":{"#tail":"\n","#text":"A model-theoretic coreference scoring scheme."},"booktitle":{"#tail":"\n","#text":"In Proc. of MUC-6,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Vilain"},{"#tail":"\n","#text":"J Burger"},{"#tail":"\n","#text":"J Aberdeen"},{"#tail":"\n","#text":"D Connolly"},{"#tail":"\n","#text":"L Hirschman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"R. Weischedel and A. Brunstein. 2005. BBN pronoun coreference and entity type corpus. Linguistica Data Consortium."},"#text":"\n","marker":{"#tail":"\n","#text":"Weischedel, Brunstein, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" detection. Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al (2006)). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al (2004)); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in thei","@endWordPosition":"1070","@position":"6574","annotationId":"T26","@startWordPosition":"1067","@citStr":"Weischedel and Brunstein, 2005"}},"title":{"#tail":"\n","#text":"BBN pronoun coreference and entity type corpus. Linguistica Data Consortium."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"R Weischedel"},{"#tail":"\n","#text":"A Brunstein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference resolution using competitive learning approach. In Proc. of the ACL, pages 176?183."},"#text":"\n","pages":{"#tail":"\n","#text":"176--183"},"marker":{"#tail":"\n","#text":"Yang, Zhou, Su, Tan, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj , andits closest antecedent, NPi; and a negative instance iscreated for NPj paired with each of the interveningNPs, NPi+1, NPi+2, . . ., NPj?1. Each instance is rep-resented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. Lexical features. Nine features allow different types of string matching operations to be performed on the given pair of NPs, NPx and NPy10, including(1) exact string match for pronouns, proper nouns, and non-pronominal NPs (both before and after determiners are removed); (2) substring match for proper nouns and non-pronominal NPs; and (3) head noun match. In addition, one feature tests whether all the words that appear in one NP also appear in the other NP. Finally, a nationality matching feature is used to match, for instance, British with Britain. Grammatical features.","@endWordPosition":"3655","@position":"21672","annotationId":"T27","@startWordPosition":"3652","@citStr":"Yang et al (2003)"}},"title":{"#tail":"\n","#text":"Coreference resolution using competitive learning approach."},"booktitle":{"#tail":"\n","#text":"In Proc. of the ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"X Yang"},{"#tail":"\n","#text":"G Zhou"},{"#tail":"\n","#text":"J Su"},{"#tail":"\n","#text":"C L Tan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of the ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Yarowsky, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Pi?s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin?s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its success in the related tasks of word sense disambiguation (Yarowsky, 1995) and NE classification (Collins and Singer, 1999). We apply add-one smoothing to smooth the class posteriors. 1-Nearest Neighbor (1-NN): We use the 1-NN classifier as implemented in TiMBL (Daelemans et al, 2004), employing dot product as the similarity function (which defines similarity as the number of common feature-value pairs between two instances). All other parameters are set to their default values. Maximum Entropy (ME): We employ Lin?s ME implementation7 , using a Gaussian prior for smoothing and running the algorithm until convergence. Naive Bayes (NB): We use an in-house implementati","@endWordPosition":"2245","@position":"13524","annotationId":"T28","@startWordPosition":"2244","@citStr":"Yarowsky, 1995"}},"title":{"#tail":"\n","#text":"Unsupervised word sense disambiguation rivaling supervised methods."},"booktitle":{"#tail":"\n","#text":"In Proc. of the ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Yarowsky"}}}]}}]}}
