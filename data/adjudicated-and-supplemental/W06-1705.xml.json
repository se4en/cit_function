{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","author":[{"#tail":"\n","@confidence":"0.529625","#text":"\nPaul Rayson\nComputing Department,\n"},{"#tail":"\n","@confidence":"0.5201095","#text":"\nJames Walkerdine\nComputing Department,\n"},{"#tail":"\n","@confidence":"0.960526","#text":"\nWilliam H. Fletcher\n"},{"#tail":"\n","@confidence":"0.92166","#text":"\nAdam Kilgarriff\n"}],"footnote":[{"#tail":"\n","@confidence":"0.994778","#text":"\n1 See, for example, those distributed by the Linguistic\nData Consortium: http://www.ldc.upenn.edu/\n"},{"#tail":"\n","@confidence":"0.98905925","#text":"\n2 http://setiathome.ssl.berkeley.edu/\n3 http://www.smi.ucd.ie/hyppia/,\nhttp://parcels.sourceforge.net and\nhttp://tidy.sourceforge.net.\n"},{"#tail":"\n","@confidence":"0.998324333333333","#text":"\n4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html\n5 http://www.comp.leeds.ac.uk/amalgam/amalgam/\namalghome.htm\n6 http://www.connexor.com\n7 http://homepage.mac.com/bncweb/home.html\n8 http://view.byu.edu/\n"},{"#tail":"\n","@confidence":"0.767924333333333","#text":"\nPERC Member Institutions. For more details, see\nhttp://www.perc21.org/\n13 http://multivalent.sourceforge.net/\n"}],"title":{"#tail":"\n","@confidence":"0.341882","#text":"\nAnnotated web as corpus\n"},"@confidence":"0.000005","reference":[{"#tail":"\n","@confidence":"0.959933169014085","#text":"\nBaroni, M. and Bernardini, S. (2004). BootCaT:\nBootstrapping Corpora and Terms from the Web.\nIn Proceedings of LREC2004, Lisbon, pp. 1313-\n1316.\nBaroni, M. and Sharoff, S. (2005). Creating special-\nized and general corpora using automated search\nengine queries. Web as Corpus Workshop, Bir-\nmingham University, UK, 14th July 2005.\nCarroll, J., R. Evans and E. Klein (2005) Supporting\ntext mining for e-Science: the challenges for Grid-\nenabled natural language processing. In Workshop\non Text Mining, e-Research And Grid-enabled\nLanguage Technology at the Fourth UK e-Science\nProgramme All Hands Meeting (AHM2005), Not-\ntingham, UK.\n14 This corpus has already been manually re-typed at\nShogakukan Inc. from PDF originals downloaded\nfrom the web.\nChakrabarti, S. (2002) Mining the Web: Discovering\nKnowledge from Hypertext Data. Morgan Kauf-\nmann.\nClark, S. and Curran, J. R.. (2004). Parsing the wsj\nusing ccg and log-linear models. In Proceedings of\nthe 42nd Annual Meeting of the Association for\nComputational Linguistics (ACL ?04).\nCorley, S., Corley, M., Keller, F., Crocker, M., &\nTrewin, S. (2001). Finding Syntactic Structure in\nUnparsed Corpora: The Gsearch Corpus Query\nSystem. Computers and the Humanities, 35, 81-94.\nCurran, J.R. (2003). Blueprint for a High Performance\nNLP Infrastructure. In Proc. of Workshop on Soft-\nware Engineering and Architecture of Language\nTechnology Systems (SEALTS) Edmonton, Canada,\n2003, pp. 40 ? 45.\nEdmonds, P and Kilgarriff, A. (2002). Introduction to\nthe special issue on evaluating word sense disam-\nbiguation systems. Journal of Natural Language\nEngineering, 8 (2), pp. 279-291.\nFletcher, W. H. (2001). Concordancing the Web with\nKWiCFinder. Third North American Symposium\non Corpus Linguistics and Language Teaching,\nBoston, MA, 23-25 March 2001.\nFletcher, W. H. (2004a). Facilitating the compilation\nand dissemination of ad-hoc Web corpora. In G.\nAston, S. Bernardini and D. Stewart (eds.), Cor-\npora and Language Learners, pp. 271 ? 300, John\nBenjamins, Amsterdam.\nFletcher, W. H. (2004b). Making the Web More Use-\nful as a Source for Linguistic Corpora. In Ulla\nConnor and Thomas A. Upton (eds.) Applied Cor-\npus Linguistics. A Multidimensional Perspective.\nRodopi, Amsterdam, pp. 191 ? 205.\nGranger, S., and Rayson, P. (1998). Automatic profil-\ning of learner texts. In S. Granger (ed.) Learner\nEnglish on Computer. Longman, London and New\nYork, pp. 119-131.\nHughes, B, Bird, S., Haejoong, K., and Klein, E.\n(2004). Experiments with data-intensive NLP on a\ncomputational grid. Proceedings of the Interna-\ntional Workshop on Human Language Technology.\nhttp://eprints.unimelb.edu.au/archive/00000503/.\nHughes, D., Gilleade, K., Walkerdine, J. and Mariani,\nJ., Exploiting P2P in the Creation of Game Worlds.\nIn the proceedings of ACM GDTW 2005, Liver-\npool, UK, 8th-9th November, 2005.\nHughes, D. and Walkerdine, J. (2005), Distributed\nVideo Encoding Over A Peer-to-Peer Network. In\nthe proceedings of PREP 2005, Lancaster, UK,\n30th March - 1st April, 2005\nKehoe, A. and Renouf, A. (2002) WebCorp: Applying\nthe Web to Linguistics and Linguistics to the Web.\n"},{"#tail":"\n","@confidence":"0.999012581818182","#text":"\nWorld Wide Web 2002 Conference, Honolulu, Ha-\nwaii.\nKeller, F., Lapata, M. and Ourioupina, O. (2002).\nUsing the Web to Overcome Data Sparseness. Pro-\nceedings of the Conference on Empirical Methods\nin Natural Language Processing, Philadelphia,\nJuly 2002, pp. 230-237.\nKennedy, G. (1998). An introduction to corpus lin-\nguistics. Longman, London.\nKilgarriff, A. (2001). Web as corpus. In Proceedings\nof Corpus Linguistics 2001, Lancaster University,\n29 March - 2 April 2001, pp. 342 ? 344.\nKilgarriff, A. (2003). Linguistic Search Engine. In\nproceedings of Workshop on Shallow Processing of\nLarge Corpora (SProLaC 2003), Lancaster Uni-\nversity, 28 - 31 March 2003, pp. 53 ? 58.\nKilgarriff, A. and Grefenstette, G (2003). Introduction\nto the Special Issue on the Web as Corpus. Compu-\ntational Linguistics, 29: 3, pp. 333-347.\nMair, C. (2005). The corpus-based study of language\nchange in progress: The extra value of tagged cor-\npora. Presentation at the AAACL/ICAME Confer-\nence, Ann Arbor, May 2005.\nResnik, P. and Elkiss, A. (2003) The Linguist's Search\nEngine: Getting Started Guide. Technical Report:\nLAMP-TR-108/CS-TR-4541/UMIACS-TR-2003-\n109, University of Maryland, College Park, No-\nvember 2003.\nRobb, T. (2003) Google as a Corpus Tool? In ETJ\nJournal, Volume 4, number 1, Spring 2003.\nRundell, M. (2000). &quot;The biggest corpus of all&quot;, Hu-\nmanising Language Teaching. 2:3; May 2000.\nShirky, C. (2001) Listening to Napster, in Peer-to-\nPeer: Harnessing the power of Disruptive Tech-\nnologies, O'Reilly.\nTurney, P. (2001). Word Sense Disambiguation by\nWeb Mining for Word Co-occurrence Probabili-\nties. In proceedings of SENSEVAL-3, Barcelona,\nSpain, July 2004 pp. 239-242.\nVeronis, J. (2005). Web: Google's missing pages:\nmystery solved?\nhttp://aixtal.blogspot.com/2005/02/web-googles-\nmissing-pages-mystery.html (accessed April 28,\n2005).\nWalkerdine, J., Gilleade, K., Hughes, D., Rayson, P.,\nSimms, J., Mariani, J., and Sommerville, I. A\nFramework for P2P Application Development. Pa-\nper submitted to Software Practice and Experience.\nWalkerdine, J. and Rayson, P. (2004) P2P-4-DL:\nDigital Library over Peer-to-Peer. In Caronni G.,\nWeiler N., Shahmehri N. (eds.) Proceedings of\nFourth IEEE International Conference on Peer-to-\nPeer Computing (PSP2004) 25-27 August 2004,\nZurich, Switzerland. IEEE Computer Society\nPress, pp. 264-265.\n"}],"#tail":"\n","bodyText":[{"#tail":"\n","@confidence":"0.994892928571429","#text":"\nThis paper presents a proposal to facili-\ntate the use of the annotated web as cor-\npus by alleviating the annotation bottle-\nneck for corpus data drawn from the web.\nWe describe a framework for large-scale\ndistributed corpus annotation using peer-\nto-peer (P2P) technology to meet this\nneed. We also propose to annotate a large\nreference corpus in order to evaluate this\nframework. This will allow us to investi-\ngate the affordances offered by distrib-\nuted techniques to ensure replicability of\nlinguistic research based on web-derived\ncorpora.\n"},{"#tail":"\n","@confidence":"0.9994","#text":"\nLinguistic annotation of corpora contributes cru-\ncially to the study of language at several levels:\nmorphology, syntax, semantics, and discourse.\nIts significance is reflected both in the growing\ninterest in annotation software for word sense\ntagging (Edmonds and Kilgarriff, 2002) and in\nthe long-standing use of part-of-speech taggers,\nparsers and morphological analysers for data\nfrom English and many other languages.\nLinguists, lexicographers, social scientists and\nother researchers are using ever larger amounts\nof corpus data in their studies. In corpus linguis-\ntics the progression has been from the 1 million-\nword Brown and LOB corpora of the 1960s, to\nthe 100 million-word British National Corpus of\nthe 1990s. In lexicography this progression is\nparalleled, for example, by Collins Dictionaries?\ninitial 10 million word corpus growing to their\ncurrent corpus of around 600 million words. In\naddition, the requirement for mega- and even\ngiga-corpora1 extends to other applications, such\nas lexical frequency studies, neologism research,\nand statistical natural language processing where\nmodels of sparse data are built. The motivation\nfor increasingly large data sets remains the same.\nDue to the Zipfian nature of word frequencies,\naround half the word types in a corpus occur\nonly once, so tremendous increases in corpus\nsize are required both to ensure inclusion of es-\nsential word and phrase types and to increase the\nchances of multiple occurrences of a given type.\nIn corpus linguistics building such mega-\ncorpora is beyond the scope of individual re-\nsearchers, and they are not easily accessible\n(Kennedy, 1998: 56) unless the web is used as a\ncorpus (Kilgarriff and Grefenstette, 2003). In-\ncreasingly, corpus researchers are tapping the\nWeb to overcome the sparse data problem (Kel-\nler et al, 2002). This topic generated intense in-\nterest at workshops held at the University of Hei-\ndelberg (October 2004), University of Bologna\n(January 2005), University of Birmingham (July\n2005) and now in Trento in April 2006. In addi-\ntion, the advantages of using linguistically anno-\ntated data over raw data are well documented\n(Mair, 2005; Granger and Rayson, 1998). As the\nsize of a corpus increases, a near linear increase\nin computing power is required to annotate the\ntext. Although processing power is steadily\ngrowing, it has already become impractical for a\nsingle computer to annotate a mega-corpus.\nCreating a large-scale annotated corpus from\nthe web requires a way to overcome the limita-\ntions on processing power. We propose distrib-\nuted techniques to alleviate the limitations on the\n"},{"#tail":"\n","@confidence":"0.994667595744681","#text":"\nvolume of data that can be tagged by a single\nprocessor. The task of annotating the data will be\nshared by computers at collaborating institutions\naround the world, taking advantage of processing\npower and bandwidth that would otherwise go\nunused. Such large-scale parallel processing re-\nmoves the workload bottleneck imposed by a\nserver based structure. This allows for tagging a\ngreater amount of textual data in a given amount\nof time while permitting other users to use the\nsystem simultaneously. Vast amounts of data can\nbe analysed with distributed techniques. The fea-\nsibility of this approach has been demonstrated\nby the SETI@home project2.\nThe framework we propose can incorporate\nother annotation or analysis systems, for exam-\nple, lemmatisation, frequency profiling, or shal-\nlow parsing. To realise and evaluate the frame-\nwork, it will be developed for a peer-to-peer\n(P2P) network and deployed along with an exist-\ning lexicographic toolset, the Sketch Engine. A\nP2P approach allows for a low cost implementa-\ntion that draws upon available resources (existing\nuser PCs). As a case study for evaluation, we\nplan to collect a large reference corpus from the\nweb to be hosted on servers from Lexical Com-\nputing Ltd. We can evaluate annotation speed\ngains of our approach comparatively against the\nsingle server version by utilising processing\npower in computer labs at Lancaster University\nand the United States Naval Academy (USNA)\nand we will call for volunteers from the corpus\ncommunity to be involved in the evaluation as\nwell.\nA key aspect of our case study research will be\nto investigate extending corpus collection to new\ndocument types. Most web-derived corpora have\nexploited raw text or HTML pages, so efforts\nhave focussed on boilerplate removal and clean-\nup of these formats with tools like Hyppia-BTE,\nTidy and Parcels 3 (Baroni and Sharoff, 2005).\nOther document formats such as Adobe PDF and\nMS-Word have been neglected due to the extra\nconversion and clean-up problems they entail.\nBy excluding PDF documents, web-derived cor-\npora are less representative of certain genres\nsuch as academic writing.\n"},{"#tail":"\n","@confidence":"0.985837978723404","#text":"\nThe vast majority of previous work on corpus\nannotation has utilised either manual coding or\nautomated software tagging systems, or else a\nsemi-automatic combination of the two ap-\nproaches e.g. automated tagging followed by\nmanual correction. In most cases a stand-alone\nsystem or client-server approach has been taken\nby annotation software using batch processing\ntechniques to tag corpora. Only a handful of\nweb-based or email services (CLAWS4, Amal-\ngam5, Connexor6) are available, for example, in\nthe application of part-of-speech tags to corpora.\nExisting tagging systems are ?small scale? and\ntypically impose some limitation to prevent over-\nload (e.g. restricted access or document size).\nLarger systems to support multiple document\ntagging processes would require resources that\ncannot be realistically provided by existing sin-\ngle-server systems. This corpus annotation bot-\ntleneck becomes even more problematic for vo-\nluminous data sets drawn from the web. The use\nof the web as a corpus for teaching and research\non language has been proposed a number of\ntimes (Kilgarriff, 2001; Robb, 2003; Rundell,\n2000; Fletcher, 2001, 2004b) and received a spe-\ncial issue of the journal Computational Linguis-\ntics (Kilgarriff and Grefenstette, 2003). Studies\nhave used several different methods to mine web\ndata. Turney (2001) extracts word co-occurrence\nprobabilities from unlabelled text collected from\na web crawler. Baroni and Bernardini (2004)\nbuilt a corpus by iteratively searching Google for\na small set of seed terms. Prototypes of Internet\nsearch engines for linguists, corpus linguists and\nlexicographers have been proposed: WebCorp\n(Kehoe and Renouf, 2002), KWiCFinder\n(Fletcher, 2004a) and the Linguist?s Search En-\ngine (Kilgarriff, 2003; Resnik and Elkiss, 2003).\nA key concern in corpus linguistics and related\ndisciplines is verifiability and replicability of the\nresults of studies. Word frequency counts in\ninternet search engines are inconsistent and unre-\nliable (Veronis, 2005). Tools based on static cor-\npora do not suffer from this problem, e.g.\nBNCweb7, developed at the University of Zurich,\nand View 8 (Variation in English Words and\nPhrases, developed at Brigham Young University)\n"},{"#tail":"\n","@confidence":"0.993379513761468","#text":"\nare both based on the British National Corpus.\nBoth BNCweb and View enable access to anno-\ntated corpora and facilitate searching on part-of-\nspeech tags. In addition, PIE9 (Phrases in Eng-\nlish), developed at USNA, which performs\nsearches on n-grams (based on words, parts-of-\nspeech and characters), is currently restricted to\nthe British National Corpus as well, although\nother static corpora are being added to its data-\nbase. In contrast, little progress has been made\ntoward annotating sizable sample corpora from\nthe web.\n?Real-time? linguistic analysis of web data at\nthe syntactic level has been piloted by the Lin-\nguist?s Search Engine (LSE). Using this tool,\nlinguists can either perform syntactic searches\nvia parse trees on a pre-analysed web collection\nof around three million sentences from the Inter-\nnet Archive (www.archive.org) or build their\nown collections from AltaVista search engine\nresults. The second method pushes the new col-\nlection onto a queue for the LSE annotator to\nanalyse. A new collection does not become\navailable for analysis until the LSE completes\nthe annotation process, which may entail signifi-\ncant delay with multiple users of the LSE server.\nThe Gsearch system (Corley et al, 2001) also\nselects sentences by syntactic criteria from large\non-line text collections. Gsearch annotates cor-\npora with a fast chart parser to obviate the need\nfor corpora with pre-existing syntactic mark-up.\nIn contrast, the Sketch Engine system to assist\nlexicographers to construct dictionary entries\nrequires large pre-annotated corpora. A word\nsketch is an automatic one-page corpus-derived\nsummary of a word's grammatical and colloca-\ntional behaviour. Word Sketches were first used\nto prepare the Macmillan English Dictionary for\nAdvanced Learners (2002, edited by Michael\nRundell). They have also served as the starting\npoint for high-accuracy Word Sense Disam-\nbiguation. More recently, the Sketch Engine was\nused to develop the new edition of the Oxford\nThesaurus of English (2004, edited by Maurice\nWaite).\nParallelising or distributing processing has\nbeen suggested before. Clark and Curran?s (2004)\nwork is in parallelising an implementation of\nlog-linear parsing on the Wall Street Journal\nCorpus, whereas we focus on part-of-speech tag-\nging of a far larger and more varied web corpus,\na technique more widely considered a prerequi-\nsite for corpus linguistics research. Curran (2003)\n9 http://pie.usna.edu/\nsuggested distributed processing in terms of web\nservices but only to ?allow components devel-\noped by different researchers in different loca-\ntions to be composed to build larger systems?\nand not for parallel processing. Most signifi-\ncantly, previous investigations have not exam-\nined three essential questions: how to apply dis-\ntributed techniques to vast quantities of corpus\ndata derived from the web, how to ensure that\nweb-derived corpora are representative, and how\nto provide verifiability and replicability. These\ncore foci of our work represent crucial innova-\ntions lacking in prior research. In particular, rep-\nresentativeness and replicability are key research\nconcerns to enhance the reliability of web data\nfor corpora.\nIn the areas of Natural Language Processing\n(NLP) and computational linguistics, proposals\nhave been made for using the computational Grid\nfor data-intensive NLP and text-mining for e-\nScience (Carroll et al, 2005; Hughes et al 2004).\nWhile such an approach promises much in terms\nof emerging infrastructure, we wish to exploit\nexisting computing infrastructure that is more\naccessible to linguists via a P2P approach. In\nsimple terms, P2P is a technology that takes ad-\nvantage of the resources and services available at\nthe edge of the Internet (Shirky, 2001). Better\nknown for file-sharing and Instant Messenger\napplications, P2P has increasingly been applied\nin distributed computational systems. Examples\ninclude SETI@home (looking for radio evidence\nof extraterrestrial life), ClimatePrediction.net\n(studying climate change), Predictor@home (in-\nvestigating protein-related diseases) and Ein-\nstein@home (searching for gravitational signals).\nA key advantage of P2P systems is that they\nare lightweight and geared to personal computing\nwhere informal groups provide unused process-\ning power to solve a common problem. Typically,\nP2P systems draw upon the resources that al-\nready exist on a network (e.g. home or work\nPCs), thus keeping the cost to resource ratio low.\nFor example the fastest supercomputer cost over\n$110 million to develop and has a peak perform-\nance of 12.3 TFLOPS (trillions of floating-point\noperations per second). In contrast, a typical day\nfor the SETI@home project involved a perform-\nance of over 20 TFLOPS, yet cost only $700,000\nto develop; processing power is donated by user\nPCs. This high yield for low start-up cost makes\nit ideal for cheaply developing effective compu-\ntational systems to realise, deploy and evaluate\nour framework. The deployment of computa-\ntional based P2P systems is supported by archi-\n"},{"#tail":"\n","@confidence":"0.996477714285714","#text":"\ntectures such as BOINC10, which provide a plat-\nform on which volunteer based distributed com-\nputing systems can be built. Lancaster's own P2P\nApplication Framework (Walkerdine et al, sub-\nmitted) also supports higher-level P2P applica-\ntion development and can be adapted to make\nuse of the BOINC architecture.\n"},{"#tail":"\n","@confidence":"0.983797045454545","#text":"\nOur research hypothesis is that distributed com-\nputational techniques can alleviate the annotation\nbottleneck for processing corpus data from the\nweb. This leads us to a number of research ques-\ntions:\n? How can corpus data from the web be di-\nvided into units for processing via distrib-\nuted techniques?\n? Which corpus annotation techniques are\nsuitable for distributed processing?\n? Can distributed techniques assist in corpus\nclean-up and conversion to allow inclu-\nsion of a wider variety of genres and to\nsupport more representative corpora?\nIn the early stages of our proposed research,\nwe are focussing on grammatical word-class\nanalysis (part-of-speech tagging) of web-derived\ncorpora of English and aspects of corpus clean-\nup and conversion. Clarifying copyright issues\nand exploring models for legal dissemination of\ncorpora compiled from web data are key objec-\ntives of this stage of the investigation as well.\n"},{"#tail":"\n","@confidence":"0.88585096","#text":"\nThe initial focus of the work will be to develop\nthe framework for distributed corpus annotation.\nSince existing solutions have been centralised in\nnature, we first must examine the consequences\nthat a distributed approach has for corpus annota-\ntion and identify issues to address.\nA key concern will be handling web pages\nwithin the framework, as it is essential to mini-\nmise the amount of data communicated between\npeers. Unlike the other distributed analytical sys-\ntems mentioned above, the size of text document\nand analysis time is largely proportional for cor-\npora annotation. This places limitations on work\nunit size and distribution strategies. In particular,\nthree areas will be investigated:\n? Mechanisms for crawling/discovery of a\nweb corpus domain - how to identify\npages to include in a web corpus. Also\n10 BOINC, Berkeley Open Infrastructure for Network\nComputing. http://boinc.berkeley.edu.\ninvestigate appropriate criteria for han-\ndling pages which are created or modi-\nfied dynamically.\n? Mechanisms to generate work units for\ndistributed computation - how to split\nthe corpus into work units and reduce the\ncommunication / computation time ratio\nthat is crucial for such systems to be ef-\nfective.\n? Mechanisms to support the distribution\nof work units and collection of results -\nhow to handle load balancing. What data\nshould be sent to peers and how is the\nprocessed information handled and ma-\nnipulated? What mechanisms should be\nin place to ensure correctness of results?\nHow can abuse be prevented and secu-\nrity concerns of collaborating institutions\nbe addressed? BOINC already provides\na good platform for this, and these as-\npects will be investigated within the pro-\nject.\nAnalysis of existing distributed computation\nsystems will help to inform the design of the\nframework and tackle some of these issues. Fi-\nnally, the framework will also cater for three\ncommon strategies for corpus annotation:\n? Site based corpus annotation - in which\nthe user can specify a web site to anno-\ntate\n? Domain based corpus annotation - in\nwhich the user specifies a content do-\nmain (with the use of keywords) to an-\nnotate\n? Crawler based corpus annotation - more\ngeneral web based corpus annotation in\nwhich crawlers are used to locate web\npages\nFrom a computational linguistic view, the\nframework will also need to take into account the\ngranularity of the unit (for example, POS tagging\nrequires sentence-units, but anaphoric annotation\nneeds paragraphs or larger). Secondly, we need\nto investigate techniques for identifying identical\ndocuments, virtually identical documents and\nhighly repetitive documents, such as those pio-\nneered by Fletcher (2004b) and shingling tech-\nniques described by Chakrabarti (2002).\nThe second stage of our work will involve im-\nplementing the framework within a P2P envi-\nronment. We have already developed a prototype\nof an object-oriented application environment to\nsupport P2P system development using JXTA\n(Sun's P2P API). We have designed this envi-\nronment so that specific application functionality\n"},{"#tail":"\n","@confidence":"0.999017090909091","#text":"\ncan be captured within plug-ins that can then in-\ntegrate with the environment and utilise its func-\ntionality. This system has been successfully\ntested with the development of plug-ins support-\ning instant messaging, distributed video encoding\n(Hughes and Walkerdine, 2005), distributed vir-\ntual worlds (Hughes et al, 2005) and digital li-\nbrary management (Walkerdine and Rayson,\n2004). It is our intention to implement our dis-\ntributed corpus annotation framework as a plug-\nin. This will involve implementing new func-\ntionality and integrating this with our existing\nannotation tools (such as CLAWS11). The devel-\nopment environment is also flexible enough to\nutilise the BOINC platform, and such support\nwill be built into it.\nUsing the P2P Application Framework as a\nbasis for the development secures several advan-\ntages. First, it reduces development time by al-\nlowing the developer to reuse existing function-\nality; secondly, it already supports essential as-\npects such as system security; and thirdly, it has\nalready been used successfully to deploy compa-\nrable P2P applications. A lightweight version of\nthe application framework will be bundled with\nthe corpus annotation plug-in, and this will then\nbe made publicly available for download in\nopen-source and executable formats. We envis-\nage our end-users will come from a variety of\ndisciplines such as language engineering and lin-\nguistics. For the less-technical users, the proto-\ntype will be packaged as a screensaver or instant\nmessaging client to facilitate deployment.\n"},{"#tail":"\n","@confidence":"0.999353588235294","#text":"\nWe will evaluate the framework and prototype\ndeveloped by applying it as a pre-processor step\nfor the Sketch Engine system. The Sketch Engine\nrequires a large well-balanced corpus which has\nbeen part-of-speech tagged and shallow parsed to\nfind subjects, objects, heads, and modifiers. We\nwill use the existing non-distributed processing\ntools on the Sketch Engine as a baseline for a\ncomparative evaluation of the AWAC frame-\nwork instantiation by utilising processing power\nand bandwidth in learning labs at Lancaster Uni-\nversity and USNA during off hours.\nWe will explore techniques to make the result-\ning annotated web corpus data available in static\nform to enable replication and verification of\ncorpus studies based on such data. The initial\nsolution will be to store the resulting reference\n"},{"#tail":"\n","@confidence":"0.993441534883721","#text":"\ncorpus in the Sketch Engine. We will also inves-\ntigate whether the distributed environment un-\nderlying our approach offers a solution to the\nproblem of reproducibility in web-based corpus\nstudies based in general. Current practise else-\nwhere includes the distribution of URL lists, but\ngiven the dynamic nature of the web, this is not\nsufficiently robust. Other solutions such as com-\nplete caching of the corpora are not typically\nadopted due to legal concerns over copyright and\nredistribution of web data, issues considered at\nlength by Fletcher (2004a). Other requirements\nfor reference corpora such as retrieval and stor-\nage of metadata for web pages are beyond the\nscope of what we propose here.\nTo improve the representative nature of web-\nderived corpora, we will research techniques to\nenable the importing of additional document\ntypes such as PDF. We will reuse and extend\ntechniques implemented in the collection, encod-\ning and annotation of the PERC Corpus of Pro-\nfessional English12. A majority of this corpus has\nbeen collected by conversion of on-line academic\njournal articles from PDF to XML with a combi-\nnation of semi-automatic tools and techniques\n(including Adobe Acrobat version 6). Basic is-\nsues such as character encoding, table/figure ex-\ntraction and maintaining text flow around em-\nbedded images need to be dealt with before an-\nnotation processing can begin. We will compara-\ntively evaluate our techniques against others such\nas pdf2txt, and Multivalent PDF ExtractText13.\nPart of the evaluation will be to collect and anno-\ntate a sample corpus. We aim to collect a corpus\nfrom the web that is comparable to the BNC in\ncontent and annotation. This corpus will be\ntagged using the P2P framework. It will form a\ntest-bed for the framework and we will utilise the\nnon-distributed annotation system on the Sketch\nEngine as a baseline for comparison and evalua-\ntion. To evaluate text conversion and clean-up\nroutines for PDF documents, we will use a 5-\nmillion-word gold-standard sub-corpus extracted\n"},{"#tail":"\n","@confidence":"0.788531666666667","#text":"\njor research project of PERC (the Professional Eng-\nlish Research Consortium) currently underway that,\nwhen finished, will consist of a 100-million-word\ncomputerised database of English used by profession-\nals in science, engineering, technology and other\nfields. Lancaster University and Shogakukan Inc. are\n"},{"#tail":"\n","@confidence":"0.7364425","#text":"\nfrom the PERC Corpus of Professional\nEnglish14.\n"},{"#tail":"\n","@confidence":"0.999152555555556","#text":"\nFuture work includes an analysis of the balance\nbetween computational and bandwidth require-\nments. It is essential in distributing the corpus\nannotation to achieve small amounts of data\ntransmission in return for large computational\ngains for each work-unit.\nIn this paper, we have discussed the require-\nment for annotation of web-derived corpus data.\nCurrently, a bottleneck exists in the tagging of\nweb-derived corpus data due to the voluminous\namount of corpus processing involved. Our pro-\nposal is to construct a framework for large-scale\ndistributed corpus annotation using existing peer-\nto-peer technology. We have presented the chal-\nlenges that lie ahead for such an approach. Work\nis now underway to address the clean-up of PDF\ndata for inclusion into corpora downloaded from\nthe web.\n"},{"#tail":"\n","@confidence":"0.995653875","#text":"\nWe wish to thank the anonymous reviewers who\ncommented our paper. We are grateful to Shoga-\nkukan Inc. (Tokyo, Japan) for supporting re-\nsearch at Lancaster University into the process of\nconversion and clean-up of PDF to text, and to\nthe Professional English Research Consortium\nfor the provision of the gold-standard corpus for\nour evaluation.\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.600959","#text":"\nLancaster University, UK\n"},{"#tail":"\n","@confidence":"0.725966","#text":"\nLancaster University, UK\n"},{"#tail":"\n","@confidence":"0.5902765","#text":"\nUnited States Naval\nAcademy, USA\n"},{"#tail":"\n","@confidence":"0.934944","#text":"\nLexical Computing Ltd., UK\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.990406","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.997727","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.99346","@genericHeader":"related work","#text":"\n2 Related Work\n"},{"#tail":"\n","@confidence":"0.956656","@genericHeader":"method","#text":"\n3 Research hypothesis and aims\n"},{"#tail":"\n","@confidence":"0.998771","@genericHeader":"method","#text":"\n4 Methodology\n"},{"#tail":"\n","@confidence":"0.994852","@genericHeader":"method","#text":"\n5 Evaluation\n"},{"#tail":"\n","@confidence":"0.47522","@genericHeader":"method","#text":"\n12 The Corpus of Professional English (CPE) is a ma-\n"},{"#tail":"\n","@confidence":"0.98035","@genericHeader":"conclusions","#text":"\n6 Conclusion\n"},{"#tail":"\n","@confidence":"0.943984","@genericHeader":"acknowledgments","#text":"\nAcknowledgements\n"},{"#tail":"\n","@confidence":"0.972857","@genericHeader":"references","#text":"\nReferences\n"}],"page":[{"#tail":"\n","@confidence":"0.999716","#text":"\n27\n"},{"#tail":"\n","@confidence":"0.999489","#text":"\n28\n"},{"#tail":"\n","@confidence":"0.991028","#text":"\n29\n"},{"#tail":"\n","@confidence":"0.991997","#text":"\n30\n"},{"#tail":"\n","@confidence":"0.646735","#text":"\n11 http://www.comp.lancs.ac.uk/ucrel/claws/\n"},{"#tail":"\n","@confidence":"0.991634","#text":"\n31\n"},{"#tail":"\n","@confidence":"0.977928","#text":"\n32\n"},{"#tail":"\n","@confidence":"0.9932835","#text":"\n33\n34\n"}],"email":[{"#tail":"\n","@confidence":"0.808394","#text":"\np.rayson@lancs.ac.uk\n"},{"#tail":"\n","@confidence":"0.97664","#text":"\nj.walkerdine@lancs.ac.uk\n"},{"#tail":"\n","@confidence":"0.979213","#text":"\nfletcher@usna.edu\n"},{"#tail":"\n","@confidence":"0.989685","#text":"\nadam@lexmasterclass.com\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.587192","#tail":"\n","@no":"0","address":{"#tail":"\n","@confidence":"0.839341","#text":"Academy, USA"},"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.9859655","#text":"Computing Department, Lancaster University, UK"},{"#tail":"\n","@confidence":"0.97889","#text":"Computing Department, Lancaster University, UK"},{"#tail":"\n","@confidence":"0.966758","#text":"United States Naval"},{"#tail":"\n","@confidence":"0.945121","#text":"Lexical Computing Ltd., UK"}],"author":[{"#tail":"\n","@confidence":"0.999945","#text":"Paul Rayson"},{"#tail":"\n","@confidence":"0.999446","#text":"James Walkerdine"},{"#tail":"\n","@confidence":"0.998945","#text":"William H Fletcher"},{"#tail":"\n","@confidence":"0.999523","#text":"Adam Kilgarriff"}],"abstract":{"#tail":"\n","@confidence":"0.991007866666667","#text":"This paper presents a proposal to facilitate the use of the annotated web as corpus by alleviating the annotation bottleneck for corpus data drawn from the web. We describe a framework for large-scale distributed corpus annotation using peerto-peer (P2P) technology to meet this need. We also propose to annotate a large reference corpus in order to evaluate this framework. This will allow us to investigate the affordances offered by distributed techniques to ensure replicability of linguistic research based on web-derived corpora."},"title":{"#tail":"\n","@confidence":"0.981878","#text":"Annotated web as corpus"},"email":[{"#tail":"\n","@confidence":"0.973854","#text":"p.rayson@lancs.ac.uk"},{"#tail":"\n","@confidence":"0.987009","#text":"j.walkerdine@lancs.ac.uk"},{"#tail":"\n","@confidence":"0.99986","#text":"fletcher@usna.edu"},{"#tail":"\n","@confidence":"0.998547","#text":"adam@lexmasterclass.com"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Baroni, M. and Bernardini, S. (2004). BootCaT: Bootstrapping Corpora and Terms from the Web."},"#text":"\n","marker":{"#tail":"\n","#text":"Baroni, Bernardini, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"vided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable (Veronis, 2005). Tools based on static corpora do not suffer from th","@endWordPosition":"1111","@position":"7289","annotationId":"T1","@startWordPosition":"1108","@citStr":"Baroni and Bernardini (2004)"}},"booktitle":{"#tail":"\n","#text":"BootCaT: Bootstrapping Corpora and Terms from the Web."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Baroni"},{"#tail":"\n","#text":"S Bernardini"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In Proceedings of LREC2004, Lisbon, pp. 1313-1316."},"#text":"\n","pages":{"#tail":"\n","#text":"1313--1316"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Lisbon,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of LREC2004,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Baroni, M. and Sharoff, S. (2005). Creating specialized and general corpora using automated search engine queries. Web as Corpus Workshop, Birmingham University, UK, 14th July 2005."},"#text":"\n","marker":{"#tail":"\n","#text":"Baroni, Sharoff, 2005"},"location":{"#tail":"\n","#text":"Birmingham University, UK,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tation speed gains of our approach comparatively against the single server version by utilising processing power in computer labs at Lancaster University and the United States Naval Academy (USNA) and we will call for volunteers from the corpus community to be involved in the evaluation as well. A key aspect of our case study research will be to investigate extending corpus collection to new document types. Most web-derived corpora have exploited raw text or HTML pages, so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE, Tidy and Parcels 3 (Baroni and Sharoff, 2005). Other document formats such as Adobe PDF and MS-Word have been neglected due to the extra conversion and clean-up problems they entail. By excluding PDF documents, web-derived corpora are less representative of certain genres such as academic writing. 2 http://setiathome.ssl.berkeley.edu/ 3 http://www.smi.ucd.ie/hyppia/, http://parcels.sourceforge.net and http://tidy.sourceforge.net. 2 Related Work The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g. aut","@endWordPosition":"846","@position":"5447","annotationId":"T2","@startWordPosition":"843","@citStr":"Baroni and Sharoff, 2005"}},"title":{"#tail":"\n","#text":"Creating specialized and general corpora using automated search engine queries. Web as Corpus Workshop,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Baroni"},{"#tail":"\n","#text":"S Sharoff"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Carroll, J., R. Evans and E. Klein (2005) Supporting text mining for e-Science: the challenges for Gridenabled natural language processing. In Workshop on Text Mining, e-Research And Grid-enabled Language Technology at the Fourth UK e-Science Programme All Hands Meeting (AHM2005), Nottingham, UK. 14 This corpus has already been manually re-typed at Shogakukan Inc. from PDF originals downloaded from the web."},"#text":"\n","marker":{"#tail":"\n","#text":"Carroll, Evans, Klein, 2005"},"location":{"#tail":"\n","#text":"Nottingham, UK."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ly distributed techniques to vast quantities of corpus data derived from the web, how to ensure that web-derived corpora are representative, and how to provide verifiability and replicability. These core foci of our work represent crucial innovations lacking in prior research. In particular, representativeness and replicability are key research concerns to enhance the reliability of web data for corpora. In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience (Carroll et al, 2005; Hughes et al 2004). While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001). Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems. Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying ","@endWordPosition":"1752","@position":"11611","annotationId":"T3","@startWordPosition":"1749","@citStr":"Carroll et al, 2005"}},"title":{"#tail":"\n","#text":"Supporting text mining for e-Science: the challenges for Gridenabled natural language processing."},"booktitle":{"#tail":"\n","#text":"In Workshop on Text Mining, e-Research And Grid-enabled Language Technology at the Fourth UK e-Science Programme All Hands Meeting (AHM2005),"},"@valid":"false","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Carroll"},{"#tail":"\n","#text":"R Evans"},{"#tail":"\n","#text":"E Klein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Chakrabarti, S. (2002) Mining the Web: Discovering Knowledge from Hypertext Data. Morgan Kaufmann."},"#text":"\n","marker":{"#tail":"\n","#text":"Chakrabarti, 2002"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"use of keywords) to annotate ? Crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by Fletcher (2004b) and shingling techniques described by Chakrabarti (2002). The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API). We have designed this environment so that specific application functionality 30 can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging, distributed video encoding (Hughes and Walkerdine, 2005), distrib","@endWordPosition":"2631","@position":"17199","annotationId":"T4","@startWordPosition":"2630","@citStr":"Chakrabarti (2002)"}},"title":{"#tail":"\n","#text":"Mining the Web: Discovering Knowledge from Hypertext Data."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"S Chakrabarti"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Clark, S. and Curran, J. R.. (2004). Parsing the wsj using ccg and log-linear models. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ?04)."},"#text":"\n","marker":{"#tail":"\n","#text":"Clark, Curran, 2004"},"title":{"#tail":"\n","#text":"Parsing the wsj using ccg and log-linear models."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ?04)."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Clark"},{"#tail":"\n","#text":"J R Curran"}]}},{"volume":{"#tail":"\n","#text":"35"},"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Corley, S., Corley, M., Keller, F., Crocker, M., & Trewin, S. (2001). Finding Syntactic Structure in Unparsed Corpora: The Gsearch Corpus Query System. Computers and the Humanities, 35, 81-94."},"#text":"\n","pages":{"#tail":"\n","#text":"81--94"},"marker":{"#tail":"\n","#text":"Corley, Corley, Keller, Crocker, Trewin, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"en piloted by the Linguist?s Search Engine (LSE). Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org) or build their own collections from AltaVista search engine results. The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system (Corley et al, 2001) also selects sentences by syntactic criteria from large on-line text collections. Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour. Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell). They have also ser","@endWordPosition":"1431","@position":"9465","annotationId":"T5","@startWordPosition":"1428","@citStr":"Corley et al, 2001"}},"title":{"#tail":"\n","#text":"Finding Syntactic Structure in Unparsed Corpora: The Gsearch Corpus Query System. Computers and the Humanities,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Corley"},{"#tail":"\n","#text":"M Corley"},{"#tail":"\n","#text":"F Keller"},{"#tail":"\n","#text":"M Crocker"},{"#tail":"\n","#text":"S Trewin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Curran, J.R. (2003). Blueprint for a High Performance NLP Infrastructure. In Proc. of Workshop on Software Engineering and Architecture of Language Technology Systems (SEALTS) Edmonton, Canada, 2003, pp. 40 ? 45."},"#text":"\n","pages":{"#tail":"\n","#text":"40--45"},"marker":{"#tail":"\n","#text":"Curran, 2003"},"location":{"#tail":"\n","#text":"Edmonton, Canada,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Rundell). They have also served as the starting point for high-accuracy Word Sense Disambiguation. More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite). Parallelising or distributing processing has been suggested before. Clark and Curran?s (2004) work is in parallelising an implementation of log-linear parsing on the Wall Street Journal Corpus, whereas we focus on part-of-speech tagging of a far larger and more varied web corpus, a technique more widely considered a prerequisite for corpus linguistics research. Curran (2003) 9 http://pie.usna.edu/ suggested distributed processing in terms of web services but only to ?allow components developed by different researchers in different locations to be composed to build larger systems? and not for parallel processing. Most significantly, previous investigations have not examined three essential questions: how to apply distributed techniques to vast quantities of corpus data derived from the web, how to ensure that web-derived corpora are representative, and how to provide verifiability and replicability. These core foci of our work represent crucial innovations lacking","@endWordPosition":"1608","@position":"10650","annotationId":"T6","@startWordPosition":"1607","@citStr":"Curran (2003)"}},"title":{"#tail":"\n","#text":"Blueprint for a High Performance NLP Infrastructure."},"booktitle":{"#tail":"\n","#text":"In Proc. of Workshop on Software Engineering and Architecture of Language Technology Systems (SEALTS)"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J R Curran"}}},{"date":{"#tail":"\n","#text":"2002"},"issue":{"#tail":"\n","#text":"2"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"istributed corpus annotation using peerto-peer (P2P) technology to meet this need. We also propose to annotate a large reference corpus in order to evaluate this framework. This will allow us to investigate the affordances offered by distributed techniques to ensure replicability of linguistic research based on web-derived corpora. 1 Introduction Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse. Its significance is reflected both in the growing interest in annotation software for word sense tagging (Edmonds and Kilgarriff, 2002) and in the long-standing use of part-of-speech taggers, parsers and morphological analysers for data from English and many other languages. Linguists, lexicographers, social scientists and other researchers are using ever larger amounts of corpus data in their studies. In corpus linguistics the progression has been from the 1 millionword Brown and LOB corpora of the 1960s, to the 100 million-word British National Corpus of the 1990s. In lexicography this progression is paralleled, for example, by Collins Dictionaries? initial 10 million word corpus growing to their current corpus of around 60","@endWordPosition":"165","@position":"1171","annotationId":"T7","@startWordPosition":"162","@citStr":"Edmonds and Kilgarriff, 2002"}},"title":{"#tail":"\n","#text":"Introduction to the special issue on evaluating word sense disambiguation systems."},"volume":{"#tail":"\n","#text":"8"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Edmonds, P and Kilgarriff, A. (2002). Introduction to the special issue on evaluating word sense disambiguation systems. Journal of Natural Language Engineering, 8 (2), pp. 279-291."},"journal":{"#tail":"\n","#text":"Journal of Natural Language Engineering,"},"#text":"\n","pages":{"#tail":"\n","#text":"279--291"},"marker":{"#tail":"\n","#text":"Edmonds, Kilgarriff, 2002"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P Edmonds"},{"#tail":"\n","#text":"A Kilgarriff"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Fletcher, W. H. (2001).  Concordancing the Web with KWiCFinder.  Third North American Symposium on Corpus Linguistics and Language Teaching, Boston, MA, 23-25 March 2001."},"#text":"\n","pages":{"#tail":"\n","#text":"23--25"},"marker":{"#tail":"\n","#text":"Fletcher, 2001"},"location":{"#tail":"\n","#text":"Boston, MA,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"on of part-of-speech tags to corpora. Existing tagging systems are ?small scale? and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Ki","@endWordPosition":"1066","@position":"6976","annotationId":"T8","@startWordPosition":"1065","@citStr":"Fletcher, 2001"}},"title":{"#tail":"\n","#text":"Concordancing the Web with KWiCFinder. Third North American"},"booktitle":{"#tail":"\n","#text":"Symposium on Corpus Linguistics and Language Teaching,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W H Fletcher"}}},{"date":{"#tail":"\n","#text":"2004"},"editor":{"#tail":"\n","#text":"In G. Aston, S. Bernardini and D. Stewart (eds.),"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"riff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable (Veronis, 2005). Tools based on static corpora do not suffer from this problem, e.g. BNCweb7, developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) 4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html 5 http://www.comp.leeds.ac.uk/amalgam/a","@endWordPosition":"1147","@position":"7537","annotationId":"T9","@startWordPosition":"1146","@citStr":"Fletcher, 2004"},{"#tail":"\n","#text":"in which the user specifies a content domain (with the use of keywords) to annotate ? Crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by Fletcher (2004b) and shingling techniques described by Chakrabarti (2002). The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API). We have designed this environment so that specific application functionality 30 can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging, distri","@endWordPosition":"2623","@position":"17140","annotationId":"T10","@startWordPosition":"2622","@citStr":"Fletcher (2004"},{"#tail":"\n","#text":" resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine. We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general. Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust. Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by Fletcher (2004a). Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here. To improve the representative nature of webderived corpora, we will research techniques to enable the importing of additional document types such as PDF. We will reuse and extend techniques implemented in the collection, encoding and annotation of the PERC Corpus of Professional English12. A majority of this corpus has been collected by conversion of on-line academic journal articles from PDF to XML with a combination of semi-automatic tools and tec","@endWordPosition":"3134","@position":"20436","annotationId":"T11","@startWordPosition":"3133","@citStr":"Fletcher (2004"}]},"title":{"#tail":"\n","#text":"Facilitating the compilation and dissemination of ad-hoc Web corpora."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Fletcher, W. H. (2004a). Facilitating the compilation and dissemination of ad-hoc Web corpora. In G. Aston, S. Bernardini and D. Stewart (eds.), Corpora and Language Learners, pp. 271 ? 300, John Benjamins, Amsterdam."},"#text":"\n","pages":{"#tail":"\n","#text":"271--300"},"marker":{"#tail":"\n","#text":"Fletcher, 2004"},"location":{"#tail":"\n","#text":"John Benjamins, Amsterdam."},"booktitle":{"#tail":"\n","#text":"Corpora and Language Learners,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W H Fletcher"}}},{"date":{"#tail":"\n","#text":"2004"},"editor":{"#tail":"\n","#text":"In Ulla Connor and Thomas A. Upton (eds.)"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"riff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable (Veronis, 2005). Tools based on static corpora do not suffer from this problem, e.g. BNCweb7, developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) 4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html 5 http://www.comp.leeds.ac.uk/amalgam/a","@endWordPosition":"1147","@position":"7537","annotationId":"T12","@startWordPosition":"1146","@citStr":"Fletcher, 2004"},{"#tail":"\n","#text":"in which the user specifies a content domain (with the use of keywords) to annotate ? Crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by Fletcher (2004b) and shingling techniques described by Chakrabarti (2002). The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API). We have designed this environment so that specific application functionality 30 can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging, distri","@endWordPosition":"2623","@position":"17140","annotationId":"T13","@startWordPosition":"2622","@citStr":"Fletcher (2004"},{"#tail":"\n","#text":" resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine. We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general. Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust. Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by Fletcher (2004a). Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here. To improve the representative nature of webderived corpora, we will research techniques to enable the importing of additional document types such as PDF. We will reuse and extend techniques implemented in the collection, encoding and annotation of the PERC Corpus of Professional English12. A majority of this corpus has been collected by conversion of on-line academic journal articles from PDF to XML with a combination of semi-automatic tools and tec","@endWordPosition":"3134","@position":"20436","annotationId":"T14","@startWordPosition":"3133","@citStr":"Fletcher (2004"}]},"title":{"#tail":"\n","#text":"Making the Web More Useful as a Source for Linguistic Corpora."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Fletcher, W. H. (2004b). Making the Web More Useful as a Source for Linguistic Corpora. In Ulla Connor and Thomas A. Upton (eds.) Applied Corpus Linguistics. A Multidimensional Perspective. Rodopi, Amsterdam, pp. 191 ? 205."},"#text":"\n","pages":{"#tail":"\n","#text":"191--205"},"marker":{"#tail":"\n","#text":"Fletcher, 2004"},"location":{"#tail":"\n","#text":"Rodopi, Amsterdam,"},"booktitle":{"#tail":"\n","#text":"Applied Corpus Linguistics. A Multidimensional Perspective."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"W H Fletcher"}}},{"date":{"#tail":"\n","#text":"1998"},"editor":{"#tail":"\n","#text":"In S. Granger (ed.)"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"he scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al, 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005; Granger and Rayson, 1998). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus. Creating a large-scale annotated corpus from the web requires a way to overcome the limitations on processing power. We propose distributed techniques to alleviate the limitations on the 1 See, for example, those distributed by the Linguistic Data Consortium: http://www.ldc.upenn.edu/ 27 volume of data that can be tagged by a single processor. The task of","@endWordPosition":"463","@position":"3053","annotationId":"T15","@startWordPosition":"460","@citStr":"Granger and Rayson, 1998"}},"title":{"#tail":"\n","#text":"Automatic profiling of learner texts."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Granger, S., and Rayson, P. (1998). Automatic profiling of learner texts. In S. Granger (ed.) Learner English on Computer. Longman, London and New York, pp. 119-131."},"#text":"\n","pages":{"#tail":"\n","#text":"119--131"},"marker":{"#tail":"\n","#text":"Granger, Rayson, 1998"},"location":{"#tail":"\n","#text":"New York,"},"booktitle":{"#tail":"\n","#text":"Learner English on Computer. Longman, London and"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Granger"},{"#tail":"\n","#text":"P Rayson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Hughes, B, Bird, S., Haejoong, K., and Klein, E. (2004). Experiments with data-intensive NLP on a computational grid. Proceedings of the International Workshop on Human Language Technology. http://eprints.unimelb.edu.au/archive/00000503/. Hughes, D., Gilleade, K., Walkerdine, J. and Mariani, J., Exploiting P2P in the Creation of Game Worlds."},"#text":"\n","marker":{"#tail":"\n","#text":"Hughes, Bird, Haejoong, Klein, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ques to vast quantities of corpus data derived from the web, how to ensure that web-derived corpora are representative, and how to provide verifiability and replicability. These core foci of our work represent crucial innovations lacking in prior research. In particular, representativeness and replicability are key research concerns to enhance the reliability of web data for corpora. In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience (Carroll et al, 2005; Hughes et al 2004). While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001). Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems. Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Pre","@endWordPosition":"1756","@position":"11631","annotationId":"T16","@startWordPosition":"1753","@citStr":"Hughes et al 2004"}},"title":{"#tail":"\n","#text":"Experiments with data-intensive NLP on a computational grid."},"booktitle":{"#tail":"\n","#text":"Proceedings of the International Workshop on Human Language Technology. http://eprints.unimelb.edu.au/archive/00000503/."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"B Hughes"},{"#tail":"\n","#text":"S Bird"},{"#tail":"\n","#text":"K Haejoong"},{"#tail":"\n","#text":"E Klein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"In the proceedings of ACM GDTW 2005, Liverpool, UK, 8th-9th November, 2005."},"#text":"\n","marker":{"#tail":"\n","#text":"2005"},"location":{"#tail":"\n","#text":"Liverpool, UK, 8th-9th"},"booktitle":{"#tail":"\n","#text":"In the proceedings of ACM GDTW 2005,"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Hughes, D. and Walkerdine, J. (2005), Distributed Video Encoding Over A Peer-to-Peer Network. In the proceedings of PREP 2005, Lancaster, UK, 30th March - 1st April, 2005 Kehoe, A. and Renouf, A. (2002) WebCorp: Applying the Web to Linguistics and Linguistics to the Web."},"#text":"\n","marker":{"#tail":"\n","#text":"Hughes, Walkerdine, 2005"},"location":{"#tail":"\n","#text":"Lancaster, UK, 30th"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"iques described by Chakrabarti (2002). The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API). We have designed this environment so that specific application functionality 30 can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging, distributed video encoding (Hughes and Walkerdine, 2005), distributed virtual worlds (Hughes et al, 2005) and digital library management (Walkerdine and Rayson, 2004). It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS11). The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it. Using the P2P Application Framework as a basis for the development secures several advantages. First, it reduces development time by allowing the devel","@endWordPosition":"2721","@position":"17790","annotationId":"T17","@startWordPosition":"2718","@citStr":"Hughes and Walkerdine, 2005"}},"title":{"#tail":"\n","#text":"Distributed Video Encoding Over A Peer-to-Peer Network."},"booktitle":{"#tail":"\n","#text":"In the proceedings of PREP 2005,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Hughes"},{"#tail":"\n","#text":"J Walkerdine"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"World Wide Web 2002 Conference, Honolulu, Hawaii."},"#text":"\n","marker":{"#tail":"\n","#text":"Web, 2002"},"location":{"#tail":"\n","#text":"Conference, Honolulu, Hawaii."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"World Wide Web"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Keller, F., Lapata, M. and Ourioupina, O. (2002)."},"#text":"\n","marker":{"#tail":"\n","#text":"Keller, Lapata, Ourioupina, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"me. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al, 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005; Granger and Rayson, 1998). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus. Crea","@endWordPosition":"406","@position":"2700","annotationId":"T18","@startWordPosition":"402","@citStr":"Keller et al, 2002"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"F Keller"},{"#tail":"\n","#text":"M Lapata"},{"#tail":"\n","#text":"O Ourioupina"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Using the Web to Overcome Data Sparseness. Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia, July 2002, pp. 230-237."},"#text":"\n","pages":{"#tail":"\n","#text":"230--237"},"marker":{"#tail":"\n","#text":"2002"},"location":{"#tail":"\n","#text":"Philadelphia,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rds) to annotate ? Crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by Fletcher (2004b) and shingling techniques described by Chakrabarti (2002). The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API). We have designed this environment so that specific application functionality 30 can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging, distributed video encoding (Hughes and Walkerdine, 2005), distrib","@endWordPosition":"2631","@position":"17199","annotationId":"T19","@startWordPosition":"2631","@citStr":"(2002)"}},"title":{"#tail":"\n","#text":"Using the Web to Overcome Data Sparseness."},"booktitle":{"#tail":"\n","#text":"Proceedings of the Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Kennedy, G. (1998). An introduction to corpus linguistics. Longman, London."},"#text":"\n","marker":{"#tail":"\n","#text":"Kennedy, 1998"},"location":{"#tail":"\n","#text":"Longman, London."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"quency studies, neologism research, and statistical natural language processing where models of sparse data are built. The motivation for increasingly large data sets remains the same. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al, 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005; Granger and Rayson, 1998). As the size of a corpus increases, a near linear increase ","@endWordPosition":"374","@position":"2513","annotationId":"T20","@startWordPosition":"373","@citStr":"Kennedy, 1998"}},"title":{"#tail":"\n","#text":"An introduction to corpus linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"G Kennedy"}}},{"date":{"#tail":"\n","#text":"2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" are available, for example, in the application of part-of-speech tags to corpora. Existing tagging systems are ?small scale? and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, ","@endWordPosition":"1060","@position":"6933","annotationId":"T21","@startWordPosition":"1059","@citStr":"Kilgarriff, 2001"}},"title":{"#tail":"\n","#text":"Web as corpus."},"volume":{"#tail":"\n","#text":"29"},"#tail":"\n","institution":{"#tail":"\n","#text":"Lancaster University,"},"rawString":{"#tail":"\n","#text":"Kilgarriff, A. (2001). Web as corpus. In Proceedings of Corpus Linguistics 2001, Lancaster University, 29 March - 2 April 2001, pp. 342 ? 344."},"#text":"\n","pages":{"#tail":"\n","#text":"342--344"},"marker":{"#tail":"\n","#text":"Kilgarriff, 2001"},"booktitle":{"#tail":"\n","#text":"In Proceedings of Corpus Linguistics"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Kilgarriff"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Kilgarriff, A. (2003). Linguistic Search Engine. In proceedings of Workshop on Shallow Processing of Large Corpora (SProLaC 2003), Lancaster University, 28 - 31 March 2003, pp. 53 ? 58."},"#text":"\n","pages":{"#tail":"\n","#text":"53--58"},"marker":{"#tail":"\n","#text":"Kilgarriff, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"01, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable (Veronis, 2005). Tools based on static corpora do not suffer from this problem, e.g. BNCweb7, developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) 4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html 5 http://www.comp.leeds.ac.uk/amalgam/amalgam/ amalghome.htm 6 http://www.connexor.com 7 htt","@endWordPosition":"1155","@position":"7590","annotationId":"T22","@startWordPosition":"1154","@citStr":"Kilgarriff, 2003"}},"title":{"#tail":"\n","#text":"Linguistic Search Engine."},"booktitle":{"#tail":"\n","#text":"In proceedings of Workshop on Shallow Processing of Large Corpora (SProLaC 2003), Lancaster University, 28 - 31"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Kilgarriff"}}},{"volume":{"#tail":"\n","#text":"29"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Kilgarriff, A. and Grefenstette, G (2003). Introduction to the Special Issue on the Web as Corpus. Computational Linguistics, 29: 3, pp. 333-347."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"333--347"},"marker":{"#tail":"\n","#text":"Kilgarriff, Grefenstette, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ural language processing where models of sparse data are built. The motivation for increasingly large data sets remains the same. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al, 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005; Granger and Rayson, 1998). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing pow","@endWordPosition":"387","@position":"2589","annotationId":"T23","@startWordPosition":"384","@citStr":"Kilgarriff and Grefenstette, 2003"},{"#tail":"\n","#text":" impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiabi","@endWordPosition":"1083","@position":"7090","annotationId":"T24","@startWordPosition":"1080","@citStr":"Kilgarriff and Grefenstette, 2003"}]},"title":{"#tail":"\n","#text":"Introduction to the Special Issue on the Web as Corpus."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Kilgarriff"},{"#tail":"\n","#text":"G Grefenstette"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Mair, C. (2005). The corpus-based study of language change in progress: The extra value of tagged corpora. Presentation at the AAACL/ICAME Conference, Ann Arbor, May 2005."},"#text":"\n","marker":{"#tail":"\n","#text":"Mair, 2005"},"location":{"#tail":"\n","#text":"Ann Arbor,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al, 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005; Granger and Rayson, 1998). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus. Creating a large-scale annotated corpus from the web requires a way to overcome the limitations on processing power. We propose distributed techniques to alleviate the limitations on the 1 See, for example, those distributed by the Linguistic Data Consortium: http://www.ldc.upenn.edu/ 27 volume of data that can be tagged by a si","@endWordPosition":"459","@position":"3026","annotationId":"T25","@startWordPosition":"458","@citStr":"Mair, 2005"}},"title":{"#tail":"\n","#text":"The corpus-based study of language change in progress: The extra value of tagged corpora."},"booktitle":{"#tail":"\n","#text":"Presentation at the AAACL/ICAME Conference,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"C Mair"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical Report: LAMP-TR-108/CS-TR-4541/UMIACS-TR-2003-109,"},"date":{"#tail":"\n","#text":"2003"},"institution":{"#tail":"\n","#text":"University of Maryland, College Park,"},"rawString":{"#tail":"\n","#text":"Resnik, P. and Elkiss, A. (2003) The Linguist's Search Engine: Getting Started Guide. Technical Report: LAMP-TR-108/CS-TR-4541/UMIACS-TR-2003-109, University of Maryland, College Park, November 2003."},"#text":"\n","marker":{"#tail":"\n","#text":"Resnik, Elkiss, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"eived a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable (Veronis, 2005). Tools based on static corpora do not suffer from this problem, e.g. BNCweb7, developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) 4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html 5 http://www.comp.leeds.ac.uk/amalgam/amalgam/ amalghome.htm 6 http://www.connexor.com 7 http://homepage.mac.com/bncwe","@endWordPosition":"1159","@position":"7616","annotationId":"T26","@startWordPosition":"1156","@citStr":"Resnik and Elkiss, 2003"}},"title":{"#tail":"\n","#text":"The Linguist's Search Engine: Getting Started Guide."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"P Resnik"},{"#tail":"\n","#text":"A Elkiss"}]}},{"volume":{"#tail":"\n","#text":"4"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Robb, T. (2003) Google as a Corpus Tool? In ETJ Journal, Volume 4, number 1, Spring 2003."},"journal":{"#tail":"\n","#text":"ETJ Journal,"},"#text":"\n","marker":{"#tail":"\n","#text":"Robb, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"r example, in the application of part-of-speech tags to corpora. Existing tagging systems are ?small scale? and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and t","@endWordPosition":"1062","@position":"6945","annotationId":"T27","@startWordPosition":"1061","@citStr":"Robb, 2003"}},"title":{"#tail":"\n","#text":"Google as a Corpus Tool? In"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"T Robb"}}},{"volume":{"#tail":"\n","#text":"2"},"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Rundell, M. (2000). &quot;The biggest corpus of all&quot;, Humanising Language Teaching. 2:3; May 2000."},"journal":{"#tail":"\n","#text":"Humanising Language Teaching."},"#text":"\n","marker":{"#tail":"\n","#text":"Rundell, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"n the application of part-of-speech tags to corpora. Existing tagging systems are ?small scale? and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s S","@endWordPosition":"1064","@position":"6960","annotationId":"T28","@startWordPosition":"1063","@citStr":"Rundell, 2000"}},"title":{"#tail":"\n","#text":"The biggest corpus of all&quot;,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Rundell"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Shirky, C. (2001) Listening to Napster, in Peer-toPeer: Harnessing the power of Disruptive Technologies, O'Reilly."},"#text":"\n","marker":{"#tail":"\n","#text":"Shirky, 2001"},"location":{"#tail":"\n","#text":"O'Reilly."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rns to enhance the reliability of web data for corpora. In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience (Carroll et al, 2005; Hughes et al 2004). While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001). Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems. Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals). A key advantage of P2P systems is that they are lightweight and geared to personal computing where informal groups provide unused processing power to solve a common problem. Typically, P2P systems draw upon the resou","@endWordPosition":"1809","@position":"11957","annotationId":"T29","@startWordPosition":"1808","@citStr":"Shirky, 2001"}},"title":{"#tail":"\n","#text":"Listening to Napster, in Peer-toPeer: Harnessing the power of Disruptive Technologies,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"C Shirky"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Turney, P. (2001). Word Sense Disambiguation by Web Mining for Word Co-occurrence Probabilities. In proceedings of SENSEVAL-3, Barcelona, Spain, July 2004 pp. 239-242."},"#text":"\n","pages":{"#tail":"\n","#text":"239--242"},"marker":{"#tail":"\n","#text":"Turney, 2001"},"location":{"#tail":"\n","#text":"Barcelona, Spain,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in in","@endWordPosition":"1095","@position":"7167","annotationId":"T30","@startWordPosition":"1094","@citStr":"Turney (2001)"}},"title":{"#tail":"\n","#text":"Word Sense Disambiguation by Web Mining for Word Co-occurrence Probabilities."},"booktitle":{"#tail":"\n","#text":"In proceedings of SENSEVAL-3,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"P Turney"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Veronis, J. (2005). Web: Google's missing pages: mystery solved? http://aixtal.blogspot.com/2005/02/web-googlesmissing-pages-mystery.html (accessed April 28, 2005). Walkerdine, J., Gilleade, K., Hughes, D., Rayson, P., Simms, J., Mariani, J., and Sommerville, I. A Framework for P2P Application Development. Paper submitted to Software Practice and Experience."},"#text":"\n","marker":{"#tail":"\n","#text":"Veronis, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"led text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist?s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable (Veronis, 2005). Tools based on static corpora do not suffer from this problem, e.g. BNCweb7, developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) 4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html 5 http://www.comp.leeds.ac.uk/amalgam/amalgam/ amalghome.htm 6 http://www.connexor.com 7 http://homepage.mac.com/bncweb/home.html 8 http://view.byu.edu/ 28 are both based on the British National Corpus. Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags. In addition, PIE9 (Phrases in E","@endWordPosition":"1191","@position":"7836","annotationId":"T31","@startWordPosition":"1190","@citStr":"Veronis, 2005"}},"title":{"#tail":"\n","#text":"Web: Google's missing pages: mystery solved? http://aixtal.blogspot.com/2005/02/web-googlesmissing-pages-mystery.html (accessed"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Veronis"}}},{"date":{"#tail":"\n","#text":"2004"},"editor":{"#tail":"\n","#text":"In Caronni G., Weiler N., Shahmehri N. (eds.)"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"thin a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API). We have designed this environment so that specific application functionality 30 can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging, distributed video encoding (Hughes and Walkerdine, 2005), distributed virtual worlds (Hughes et al, 2005) and digital library management (Walkerdine and Rayson, 2004). It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS11). The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it. Using the P2P Application Framework as a basis for the development secures several advantages. First, it reduces development time by allowing the developer to reuse existing functionality; secondly, it already supports essential aspects such as system security;","@endWordPosition":"2738","@position":"17900","annotationId":"T32","@startWordPosition":"2735","@citStr":"Walkerdine and Rayson, 2004"}},"title":{"#tail":"\n","#text":"P2P-4-DL: Digital Library over Peer-to-Peer."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Walkerdine, J. and Rayson, P. (2004) P2P-4-DL: Digital Library over Peer-to-Peer. In Caronni G., Weiler N., Shahmehri N. (eds.) Proceedings of Fourth IEEE International Conference on Peer-toPeer Computing (PSP2004) 25-27 August 2004, Zurich, Switzerland. IEEE Computer Society Press, pp. 264-265."},"#text":"\n","pages":{"#tail":"\n","#text":"25--27"},"marker":{"#tail":"\n","#text":"Walkerdine, Rayson, 2004"},"publisher":{"#tail":"\n","#text":"IEEE Computer Society Press,"},"location":{"#tail":"\n","#text":"Zurich, Switzerland."},"booktitle":{"#tail":"\n","#text":"Proceedings of Fourth IEEE International Conference on Peer-toPeer Computing (PSP2004)"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J Walkerdine"},{"#tail":"\n","#text":"P Rayson"}]}}]}}]}}
