 significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others. Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said . . . , or they can be just capitalized common words, as in White elephants are . . . . The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably. Church (1995, p. 294) studied, among other simple text normalization techniques, the effect of case normalization for different words and showed that ?sometimes case variants refer to the same thing (hurricane and Hurricane), sometimes they refer to different things (continental and Continental) and sometimes they don?t refer to much of anything (e.g., anytime and Anytime).? Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not. ? Institute for Communicating and Collaborative Systems, Division of Informatics, 2
and Hurricane), sometimes they refer to different things (continental and Continental) and sometimes they don?t refer to much of anything (e.g., anytime and Anytime).? Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not. ? Institute for Communicating and Collaborative Systems, Division of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: mikheev@cogsci.ed.ac.uk 290 Computational Linguistics Volume 28, Number 3 Proper names are the main concern of the named-entity recognition subtask (Chinchor 1998) of information extraction. The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1 There the disambiguation of the first word in a sentence (and in other ambiguous positions) is one of the central problems: about 20% of named entities occur in ambiguous positions. For instance, the word Black in the sentenceinitial position can stand for a person?s surname but can also refer to the color. Even in multiword capitalized phrases, the first word can belong to the rest of the phrase or 
ing, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary. In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary. Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD problem can be found in Palmer and Hearst (1997). The disambiguation of capitalized words and sentence boundaries presents a chicken-and-egg problem. If we know that a capitalized word that follows a period is a common word, we can safely assign such period as sentence terminal. On the other hand, if we know that a period is not sentence terminal, then we can conclude that the following capitalized word is a proper name. Another frequent source of ambiguity in end-of-sentence marking is introduced by abbreviations: if we know that the word that precedes a period is not an abbreviation, then almost certainly this period denotes a sentence bo
 word tokens that it is intended to handle. Obviously, we want the system to handle all cases as accurately as possible. Sometimes, however, it is beneficial to assign only cases in which the system is confident enough, leaving the rest to be handled by other methods. In this case apart from the error rate (which corresponds to precision or accuracy as 1?error rate) we also measure the system?s coverage or recall coverage = correctly assigned all to be assigned 2.1 Corpora for Evaluation There are two corpora normally used for evaluation in a number of text-processing tasks: the Brown corpus (Francis and Kucera 1982) and the Wall Street Journal (WSJ) corpus, both part of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). The Brown corpus represents general English. It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech. The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure. Altogether there are about 500 documents in the Brown corpus, with an average length of 
ime, a word can belong to multiple lists. Since we have four lists, we have four types: ? common word (as opposed to proper name) ? common word that is a frequent sentence starter 297 Mikheev Periods, Capitalized Words, etc. ? frequent proper name ? abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts. For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times (NYT) corpus that was supplied as training data for the 7th Message Understanding Conference (MUC-7) (Chinchor 1998). We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain. The first list on which our method relies is a list of common words. This list includes common words for a given language, but no sup
veloped for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain. The first list on which our method relies is a list of common words. This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list. A variety of such lists for many languages are already available (e.g., Burnage 1990). Words in such lists are usually supplemented with morphological and POS information (which is not required by our method). We do not have to rely on pre-existing resources, however. A list of common words can be easily obtained automatically from a raw (unannotated in any way) text collection by simply collecting and counting lowercased words in it. We generated such list from the NYT collection. To account for potential spelling and capitalization errors, we included in the list of common words only words that occurred lower-cased at least three times in the NYT texts. The list of common wo
alized words that follow a potential sentence boundary punctuation sign. Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies, which were briefly described in Mikheev (1999). 301 Mikheev Periods, Capitalized Words, etc. 7.1 The Sequence Strategy The first DCA strategy for the disambiguation of ambiguous capitalized words is to explore sequences of words extracted from contexts in which the same words are used unambiguously with respect to their capitalization. We call this the sequence strategy. The rationale behind this strategy is that if there is a phrase of two or more capitalized words starting from an unambiguous position (e.g., following a lower-cased word), the system can be reasonably confident that even when the same phrase starts from an unreliable pos
istics to feed the SBD rule set, as described in Section 3.2. Here we see a significant impact of the infelicities in the disambiguation of capitalized words and abbreviations on the performance of the SBD rule set. Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8?1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al 1995): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus. In ro
Section 3.2. Here we see a significant impact of the infelicities in the disambiguation of capitalized words and abbreviations on the performance of the SBD rule set. Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8?1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al 1995): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus. In row D of Table 4, we summarized our main results
viations on the performance of the SBD rule set. Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8?1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al 1995): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus. In row D of Table 4, we summarized our main results: the results obtained by the application of our SBD rule set, which uses the information provided by the DCA 
te of 0.8?1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al 1995): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus. In row D of Table 4, we summarized our main results: the results obtained by the application of our SBD rule set, which uses the information provided by the DCA to capitalized word disambiguation applied together with lexical lookup (as described in Section 7.5), and the abbreviation-handling strategy, which included the guessing heuristics, the DCA, and the list of 270 abbreviations (as described in Section 6). As can be seen in the table
en the performance of our method significantly deteriorates on longer documents. We also evaluated the performance of the method on different subcorpora of the Brown corpus: the most difficult subdomains proved to be scientific texts, spoken-language transcripts, and journalistic texts, whereas fiction was the easiest genre for the system. 10. Incorporating DCA into a POS Tagger To test our hypothesis that DCA can be used as a complement to a local-context approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger. Unlike other POS taggers, this POS tagger (Mikheev 2000) was also trained to disambiguate sentence boundaries. 10.1 Training a POS Tagger In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations. Consequently a POS tagger can naturally treat them similarly to any other ambiguous words. There is, however, one difference in the implementation of such a tagger. Normally, a POS 307 Mikheev Periods, Capitalized Words, etc. x x x x x x x o o o o o o o c c c c c c c Number of Documents Cap.Word error rate SBD error rate 50 100 200 500 1000 2000 3000 1 2 3 200 6
 etc. x x x x x x x o o o o o o o c c c c c c c Number of Documents Cap.Word error rate SBD error rate 50 100 200 500 1000 2000 3000 1 2 3 200 600 Doc. Length Error Rate Number of Docs 400 Figure 2 Distribution of the error rate and the number of documents across the document length (measured in word tokens) in the WSJ corpus. tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992], Brill?s [Brill 1995a], and MaxEnt [Ratnaparkhi 1996]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and th
 o o o o o c c c c c c c Number of Documents Cap.Word error rate SBD error rate 50 100 200 500 1000 2000 3000 1 2 3 200 600 Doc. Length Error Rate Number of Docs 400 Figure 2 Distribution of the error rate and the number of documents across the document length (measured in word tokens) in the WSJ corpus. tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992], Brill?s [Brill 1995a], and MaxEnt [Ratnaparkhi 1996]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word 
umber of Documents Cap.Word error rate SBD error rate 50 100 200 500 1000 2000 3000 1 2 3 200 600 Doc. Length Error Rate Number of Docs 400 Figure 2 Distribution of the error rate and the number of documents across the document length (measured in word tokens) in the WSJ corpus. tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992], Brill?s [Brill 1995a], and MaxEnt [Ratnaparkhi 1996]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an u
ious word strongly influences period disambiguation, it was included in the standard trigram model. We decided to train the tagger with the minimum of preannotated resources. First, we used 20,000 tagged words to ?bootstrap? the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques (e.g., Baum-Welch [Baum and Petrie 1966] or Brill?s [Brill 1995b]) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns. Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique. After bootstrapping we applied the forward-backward (Baum-Welch) algorithm (Baum and Petrie 1966) and trained our tagger in the unsupervised m
od disambiguation, it was included in the standard trigram model. We decided to train the tagger with the minimum of preannotated resources. First, we used 20,000 tagged words to ?bootstrap? the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques (e.g., Baum-Welch [Baum and Petrie 1966] or Brill?s [Brill 1995b]) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns. Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique. After bootstrapping we applied the forward-backward (Baum-Welch) algorithm (Baum and Petrie 1966) and trained our tagger in the unsupervised mode, that is, without us
asting Corporation (BBC) news in Russian. We collected this corpus from the Internet ?http://news.bbc.co.uk/hi/russian/world/default.htm? over a period of 30 days. This gave us a corpus of 300 short documents (one or two paragraphs each). We automatically created the supporting resources from 364,000 documents from the Russian corpus of the European Corpus Initiative, using the method described in section 5. Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue. Before using the DCA method, we applied a Russian morphological processor (Mikheev and Liubushkina 1995) to convert each word in the text to its main form: nominative case singular for nouns and adjectives, infinitive for verbs, etc. For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms. Since the documents in the BBC news corpus were rather short, we applied the cache module, as described in Section 11.1. This allowed us to reuse information across the documents. Russian proved to be a simpler case than English for our tasks. First, on average, Russian words are longer than English words: thus the i
a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model. Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word?s last 311 Mikheev Periods, Capitalized Words, etc. occurrence. In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to
ing standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word?s last 311 Mikheev Periods, Capitalized Words, etc. occurrence. In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names. They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikhe
g text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (?one sense per discourse?). Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale, Church, and Yarowsky?s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to ?one sense per collocation? idea of Yarowsky (1993). The description of the EAGLE workbench for linguistic engineering (Baldwin et al 1997) mentions a case normalization module that uses a heuristic in
 on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (?one sense per discourse?). Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale, Church, and Yarowsky?s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to ?one sense per collocation? idea of Yarowsky (1993). The description of the EAGLE workbench for linguistic engineering (Baldwin et al 1997) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should 
e in a document or discourse (?one sense per discourse?). Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale, Church, and Yarowsky?s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to ?one sense per collocation? idea of Yarowsky (1993). The description of the EAGLE workbench for linguistic engineering (Baldwin et al 1997) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document. This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions. It is quite similar to our method for capitalized-word disambiguation. The description of the EAGLE case normalization module provided by Baldwin et al is, however, very brief
een applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale, Church, and Yarowsky?s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to ?one sense per collocation? idea of Yarowsky (1993). The description of the EAGLE workbench for linguistic engineering (Baldwin et al 1997) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document. This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions. It is quite similar to our method for capitalized-word disambiguation. The description of the EAGLE case normalization module provided by Baldwin et al is, however, very brief and provides no performance evaluation or other details. 12.2 Research in Text Preproce
however, very brief and provides no performance evaluation or other details. 12.2 Research in Text Preprocessing 12.2.1 Sentence Boundary Disambiguation. There exist two large classes of SBD systems: rule based and machine learning. The rule-based systems use manually built rules that are usually encoded in terms of regular-expression grammars supplemented with lists of abbreviations, common words, proper names, etc. To put together a few rules is fast and easy, but to develop a rule-based system with good performance is quite a labor-consuming enterprise. For instance, the Alembic workbench (Aberdeen et al. 1995) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex. Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus, the second class of SBD systems employs machine learning techniques such as decision tree classifiers (Riley 1989), neural n
Aberdeen et al. 1995) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex. Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus, the second class of SBD systems employs machine learning techniques such as decision tree classifiers (Riley 1989), neural networks (Palmer and Hearst 1994), and maximum-entropy modeling (Reynar and Ratnaparkhi 1997). Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign. Although training of such 312 Computational Linguistics Volume 28, Number 3 systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase. 
 a sentence-splitting module that employs over 100 regular-expression rules written in Flex. Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus, the second class of SBD systems employs machine learning techniques such as decision tree classifiers (Riley 1989), neural networks (Palmer and Hearst 1994), and maximum-entropy modeling (Reynar and Ratnaparkhi 1997). Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign. Although training of such 312 Computational Linguistics Volume 28, Number 3 systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase. The main difference between the existing m
lar-expression rules written in Flex. Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus, the second class of SBD systems employs machine learning techniques such as decision tree classifiers (Riley 1989), neural networks (Palmer and Hearst 1994), and maximum-entropy modeling (Reynar and Ratnaparkhi 1997). Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign. Although training of such 312 Computational Linguistics Volume 28, Number 3 systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase. The main difference between the existing machine learning and rule-based methods for the SBD task and 
the period. Our approach to SBD is closer in spirit to machine learning methods because its retargeting does not require rule reengineering and can be done completely automatically. Unlike traditional machine learning SBD approaches, however, our approach does not require annotated data for training. 12.2.2 Disambiguation of Capitalized Words. Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. As Church (1988) rightly pointed out, however, ?Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not. Estimates from the Brown Corpus can be misleading. For example, the capitalized word ?Acts? is found twice in the Brown Corpus, both times as a proper noun (in a title). It would be misleading to infer from this evidence that the word ?Acts? is always a proper noun.? In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes
Brown Corpus, both times as a proper noun (in a title). It would be misleading to infer from this evidence that the word ?Acts? is always a proper noun.? In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name. Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998). In some systems such dependencies are learned from labeled examples (Bikel et al 1997). The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data. Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port. Both POS taggers and named-entity recognizers are normally built using the localcontext paradigm. In contrast, we opted for a method that relies on the entire distribution of a word in a document. Altho
for a method that relies on the entire distribution of a word in a document. Although it is possible to train some classes of POS taggers without supervision, this usually leads to suboptimal performance. Thus the majority of taggers are trained using at least some labeled data. Named-entity recognition systems are usually hand-crafted or trained from labeled data. As was shown above, our method does not require supervised training. 12.2.3 Disambiguation of Abbreviations. Not much information has been published on abbreviation identification. One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing. This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is 313 Mikheev Periods, Capitalized Words, etc. that this does not presuppose the existence of a corpus in which the current document is similar to other documents. Park and Byrd (2001) recently described a hyb
scribed in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing. This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is 313 Mikheev Periods, Capitalized Words, etc. that this does not presuppose the existence of a corpus in which the current document is similar to other documents. Park and Byrd (2001) recently described a hybrid method for finding abbreviations and their definitions. This method first applies an ?abbreviation recognizer? that generates a set of ?candidate abbreviations? for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK). The abbreviation recognizer for these purposes is allowed to overgenerate significantly. There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained. Therefore 
pproach is robust to domain shifts and new lexica and closely targeted to each document. 314 Computational Linguistics Volume 28, Number 3 A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention. The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance, Palmer and Hearst (1997) report that the SATZ system (decision tree variant) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16,000 words. This is a relatively small training set that can be manually marked in a few hours? time. But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%). The performance of our system does not depend on the availability of labeled training examples. For its ?training,? it uses a raw (unannotated in any way) corpus of texts. Although it need
ems.3 The operational speed is about 10% higher than the training speed because, apart from applying the system to the training corpus, training also involves collecting, thresholding, and sorting of the word lists?all done automatically but at extra time cost. Training on the 300,000-word NYT text collection took about two minutes. Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reac
lecting, thresholding, and sorting of the word lists?all done automatically but at extra time cost. Training on the 300,000-word NYT text collection took about two minutes. Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four do
 we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on 3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their average sentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha 3000). 315 Mikheev Periods, Capitalized Words, etc. optical character reader?generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger reported in Mikheev (2000), which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage. Our DCA uses information derived from the entire document and thus can be used as a complement to approaches based on the local context. When we incorporated the DCA system into a POS tagger (Section 8), we measured a 30?35% cut in the error rate on proper-name identification in comparison to DCA or the POS-tagging approaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20% error rate on the Brown corpus and a 0.3
-cased texts (e.g., texts written in all capital or all lower-cased letters) or on 3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their average sentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha 3000). 315 Mikheev Periods, Capitalized Words, etc. optical character reader?generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger reported in Mikheev (2000), which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage. Our DCA uses information derived from the entire document and thus can be used as a complement to approaches based on the local context. When we incorporated the DCA system into a POS tagger (Section 8), we measured a 30?35% cut in the error rate on proper-name identification in comparison to DCA or the POS-tagging approaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20% error rate on the Brown corpus and a 0.31% error rate on the WSJ corpus, which corres
