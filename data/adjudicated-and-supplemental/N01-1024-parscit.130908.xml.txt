automatically induce the morphology structures of a language. Our algorithm takes as input a large corpus and produces as output a set of conflation sets indicating the various inflected and derived forms for each word in the language. As an example, the conflation set of the word “abuse” would contain “abuse”, “abused”, “abuses”, “abusive”, “abusively”, and so forth. Our algorithm extends earlier approaches to morphology induction by combining various induced information sources: the semantic relatedness of the affixed forms using a Latent Semantic Analysis approach to corpusbased semantics (Schone and Jurafsky, 2000), affix frequency, syntactic context, and transitive closure. Using the hand-labeled CELEX lexicon (Baayen, et al., 1993) as our gold standard, the current version of our algorithm achieves an F-score of 88.1% on the task of identifying conflation sets in English, outperforming earlier algorithms. Our algorithm is also applied to German and Dutch and evaluated on its ability to find prefixes, suffixes, and circumfixes in these languages. To our knowledge, this serves as the first evaluation of complete regular morphological induction of German or Dutch (although researchers such as Nakisa and 
nalysis. We here briefly describe work in each category. 2.1 Using a Knowledge Source to Bootstrap Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a corpus to refine Porter stemmer output. Gaussier (1999) induces derivational morphology using an inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing suffixes. Kazakov (1997) does something akin to this using MDL as a fitness metric for evolutionary computing. DéJean (1998) uses a strategy similar to that of
search is also knowledge-free but attempts to induce, for each morphological variants of each other. With the exception of word segmentation, we provided no human information to our system. We applied our system to an English corpus and evaluated by comparing each word’s conflation set as produced by our algorithm to those derivable from CELEX. 2.4 Problems with earlier approaches word of a corpus, a complete analysis. Since our Most of the existing algorithms described focus on approach falls into this category (expanding upon suffixing in inflectional languages (though our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and DéJean describe work on prefixes). we describe work in this area in more detail. None of these algorithms consider the general 2.3.1 Jacquemin’s multiword approach Jacquemin (1997) deems pairs of word n-grams as morphologically related if two words in the first ngram have the same first few letters (or stem) as two words in the second n-gram and if there is a suffix for each stem whose length is less than k. He also clusters groups of words having the same kinds of word endings, which gives an added performance boost. He applies his algorithm to a French term list and scores b
-maximization algorithm (EM) and MDL as well as some triage procedures to help eliminate inappropriate parses for every word in a corpus. He collects the possible suffixes for each stem and calls these signatures which give clues about word classes. With the exceptions of capitalization removal and some word segmentation, Goldsmith's algorithm is otherwise knowledge-free. His algorithm, Linguistica, is freely available on the Internet. Goldsmith applies his algorithm to various languages but evaluates in English and French. 2.3.3 Schone and Jurafsky: induced semantics In our earlier work, we (Schone and Jurafsky (2000)) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes. We then applied Latent Semantic Analysis (Deerwester, et al., 1990) as a method of automatically determining semantic relatedness between word pairs. Using statistics from the conditions of circumfixing or infixing, nor are they applicable to other language types such as agglutinative languages (Sproat, 1992). Additionally, most approaches have centered around statistics of orthographic properties. We had noted previously (Schone and Jura
ree languages and compare our results to those the search for suffixes unless we first remove all that the Goldsmith and Schone/Jurafsky algorithms candidate prefixes. Therefore, we build a lexicon would have obtained on our same data. We show consisting of all words in our corpus and identify all how each of our additions result in progressively word beginnings with frequencies in excess of some better overall solutions. threshold (T ). We call these pseudo-prefixes. We 1 3 Current Approach Figure 1: Strategy and evaluation 3.1 Finding Candidate Circumfix Pairings As in our earlier approach (Schone and Jurafsky, 2000), we begin by generating, from an untagged corpus, a list of word pairs that might be morphological variants. Our algorithm has changed somewhat, though, since we previously sought word pairs that vary only by a prefix or a suffix, yet we now wish to generalize to those with circumfixing differences. We use “circumfix” to mean true circumfixes like the German ge-/-t as well as combinations of prefixes and suffixes. It should be mentioned also that we assume the existence of languages having valid circumfixes that are not composed merely of a prefix and a suffix that appear independently elsewh
 frequency of word i as seen in document j of the corpus. The SVD decomposes M into the product of three matrices, U, D, and V such T that U and V are orthogonal matrices and D is a T diagonal matrix whose entries are the singular values of M. The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace. This methodology is well-described in the literature (Landauer, et al., 1998; Manning and Schütze, 1999). In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000)). Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w’s row, the first N columns denote words that precede w by up to 50 words, and the second N � columns represent those words that follow by up to NCS(µ,) =f NCS exp[ ((x-µ)/)2]dx 50 words. Since SVDs are more designed to work then, if there were nR items in the rules
ne would like to be able to obtain a separate semantic vector for every word (not just those in the top N). SVD computations can be expensive and impractical for large values of N. Yet due to the fact that U and VT are orthogonal matrices, we can start with a matrix of reasonablesized N and “fold in” the remaining terms, which is the approach we have followed. For details about folding in terms, the reader is referred to Manning and Schütze (1999, p. 563). 3.3 Correlating Semantic Vectors To correlate these semantic vectors, we use normalized cosine scores (NCSs) as we had illustrated before (Schone and Jurafsky (2000)). The normalized cosine score between two words w1 and w2 is determined by first computing cosine values between each word’s semantic vector and 200 other randomly selected semantic vectors. This provides a mean (µ) and variance (� ) of correlation 2 for each word. The NCS is given to be NCS w w2 - min cos(S2W]w S2w2)-µk 1 ( 1, ) ke(1,2) ak ( ) We had previously illustrated NCS values on various PPMVs and showed that this type of score seems to be appropriately identifying semantic relationships. (For example, the PPMVs of car/cars and ally/allies had NCS values of 5.6 and 6.5 respectively, w
 English, for example, the highest frequency rule is -s—e. CELEX suggests that 99.7% of our PPMVs for this rule would be true. However, since the purely semantic-based approach tends to select only relationships with contextually similar meanings, only 92% of the PPMVs are retained. This suggests that one might improve the analysis by supplementing semantic probabilities with orthographic-based probabilities (Prorth). Our approach to obtaining Prorth is motivated by an appeal to minimum edit distance (MED). MED has been applied to the morphology induction problem by other researchers (such as Yarowsky and Wicentowski, 2000). MED determines the minimum-weighted set of insertions, substitutions, and deletions required to transform one word into another. For example, only a single deletion is required to transform “rates” into “rate” whereas two substitutions and an insertion are required to transform it into “rating.” Effectively, if Cost(-) is transforming cost, Cost(rates—rate) = Cost(s—e) whereas Cost(rates—rating)=Cost(es—ing). More generally, suppose word X has circumfix C1=B1/E1 and pseudo-stem -S-, and word Y has circumfix C2 =B2/E2 also with pseudo-stem -S-. Then, Cost(X—Y)=Cost(B1SE1—B2SE2)=Cost(C 1 —C 2)
T-6 p.. We the words in the graph. Due to the difficulty in combine the probabilities of all independent paths developing a scoring algorithm to compare directed between X and Z according to Figure 5: graphs, we will follow our earlier approach and only Figure 5: Pseudocode for Branching Probability function BranchProbBetween(X,Z) prob=0 foreach independent path Œj prob = prob+Pr�j(X<Z) - (prob*Pr (X<Z) ) �j return prob If the returned probability exceeds T5, we declare X and Z to be morphological variants of each other. 4 Evaluation We compare this improved algorithm to our former algorithm (Schone and Jurafsky (2000)) as well as to Goldsmith's Linguistica (2000). We use as input to our system 6.7 million words of English newswire, 2.3 million of German, and 6.7 million of Dutch. Our gold standards are the hand-tagged morphologically-analyzed CELEX lexicon in each of these languages (Baayen, et al., 1993). We apply the algorithms only to those words of our corpora with frequencies of 10 or more. Obviously this cutoff slightly limits the generality of our results, but it also greatly decreases processing time for all of Figure 6: Morphologic relations of “conduct” compare induced conflation sets to those of
h of the three languages. Furthermore, using ten-fold cross validation on the English data, we find that Fscore differences of the S column are each statistically significant at least at the 95% level. Table 5: Computation of F-Scores Algorithms English German Dutch S C A S C S C None 62.8 59.9 51.7 75.8 63.0 74.2 70.0 Goldsmith 81.8 84.0 75.8 S/J2000 85.2 88.3 82.2 +orthogrph 85.7 82.2 76.9 89.3 76.1 84.5 78.9 + syntax 87.5 84.0 79.0 91.6 78.2 85.6 79.4 + transitive 88.1 84.5 79.7 92.3 78.9 85.8 79.6 5 Conclusions We have illustrated three extensions to our earlier morphology induction work (Schone and Jurafsky (2000)). In addition to induced semantics, we incorporated induced orthographic, syntactic, and transitive information resulting in almost a 20% relative reduction in overall induction error. We have also extended the work by illustrating performance in German and Dutch where, to our knowledge, complete morphology induction performance measures have not previously been obtained. Lastly, we showed a mechanism whereby circumfixes as well as combinations of prefixing and suffixing can be induced in lieu of the suffixonly strategies prevailing in most previous research. For the future, we expect improve
