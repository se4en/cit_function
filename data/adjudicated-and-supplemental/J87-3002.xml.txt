l language parsing systems. 1 INTRODUCTION The grammar coding system employed by the Longman Dictionary of Contemporary English (henceforth LDOCE) is the most comprehensive description of grammatical properties of words to be found in any published dictionary available in machine readable form. This paper describes the extraction of this, and other, information from LDOCE and discusses the utility of the coding system for automated natural language processing. Recent developments in linguistics, and especially on grammatical theory - - for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al, 1985), Lex- ical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) - - and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial frag- ments of natural anguage. These developments also emphasise that if natural anguage processing systems are to be able to handle the grammatical nd semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central compo- nent of the 
ding system employed by the Longman Dictionary of Contemporary English (henceforth LDOCE) is the most comprehensive description of grammatical properties of words to be found in any published dictionary available in machine readable form. This paper describes the extraction of this, and other, information from LDOCE and discusses the utility of the coding system for automated natural language processing. Recent developments in linguistics, and especially on grammatical theory - - for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al, 1985), Lex- ical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) - - and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial frag- ments of natural anguage. These developments also emphasise that if natural anguage processing systems are to be able to handle the grammatical nd semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central compo- nent of the parsing system. Real-time parsing imposes stringent requirement
perties of words to be found in any published dictionary available in machine readable form. This paper describes the extraction of this, and other, information from LDOCE and discusses the utility of the coding system for automated natural language processing. Recent developments in linguistics, and especially on grammatical theory - - for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al, 1985), Lex- ical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) - - and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial frag- ments of natural anguage. These developments also emphasise that if natural anguage processing systems are to be able to handle the grammatical nd semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central compo- nent of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environ- ment; at the very least it must allow frequent and rapid access to the information in the dictionary
ts of natural anguage. These developments also emphasise that if natural anguage processing systems are to be able to handle the grammatical nd semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central compo- nent of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environ- ment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects (Boguraev, 1987; Russell et al, 1986; Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological nd syntactic analyser for English. One motivation for our interest in machine readable dictionaries i to attempt to provide a substan- tial lexicon with lexical entries containing rammatical information compatible with the grammatical frame- work employed by the analyser. The idea of using the machine readable source of a published ictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name b
guage. These developments also emphasise that if natural anguage processing systems are to be able to handle the grammatical nd semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central compo- nent of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environ- ment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects (Boguraev, 1987; Russell et al, 1986; Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological nd syntactic analyser for English. One motivation for our interest in machine readable dictionaries i to attempt to provide a substan- tial lexicon with lexical entries containing rammatical information compatible with the grammatical frame- work employed by the analyser. The idea of using the machine readable source of a published ictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications
ents also emphasise that if natural anguage processing systems are to be able to handle the grammatical nd semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central compo- nent of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environ- ment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects (Boguraev, 1987; Russell et al, 1986; Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological nd syntactic analyser for English. One motivation for our interest in machine readable dictionaries i to attempt to provide a substan- tial lexicon with lexical entries containing rammatical information compatible with the grammatical frame- work employed by the analyser. The idea of using the machine readable source of a published ictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications. Most of the work on automate
machine readable dictionaries i to attempt to provide a substan- tial lexicon with lexical entries containing rammatical information compatible with the grammatical frame- work employed by the analyser. The idea of using the machine readable source of a published ictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications. Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch pro- cessing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics. Permission tocopy without fee all or part of this material is granted provided that he copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, orto republish, requires afee and/or specific permission. 0362-613X/87/030203-218503.00 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 203 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing on developing dictionary servers for office automation
ovided that he copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, orto republish, requires afee and/or specific permission. 0362-613X/87/030203-218503.00 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 203 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typi- cally generated by hand. Two exceptions to this gener- alisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al, 1982; Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). How- eve
epublish, requires afee and/or specific permission. 0362-613X/87/030203-218503.00 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 203 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typi- cally generated by hand. Two exceptions to this gener- alisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al, 1982; Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). How- ever, as yet few results have been published concerning the utility of electronic versions of published ictionar- ies as sources for such lexicons. In this paper we provide
omputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 203 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typi- cally generated by hand. Two exceptions to this gener- alisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al, 1982; Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). How- ever, as yet few results have been published concerning the utility of electronic versions of published ictionar- ies as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective. We chose to
ehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typi- cally generated by hand. Two exceptions to this gener- alisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al, 1982; Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). How- ever, as yet few results have been published concerning the utility of electronic versions of published ictionar- ies as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective. We chose to employ LDOCE as the machine read- able source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural anguage processing sys- tem. Most prominent among these are the rich gram- matical subcategorisations
 as the machine read- able source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural anguage processing sys- tem. Most prominent among these are the rich gram- matical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational nd semantic odes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary. (Michiels (1982) contains further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al (1985) and Alshawi (1987) describe further esearch in Cambridge utilising different types of information avail- able in LDOCE. The information available in the dictionary is both very rich and diverse, but also typically only semi- formalised, as it is intended for human, rather than machine, interpetation. As a consequence the programs we are developing, both to restructure and to exploit this information, need to undergo constant revision as the
y appropriate for use as the core knowledge base of a natural anguage processing sys- tem. Most prominent among these are the rich gram- matical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational nd semantic odes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary. (Michiels (1982) contains further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al (1985) and Alshawi (1987) describe further esearch in Cambridge utilising different types of information avail- able in LDOCE. The information available in the dictionary is both very rich and diverse, but also typically only semi- formalised, as it is intended for human, rather than machine, interpetation. As a consequence the programs we are developing, both to restructure and to exploit this information, need to undergo constant revision as they are being used. The system we describe is not intended for off-line use, where one might attempt to derive, completely automatically, a lexicon for natur
the core knowledge base of a natural anguage processing sys- tem. Most prominent among these are the rich gram- matical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational nd semantic odes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary. (Michiels (1982) contains further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al (1985) and Alshawi (1987) describe further esearch in Cambridge utilising different types of information avail- able in LDOCE. The information available in the dictionary is both very rich and diverse, but also typically only semi- formalised, as it is intended for human, rather than machine, interpetation. As a consequence the programs we are developing, both to restructure and to exploit this information, need to undergo constant revision as they are being used. The system we describe is not intended for off-line use, where one might attempt to derive, completely automatically, a lexicon for natural language analysi
uistic information, makes a dictio- nary difficult o represent as a structured atabase of a standard, eg. relational, type. In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natu- ral language processing systems, we need to provide fast access from Lisp to data held in secondary storage. Lisp is not particularly well suited for interfacing to complex, structured objects, and it was not our inten- tion to embark on a major effort involving the develop- ment of a formal model of a dictionary (of the style described in, eg., Tompa 1986); on the other hand a method of access was clearly required, which was flexible enough to support a range of applications in- tending to make use of the LDOCE tape. The requirement for having the dictionary entries in a form convenient for symbolic manipulation from within Lisp was furthermore augmented by the constraint that all the information present in the typesetting tape should be carried over to the on-line version of LDOCE, since it is impossible to say in advance which records and fields of an entry would, or would not, be of potential use to a natural anguage processing program. Fina
nstrained in any way by the method of access, as we do not have a very clear idea what form the restructured dictionary may eventually take. Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the s-expression, we adopted the approach of converting the tape source into a set of list structures, one per entry. Our task was made possible by the fact that while far from being a database in the accepted sense of the word, the LDOCE typesetting tape is the only truly computerised dictionary of English (Michiels, 1983). 204 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing The logical structure of a dictionary entry is reflected on the tape as a sequence of typed records (see Figure 1), each with additional internal segmentation, where rec- ords and fields correspond to separate units in an entry, such as headword, pronunciation, grammar code, word senses, and so forth. (Record-type homograph (Seq-number E-code I-code)) (Record-type headword (Serial-no Main-entry)) (Record-type pronunciation (Phonetic)) (Record-
ss file, paired together with an indexing file from which the disc addresses of dictionary entries for words and compounds can be computed. A series of systems in Cambridge are implemented in Lisp running under Unix TM. They all make use of an efficient dictionary access system which services re- quests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated infor- mation can be retrieved. As Alshawi (1987) points out, given that no situations were envisaged where the information from the tape would be altered once in- stalled in secondary storage, this simple and conven- Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 205 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing tional access trategy is perfectly adequate. The use of such standard atabase indexing techniques makes it possible for an active dictionary process to be very undemanding with respect o main memory utilisation. For reasons of efficiency and flexibility of customisa- tion, namely
bsequent positions in the dictionary) related to blow. While no application currently makes use of this facil- ity, the motivation for such an approach to dictionary access comes from envisaging a parser which will operate on the basis of the on-line LDOCE; and any serious parser must be able to recognise compounds before it segments its input into separate words. From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords, dictionary search through the pronuncia- tion field is available; Carter (1987) has merged infor- mation from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic lass and syllable structure (Huttenlocher and Zue, 1983). In addition, a fully flexible access system allows the re- trieval of dictionary entries on the basis of constraints specifying any combination ofphonetic, lexical, syntac- tic, and semantic information (Boguraev et al, 1987). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample. 3 THE FORMAT OF T
l operate on the basis of the on-line LDOCE; and any serious parser must be able to recognise compounds before it segments its input into separate words. From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords, dictionary search through the pronuncia- tion field is available; Carter (1987) has merged infor- mation from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic lass and syllable structure (Huttenlocher and Zue, 1983). In addition, a fully flexible access system allows the re- trieval of dictionary entries on the basis of constraints specifying any combination ofphonetic, lexical, syntac- tic, and semantic information (Boguraev et al, 1987). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample. 3 THE FORMAT OF THE GRAMMAR CODES The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields - - head word, pronunciation, grammar codes, definitions, examples, and so forth. Howe
which allows access into the dictionary via different routes. In addition to headwords, dictionary search through the pronuncia- tion field is available; Carter (1987) has merged infor- mation from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic lass and syllable structure (Huttenlocher and Zue, 1983). In addition, a fully flexible access system allows the re- trieval of dictionary entries on the basis of constraints specifying any combination ofphonetic, lexical, syntac- tic, and semantic information (Boguraev et al, 1987). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample. 3 THE FORMAT OF THE GRAMMAR CODES The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields - - head word, pronunciation, grammar codes, definitions, examples, and so forth. However, each of these fields requires further decoding and re- structuring to provide client programs with easy access to the information they require (see Calzolari (1984) for further discussion). For this purpose the formatting 
tic, lexical, syntac- tic, and semantic information (Boguraev et al, 1987). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample. 3 THE FORMAT OF THE GRAMMAR CODES The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields - - head word, pronunciation, grammar codes, definitions, examples, and so forth. However, each of these fields requires further decoding and re- structuring to provide client programs with easy access to the information they require (see Calzolari (1984) for further discussion). For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this informa- tion. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used. These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character. In Figure 1 above the defini- tion of rivet as verb includes the noun definition of &quot;RIVET 1'
typesetting tape and after re- structuring. be l ieve v ... B \[TSa,b;V3;X(to be)l, (to be)7\] (7 300 !< TSa l , b !; V3 !; X (*46 to be *44) 1 !, (*46 to be *44) 7 !< ) sense-no 3 head: TSa head: T5b head: V3 head: X1 r ight opt iona l ( to be) head: X7 r ight opt iona l ( to be) Figure 4 LDOCE provides considerably more syntactic infor- mation than a traditional dictionary. The Longman lexicographers have developed a grammar coding sys- tem capable of representing in compact form a non- trivial amount of information, usually to be found only in large descriptive grammars of English (such as Quirk et al, 1985). A grammar code describes a particular pattern of behaviour of a word. Patterns are descriptive, and are used to convey a range of information: eg. distinctions between count and mass nouns (dog vs. desire), predicative, postpositive and attributive adjec- tives (asleep vs. elect vs. jokular), noun complementa- tion (fondness, fact) and, most importantly, verb com- plementation a d valency. Grammar codes typically contain a capital letter, followed by a number and, occasionally, a small letter, for example \[T5a\] or \[V3\]. The capital etters encode information &quot;about he way a word works in 
. desire), predicative, postpositive and attributive adjec- tives (asleep vs. elect vs. jokular), noun complementa- tion (fondness, fact) and, most importantly, verb com- plementation a d valency. Grammar codes typically contain a capital letter, followed by a number and, occasionally, a small letter, for example \[T5a\] or \[V3\]. The capital etters encode information &quot;about he way a word works in a sentence Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 207 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing or about the position it can fill&quot; (Procter, 1978: xxviii); the numbers &quot;give information about he way the rest of a phrase or clause is made up in relation to the word described&quot; (ibid.). For example, &quot;T&quot; denotes a transi- tive verb with one object, while &quot;5&quot; specifies that what follows the verb must be a sentential complement introduced by that. (The small letters, eg. &quot;a&quot; in the case above, provide further information typically re- lated to the status of various complementisers, adverbs and prepositions in compound verb constructions: eg. &quot;a&quot; indicates that the word that can be left out between a verb and the following clause.) As another
 system by different lexicographers. For example, when codes containing &quot;to be&quot; are elided they mostly occur as illustrated in Figure 4 above. However, sometimes this is represented as \[L(to be)l,9\]. Presumably this kind of inconsistency arose because one member of the team of lexicographers realised that this form of elision saved more space. This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic hecking procedure is attempted (see Mi- chiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project (Akkerman et al, 1985; Akkerman, 1986). In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation a d expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis. In the medium term this ap- proach, though time consuming, will be of some utility for producing more reliable lexicons for natural lan- guage processing. 208
exicographers. For example, when codes containing &quot;to be&quot; are elided they mostly occur as illustrated in Figure 4 above. However, sometimes this is represented as \[L(to be)l,9\]. Presumably this kind of inconsistency arose because one member of the team of lexicographers realised that this form of elision saved more space. This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic hecking procedure is attempted (see Mi- chiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project (Akkerman et al, 1985; Akkerman, 1986). In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation a d expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis. In the medium term this ap- proach, though time consuming, will be of some utility for producing more reliable lexicons for natural lan- guage processing. 208 Computational Li
oach to lexicon development, since this allows the restructuring programs to be progres- sively refined as these problems emerge. Any attempt at batch processing without extensive initial testing of this kind would inevitably result in an incomplete and pos- sibly inaccurate l xicon. 4 THE CONTENT OF THE GRAMMAR CODES Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language process- ing. The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of Quirk et al (1972, 1985). The codes are doubly articulated; capital letters represent he grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in. Most of the subcategorisation frames are specified by syntactic ategory, but some are very ill-specified; for instance, 9 is defined as &quot;needs a descriptive word or phrase&quot;. In practice many adver- bial and predicative complements will satisfy this code, when attached to a verb; for example, put \[xg\] where the code marks a locative adverbial prepositional phrase vs. make under sense
5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assign- ment of the V code seem to be purely syntactic. Michiels (1982) and Akkerman et al (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. Ingria (1984) compre- hensively compares different approaches to comple- mentation within grammatical theory providing atouch- stone against which the LDOCE scheme can be evaluated. Most automated parsing systems employ grammars which carefully distinguish syntactic and semantic in- formation, therefore, if the information provided by the Longman grammar code system is to be of use, we need to be able to separate out
d to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assign- ment of the V code seem to be purely syntactic. Michiels (1982) and Akkerman et al (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. Ingria (1984) compre- hensively compares different approaches to comple- mentation within grammatical theory providing atouch- stone against which the LDOCE scheme can be evaluated. Most automated parsing systems employ grammars which carefully distinguish syntactic and semantic in- formation, therefore, if the information provided by the Longman grammar code system is to be of use, we need to be able to separate out this information and map 
used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assign- ment of the V code seem to be purely syntactic. Michiels (1982) and Akkerman et al (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. Ingria (1984) compre- hensively compares different approaches to comple- mentation within grammatical theory providing atouch- stone against which the LDOCE scheme can be evaluated. Most automated parsing systems employ grammars which carefully distinguish syntactic and semantic in- formation, therefore, if the information provided by the Longman grammar code system is to be of use, we need to be able to separate out this information and map it into a representation scheme compatible with the type of lexicon used by such parsing systems. The program which transforms the LDOCE grammar codes into lexical ent
ve developed for the verb codes. Extending the system to handle nouns, adjectives and adverbs would present no problems of principle. However, the LDOCE coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system. No attempt has been made to map any closed class entries from LDOCE, as a 3,000 word lexicon containing most closed class items has been developed independently b one of the groups collaborating with us to develop the general purpose morphological nd syntactic analyser (see the Introduction and Russell et al, 1986). Initially the transformation f the LDOCE codes was performed on a code-by-code basis, within a code field associated with each individual word sense. This ap- proach is adequate if all that is required is an indication of the subcategorisation frames relevant o any partic- ular sense. In the main, the code numbers determine a unique subcategorisation. Thus the entries can be used to select the appropriate VP rules from the grammar (assuming a GPSG-style approach to subcategorisation) Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 209 Bran Boguraev and Ted Briscoe Large
tial indications of the semantic nature of the relevant sense of the verb. The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense. In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of Michiels (1982), we classify verbs as Subject Equi, Object Equi, Subject Raising or Object Raising for each sense which has a predicate complement code associated with it. These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For exam- ple, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in (1)
rficial syntactic form to an underlying logical representation. For exam- ple, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in (1) John believes the Earth to be round it will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see Williams (1980), for further discussion). The five rules which are applied to the grammar codes associated with a verb sense are ordered in a way which reflects the filtering of the verb sense through a series of syntactic tests. Verb senses with an \[it + 15\] code are classified as Subject Raising. Next, verb senses which contain a \[V\] or \[X\] code and one of \[D5\], \[D5a\], \[D6\] or \[D6a\] codes are classified as Object Equi. Then, verb senses which contain a \[V\] or \[X\] code and a IT5\] or \[T5a\] code in the associated grammar code field, (but none of the D codes mentioned above), are classifie
( l ) ass,l ine(I) decline(S) \[WvS;/t+15\] (Type 1 SRaising) \[Wv4;IO;Tl:( of, against) ifa;D5a;V3\] (Type 30Equi) \[Wv4;T1,Sa,b;X(to be)l,7\] (Type 20Raising) \[T1,3;I0\] (Type 2 SEqui) Figure 9 (2) John believes that the Earth is round. (3) *John forces that the Earth is round. Secondly, if a verb takes a direct object and a sentential complement, it will be an Equi verb, as examples in (4) and (5) illustrate. (4) John persuaded Mary that the Earth is round. (5) *John believed Mary that the Earth is round. Clearly, there are other syntactic and semantic tests for this distinction, (see eg. Perlmutter and Soames, 1979:472), but these are the only ones which are explicit in the LDOCE coding system. Once the semantic type for a verb sense has been determined, the sequence of codes in the associated code field is translated, as before, on a code-by-code basis. However, when a predicate complement code is encountered, the semantic type is used to determine the type assignment, asillustrated in Figures 4 and 8 above. Where no predicate complement is involved, the letter code is usually sufficient to determine the logical prop- erties of the verb involved. For example, T codes nearly always translate into two-pl
rd qualifiers can be utilised straightforwardly is with di- transitive verbs such as give and donate. Give is coded as \[Dl(to)\] which allows us to recover the information that this verb permits dative movement and requires a prepositional phrase headed by &quot;to&quot;: (Takes NP NP ToPP) and (Takes NP NP NP). On the other hand, donate is coded \[T1 (to)\], which tells us that it does not undergo dative movement but does require a prepositional phrase headed by &quot;to&quot;: (Takes NP NP ToPP). There are many more distinctions which are con- veyed by the conjunction of grammar codes and word qualifiers (see Michiels, 1982, for further details). How- ever, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries. 5 LEXICAL ENTRIES FOR PATR-II The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms. To demonstrate hat this is possible we have implemented a system which con- structs dictionary entries for the PATR-II system (Shieber, 1984 and references therein). PATR-II was chosen because it has 
 the conjunction of grammar codes and word qualifiers (see Michiels, 1982, for further details). How- ever, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries. 5 LEXICAL ENTRIES FOR PATR-II The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms. To demonstrate hat this is possible we have implemented a system which con- structs dictionary entries for the PATR-II system (Shieber, 1984 and references therein). PATR-II was chosen because it has been reimplemented in Cam- bridge and was therefore, available; however, the task would be nearly identical if we were constructing en- tries for a system based on GPSG, FUG or LFG. We word storm: w Jense ~ <head t rans sense-no> = 1 V TakesNP Dyadic worddag storm: \ [cat: v head: \[aux: fa l se t rans: \[pred: storm sense-no: 1 arg l : <DGIS> - \[\] arg2: <DG16> = \ [ \ ] \ ] \ ] syncat : \ [ f i r s t : \ [cat: NP head: \ [ trans: <DG15>\]\] res t : \ [ f i r s t : \ [cat: NP head: \ [ trans: <DC16>\]\] res t : \ [ f i r s t : lambd
he source dictionary and the reliability of the more ambi- tious (and potentially controversial) aspects of the grammar code transformation rules. It is not clear, in particular, that the rules for computing semantic types for verbs are well enough motivated linguistically or that the LDOCE lexicographers were sensitive nough to the different transformational potential of the various classes of verbs to make a rule such as our one for Object Raising viable. We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in Rosenbaum (1967) and Stock- well et al (1973). Figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system. The overall error rate of the system was 14%; how- ever, as the table illustrates, the rules discussed above classify verbs into Subject Raising, Subject Equi and persuade v I \[TI (of); D5\] to cause to feel certain; CONVINCE: She waa not persuaded o,f the truth o.f hi~ ~ement = \[Tl(into, out o~; V3\] to cause to do something by reasoning, arguing, begging, etc.: try to persuade him to let .a go with 
~lou help me up (the a~,o)~ I T~ a,'~ he~ps h~,. (to) ,~k, I Yo,,, o~u helps a lot. I Can I help ( ,~ yo,,, wo~k)~ ........ detest ... \[T1,4\] to hate with very strong feeling: I deter people who decelse and tell lies. I dn, . ~i shootir~ and k~lin? ......... (7) I prefer that he come on Monday . (8) ?I pre fer that he marr ies Jul ie. This example also highlights a deficiency in the LDOCE coding system since pre fer occurs much more naturally with a sentential complement if it collocates with a modal such as &quot;would&quot;. This deficiency is rectified in the verb classification system employed by Jackendoff and Grimshaw (1985) in the Brandeis verb catalogue. The main source of error comes from the misclassi- fication of Object Raising into Object Equi verbs. Ar- guably, these errors also derive mostly from errors in the dictionary, rather than a defect of the rule. 66% of the Object Raising verbs were misclassified as Object Equi verbs, because the cooccurrence of the T5 and V (2, 3, or 4) codes in the same code fields, as predicted by the Object Raising rule above, was not confirmed by LDOCE. All the 14 verbs misclassified contain V codes and 10 of these also contain T5 codes. However, the Longman lexicographers t
here. Similarly, expect is not given a V3 code under sense 1 (Figure 19), however the grammaticality of (10) I expect him to pass the exam with the relevant interpretation suggests that it should be assigned a V3 code. Alternatively, sense 5, which is assigned a V3 code, seems uspiciously similar to sense 1. The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require. None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Rais- ing rule. In addition, Moulin et al (1985) note that our Object Raising rule would assign mean to this category incorrectly. Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e. &quot;intend&quot;), however, when it is used in this sense it must be treated as an Object Equi verb. This small experiment demonstrates a number of points. Firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in LDOCE. Of the 139 verbs tested, we only found code omissions in 10 cases. Secondly though, when we consider the interaction between the assignments
dual codes to verbs is on the whole relatively accurate in LDOCE. Of the 139 verbs tested, we only found code omissions in 10 cases. Secondly though, when we consider the interaction between the assignments of codes and word sense classification, LDOCE appears less reliable. This is the primary source of error in the case of the Object Raising rule. Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. Ideally, to distinguish between raising and equi verbs, a number of syntactic criteria should be employed (Perlmutter and Soames, 1979:460ff.). However, only two of these criteria are explicit in the coding system. On the basis of the results obtained, we explored the possibility of modifying the Object Raising rule to take account of the cooccurrence ofT5 and T5a codes and V or X codes within a homograph, rather than within a word sense. An exhaustive search of the dictionary produced 24 verbs coded in this fashion. Ten of these were listed as Object Raising verbs in the published lists used in the above experiment. Five more verbs were classified as Equi in the published lists. Of the remaining nine verbs which did not app
ter this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system. This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic onstruction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. Leech et al, 1983). However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly. 8 ACKNOWLEDGEMENTS We would like to thank the Longman Group Limited for kindly allowing us access to the LDOCE typesetting tape for research purposes. We also thank Steve Pul- man, Graham Russell and Karen Sparck Jones for their comments on the first draft, which substantially im- proved this paper. Part of the research reported here was funded by the UK Science and Engineering Re- search Council (Grant No. GR/D/05554) under the Al- vey Programme.
