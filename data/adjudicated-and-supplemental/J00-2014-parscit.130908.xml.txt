generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.1 Whether this is always possible under an appropriate definition of &quot;simple constraints&quot; (e.g., Eisner 1997b) is of course an empirical question. 2. Relevance Before looking at Kager 's textbook in detail, it is worth pausing to ask what broader implications Optimality Theory might have for computational linguistics. If you are an academic phonologist, you already know OT by now. If you are not, should you take the time to learn? So far, OT has served CL mainly as a source of interesting new problems—both theoretical and (assuming a lucrative market for phonology workbench utilities) prac1 This style of analysis is shared by Autolexical Grammar (Sadock 1985), which has focused more on (morpho)synta
interaction, and about the usefulness of positing hidden structure (e.g., prosodic constituency) to which multiple features may refer. For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-speech tagging. Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from E*, based on criteria of well-formedness (transition probabilities) and faithfulness to the input (emission probabilities). But typical OT grammars offer much richer finite-state models of left context (Eisner 1997a) than provided by the traditional HMM finite-state topologies. Now, among methods that use a Gibbs distribution to choose among linguistic forms, OT generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs. When is this appropriate? It seems to me that there are three possible uses. First, there are categorical phenomena for which strict feature ranking may genuinely suffice. As Kager demonstrates in this textbook, phonology may well fall into this class—although the claim depends on what features are allowe
 be suited to OT analysis. 2 Each constraint/feature is weighted so highly that it can overwhelm the total of all lower-ranked constraints, and even the lowest-ranked constraint is weighted very highly. Recall that the incompatibility of some feature combinations (i.e., nonorthogonality of features) is always what makes it nontrivial to normalize or sample a Gibbs distribution, just as it makes it nontrivial to find optimal forms in OT. 287 Computational Linguistics Volume 26, Number 2 Second, weights are an annoyance when writing grammars by hand. In some cases rankings may work well enough. Samuelsson and Voutilainen (1997) report excellent part-of-speech tagging results using a handcrafted approach that is close to OT.3 More speculatively, imagine an OT grammar for stylistic revision of parsed sentences. The tension between preserving the original author's text (faithfulness to the underlying form) and making it readable in various ways (well-formedness) is right up OT's alley. The same applies to document layout: I have often wished I could write OT-style TeX macros! Third, even in statistical corpus-based NLP, estimating a full Gibbs distribution is not always feasible. Even if strict ranking is not quite acc
