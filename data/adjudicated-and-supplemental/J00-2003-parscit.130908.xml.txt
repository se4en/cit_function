ontext C receives the pronunciation (i.e., phoneme substring) D. Such rules can also be straightforwardly cast in the IF. . . THEN form commonly featured in high-level programming languages and employed in expert, knowledge-based systems technology. They also constitute a formal model of universal computation (Post 1943). Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern. Typical letter-to-sound rule sets are those described by Ainsworth (1973), McIlroy (1973), Elovitz et al. (1976), Hurmicutt (1976), and Divay and Vitale (1997). Because of the complexities of English spelling-to-sound correspondence detailed in the previous section, more than one rule generally applies at each stage of transcription. The potential conflicts that arise are resolved by maintaining the rules in a set of sublists, grouped by (initial) letter and with each sublist ordered by specificity. Typically, the most specific rule is at the top and most general at the bottom. In the Elovitz et al. rules, for instance, transcription is a one-pass, left-to-right process. For the particular target letter (i.e., the initial letter of the substring cur
ge processing that the representation of language should be done within the rule-based paradigm alone. For historical reasons, this traditional position is largely the result of the influence of Chomsky and his efforts to define language in terms of formal mathematics.... The contrasting view, taken here, is that language is not something that can be described in a neat and tidy way. This is also the perspective adopted here. It is also conceivable that data-driven techniques can actually outperform traditional rules. However, this possibility is not usually given much credence. For instance, Divay and Vitale (1997) recently wrote: &quot;To our knowledge, learning algorithms, although promising, have not (yet) reached the level of rule sets developed by humans&quot; (p. 520). Dutoit (1997) takes this further, stating &quot;such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores&quot; (p. 115, note 14). Pronunciation by analogy (PbA) is a data-driven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by Glushko (1979) and Kay and Marcel (1981). It was first proposed for ITS appl
t (yet) reached the level of rule sets developed by humans&quot; (p. 520). Dutoit (1997) takes this further, stating &quot;such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores&quot; (p. 115, note 14). Pronunciation by analogy (PbA) is a data-driven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by Glushko (1979) and Kay and Marcel (1981). It was first proposed for ITS applications over a decade ago by Dedina and Nusbaum (1986, 1991). See also the work of Byrd and Chodorow (1985), which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis. As detailed by Damper (1995) and Damper and Eastmond (1997), PbA shares many similarities with the artificial intelligence paradigms variously called case-based, memory-based, or instance-based reasoning as applied to letter-to-phoneme conversion (Stanfill and Waltz 1986; Lehnert 1987; Stanfill 1987, 1988; Golding 1991; Golding and Rosenbloom 1991; van den Bosch and Daelemans 1993). PbA exploits the phonological knowledge implicitly contained in a dictionary of w
, good) data with inaccurate or biased data, especially if the uncertainties or variances of the data are unknown&quot; (p. 8). Methods of information fusion include &quot;voting methods, Bayesian inference, Dempster-Shafer 's method, generalized evidence processing theory, and various ad hoc techniques&quot; (Hall 1992, 135). Clearly, the above characterization is very wide ranging. Consequently, fusion has been applied to a wide variety of pattern recognition and decision theoretic problems— using a plethora of theories, techniques, and tools—including some applications in computational linguistics (e.g., Brill and Wu 1998; van Halteren, Zavrel, and Daelemans 1998) and speech technology (e.g., Bowles and Damper 1989; Romary and Pierre11989). According to Abbott (1999, 290), &quot;While the reasons [that] combining models works so well are not rigorously understood, there is ample evidence that improvements over single models are typical.... A strong case can be made for combining models across algorithm families as a means of providing uncorrelated output estimates.&quot; Our purpose in this paper is to study and exploit such fusion by model (or strategy) combination as a way of achieving performance gains in PbA. 6. Mul
