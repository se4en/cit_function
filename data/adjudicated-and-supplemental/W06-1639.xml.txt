abase (http://thomas.loc.gov) of congresonline accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially society-changing effect. In the United States, for example, governmental bodies are providing and soliciting political documents via the Internet, with lofty goals in mind: electronic rulemaking (eRulemaking) initiatives involving the ?electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process?, may ?[alter] the citizen-government relationship? (Shulman and Schlosberg, 2002). Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics2, or at least on political journalism3. Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process. Evaluative and persuasive documents, such as a politician?s speech regarding a bill or a blogger?s commentary on a legislative proposal, form a particularly interesting type o
ant to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process. Evaluative and persuasive documents, such as a politician?s speech regarding a bill or a blogger?s commentary on a legislative proposal, form a particularly interesting type of politically oriented text. People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that (U.S.) bills often reach several hundred pages in length (Smith et al, 2005). Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist. 2E.g., ?Internet injects sweeping change into U.S. politics?, Adam Nagourney, The New York Times, April 2, 2006. 3E.g., ?The End of News??, Michael Massing, The New York Review of Books, December 1, 2005. 327 plicitly solicited in the eRulemaking scenario. In the analysis of evaluative language, it is fundamentally necessary to determine whether the author/speaker supports or disapproves of the topic of discussion. In this pap
aker segment of text) represents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity b
sents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the text
or or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be label
to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bh
e of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005;
whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most inte
aker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniqu
oposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable 
ithin the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we 
d Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easil
for an active bibliography). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via t
graphy). In particular, since we treat each individual speech within a debate as a single ?document?, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al, 2002; Turney, 2002; Dave et al, 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), w
em-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data. Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants? political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (?I second that!?) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf. Agrawal et al (2003)). Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text. Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually. Intuition suggests that this is true of the data with which we experiment, for several reasons. First, U.S. congressional debates contain very rich language and cover an extremely wide var
ple, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data. Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants? political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (?I second that!?) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf. Agrawal et al (2003)). Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text. Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually. Intuition suggests that this is true of the data with which we experiment, for several reasons. First, U.S. congressional debates contain very rich language and cover an extremely wide variety of topics, ranging from 
e set because our goal is to examine classification of speech segments in the context of the surrounding discussion. 3 Method The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation. As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate. Our classification framework, directly inspired by Blum and Chawla (2001), integrates both perspectives, optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label. In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships. 329 3.1 Classification framework Let s1, s2, . . . , sn be the sequence of speech segments within a given debate, and let Y and N stand for the ?yea? and ?nay? class, respectively. Assume we have a non-negative function in
e linked speech segments receive the same label. Then, any class assignment c = c(s1), c(s2), . . . , c(sn) can be assigned a cost ? s ind(s, c(s))+ ? s,s?: c(s) 6=c(s?) ? ` between s,s? str(`), where c(s) is the ?opposite? class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document 
ents receive the same label. Then, any class assignment c = c(s1), c(s2), . . . , c(sn) can be assigned a cost ? s ind(s, c(s))+ ? s,s?: c(s) 6=c(s?) ? ` between s,s? str(`), where c(s) is the ?opposite? class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y
, any class assignment c = c(s1), c(s2), . . . , c(sn) can be assigned a cost ? s ind(s, c(s))+ ? s,s?: c(s) 6=c(s?) ? ` between s,s? str(`), where c(s) is the ?opposite? class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and u
ph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al, 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def= ? ??? ??? 1 d(s) > 2?s;( 1 + d(s)2?s ) /2 |d(s)| ? 2?s; 0 d(s) < ?2?s where ?s is the standard deviat
ing speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al, 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def= ? ??? ??? 1 d(s) > 2?s;( 1 + d(s)2?s ) /2 |d(s)| ? 2?s; 0 d(s) < ?2?s where ?s is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s,N ) def= 1? ind(s,Y). We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class. 3.3 Relationships between speech segments A wide range of relationships between text segme
ation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al, 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def= ? ??? ??? 1 d(s) > 2?s;( 1 + d(s)2?s ) /2 |d(s)| ? 2?s; 0 d(s) < ?2?s where ?s is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s,N ) def= 1? ind(s,Y). We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class. 3.3 Relationships between speech segments A wide range of relationships between text segments can be modeled as
to change the train/development/test sets in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP
evelopment/test sets in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of nea
 in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detecti
and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et
 we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), i
is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that
ork Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al, 2005; Cardie et al, 2006; Kwon et al, 2006). There has also been work focused upon determining the political leaning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem
ning (e.g., ?liberal? vs. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do no
s. ?conservative?) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ?unlabeled? texts) (Laver et al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to
t al, 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen
sites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also 
s of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-docume
tion to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang
ically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg 
llard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al, 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al, 2000), citation analysis (Lehnert et al, 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty 
onal rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum an
2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It
ns, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to 
hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that 
lationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it 
unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underl
lho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem th
quential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in
s between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice ea
nversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on spe
iment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on d
 different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual refere
as considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statemen
ocument similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that t
Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that the integration of ev
aryya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al, 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al, 2001; Getoor et al, 2002; Taskar et al., 2002; Taskar et al, 2003; Taskar et al, 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that the integration of even very limited information r
