al Linguistics Volume 33, Number 1 statistical techniques to overcome the brittleness often associated with knowledgebased approaches? We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm
 overcome the brittleness often associated with knowledgebased approaches? We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al 2000) provides a task-based mode
text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al 2000) provides a task-based model of the clinical information-seeking process. The PICO framework (Richardson et al 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, an
propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al 2000) provides a task-based model of the clinical information-seeking process. The PICO framework (Richardson et al 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomed
k-based model of the clinical information-seeking process. The PICO framework (Richardson et al 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians? questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994
s of a system. The confluence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians? questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant
 many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians? questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately 
2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians? questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts. Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology?better information systems to provide d
k factors). 65 Computational Linguistics Volume 33, Number 1 Diagnosis: This encompasses two primary types: Differential diagnosis: Identifying and ranking by likelihood potential diseases based on findings observed in a patient. Diagnostic test: Selecting and interpreting diagnostic tests for a patient, considering their precision, accuracy, acceptability, cost, and safety. Etiology/Harm: Identifying factors that cause a disease or condition in a patient. Prognosis: Estimating a patient?s likely course over time and anticipating likely complications. These activities represent what Ingwersen (1999) calls ?work tasks.? It is important to note that they exist independently of information needs, namely, searching is not necessarily implicated in any of these activities. We are, however, interested in situations where questions arise during one of these clinical tasks?only then does the physician engage in information-seeking behavior. These activities translate into natural ?search tasks.? For therapy, the search task is usually therapy selection (for example, determining which course of action is the best treatment for a disease) or prevention (for example, selecting preemptive measures w
 the types of studies relevant to each of the four tasks have been extensively studied by the Hedges Project at the McMaster University (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). The results of this research are implemented in the PubMed Clinical Queries tools, which can be used to retrieve task-specific citations (more about this in the next section). The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question. The following four components have been identified as the key elements of a question related to patient care (Richardson et al 1995):  What is the primary problem or disease? What are the characteristics of the patient (e.g., age, gender, or co-existing conditions)?  What is the main intervention (e.g., a diagnostic test, medication, or therapeutic procedure)?  What is the main intervention compared to (e.g., no intervention, another drug, another therapeutic procedure, or a placebo)?  What is the desired effect of the intervention (e.g., cure a disease, relieve or eliminate symptoms, reduce side effects, or lower cost)? These four elements are often referenced with the mnemonic PICO, which stands for Patient/Problem, 
ly, the third facet serves as a tool for appraising the strength of evidence presented in the study, that is, how much confidence should a physician have in the results? Several taxonomies for appraising the strength of evidence based on the type and quality of the study have been developed. We chose the Strength of Recommendations Taxonomy (SORT) as the basis for determining the potential upper bound on the 66 Demner-Fushman and Lin Answering Clinical Questions quality of evidence, due to its emphasis on the use of patient-oriented outcomes and its attempt to unify other existing taxonomies (Ebell et al 2004). There are three levels of recommendations according to SORT:  A-level evidence is based on consistent, good-quality patient outcome-oriented evidence presented in systematic reviews, randomized controlled clinical trials, cohort studies, and meta-analyses.  B-level evidence is inconsistent, limited-quality, patient-oriented evidence in the same types of studies.  C-level evidence is based on disease-oriented evidence or studies less rigorous than randomized controlled clinical trials, cohort studies, systematic reviews, and meta-analyses. A question-answering system designed to support th
structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a 1 http://www.nlm.nih.gov/pubs/factsheets/medline.html 2 Commonly referred to as MeSH terms or MeSH headings, although technically the latter is redundant. 67 Computational Linguistics Volume 33, Number 1 separate thesaurus. Indexing is performed by approximately 100 indexers with at least bachelor?s degrees in life sciences and formal training in indexing provided by NLM. Since mid-2002, the Library has been employing software that automatically suggests MeSH headings based on content (Aronson et al 2004). Nevertheless, the indexing process remains firmly human-centered. As a concrete example, an abstract titled ?Antipyretic efficacy of ibuprofen vs. acetaminophen? might have the following MeSH headings associated with it: MH - Acetaminophen/*therapeutic use MH - Child MH - Comparative Study MH - Fever/*drug therapy MH - Ibuprofen/*therapeutic use To represent different aspects of the topic described by a particular MeSH heading, up to three subheadings may be assigned, as indicated by the slash notation. In this example, a trained user could interpret from the MeSH terms that the article is a
Section 12. Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative, we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail?this is the standard pipeline architecture commonly employed in other question-answering systems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001). The architecture of our system is shown in Figure 1. The query formulator is responsible for converting a clinical question (in the form of a query frame) into a PubMed search query. Presently, these queries are already encoded in our test collection (see Section 6). PubMed returns an initial list of MEDLINE citations, which is then analyzed by our knowledge extractors (see Section 5). The input to the semantic matcher, which implements our EBM citation scoring algorithm, is the query frame and annotated MEDLINE citations. The module outputs a ranked list of c
would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative, we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail?this is the standard pipeline architecture commonly employed in other question-answering systems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001). The architecture of our system is shown in Figure 1. The query formulator is responsible for converting a clinical question (in the form of a query frame) into a PubMed search query. Presently, these queries are already encoded in our test collection (see Section 6). PubMed returns an initial list of MEDLINE citations, which is then analyzed by our knowledge extractors (see Section 5). The input to the semantic matcher, which implements our EBM citation scoring algorithm, is the query frame and annotated MEDLINE citations. The module outputs a ranked list of citations that have been scored i
ee capabilities in an implemented clinical question-answering system and conducted three separate evaluations to assess the effectiveness of our developed capabilities. We do not tackle the query formulator, although see discussion in Section 12. Overall, results indicate that our implemented system significantly outperforms the PubMed baseline. 5. Knowledge Extraction for Evidence-Based Medicine The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section, which elaborates on preliminary results reported in Demner-Fushman and Lin (2005), describes extraction algorithms for population, problems, interventions, outcomes, and the strength of evidence. For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases. Our knowledge extractors rely extensively on MetaMap (Aronson 2001), a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus. Many of our algorithms oper
ty integral to clinical question answering. This section, which elaborates on preliminary results reported in Demner-Fushman and Lin (2005), describes extraction algorithms for population, problems, interventions, outcomes, and the strength of evidence. For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases. Our knowledge extractors rely extensively on MetaMap (Aronson 2001), a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus. Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional feature we take advantage of (when present) is explicit section markers present in some abstracts. These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess th
a great deal of variation in the actual headings. Even when present, the headings are not organized in a manner focused on patient care. In addition, abstracts of much high-quality work remain unstructured. For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence. The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts, created through an effort led by the first author at the National Library of Medicine (Demner-Fushman et al 2006). As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac70 Demner-Fushman and Lin Answering Clinical Questions tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques (Demner-Fushman et al 2006). These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily 
 model different user behaviors ranging from naive to expert (where advanced search features were employed). With the exception of 50 citations retrieved to answer a question about childhood immunization, the rest of the results were retrieved by querying on a disease, for example, diabetes. Of the 633 citations, 100 abstracts were also fully annotated with population, problems, and interventions. These 100 abstracts were set aside as a held-out test set. Of the remaining citations, 275 were used for training and rule derivation, as described in the following sections. After much exploration, Demner-Fushman et al (2006) discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues. Consider the following segment: This double-blind, placebo-controlled, randomized, 3-period, complete block, 6-week crossover study examined the efficacy of simvastatin in adult men and women (N = 151) with stable type 2 DM, low density lipoprotein-cholesterol 100 mg/dL, HDL-C < 40 mg/dL, and fasting triglyceride level > 150 and < 700 mg/dL. All annotators agreed that the sentence contained the problem, population, and intervent
om co-occurring conditions identified in the abstract. 5.4 Evaluation of Problem Extractor Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem. For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers? tasks in assigning terms is to identify the main topic of the article (sometimes a disorder). For this evaluation, we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in Sneiderman et al (2005). We applied our problem extractor on different segments of the abstract: the title only, the title and first two sentences, and the entire abstract. These results are shown in Table 2. Here, a problem was considered correctly identified only if it shared the same concept ID as the ground truth problem (from the MeSH heading). The performance of our best variant (abstract title and first two sentences) approaches the upper bound on MetaMap performance?which is limited by human agreement on the identification of semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (20
05). We applied our problem extractor on different segments of the abstract: the title only, the title and first two sentences, and the entire abstract. These results are shown in Table 2. Here, a problem was considered correctly identified only if it shared the same concept ID as the ground truth problem (from the MeSH heading). The performance of our best variant (abstract title and first two sentences) approaches the upper bound on MetaMap performance?which is limited by human agreement on the identification of semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003). Although problem extraction largely depends on disease coverage in UMLS and MetaMap performance, the error rate could be further reduced by more sophisticated recognition of implicitly stated problems. For example, with respect to a question about immunization in children, an abstract about the measles-mumps-rubella vaccination never mentioned the disease without the word vaccination; hence, no concept of the type DISEASE OR SYNDROME was identified. 5.5 Intervention Extractor The intervention extractor identifies both the intervention and comparison elements in a PICO frame; processing of th
Map, which sets the maximum score to 8. The two phrases dropout rate and adverse events contribute one point each to the cumulative score, which results in a likelihood estimate of 0.25 for this sentence. The unigram ?bag of words? classifier is a naive Bayes classifier implemented with the API provided by the MALLET toolkit.4 This classifier outputs the probability of a class assignment. The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure (Yang and Pedersen 1997), and then selected only the positive outcome predictors using odds ratio (Mladenic and Grobelnik 1999). Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment. The position classifier returns the maximum likelihood estimate that a sentence is an outcome based on its position in the abstract (for structured abstracts, with respect to the results or conclusions sections; for unstructured abstract
 one point each to the cumulative score, which results in a likelihood estimate of 0.25 for this sentence. The unigram ?bag of words? classifier is a naive Bayes classifier implemented with the API provided by the MALLET toolkit.4 This classifier outputs the probability of a class assignment. The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure (Yang and Pedersen 1997), and then selected only the positive outcome predictors using odds ratio (Mladenic and Grobelnik 1999). Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment. The position classifier returns the maximum likelihood estimate that a sentence is an outcome based on its position in the abstract (for structured abstracts, with respect to the results or conclusions sections; for unstructured abstracts, with respect to the end of the abstract). The abstract length classifier returns a smoothed (add one
lem or an intervention. The outputs of our basic classifiers are combined using a simple weighted linear interpolation scheme: Soutcome = ?1Scues + ?2Sunigram + ?3Sn-gram + ?4Sposition + ?5Slength + ?6Ssemantic type (1) We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition. The second involved a more principled method using confidence values generated by the base classifiers and least squares lin4 http://mallet.cs.umass.edu/ 76 Demner-Fushman and Lin Answering Clinical Questions ear regression adapted for classification (Ting and Witten 1999), which can be described by the following equation: LR(x) = N ? k=1 ?kPk(X) (2) Pk is the probability that a sentence specifies an outcome, as determined by classifier k (for classifiers that do not return actual probabilities, we normalized the scores and treated them as such). To predict the class of a sentence, the probabilities generated by n classifiers are combined using the coefficients (?0, ...,?n). These values are determined in the training stage as follows: Probabilities predicted by base classifiers for each sentence are represented in an N ? M matrix A, where M is the number of se
sess the quality of a particular citation for clinical purposes. Metadata associated with most MEDLINE citations (MeSH terms) are extensively used to determine the strength of evidence and in our EBM citation scoring algorithm (Section 6). The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study. Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy (Ebell et al 2004). 5.10 Sample Output A complete example of our knowledge extractors working in unison is shown in Figure 2, which contains an abstract retrieved in response to the following question: ?In children with an acute febrile illness, what is the efficacy of single-medication therapy with acetaminophen or ibuprofen in reducing fever?? (Kauffman, Sawyer, and Scheinbaum 1992). Febrile illness is the only concept mapped to DISORDER, and hence is identified as the primary problem. 37 otherwise healthy children aged 2 to 12 years is correctly identified as the population. Acetaminophen, ibuprofen, and pla
listic sampling of the scenarios that a clinical question-answering system would be confronted with. These questions were minimally modified from their original form as downloaded from the World Wide Web. In a few cases, a single question actually consisted of several smaller questions; such clusters were simplified by removing questions more peripheral to the central clinical problem. All questions were manually classified into one of the four clinical tasks; the distribution of the questions roughly follows the prevalence of each task type as observed in natural settings, noted by Ely et al (1999). The final step in the preparation process was manual translation of the natural language questions into PICO query frames. Our collection was divided into a development set and a blind held-out test set for verification purposes. The breakdown of these questions into the four clinical tasks and the development/test split is shown in Table 6. An example of each question type from our development set is presented here, along with its query frame: Does quinine reduce leg cramps for young athletes? (Therapy) search task: therapy selection primary problem: leg cramps co-occurring problems: muscle
ks falling into each clinical task, we gathered a list of terms that are positive and negative indicators of relevance. 82 Demner-Fushman and Lin Answering Clinical Questions The task score, Stask, is given by: Stask = ? t?MeSH ?(t) (7) The function ?(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task. Note that although our current system uses MeSH headings assigned by human indexers, manually assigned terms can be replaced with automatic processing if needed (Aronson et al 2004). Below, we enumerate the relevant indicator terms by clinical task. However, there is a set of negative indicators common to all tasks; these were extracted from the set of genomics articles provided for the secondary task in the TREC 2004 genomics track evaluation (Hersh, Bhupatiraju, and Corley 2004); examples include genetics and cell physiology. The positive and negative weights assigned to each term heuristically encode the relative importance of different MeSH headings and are derived from the Clinical Queries filters in PubMed, from the JAMA EBM tutorial series on critical appraisal of
l; for one, it is debatable which metric should be optimized. The test questions were hidden during the system 85 Computational Linguistics Volume 33, Number 1 development phase and served as a blind held-out test set for assessing the generality of our algorithm. In our experiment, we collected the following metrics, all computed automatically using our relevance judgments:  Precision at ten retrieved documents (P10) measures the fraction of relevant documents in the top ten results.  Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved (Baeza-Yates and Ribeiro-Neto 1999). It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.  Mean Reciprocal Rank (MRR) is a measure of how far down a hit list the user must browse before encountering the first relevant result. The score is equal to the reciprocal of the rank, that is, a relevant document at rank 1 gets a score of 1, 1/2 at rank 2, 1/3 at rank 3, and so on. Note that this measure only captures the appearance of the first relevant document. Furthermore, due to its discretization, MRR values are noisy on small collections.  Tot
an directly act on. Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion?it need not be repeated unless the physician wishes to ?drill down?; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these features are also beyond the capabilities of current summarization systems. It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objective
mputational linguistics?redundancy detection for multi-document summarization?seems easy by comparison. Furthermore, it is unclear if textual strings make ?good answers.? Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Perhaps some variation of multi-level bulleted lists, appropriately integrated with interface elements for expanding and hiding items, might provide physicians a better overview of the information landscape; see, for example, Demner-Fushman and Lin (2006). Recognizing this complex set of issues, we decided to take a simple extractive approach to answer generation. For each abstract in our reranked list of citations, our system produces an answer by combining the title of the abstract and the top three outcome sentences (in the order they appeared in the abstract). We employed the outcome scores generated by the regression model. No attempt was made to synthesize information from multiple citations. A formal evaluation of this simple approach to answer generation is presented in the next section. 10. Evaluation of Clinical Answers Evaluation of
itle of the abstract and the top three outcome sentences (in the order they appeared in the abstract). We employed the outcome scores generated by the regression model. No attempt was made to synthesize information from multiple citations. A formal evaluation of this simple approach to answer generation is presented in the next section. 10. Evaluation of Clinical Answers Evaluation of answers within a clinical setting involves a complex decision that must not only take into account topical relevance (i.e., ?Does the answer address the information need??), but also situational relevance (e.g., Saracevic 1975, Barry and Schamber 94 Demner-Fushman and Lin Answering Clinical Questions 1998). The latter factor includes many issues such as the strength of evidence, recency of results, and reputation of the journal. Clinicians need to carefully consider all these elements before acting on any information for the purposes of patient care. Within the framework of evidence-based medicine, the physician is the final arbiter of how clinical answers are integrated into the broader activities of medical care, but this complicates any attempt to evaluate answers generated by our system. In assessing answers pr
nd Discussion Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges P
re exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particul
uestion answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-b
 of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the
o the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because seco
as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary 
possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of s
and demonstrates that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering. In addition to question answering, multi-document summarization provides 
that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering. In addition to question answering, multi-document summarization provides a complementary approac
n a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering. In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs. The PERSIVAL project, the most comprehensive study of such techniques applied on medical texts to date, leverages patient records to generate personalized summaries in response to physicians? queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project. The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers (e.g., Jacquemart and Zweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 20
e, leverages patient records to generate personalized summaries in response to physicians? queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project. The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers (e.g., Jacquemart and Zweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005. Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine. Finally, the evaluation of answers to complex questions remains an open research problem. Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems. In Sections 9 and 10, we have d
enerate personalized summaries in response to physicians? queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project. The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers (e.g., Jacquemart and Zweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005. Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine. Finally, the evaluation of answers to complex questions remains an open research problem. Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems. In Sections 9 and 10, we have discussed many of thes
d for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems. In Sections 9 and 10, we have discussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of ?information nuggets? may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ?other? questions (Voorhees 2003). A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken. 12. Future Work The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously, a user study (Lin et al 20
iscussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of ?information nuggets? may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ?other? questions (Voorhees 2003). A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken. 12. Future Work The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously, a user study (Lin et al 2003) has shown that people are reluctant to type full natural language questions, even after being told that they were using a questionanswering system and that typing complete questions would result in better performance.
orhees 2003). A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken. 12. Future Work The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously, a user study (Lin et al 2003) has shown that people are reluctant to type full natural language questions, even after being told that they were using a questionanswering system and that typing complete questions would result in better performance. We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest 98 Demner-Fushman and Lin Answering Clinical Questions the upfront effort necessary to accomplish this. Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance o
rom the entire MEDLINE database and directly index the results. Given access to more resources, a system could index identified PICO elements and directly match queries against a knowledge store. Finally, answer generation remains an area that awaits further exploration, although we would have to first define what a good answer should be. We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple 99 Computational Linguistics Volume 33, Number 1 abstracts; although see Demner-Fushman and Lin (2006). Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements. To address these very difficult challenges, finer-grained semantic analysis of medical texts is required. 13. Conclusion Our experiments in clinical question answering provide some answers to the broader research question regarding the role of knowledge-based and statistical techniques in advanced question answering. This work demonstrates that the two approaches are complementary and can be seamlessly integrated into algorithms that draw from the best of both worlds. E
