fashion. Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. 1 Introduction The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all language
raining data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. 1 Introduction The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre 
f the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. 1 Introduction The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds 
rkish. 1 Introduction The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store
ple languages using a single dependency parser that has the capacity to learn from treebank data. Our methodology for performing this task is based on four essential components: ? A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). ? History-based feature models for predicting the next parser action (Black et al, 1992). ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and adding arcs using four elementary actions 
. ? Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? SHIFT: Push next onto the stack. ? REDUCE: Pop the stack. ? RIGHT-ARC(r): Add an arc labeled r from top to next; push next onto the stack. ? LEFT-ARC(r): Add an arc labeled r from next to top; pop the stack. Although the parser only derives projective graphs, the fact tha
parser actions (Kudo and Matsumoto, 2002). ? Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). All experiments have been performed using MaltParser (Nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/MaltParser.html 2 Parsing Methodology 2.1 Parsing Algorithm The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre (2003) and extended to labeled dependency parsing by Nivre et al (2004). The algorithm builds a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? SHIFT: Push next onto the stack. ? REDUCE: Pop the stack. ? RIGHT-ARC(r): Add an arc labeled r from top to next; push next onto the stack. ? LEFT-ARC(r): Add an arc labeled r from next to top; pop the stack. Although the parser only derives projective graphs, the fact that graphs are labeled allows non-projective dependencies to be cap
 a starting point for languagespecific feature selection. The features of this model are shown in Table 1, where rows denote tokens in a parser configuration (defined relative to the stack, the remaining input, and the partially built dependency graph), and where columns correspond to data fields. The base model contains twenty features, but note that the fields LEMMA, CPOS and FEATS are not available for all languages. 2.3 Support Vector Machines We use support vector machines3 to predict the next parser action from a feature vector representing the history. More specifically, we use LIBSVM (Chang and Lin, 2001) with a quadratic kernel K(xi, xj) = (?xTi xj +r)2 and the built-in one-versus-all strategy for multi-class classification. Symbolic features are 2The fields PHEAD and PDEPREL have not been used at all, since we rely on pseudo-projective parsing for the treatment of non-projective structures. 3We also ran preliminary experiments with memory-based learning but found that this gave consistently lower accuracy. converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.4 For some languages, we divide the training
EL have not been used at all, since we rely on pseudo-projective parsing for the treatment of non-projective structures. 3We also ran preliminary experiments with memory-based learning but found that this gave consistently lower accuracy. converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.4 For some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t. 2.4 Pseudo-Projective Parsing Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser. We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r?h, where r is the original lab
istently lower accuracy. converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.4 For some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t. 2.4 Pseudo-Projective Parsing Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser. We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r?h, where r is the original label and h is the label of the original head in the nonprojective dependency graph. Non-projective dependencies can be recovered by applying an inverse transformation to the output of the parser, using a left-to-r
re models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portug
ing algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002),
nd on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overa
ge. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to ve
 and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to very high attachment scores, but much
 and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95?90?85, for arcs of length 1, 2 and 3?6. Typical examples are Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech (Bo?hmova
e Bulgarian (Simov et al, 2005; Simov and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech (Bo?hmova? et al, 2003), Dutch (van der Beek et al, 2002) and Slovene (Dz?eroski et al, 2006), where root precision drops more drastically to about 69%, 71% and 41%, respectively, and root recall is also affected negatively. On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Mart?? Anton??n, 2002), although this cannot be e
v and Osenova, 2003), Chinese (Chen et al, 2003), Danish (Kromann, 2003), and Swedish (Nilsson et al, 2005). Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al, 2002) and Portuguese (Afonso et al, 2002), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech (Bo?hmova? et al, 2003), Dutch (van der Beek et al, 2002) and Slovene (Dz?eroski et al, 2006), where root precision drops more drastically to about 69%, 71% and 41%, respectively, and root recall is also affected negatively. On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Mart?? Anton??n, 2002), although this cannot be explained by a high proportion of non
 is found for Spanish (Civit Torruella and Mart?? Anton??n, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features. The results for Arabic (Hajic? et al, 2004; Smrz? et al, 2002) are characterized by low root accuracy 223 as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast, Turkish (Oflazer et al, 2003; Atalay et al, 2003) exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). It is noteworthy that Arabic and Turkish, being ?typological outliers?, show patterns that are different both from each other and from most of the other languages. 4.1 Swedish A more fine-grained analysis of the Swedish results reveals a high accuracy for function words, which is compatible with previous studies (Nivre, 2006). Thus, the labeled F-score is 100% for infinitive markers (IM) and subordinating conjunctions (UK), and above 95% for determiners (DT). 
 (Civit Torruella and Mart?? Anton??n, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features. The results for Arabic (Hajic? et al, 2004; Smrz? et al, 2002) are characterized by low root accuracy 223 as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast, Turkish (Oflazer et al, 2003; Atalay et al, 2003) exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). It is noteworthy that Arabic and Turkish, being ?typological outliers?, show patterns that are different both from each other and from most of the other languages. 4.1 Swedish A more fine-grained analysis of the Swedish results reveals a high accuracy for function words, which is compatible with previous studies (Nivre, 2006). Thus, the labeled F-score is 100% for infinitive markers (IM) and subordinating conjunctions (UK), and above 95% for determiners (DT). In addition, subjects
 a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast, Turkish (Oflazer et al, 2003; Atalay et al, 2003) exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). It is noteworthy that Arabic and Turkish, being ?typological outliers?, show patterns that are different both from each other and from most of the other languages. 4.1 Swedish A more fine-grained analysis of the Swedish results reveals a high accuracy for function words, which is compatible with previous studies (Nivre, 2006). Thus, the labeled F-score is 100% for infinitive markers (IM) and subordinating conjunctions (UK), and above 95% for determiners (DT). In addition, subjects (SS) have a score above 90%. In all these cases, the dependent has a configurationally defined (but not fixed) position with respect to its head. Arguments of the verb, such as objects (DO, IO) and predicative complements (SP), have a slightly lower accuracy (about 85% labeled F-score), which is due to the fact that they ?compete? in the same structural positions, whereas adverbials (labels that end in A) have even lower scores (often be
n problematic in parsing, has high accuracy. The Swedish treebank annotation treats the second conjunct as a dependent of the first conjunct and as the head of the coordinator, which seems to facilitate parsing.6 The attachment of the second conjunct to the first (CC) has a labeled F-score above 80%, while the attachment of the coordinator to the second conjunct (++) has a score well above 90%. 4.2 Turkish In Turkish, very essential syntactic information is contained in the rich morphological structure, where 6The analysis is reminiscent of the treatment of coordination in the Collins parser (Collins, 1999). concatenated suffixes carry information that in other languages may be expressed by separate words. The Turkish treebank therefore divides word forms into smaller units, called inflectional groups (IGs), and the task of the parser is to construct dependencies between IGs, not (primarily) between word forms (Eryig?it and Oflazer, 2006). It is then important to remember that an unlabeled attachment score of 75.8% corresponds to a word-to-word score of 82.7%, which puts Turkish on a par with languages like Czech, Dutch and Spanish. Moreover, when we break down the results according to whether t
