{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Preslav Nakov and Hwee Tou Ng. 2009. Improved statistical machine translation for resource-poor languages using related resource-rich languages. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201909, pages 1358\u20131367."},"#text":"\n","pages":{"#tail":"\n","#text":"1358--1367"},"marker":{"#tail":"\n","#text":"Nakov, Ng, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ource-rich language, with whom they overlap in vocabulary and share cognates, which offers opportunities for bi-text reuse. Example pairs of such resource rich\u2013poor languages include Spanish\u2013Catalan, Finnish\u2013Estonian, Swedish\u2013Norwegian, Russian\u2013Ukrainian, Irish\u2013 Gaelic Scottish, Standard German\u2013Swiss German, Modern Standard Arabic\u2013Dialectical Arabic (e.g., Gulf, Egyptian), Turkish\u2013Azerbaijani, etc. Previous work has already demonstrated the benefits of using a bi-text for a related resource-rich language to X (e.g., X=English) to improve machine translation from a resource-poor language to X (Nakov and Ng, 2009; Nakov and Ng, 2012). Here we take a different, orthogonal approach: we adapt the resource-rich language to get closer to the resource-poor one. We assume a small bi-text for the resource-poor language, which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the two languages. Assuming translation into the same target language X, we adapt (the source side of) a large training bi-text for a related resource-rich language and X. Training on the adapted large bi-text yields very significant improvements in translation quality compared to bot","@endWordPosition":"330","@position":"2395","annotationId":"T1","@startWordPosition":"327","@citStr":"Nakov and Ng, 2009"},{"#tail":"\n","#text":"slation when training on the adapted \u201cEP\u201d\u2013En bi-text compared to using the unadapted BP\u2013En (38.55 vs. 38.29), or when an EP\u2013English bi-text is used in addition to the adapted/unadapted one (41.07 vs. 40.91 BLEU). Unlike this work, which heavily relied on language-specific rules, our approach is statistical, and largely language-independent; moreover, our improvements are much more sizable. A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages. For example, our previous work (Nakov and Ng, 2009; Nakov and Ng, 2012) experimented with various techniques for combining a small bi-text for a resource-poor language (Indonesian or Spanish, pretending that Spanish is resource-poor) with a much larger bi-text for a related resource-rich language (Malay or Portuguese); the target language of all bi-texts was English. However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese\u2013Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay\u2013Indonesian, which us","@endWordPosition":"1031","@position":"6937","annotationId":"T2","@startWordPosition":"1028","@citStr":"Nakov and Ng, 2009"},{"#tail":"\n","#text":"an\u2013English one, and now it has been further expanded n times in order to become an \u201cIndonesian\u201d\u2013English bi-text, which means that it will dominate the concatenation due to its size. In order to counter-balance this, we repeat the smaller Indonesian\u2013English bi-text enough times so that we can make the number of sentences it contains roughly the same as for the \u201cIndonesian\u201d\u2013English bi-text; then we concatenate the two bi-texts and we train an SMT system on the resulting bi-text. Sophisticated phrase table combination. Finally, we experiment with a method for combining phrase tables proposed in (Nakov and Ng, 2009; Nakov and Ng, 2012). The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian\u2013English bi-text. The second table is built from the simple concatenation. The two tables are then merged as follows: all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one. Each phrase pair retains its original scores, which are further augmented with 1\u20133 additional feature scores indicating i","@endWordPosition":"3484","@position":"22827","annotationId":"T3","@startWordPosition":"3481","@citStr":"Nakov and Ng, 2009"},{"#tail":"\n","#text":"d 0 otherwise. We experiment using all three, the first two, or the first feature only; we also try setting the features to 0.5 instead of 0. This makes the following six combinations (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use the one that achieves the highest BLEU score on the development set. Other possibilities for combining the phrase tables include using alternative decoding paths (Birch et al., 2007), simple linear interpolation, and direct phrase table merging with extra features (CallisonBurch et al., 2006); they were previously found inferior to the last two approaches above (Nakov and Ng, 2009; Nakov and Ng, 2012). 5 Experiments We run two kinds of experiments: (a) isolated, where we train on the synthetic \u201cIndonesian\u201d\u2013 English bi-text only, and (b) combined, where we combine it with the Indonesian\u2013English bi-text. 5.1 Datasets In our experiments, we use the following datasets, normally required for Indonesian\u2013English SMT: \u2022 Indonesian\u2013English train bi-text (IN2EN): 28,383 sentence pairs; 915,192 English tokens; 796,787 Indonesian tokens; \u2022 Indon.\u2013English dev bi-text (IN2EN-dev): 2,000 sentence pairs; 36,584 English tokens; 35,708 Indonesian tokens; \u2022 Indon.\u2013English test bi-text (I","@endWordPosition":"3705","@position":"24148","annotationId":"T4","@startWordPosition":"3702","@citStr":"Nakov and Ng, 2009"}]},"title":{"#tail":"\n","#text":"Improved statistical machine translation for resource-poor languages using related resource-rich languages."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201909,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Preslav Nakov"},{"#tail":"\n","#text":"Hwee Tou Ng"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Masao Utiyama and Hitoshi Isahara. 2007. A comparison of pivot methods for phrase-based statistical machine translation. In Proceedings of the Human Language Technology Conference of NAACL, HLTNAACL \u201907, pages 484\u2013491."},"#text":"\n","pages":{"#tail":"\n","#text":"484--491"},"marker":{"#tail":"\n","#text":"Utiyama, Isahara, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese\u2013Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay\u2013Indonesian, which use unified spelling. Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches. Another alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). Unfortunately, using the resource-rich language as a pivot (poor\u2014*rich\u2014*X) would require an additional parallel poor\u2013rich bi-text, which we do not have. Pivoting over the target X (rich\u2014*X\u2014*poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich\u2013X bi-text). 287 3 Malay and Indonesian Malay and Indones","@endWordPosition":"1180","@position":"7891","annotationId":"T5","@startWordPosition":"1177","@citStr":"Utiyama and Isahara, 2007"}},"title":{"#tail":"\n","#text":"A comparison of pivot methods for phrase-based statistical machine translation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Human Language Technology Conference of NAACL, HLTNAACL \u201907,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Masao Utiyama"},{"#tail":"\n","#text":"Hitoshi Isahara"}]}}]}}}}
