{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"E. Black, F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, and S. Roukos. 1993. Towards history-based grammars: Using richer models for probabilistic parsing. In Proc. 31st Meeting of Association for Computational Linguistics, pages 31-37, Columbus, Ohio."},"#text":"\n","pages":{"#tail":"\n","#text":"31--37"},"marker":{"#tail":"\n","#text":"Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1993"},"location":{"#tail":"\n","#text":"Columbus, Ohio."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ative model of left-corner parsing, and these parameters are used to search for the most probable parse. The parser's performance (88.8% Fmeasure) is within 1% of the best current parsers for this task, despite using a small vocabulary size (512 inputs). Crucial to this success is the neural network architecture's ability to induce a finite representation of the unbounded parse history, and the biasing of this induction in a linguistically appropriate way. 1 Introduction Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001) are based on a history-based probability model (Black et al., 1993), where the probability of each decision in a parse is conditioned on the previous decisions in the parse. A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001). In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history. We perform this induction using an artifici","@endWordPosition":"140","@position":"945","annotationId":"T1","@startWordPosition":"137","@citStr":"Black et al., 1993"},{"#tail":"\n","#text":"d the input sentence. At each step, the process chooses a characteristic of the tree or predicts a word in the sentence. This sequence of decisions is the derivation of the tree, which we will denote d1,..., dm. Because there is a one-to-one mapping from phrase 131 structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models (Black et al., 1993), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d,_1, which is called the derivation history at step i. This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions. P (di ,..., = di_i) The probabilities P(dild1,..., d1)' are the parameters of the parser's probability model. To define the parameters di_i) we need to choose the ordering of the decisions in a derivation, such as a top-down or shift-red","@endWordPosition":"525","@position":"3384","annotationId":"T2","@startWordPosition":"522","@citStr":"Black et al., 1993"}]},"title":{"#tail":"\n","#text":"Towards history-based grammars: Using richer models for probabilistic parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. 31st Meeting of Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E Black"},{"#tail":"\n","#text":"F Jelinek"},{"#tail":"\n","#text":"J Lafferty"},{"#tail":"\n","#text":"D Magerman"},{"#tail":"\n","#text":"R Mercer"},{"#tail":"\n","#text":"S Roukos"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Rens Bod. 2001. What is the minimal set of fragments that achieves maximal parse accuracy? In Proc. 34th Meeting of Association for Computational Linguistics, pages 66-73."},"#text":"\n","pages":{"#tail":"\n","#text":"66--73"},"marker":{"#tail":"\n","#text":"Bod, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on parsing with a non-lexicalized model. The Tags model also does much better than the only other broad coverage neural network parser (Costa et al., 2001). The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001). The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head. 7 Discussion and Furth","@endWordPosition":"4137","@position":"24575","annotationId":"T3","@startWordPosition":"4136","@citStr":"Bod, 2001"}},"title":{"#tail":"\n","#text":"What is the minimal set of fragments that achieves maximal parse accuracy?"},"booktitle":{"#tail":"\n","#text":"In Proc. 34th Meeting of Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Rens Bod"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proc. 1st Meeting of North American Chapter of Association for Computational Linguistics, pages 132-139, Seattle, Washington."},"#text":"\n","pages":{"#tail":"\n","#text":"132--139"},"marker":{"#tail":"\n","#text":"Charniak, 2000"},"location":{"#tail":"\n","#text":"Seattle, Washington."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"3 Inducing Features of the Derivation History The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e. structures of th","@endWordPosition":"1020","@position":"6140","annotationId":"T4","@startWordPosition":"1019","@citStr":"Charniak, 2000"},{"#tail":"\n","#text":"res (Collins, 2000). In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers (Charniak, 2000). The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history featur","@endWordPosition":"1398","@position":"8319","annotationId":"T5","@startWordPosition":"1397","@citStr":"Charniak, 2000"},{"#tail":"\n","#text":"0 88.8 89.6 88.3 89.2 Table 1: Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on parsing with a non-lexicalized model. The Tags model also does much better than the only other broad coverage neural network parser (Costa et al., 2001). The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001). The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical h","@endWordPosition":"4133","@position":"24548","annotationId":"T6","@startWordPosition":"4132","@citStr":"Charniak, 2000"}]},"title":{"#tail":"\n","#text":"A maximum-entropyinspired parser."},"booktitle":{"#tail":"\n","#text":"In Proc. 1st Meeting of North American Chapter of Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Eugene Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proc. 1st Meeting of North American Chapter of Association for Computational Linguistics, pages 132-139, Seattle, Washington."},"#text":"\n","pages":{"#tail":"\n","#text":"132--139"},"marker":{"#tail":"\n","#text":"Charniak, 2000"},"location":{"#tail":"\n","#text":"Seattle, Washington."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"3 Inducing Features of the Derivation History The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e. structures of th","@endWordPosition":"1020","@position":"6140","annotationId":"T7","@startWordPosition":"1019","@citStr":"Charniak, 2000"},{"#tail":"\n","#text":"res (Collins, 2000). In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers (Charniak, 2000). The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history featur","@endWordPosition":"1398","@position":"8319","annotationId":"T8","@startWordPosition":"1397","@citStr":"Charniak, 2000"},{"#tail":"\n","#text":"0 88.8 89.6 88.3 89.2 Table 1: Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on parsing with a non-lexicalized model. The Tags model also does much better than the only other broad coverage neural network parser (Costa et al., 2001). The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001). The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical h","@endWordPosition":"4133","@position":"24548","annotationId":"T9","@startWordPosition":"4132","@citStr":"Charniak, 2000"}]},"title":{"#tail":"\n","#text":"A maximum-entropyinspired parser."},"booktitle":{"#tail":"\n","#text":"In Proc. 1st Meeting of North American Chapter of Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Eugene Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proc. 1st Meeting of North American Chapter of Association for Computational Linguistics, pages 132-139, Seattle, Washington."},"#text":"\n","pages":{"#tail":"\n","#text":"132--139"},"marker":{"#tail":"\n","#text":"Charniak, 2000"},"location":{"#tail":"\n","#text":"Seattle, Washington."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"3 Inducing Features of the Derivation History The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e. structures of th","@endWordPosition":"1020","@position":"6140","annotationId":"T10","@startWordPosition":"1019","@citStr":"Charniak, 2000"},{"#tail":"\n","#text":"res (Collins, 2000). In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers (Charniak, 2000). The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history featur","@endWordPosition":"1398","@position":"8319","annotationId":"T11","@startWordPosition":"1397","@citStr":"Charniak, 2000"},{"#tail":"\n","#text":"0 88.8 89.6 88.3 89.2 Table 1: Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on parsing with a non-lexicalized model. The Tags model also does much better than the only other broad coverage neural network parser (Costa et al., 2001). The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001). The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical h","@endWordPosition":"4133","@position":"24548","annotationId":"T12","@startWordPosition":"4132","@citStr":"Charniak, 2000"}]},"title":{"#tail":"\n","#text":"A maximum-entropyinspired parser."},"booktitle":{"#tail":"\n","#text":"In Proc. 1st Meeting of North American Chapter of Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Eugene Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures and the voted perceptron. In Proc. 35th Meeting of Association for Computational Linguistics, pages 263-270."},"#text":"\n","pages":{"#tail":"\n","#text":"263--270"},"marker":{"#tail":"\n","#text":"Collins, Duffy, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"he tree (becoming [X ... [mod Y \u2022 requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link. We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]). These transforms are undone before any evaluation is performed on the output trees. We do not believe these transforms have a major impact on performance, but we have not currently run tests without them. o ROOT 3 NP VP NNP/Mary 4 VBZ/runs 6 RB/often 132 feature sets, but then efficiency becomes a problem. Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins' previous work on re-ranking using a finite set of features (Collins, 2000). In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000). The outputs of this network are probab","@endWordPosition":"1249","@position":"7427","annotationId":"T13","@startWordPosition":"1246","@citStr":"Collins and Duffy (2002)"}},"title":{"#tail":"\n","#text":"New ranking algorithms for parsing and tagging: Kernels over discrete structures and the voted perceptron."},"booktitle":{"#tail":"\n","#text":"In Proc. 35th Meeting of Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michael Collins"},{"#tail":"\n","#text":"Nigel Duffy"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"University of Pennsylvania,"},"rawString":{"#tail":"\n","#text":"Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1999"},"location":{"#tail":"\n","#text":"Philadelphia, PA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"Penn Treebank. The neural network is used to estimate the parameters of a generative model of left-corner parsing, and these parameters are used to search for the most probable parse. The parser's performance (88.8% Fmeasure) is within 1% of the best current parsers for this task, despite using a small vocabulary size (512 inputs). Crucial to this success is the neural network architecture's ability to induce a finite representation of the unbounded parse history, and the biasing of this induction in a linguistically appropriate way. 1 Introduction Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001) are based on a history-based probability model (Black et al., 1993), where the probability of each decision in a parse is conditioned on the previous decisions in the parse. A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001). In the work presented here, we automatically induce a finite set of features ","@endWordPosition":"127","@position":"860","annotationId":"T14","@startWordPosition":"126","@citStr":"Collins, 1999"},{"#tail":"\n","#text":"). 8, and 9).2 3 Inducing Features of the Derivation History The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e.","@endWordPosition":"1018","@position":"6123","annotationId":"T15","@startWordPosition":"1017","@citStr":"Collins, 1999"},{"#tail":"\n","#text":"tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus. We used a publicly available tagger (Ratnaparkhi, 1996) to tag the words and then used these in the input to the system. 6We found that 80 hidden units produced better performance than 60 or 100. Momentum was applied throughout training. Weight decay regularization was applied at the beginning of training but reduced to zero by the end of training. 7A11 our results are computed with the evalb program following the now-standard criteria in (Collins, 1999). Length<40 All LR LP LR LP Costa-et-a101 NA NA 57.8 64.9 Manning&Carpenter97 77.6 79.9 NA NA Charniak97 (PCFG) 71.2 75.3 70.1 74.3 SSN-Tags 83.9 84.9 83.3 84.3 Ratnaparkhi99 NA NA 86.3 87.5 Collins99 88.5 88.7 88.1 88.3 Charniak00 90.1 90.1 89.6 89.5 Collins00 90.1 90.4 89.6 89.9 Bod01 90.8 90.6 89.7 89.7 SSN-Freq>200 88.8 89.6 88.3 89.2 Table 1: Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on ","@endWordPosition":"3987","@position":"23615","annotationId":"T16","@startWordPosition":"3986","@citStr":"Collins, 1999"}]},"title":{"#tail":"\n","#text":"Head-Driven Statistical Models for Natural Language Parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Collins"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"University of Pennsylvania,"},"rawString":{"#tail":"\n","#text":"Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1999"},"location":{"#tail":"\n","#text":"Philadelphia, PA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"Penn Treebank. The neural network is used to estimate the parameters of a generative model of left-corner parsing, and these parameters are used to search for the most probable parse. The parser's performance (88.8% Fmeasure) is within 1% of the best current parsers for this task, despite using a small vocabulary size (512 inputs). Crucial to this success is the neural network architecture's ability to induce a finite representation of the unbounded parse history, and the biasing of this induction in a linguistically appropriate way. 1 Introduction Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001) are based on a history-based probability model (Black et al., 1993), where the probability of each decision in a parse is conditioned on the previous decisions in the parse. A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001). In the work presented here, we automatically induce a finite set of features ","@endWordPosition":"127","@position":"860","annotationId":"T17","@startWordPosition":"126","@citStr":"Collins, 1999"},{"#tail":"\n","#text":"). 8, and 9).2 3 Inducing Features of the Derivation History The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e.","@endWordPosition":"1018","@position":"6123","annotationId":"T18","@startWordPosition":"1017","@citStr":"Collins, 1999"},{"#tail":"\n","#text":"tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus. We used a publicly available tagger (Ratnaparkhi, 1996) to tag the words and then used these in the input to the system. 6We found that 80 hidden units produced better performance than 60 or 100. Momentum was applied throughout training. Weight decay regularization was applied at the beginning of training but reduced to zero by the end of training. 7A11 our results are computed with the evalb program following the now-standard criteria in (Collins, 1999). Length<40 All LR LP LR LP Costa-et-a101 NA NA 57.8 64.9 Manning&Carpenter97 77.6 79.9 NA NA Charniak97 (PCFG) 71.2 75.3 70.1 74.3 SSN-Tags 83.9 84.9 83.3 84.3 Ratnaparkhi99 NA NA 86.3 87.5 Collins99 88.5 88.7 88.1 88.3 Charniak00 90.1 90.1 89.6 89.5 Collins00 90.1 90.4 89.6 89.9 Bod01 90.8 90.6 89.7 89.7 SSN-Freq>200 88.8 89.6 88.3 89.2 Table 1: Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on ","@endWordPosition":"3987","@position":"23615","annotationId":"T19","@startWordPosition":"3986","@citStr":"Collins, 1999"}]},"title":{"#tail":"\n","#text":"Head-Driven Statistical Models for Natural Language Parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Collins"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"University of Pennsylvania,"},"rawString":{"#tail":"\n","#text":"Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1999"},"location":{"#tail":"\n","#text":"Philadelphia, PA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"Penn Treebank. The neural network is used to estimate the parameters of a generative model of left-corner parsing, and these parameters are used to search for the most probable parse. The parser's performance (88.8% Fmeasure) is within 1% of the best current parsers for this task, despite using a small vocabulary size (512 inputs). Crucial to this success is the neural network architecture's ability to induce a finite representation of the unbounded parse history, and the biasing of this induction in a linguistically appropriate way. 1 Introduction Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001) are based on a history-based probability model (Black et al., 1993), where the probability of each decision in a parse is conditioned on the previous decisions in the parse. A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001). In the work presented here, we automatically induce a finite set of features ","@endWordPosition":"127","@position":"860","annotationId":"T20","@startWordPosition":"126","@citStr":"Collins, 1999"},{"#tail":"\n","#text":"). 8, and 9).2 3 Inducing Features of the Derivation History The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e.","@endWordPosition":"1018","@position":"6123","annotationId":"T21","@startWordPosition":"1017","@citStr":"Collins, 1999"},{"#tail":"\n","#text":"tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus. We used a publicly available tagger (Ratnaparkhi, 1996) to tag the words and then used these in the input to the system. 6We found that 80 hidden units produced better performance than 60 or 100. Momentum was applied throughout training. Weight decay regularization was applied at the beginning of training but reduced to zero by the end of training. 7A11 our results are computed with the evalb program following the now-standard criteria in (Collins, 1999). Length<40 All LR LP LR LP Costa-et-a101 NA NA 57.8 64.9 Manning&Carpenter97 77.6 79.9 NA NA Charniak97 (PCFG) 71.2 75.3 70.1 74.3 SSN-Tags 83.9 84.9 83.3 84.3 Ratnaparkhi99 NA NA 86.3 87.5 Collins99 88.5 88.7 88.1 88.3 Charniak00 90.1 90.1 89.6 89.5 Collins00 90.1 90.4 89.6 89.9 Bod01 90.8 90.6 89.7 89.7 SSN-Freq>200 88.8 89.6 88.3 89.2 Table 1: Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on ","@endWordPosition":"3987","@position":"23615","annotationId":"T22","@startWordPosition":"3986","@citStr":"Collins, 1999"}]},"title":{"#tail":"\n","#text":"Head-Driven Statistical Models for Natural Language Parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. 17th Int. Conf on Machine Learning, pages 175-182, Stanford, CA."},"#text":"\n","pages":{"#tail":"\n","#text":"175--182"},"marker":{"#tail":"\n","#text":"Collins, 2000"},"location":{"#tail":"\n","#text":"Stanford, CA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ms are undone before any evaluation is performed on the output trees. We do not believe these transforms have a major impact on performance, but we have not currently run tests without them. o ROOT 3 NP VP NNP/Mary 4 VBZ/runs 6 RB/often 132 feature sets, but then efficiency becomes a problem. Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins' previous work on re-ranking using a finite set of features (Collins, 2000). In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers (Charniak, 2000). Th","@endWordPosition":"1301","@position":"7723","annotationId":"T23","@startWordPosition":"1300","@citStr":"Collins, 2000"},{"#tail":"\n","#text":" 89.2 Table 1: Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on parsing with a non-lexicalized model. The Tags model also does much better than the only other broad coverage neural network parser (Costa et al., 2001). The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001). The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head. 7 Discussi","@endWordPosition":"4135","@position":"24563","annotationId":"T24","@startWordPosition":"4134","@citStr":"Collins, 2000"}]},"title":{"#tail":"\n","#text":"Discriminative reranking for natural language parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. 17th Int. Conf on Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. 17th Int. Conf on Machine Learning, pages 175-182, Stanford, CA."},"#text":"\n","pages":{"#tail":"\n","#text":"175--182"},"marker":{"#tail":"\n","#text":"Collins, 2000"},"location":{"#tail":"\n","#text":"Stanford, CA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ms are undone before any evaluation is performed on the output trees. We do not believe these transforms have a major impact on performance, but we have not currently run tests without them. o ROOT 3 NP VP NNP/Mary 4 VBZ/runs 6 RB/often 132 feature sets, but then efficiency becomes a problem. Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins' previous work on re-ranking using a finite set of features (Collins, 2000). In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers (Charniak, 2000). Th","@endWordPosition":"1301","@position":"7723","annotationId":"T25","@startWordPosition":"1300","@citStr":"Collins, 2000"},{"#tail":"\n","#text":" 89.2 Table 1: Percentage labeled constituent recall and precision on the testing set. tical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997). The Tags model achieves performance which is better than any previously published results on parsing with a non-lexicalized model. The Tags model also does much better than the only other broad coverage neural network parser (Costa et al., 2001). The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001). The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head. 7 Discussi","@endWordPosition":"4135","@position":"24563","annotationId":"T26","@startWordPosition":"4134","@citStr":"Collins, 2000"}]},"title":{"#tail":"\n","#text":"Discriminative reranking for natural language parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. 17th Int. Conf on Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613-632."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","pages":{"#tail":"\n","#text":"24--4"},"marker":{"#tail":"\n","#text":"Johnson, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" determines the inductive bias discussed in the previous section. The principle we apply when designing D(top,) and f (di,..., di_1) is that we want the inductive bias to reflect structural locality. For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_1, if any). For right-branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial (Johnson, 1998), as has conditioning on the left-corner child (Roark and Johnson, 1999). Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i \u2014 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases. As mentioned above, D(top) also includes top, itself, which means that the input","@endWordPosition":"2599","@position":"15560","annotationId":"T27","@startWordPosition":"2598","@citStr":"Johnson, 1998"}},"title":{"#tail":"\n","#text":"PCFG models of linguistic tree representations."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Mark Johnson"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Adwait Ratnaparkhi 1996. A maximum entropy model for part-of-speech tagging. In Proc. Conf on Empirical Methods in Natural Language Processing, pages 133-142, Univ. of Pennsylvania, PA."},"#text":"\n","pages":{"#tail":"\n","#text":"133--142"},"marker":{"#tail":"\n","#text":"Ratnaparkhi, 1996"},"location":{"#tail":"\n","#text":"Univ. of Pennsylvania, PA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"he best non-lexicalized and the best lexicalized models on the testing set.6 Standard measures of performance are shown in table 1.7 The top panel of table 1 lists the results for the non-lexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser (Costa et al., 2001), an earlier statis5In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus. We used a publicly available tagger (Ratnaparkhi, 1996) to tag the words and then used these in the input to the system. 6We found that 80 hidden units produced better performance than 60 or 100. Momentum was applied throughout training. Weight decay regularization was applied at the beginning of training but reduced to zero by the end of training. 7A11 our results are computed with the evalb program following the now-standard criteria in (Collins, 1999). Length<40 All LR LP LR LP Costa-et-a101 NA NA 57.8 64.9 Manning&Carpenter97 77.6 79.9 NA NA Charniak97 (PCFG) 71.2 75.3 70.1 74.3 SSN-Tags 83.9 84.9 83.3 84.3 Ratnaparkhi99 NA NA 86.3 87.5 Collin","@endWordPosition":"3917","@position":"23212","annotationId":"T28","@startWordPosition":"3916","@citStr":"Ratnaparkhi, 1996"}},"title":{"#tail":"\n","#text":"A maximum entropy model for part-of-speech tagging."},"booktitle":{"#tail":"\n","#text":"In Proc. Conf on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Adwait Ratnaparkhi"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Brian Roark and Mark Johnson. 1999. Efficient probabilistic top-down and left-corner parsing. In Proc. 37th Meeting of Association for Computational Linguistics, pages 421-428."},"#text":"\n","pages":{"#tail":"\n","#text":"421--428"},"marker":{"#tail":"\n","#text":"Roark, Johnson, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"on. The principle we apply when designing D(top,) and f (di,..., di_1) is that we want the inductive bias to reflect structural locality. For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_1, if any). For right-branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial (Johnson, 1998), as has conditioning on the left-corner child (Roark and Johnson, 1999). Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i \u2014 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases. As mentioned above, D(top) also includes top, itself, which means that the inputs to g always include the history features for the most recent derivatio","@endWordPosition":"2610","@position":"15632","annotationId":"T29","@startWordPosition":"2607","@citStr":"Roark and Johnson, 1999"}},"title":{"#tail":"\n","#text":"Efficient probabilistic top-down and left-corner parsing."},"booktitle":{"#tail":"\n","#text":"In Proc. 37th Meeting of Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Brian Roark"},{"#tail":"\n","#text":"Mark Johnson"}]}}]}}}}
