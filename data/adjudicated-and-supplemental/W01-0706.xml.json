{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","figure":{"#tail":"\n","@confidence":"0.958851384615385","#text":"\nPrecision \nNumber of correct proposed patterns\nNumber of proposed patterns\nRecall \nNumber of correct proposed patterns\nNumber of correct patterns\nfi\nflffi\nff!\nRecall Precision\n\nfl\nPrecision ffi Recall\n"},"address":{"#tail":"\n","@confidence":"0.916074","#text":"\nUrbana, IL 61801\n"},"author":{"#tail":"\n","@confidence":"0.980183","#text":"\nXin Li Dan Roth\n"},"equation":[{"#tail":"\n","@confidence":"0.8621105","#text":"\n( (S (NP (NP Pierre Vinken) , (ADJP\n(NP 61 years) old) ,) (VP will (VP join\n(NP the board) (PP as (NP a nonexecu-\ntive director)) (NP Nov. 29))) .))\n"},{"#tail":"\n","@confidence":"0.961219","#text":"\n\n\n\n"},{"#tail":"\n","@confidence":"0.991431888888889","#text":"\nHuh/UH ,/, well/UH ,/, um/UH\n,/, you/PRP know/VBP ,/, I/PRP\nguess/VBP it/PRP ?s/BES pretty/RB\ndeep/JJ feelings/NNS ,/, uh/UH ,/,\nI/PRP just/RB ,/, uh/UH ,/, went/VBD\nback/RB and/CC rented/VBD ,/, uh/UH\n,/, the/DT movie/NN ,/, what/WP is/VBZ\nit/PRP ,/, GOOD/JJ MORNING/NN\nVIET/NNP NAM/NNP ./.\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.993134","#text":"\n2.1 Parsers\n"},{"#tail":"\n","@confidence":"0.969376","#text":"\n2.2 Data\n"},{"#tail":"\n","@confidence":"0.996722","#text":"\n2.3 Representation\n"},{"#tail":"\n","@confidence":"0.978151","#text":"\n2.4 Performance Measure\n"},{"#tail":"\n","@confidence":"0.992146","#text":"\n3.1 Performance\n"},{"#tail":"\n","@confidence":"0.999692","#text":"\n3.2 Robustness\n"},{"#tail":"\n","@confidence":"0.998063","#text":"\n3.3 Discussion\n"}],"@confidence":"0.000172","reference":{"#tail":"\n","@confidence":"0.993852079545454","#text":"\nS. P. Abney. 1991. Parsing by chunks. In S. P. Ab-\nney R. C. Berwick and C. Tenny, editors, Principle-\nbased parsing: Computation and Psycholinguistics,\npages 257?278. Kluwer, Dordrecht.\nD. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson.\n1993. FASTUS: A finite-state processor for infor-\nmation extraction from real-world text. In Proc.\nInternational Joint Conference on Artificial Intelli-\ngence.\nS. Argamon, I. Dagan, and Y. Krymolowski. 1998.\nA memory-based approach to learning shallow nat-\nural language patterns. In COLING-ACL 98, The\n17th International Conference on Computational\nLinguistics.\nS. Buchholz, J. Veenstra, and W. Daelemans. 1999.\nCascaded grammatical relation assignment. In\nEMNLP-VLC?99, the Joint SIGDAT Conference on\nEmpirical Methods in Natural Language Process-\ning and Very Large Corpora, June.\nC. Cardie and D. Pierce. 1998. Error-driven pruning\nof Treebanks grammars for base noun phrase iden-\ntification. In Proceedings of ACL-98, pages 218?\n224.\nA. Carleson, C. Cumby, J. Rosen, and D. Roth. 1999.\nThe SNoW learning architecture. Technical Re-\nport UIUCDCS-R-99-2101, UIUC Computer Sci-\nence Department, May.\nE. Charniak. 1997a. Statistical parsing with a context-\nfree grammar and word statistics. In Proc. National\nConference on Artificial Intelligence.\nE. Charniak. 1997b. Statistical techniques for natural\nlanguage parsing. The AI Magazine.\nKenneth W. Church. 1988. A stochastic parts program\nand noun phrase parser for unrestricted text. In\nProc. of ACL Conference on Applied Natural Lan-\nguage Processing.\nM. Collins. 1996. A new statistical parser based on\nbigram lexical dependencies. In Proceedings of the\n34th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 184?191.\nM. Collins. 1997. Three generative, lexicalised mod-\nels for statistical parsing. In Proceedings of the\n35th Annual Meeting of the Association for Com-\nputational Linguistics.\nJ. P. Gee and F. Grosjean. 1983. Performance struc-\ntures:a psycholinguistic and linguistic appraisal.\nCognitive Psychology, 15:411?458.\nG. Greffenstette. 1993. Evaluation techniques for au-\ntomatic semantic extraction: comparing semantic\nand window based approaches. In ACL?93 work-\nshop on the Acquisition of Lexical Knowledge from\nText.\nR. Grishman. 1995. The NYU system for MUC-6 or\nwhere?s syntax? In B. Sundheim, editor, Proceed-\nings of the Sixth Message Understanding Confer-\nence. Morgan Kaufmann Publishers.\nZ. S. Harris. 1957. Co-occurrence and transformation\nin linguistic structure. Language, 33(3):283?340.\nE. F. Tjong Kim Sang and S. Buchholz. 2000. In-\ntroduction to the CoNLL-2000 shared task: Chunk-\ning. In Proceedings of CoNLL-2000 and LLL-2000,\npages 127?132.\nM. P. Marcus, B. Santorini, and M. Marcinkiewicz.\n1993. Building a large annotated corpus of En-\nglish: The Penn Treebank. Computational Linguis-\ntics, 19(2):313?330, June.\nM. Munoz, V. Punyakanok, D. Roth, and D. Zimak.\n1999. A learning approach to shallow parsing. In\nEMNLP-VLC?99, the Joint SIGDAT Conference on\nEmpirical Methods in Natural Language Process-\ning and Very Large Corpora, June.\nV. Punyakanok and D. Roth. 2001. The use of clas-\nsifiers in sequential inference. In NIPS-13; The\n2000 Conference on Advances in Neural Informa-\ntion Processing Systems.\nL. A. Ramshaw and M. P. Marcus. 1995. Text chunk-\ning using transformation-based learning. In Pro-\nceedings of the Third Annual Workshop on Very\nLarge Corpora.\nA. Ratnaparkhi. 1997. A linear observed time statis-\ntical parser based on maximum entropy models. In\nEMNLP-97, The Second Conference on Empirical\nMethods in Natural Language Processing, pages 1?\n10.\nD. Roth. 1998. Learning to resolve natural language\nambiguities: A unified approach. In Proc. National\nConference on Artificial Intelligence, pages 806?\n813.\n"},"#tail":"\n","bodyText":[{"#tail":"\n","@confidence":"0.997878375","#text":"\nSignificant amount of work has been\ndevoted recently to develop learning\ntechniques that can be used to gener-\nate partial (shallow) analysis of natu-\nral language sentences rather than a full\nparse. In this work we set out to evalu-\nate whether this direction is worthwhile\nby comparing a learned shallow parser\nto one of the best learned full parsers\non tasks both can perform ? identify-\ning phrases in sentences. We conclude\nthat directly learning to perform these\ntasks as shallow parsers do is advanta-\ngeous over full parsers both in terms of\nperformance and robustness to new and\nlower quality texts.\n"},{"#tail":"\n","@confidence":"0.983275574257426","#text":"\nShallow parsing is studied as an alternative to\nfull-sentence parsing. Rather than producing a\ncomplete analysis of sentences, the alternative\nis to perform only partial analysis of the syn-\ntactic structures in a text (Harris, 1957; Abney,\n1991; Greffenstette, 1993). A lot of recent work\non shallow parsing has been influenced by Ab-\nney?s work (Abney, 1991), who has suggested to\n?chunk? sentences to base level phrases. For ex-\nample, the sentence ?He reckons the current ac-\ncount deficit will narrow to only $ 1.8 billion in\nSeptember .? would be chunked as follows (Tjong\nKim Sang and Buchholz, 2000):\n[NP He ] [VP reckons ] [NP the current\naccount deficit ] [VP will narrow ] [PP\n\nThis research is supported by NSF grants IIS-9801638,\nITR-IIS-0085836 and an ONR MURI Award.\nto ] [NP only $ 1.8 billion ] [PP in ] [NP\nSeptember] .\nWhile earlier work in this direction concen-\ntrated on manual construction of rules, most of\nthe recent work has been motivated by the obser-\nvation that shallow syntactic information can be\nextracted using local information ? by examin-\ning the pattern itself, its nearby context and the\nlocal part-of-speech information. Thus, over the\npast few years, along with advances in the use\nof learning and statistical methods for acquisition\nof full parsers (Collins, 1997; Charniak, 1997a;\nCharniak, 1997b; Ratnaparkhi, 1997), significant\nprogress has been made on the use of statisti-\ncal learning methods to recognize shallow pars-\ning patterns ? syntactic phrases or words that\nparticipate in a syntactic relationship (Church,\n1988; Ramshaw and Marcus, 1995; Argamon et\nal., 1998; Cardie and Pierce, 1998; Munoz et al,\n1999; Punyakanok and Roth, 2001; Buchholz et\nal., 1999; Tjong Kim Sang and Buchholz, 2000).\nResearch on shallow parsing was inspired by\npsycholinguistics arguments (Gee and Grosjean,\n1983) that suggest that in many scenarios (e.g.,\nconversational) full parsing is not a realistic strat-\negy for sentence processing and analysis, and was\nfurther motivated by several arguments from a\nnatural language engineering viewpoint.\nFirst, it has been noted that in many natural lan-\nguage applications it is sufficient to use shallow\nparsing information; information such as noun\nphrases (NPs) and other syntactic sequences have\nbeen found useful in many large-scale language\nprocessing applications including information ex-\ntraction and text summarization (Grishman, 1995;\nAppelt et al, 1993). Second, while training a full\nparser requires a collection of fully parsed sen-\ntences as training corpus, it is possible to train a\nshallow parser incrementally. If all that is avail-\nable is a collection of sentences annotated for\nNPs, it can be used to produce this level of anal-\nysis. This can be augmented later if more infor-\nmation is available. Finally, the hope behind this\nresearch direction was that this incremental and\nmodular processing might result in more robust\nparsing decisions, especially in cases of spoken\nlanguage or other cases in which the quality of the\nnatural language inputs is low ? sentences which\nmay have repeated words, missing words, or any\nother lexical and syntactic mistakes.\nOverall, the driving force behind the work on\nlearning shallow parsers was the desire to get bet-\nter performance and higher reliability. However,\nsince work in this direction has started, a sig-\nnificant progress has also been made in the re-\nsearch on statistical learning of full parsers, both\nin terms of accuracy and processing time (Char-\nniak, 1997b; Charniak, 1997a; Collins, 1997;\nRatnaparkhi, 1997).\nThis paper investigates the question of whether\nwork on shallow parsing is worthwhile. That\nis, we attempt to evaluate quantitatively the intu-\nitions described above ? that learning to perform\nshallow parsing could be more accurate and more\nrobust than learning to generate full parses. We\ndo that by concentrating on the task of identify-\ning the phrase structure of sentences ? a byprod-\nuct of full parsers that can also be produced by\nshallow parsers. We investigate two instantiations\nof this task, ?chucking? and identifying atomic\nphrases. And, to study robustness, we run our\nexperiments both on standard Penn Treebank data\n(part of which is used for training the parsers) and\non lower quality data ? the Switchboard data.\nOur conclusions are quite clear. Indeed, shal-\nlow parsers that are specifically trained to per-\nform the tasks of identifying the phrase structure\nof a sentence are more accurate and more robust\nthan full parsers. We believe that this finding, not\nonly justifies work in this direction, but may even\nsuggest that it would be worthwhile to use this\nmethodology incrementally, to learn a more com-\nplete parser, if needed.\n"},{"#tail":"\n","@confidence":"0.999622236842105","#text":"\nIn order to run a fair comparison between full\nparsers and shallow parsers ? which could pro-\nduce quite different outputs ? we have chosen\nthe task of identifying the phrase structure of a\nsentence. This structure can be easily extracted\nfrom the outcome of a full parser and a shallow\nparser can be trained specifically on this task.\nThere is no agreement on how to define phrases\nin sentences. The definition could depend on\ndownstream applications and could range from\nsimple syntactic patterns to message units peo-\nple use in conversations. For the purpose of this\nstudy, we chose to use two different definitions.\nBoth can be formally defined and they reflect dif-\nferent levels of shallow parsing patterns.\nThe first is the one used in the chunking com-\npetition in CoNLL-2000 (Tjong Kim Sang and\nBuchholz, 2000). In this case, a full parse tree\nis represented in a flat form, producing a rep-\nresentation as in the example above. The goal\nin this case is therefore to accurately predict a\ncollection of  different types of phrases. The\nchunk types are based on the syntactic category\npart of the bracket label in the Treebank. Roughly,\na chunk contains everything to the left of and\nincluding the syntactic head of the constituent\nof the same name. The phrases are: adjective\nphrase (ADJP), adverb phrase (ADVP), conjunc-\ntion phrase (CONJP), interjection phrase (INTJ),\nlist marker (LST), noun phrase (NP), preposition\nphrase (PP), particle (PRT), subordinated clause\n(SBAR), unlike coordinated phrase (UCP), verb\nphrase (VP). (See details in (Tjong Kim Sang and\nBuchholz, 2000).)\nThe second definition used is that of atomic\nphrases. An atomic phrase represents the most\nbasic phrase with no nested sub-phrases. For ex-\nample, in the parse tree,\n"},{"#tail":"\n","@confidence":"0.9789625","#text":"\nPierre Vinken, 61 years, the board,\na nonexecutive director and Nov.\n29 are atomic phrases while other higher-level\nphrases are not. That is, an atomic phrase denotes\na tightly coupled message unit which is just\nabove the level of single words.\n"},{"#tail":"\n","@confidence":"0.998518641509434","#text":"\nWe perform our comparison using two state-of-\nthe-art parsers. For the full parser, we use the\none developed by Michael Collins (Collins, 1996;\nCollins, 1997) ? one of the most accurate full\nparsers around. It represents a full parse tree as\na set of basic phrases and a set of dependency\nrelationships between them. Statistical learning\ntechniques are used to compute the probabilities\nof these phrases and of candidate dependency re-\nlations occurring in that sentence. After that, it\nwill choose the candidate parse tree with the high-\nest probability as output. The experiments use\nthe version that was trained (by Collins) on sec-\ntions 02-21 of the Penn Treebank. The reported\nresults for the full parse tree (on section 23) are\nrecall/precision of 88.1/87.5 (Collins, 1997).\nThe shallow parser used is the SNoW-based\nCSCL parser (Punyakanok and Roth, 2001;\nMunoz et al, 1999). SNoW (Carleson et al,\n1999; Roth, 1998) is a multi-class classifier that\nis specifically tailored for learning in domains\nin which the potential number of information\nsources (features) taking part in decisions is very\nlarge, of which NLP is a principal example. It\nworks by learning a sparse network of linear func-\ntions over a pre-defined or incrementally learned\nfeature space. Typically, SNoW is used as a\nclassifier, and predicts using a winner-take-all\nmechanism over the activation value of the tar-\nget classes. However, in addition to the predic-\ntion, it provides a reliable confidence level in the\nprediction, which enables its use in an inference\nalgorithm that combines predictors to produce a\ncoherent inference. Indeed, in CSCL (constraint\nsatisfaction with classifiers), SNoW is used to\nlearn several different classifiers ? each detects\nthe beginning or end of a phrase of some type\n(noun phrase, verb phrase, etc.). The outcomes\nof these classifiers are then combined in a way\nthat satisfies some constraints ? non-overlapping\nconstraints in this case ? using an efficient con-\nstraint satisfaction mechanism that makes use of\nthe confidence in the classifier?s outcomes.\nSince earlier versions of the SNoW based\nCSCL were used only to identify single\nphrases (Punyakanok and Roth, 2001; Munoz\net al, 1999) and never to identify a collection\nof several phrases at the same time, as we do\nhere, we also trained and tested it under the exact\nconditions of CoNLL-2000 (Tjong Kim Sang and\nBuchholz, 2000) to compare it to other shallow\nparsers. Table 1 shows that it ranks among the\ntop shallow parsers evaluated there 1.\n"},{"#tail":"\n","@confidence":"0.960368190476191","#text":"\nTraining was done on the Penn Treebank (Mar-\ncus et al, 1993) Wall Street Journal data, sections\n02-21. To train the CSCL shallow parser we had\nfirst to convert the WSJ data to a flat format that\ndirectly provides the phrase annotations. This is\ndone using the ?Chunklink? program provided for\nCoNLL-2000 (Tjong Kim Sang and Buchholz,\n2000).\nTesting was done on two types of data. For\nthe first experiment, we used the WSJ section 00\n(which contains about 45,000 tokens and 23,500\nphrases). The goal here was simply to evaluate\nthe full parser and the shallow parser on text that\nis similar to the one they were trained on.\n1We note that some of the variations in the results are\ndue to variations in experimental methodology rather than\nparser?s quality. For example, in [KM00], rather than learn-\ning a classifier for each of the\n\ndifferent phrases, a discrim-\ninator is learned for each of the \n"},{"#tail":"\n","@confidence":"0.992825","#text":"\nphrase pairs which, sta-\ntistically, yields better results. [Hal00] also uses  different\nparsers and reports the results of some voting mechanism on\ntop of these.\nOur robustness test (section 3.2) makes use\nof section 4 in the Switchboard (SWB) data\n(which contains about 57,000 tokens and 17,000\nphrases), taken from Treebank 3. The Switch-\nboard data contains conversation records tran-\nscribed from phone calls. The goal here was two\nfold. First, to evaluate the parsers on a data source\nthat is different from the training source. More\nimportantly, the goal was to evaluate the parsers\non low quality data and observe the absolute per-\nformance as well as relative degradation in per-\nformance.\nThe following sentence is a typical example of\nthe SWB data.\n"},{"#tail":"\n","@confidence":"0.974682666666667","#text":"\nThe fact that it has some missing words, repeated\nwords and frequent interruptions makes it a suit-\nable data to test robustness of parsers.\n"},{"#tail":"\n","@confidence":"0.999894090909091","#text":"\nWe had to do some work in order to unify the in-\nput and output representations for both parsers.\nBoth parsers take sentences annotated with POS\ntags as their input. We used the POS tags in the\nWSJ and converted both the WSJ and the SWB\ndata into the parsers? slightly different input for-\nmats. We also had to convert the outcomes of the\nparsers in order to evaluate them in a fair way.\nWe choose CoNLL-2000?s chunking format as\nour standard output format and converted Collins?\nparser outcome into this format.\n"},{"#tail":"\n","@confidence":"0.910829","#text":"\nThe results are reported in terms of precision, re-\ncall, and  ff as defined below:\n"},{"#tail":"\n","@confidence":"0.998806166666667","#text":"\nWe have used the evaluation procedure of\nCoNLL-2000 to produce the results below. Al-\nthough we do not report significance results here,\nnote that all experiments were done on tens of\nthousands of instances and clearly all differences\nand ratios measured are statistically significant.\n"},{"#tail":"\n","@confidence":"0.993642111111111","#text":"\nWe start by reporting the results in which we com-\npare the full parser and the shallow parser on the\n?clean? WSJ data. Table 2 shows the results on\nidentifying all phrases ? chunking in CoNLL-\n2000 (Tjong Kim Sang and Buchholz, 2000) ter-\nminology. The results show that for the tasks of\nidentifying phrases, learning directly, as done by\nthe shallow parser outperforms the outcome from\nthe full parser.\n"},{"#tail":"\n","@confidence":"0.963913","#text":"\nfication (chunking) for the full and the shallow\nparser on the WSJ data. Results are shown for an\n(weighted) average of 11 types of phrases as well\nas for two of the most common phrases, NP and\n"},{"#tail":"\n","@confidence":"0.9989342","#text":"\nNext, we compared the performance of the\nparsers on the task of identifying atomic phrases2.\nHere, again, the shallow parser exhibits signifi-\ncantly better performance. Table 3 shows the re-\nsults of extracting atomic phrases.\n"},{"#tail":"\n","@confidence":"0.831454444444444","#text":"\nNext we present the results of evaluating the ro-\nbustness of the parsers on lower quality data. Ta-\nble 4 describes the results of evaluating the same\nparsers as above, (both trained as before on the\n2As a side note ? the fact that the same program could\nbe trained to recognize patterns of different level in such an\neasy way, only by changing the annotations of the training\ndata, could also be viewed as an advantage of the shallow\nparsing paradigm.\n"},{"#tail":"\n","@confidence":"0.9746132","#text":"\nidentification on the WSJ data. Results are\nshown for an (weighted) average of 11 types of\nphrases as well as for the most common phrase,\nNP. VP occurs very infrequently as an atomic\nphrase.\n"},{"#tail":"\n","@confidence":"0.828219666666667","#text":"\ndent that on this data the difference between the\nperformance of the two parsers is even more sig-\nnificant.\n"},{"#tail":"\n","@confidence":"0.9398875","#text":"\ncall for phrase identification (chunking) on the\nSwitchboard data. Results are shown for an\n(weighted) average of 11 types of phrases as well\nas for two of the most common phrases, NP, VP.\n"},{"#tail":"\n","@confidence":"0.9991229","#text":"\nThis is shown more clearly in Table 5 which\ncompares the relative degradation in performance\neach of the parsers suffers when moving from the\nWSJ to the SWB data (Table 2 vs. Table 4). While\nthe performances of both parsers goes down when\nthey are tested on the SWB, relative to the WSJ\nperformance, it is clear that the shallow parser?s\nperformance degrades more gracefully. These re-\nsults clearly indicate the higher-level robustness\nof the shallow parser.\n"},{"#tail":"\n","@confidence":"0.909287769230769","#text":"\nAnalyzing the results shown above is outside the\nscope of this short abstract. We will only provide\none example that might shed some light on the\nreasons for the more significant degradation in the\nresults of the full parser. Table 6 exhibits the re-\nsults of chunking as given by Collins? parser. The\nfour columns are the original words, POS tags,\nand the phrases ? encoded using the BIO scheme\nTable 5: Robustness: Relative degradation in\n results for Chunking. For each parser the re-\nsult shown is the ratio between the result on the\n?noisy? SWB data and the ?clean? WSJ corpus\ndata.\n"},{"#tail":"\n","@confidence":"0.966772230769231","#text":"\n(B- beginning of phrase; I- inside the phrase; O-\noutside the phrase) ? with the true annotation\nand Collins? annotation.\nThe mistakes in the phrase identification (e.g.,\nin ?word processing applications?) seem to be a\nresult of assuming, perhaps due to the ?um? and\nadditional punctuation marks, that this is a sep-\narate sentence, rather than a phrase. Under this\nassumption, the full parser tries to make it a com-\nplete sentence and decides that ?processing? is a\n?verb? in the parsing result. This seems to be\na typical example for mistakes made by the full\nparser.\n"},{"#tail":"\n","@confidence":"0.999832782608696","#text":"\nFull parsing and shallow parsing are two different\nstrategies for parsing natural languages. While\nfull parsing provides more complete information\nabout a sentence, shallow parsing is more flexi-\nble, easier to train and could be targeted for spe-\ncific, limited subtasks. Several arguments have\nbeen used in the past to argue, on an intuitive\nbasis, that (1) shallow parsing is sufficient for a\nwide range of applications and that (2) shallow\nparsing could be more reliable than full parsing in\nhandling ill-formed real-world sentences, such as\nsentences that occur in conversational situations.\nWhile the former is an experimental issue that\nis still open, this paper has tried to evaluate exper-\nimentally the latter argument. Although the ex-\nperiments reported here only compare the perfor-\nmance of one full parser and one shallow parser,\nwe believe that these state-of-the-art parsers rep-\nresent their class quite well. Our results show that\non the specific tasks for which we have trained\nthe shallow parser ? identifying several kinds of\nphrases ? the shallow parser performs more accu-\nrately and more robustly than the full parser. In\nsome sense, these results validate the research in\nthis direction.\nClearly, there are several directions for future\nwork that this preliminary work suggests. First, in\nour experiments, the Collins? parser is trained on\nthe Treebank data and tested on the lower quality\ndata. It would be interesting to see what are the\nresults if lower quality data is also used for train-\ning. Second, our decision to run the experiments\non two different ways of decomposing a sentence\ninto phrases was somewhat arbitrary (although\nwe believe that selecting phrases in a different\nway would not affect the results). It does reflect,\nhowever, the fact that it is not completely clear\nwhat kinds of shallow parsing information should\none try to extract in real applications. Making\nprogress in the direction of a formal definition of\nphrases and experimenting with these along the\nlines of the current study would also be useful.\nFinally, an experimental comparison on several\nother shallow parsing tasks such as various attach-\nments and relations detection is also an important\ndirection that will enhance this work.\n"},{"#tail":"\n","@confidence":"0.99992325","#text":"\nWe are grateful to Vasin Punyakanok for his ad-\nvice in this project and for his help in using the\nCSCL parser. We also thank Michael Collins for\nmaking his parser available to us.\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.9979855","#text":"\nDepartment of Computer Science\nUniversity of Illinois at Urbana-Champaign\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.989697","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998112","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.987447","@genericHeader":"method","#text":"\n2 Experimental Design\n"},{"#tail":"\n","@confidence":"0.988628","@genericHeader":"method","#text":"\n3 Experimental Results\n"},{"#tail":"\n","@confidence":"0.988905","@genericHeader":"conclusions","#text":"\n4 Conclusion\n"},{"#tail":"\n","@confidence":"0.999306","@genericHeader":"acknowledgments","#text":"\n5 Acknowledgments\n"},{"#tail":"\n","@confidence":"0.983812","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.756319666666667","#text":"\nTable 1: Rankings of Shallow Parsers in\nCoNLL-2000. See (Tjong Kim Sang and Buch-\nholz, 2000) for details.\n"},{"#tail":"\n","@confidence":"0.957538","#text":"\nTable 2: Precision & Recall for phrase identi-\n"},{"#tail":"\n","@confidence":"0.979862","#text":"\nTable 3: Precision & Recall for atomic phrase\n"},{"#tail":"\n","@confidence":"0.894373","#text":"\nTable 4: Switchboard data: Precision & Re-\n"},{"#tail":"\n","@confidence":"0.996254","#text":"\nTable 6: An example: a parsing mistake\n"}],"table":[{"#tail":"\n","@confidence":"0.999717423076923","#text":"\nParsers Precision( ) Recall(  )  (  )\n\nKM00 93.45 93.51 93.48\n\nHal00 93.13 93.51 93.32\n\nCSCL * 93.41 92.64 93.02\n\nTKS00 94.04 91.00 92.50\n\nZST00 91.99 92.25 92.12\n\nDej00 91.87 91.31 92.09\n\nKoe00 92.08 91.86 91.97\n\nOsb00 91.65 92.23 91.94\n\nVB00 91.05 92.03 91.54\n\nPMP00 90.63 89.65 90.14\n\nJoh00 86.24 88.25 87.23\n\nVD00 88.82 82.91 85.76\nBaseline 72.58 82.14 77.07\n"},{"#tail":"\n","@confidence":"0.925557833333333","#text":"\nVP.\nFull Parser Shallow Parser\nP R &quot;$# P R &quot;$#\nAvr 91.71 92.21 91.96 93.85 95.45 94.64\nNP 93.10 92.05 92.57 93.83 95.92 94.87\nVP 86.00 90.42 88.15 95.50 95.05 95.28\n"},{"#tail":"\n","@confidence":"0.8976024","#text":"\nFull Parser Shallow Parser\nP R &quot;$# P R &quot;$#\nAvr 88.68 90.45 89.56 92.02 93.61 92.81\nNP 91.86 92.16 92.01 93.54 95.88 94.70\nsame WSJ sections) on the SWB data. It is evi-\n"},{"#tail":"\n","@confidence":"0.9846366","#text":"\nFull Parser Shallow Parser\nP R &quot;$# P R &quot;$#\nAvr 81.54 83.79 82.65 86.50 90.54 88.47\nNP 88.29 88.96 88.62 90.50 92.59 91.54\nVP 70.61 83.53 76.52 85.30 89.76 87.47\n"},{"#tail":"\n","@confidence":"0.9809392","#text":"\nFull Parser Shallow Parser\n&quot;%# &quot;$#\nAvr .89 .93\nNP .95 .96\nVP .86 .92\n"},{"#tail":"\n","@confidence":"0.9914405","#text":"\nWORD POS TRUE Collins\nUm UH B-INTJ B-INTJ\nCOMMA COMMA O I-INTJ\nMostly RB O I-INTJ\nCOMMA COMMA O O\num UH B-INTJ B-INTJ\nCOMMA COMMA O O\nword NN B-NP &('*),+\nprocessing NN I-NP &('.-/+\napplications NNS I-NP &('*),+\nand CC O O\nCOMMA COMMA O O\nuh UH B-INTJ B-INTJ\nCOMMA COMMA O O\njust RB B-ADVP B-PP\nas IN B-PP I-PP\na DT B-NP B-NP\ndumb JJ I-NP I-NP\nterminal NN I-NP I-NP\n.. . O O\n"}],"email":{"#tail":"\n","@confidence":"0.973314","#text":"\nxli1@cs.uiuc.edu danr@cs.uiuc.edu\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.959663","#tail":"\n","@no":"0","address":{"#tail":"\n","@confidence":"0.973331","#text":"Urbana, IL 61801"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.999919","#text":"Department of Computer Science University of Illinois at Urbana-Champaign"},"author":{"#tail":"\n","@confidence":"0.997987","#text":"Xin Li Dan Roth"},"abstract":{"#tail":"\n","@confidence":"0.999369176470588","#text":"Significant amount of work has been devoted recently to develop learning techniques that can be used to generate partial (shallow) analysis of natural language sentences rather than a full parse. In this work we set out to evaluate whether this direction is worthwhile by comparing a learned shallow parser to one of the best learned full parsers on tasks both can perform ? identifying phrases in sentences. We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts."},"email":{"#tail":"\n","@confidence":"0.997281","#text":"xli1@cs.uiuc.edudanr@cs.uiuc.edu"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"date":{"#tail":"\n","#text":"1991"},"editor":{"#tail":"\n","#text":"In S. P. Abney R. C. Berwick and C. Tenny, editors,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"irection is worthwhile by comparing a learned shallow parser to one of the best learned full parsers on tasks both can perform ? identifying phrases in sentences. We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts. 1 Introduction Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993). A lot of recent work on shallow parsing has been influenced by Abney?s work (Abney, 1991), who has suggested to ?chunk? sentences to base level phrases. For example, the sentence ?He reckons the current account deficit will narrow to only $ 1.8 billion in September .? would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP  This research is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlie","@endWordPosition":"167","@position":"1048","annotationId":"T1","@startWordPosition":"166","@citStr":"Abney, 1991"}},"title":{"#tail":"\n","#text":"Parsing by chunks."},"#tail":"\n","rawString":{"#tail":"\n","#text":"S. P. Abney. 1991. Parsing by chunks. In S. P. Abney R. C. Berwick and C. Tenny, editors, Principlebased parsing: Computation and Psycholinguistics, pages 257?278. Kluwer, Dordrecht."},"#text":"\n","pages":{"#tail":"\n","#text":"257--278"},"marker":{"#tail":"\n","#text":"Abney, 1991"},"publisher":{"#tail":"\n","#text":"Kluwer,"},"location":{"#tail":"\n","#text":"Dordrecht."},"booktitle":{"#tail":"\n","#text":"Principlebased parsing: Computation and Psycholinguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"S P Abney"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. 1993. FASTUS: A finite-state processor for information extraction from real-world text. In Proc. International Joint Conference on Artificial Intelligence."},"#text":"\n","marker":{"#tail":"\n","#text":"Appelt, Hobbs, Bear, Israel, Tyson, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nts (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (Grishman, 1995; Appelt et al, 1993). Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally. If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis. This can be augmented later if more information is available. Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs i","@endWordPosition":"517","@position":"3223","annotationId":"T2","@startWordPosition":"514","@citStr":"Appelt et al, 1993"}},"title":{"#tail":"\n","#text":"FASTUS: A finite-state processor for information extraction from real-world text."},"booktitle":{"#tail":"\n","#text":"In Proc. International Joint Conference on Artificial Intelligence."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"D Appelt"},{"#tail":"\n","#text":"J Hobbs"},{"#tail":"\n","#text":"J Bear"},{"#tail":"\n","#text":"D Israel"},{"#tail":"\n","#text":"M Tyson"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"S. Argamon, I. Dagan, and Y. Krymolowski. 1998. A memory-based approach to learning shallow natural language patterns. In COLING-ACL 98, The 17th International Conference on Computational Linguistics."},"#text":"\n","marker":{"#tail":"\n","#text":"Argamon, Dagan, Krymolowski, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"t shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such a","@endWordPosition":"395","@position":"2404","annotationId":"T3","@startWordPosition":"392","@citStr":"Argamon et al., 1998"}},"title":{"#tail":"\n","#text":"A memory-based approach to learning shallow natural language patterns."},"booktitle":{"#tail":"\n","#text":"In COLING-ACL 98, The 17th International Conference on Computational Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Argamon"},{"#tail":"\n","#text":"I Dagan"},{"#tail":"\n","#text":"Y Krymolowski"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cascaded grammatical relation assignment. In EMNLP-VLC?99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, June."},"#text":"\n","marker":{"#tail":"\n","#text":"Buchholz, Veenstra, Daelemans, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"attern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic sequences have been found useful in many large-scale ","@endWordPosition":"411","@position":"2498","annotationId":"T4","@startWordPosition":"408","@citStr":"Buchholz et al., 1999"}},"title":{"#tail":"\n","#text":"Cascaded grammatical relation assignment."},"booktitle":{"#tail":"\n","#text":"In EMNLP-VLC?99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"S Buchholz"},{"#tail":"\n","#text":"J Veenstra"},{"#tail":"\n","#text":"W Daelemans"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"C. Cardie and D. Pierce. 1998. Error-driven pruning of Treebanks grammars for base noun phrase identification. In Proceedings of ACL-98, pages 218? 224."},"#text":"\n","pages":{"#tail":"\n","#text":"218--224"},"marker":{"#tail":"\n","#text":"Cardie, Pierce, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"formation can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and ","@endWordPosition":"399","@position":"2429","annotationId":"T5","@startWordPosition":"396","@citStr":"Cardie and Pierce, 1998"}},"title":{"#tail":"\n","#text":"Error-driven pruning of Treebanks grammars for base noun phrase identification."},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL-98,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"C Cardie"},{"#tail":"\n","#text":"D Pierce"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"A. Carleson, C. Cumby, J. Rosen, and D. Roth. 1999."},"#text":"\n","marker":{"#tail":"\n","#text":"Carleson, Cumby, Rosen, Roth, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997). The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001; Munoz et al, 1999). SNoW (Carleson et al, 1999; Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enab","@endWordPosition":"1408","@position":"8550","annotationId":"T6","@startWordPosition":"1405","@citStr":"Carleson et al, 1999"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Carleson"},{"#tail":"\n","#text":"C Cumby"},{"#tail":"\n","#text":"J Rosen"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical Report UIUCDCS-R-99-2101,"},"date":{"#tail":"\n"},"institution":{"#tail":"\n","#text":"UIUC Computer Science Department,"},"rawString":{"#tail":"\n","#text":"The SNoW learning architecture. Technical Report UIUCDCS-R-99-2101, UIUC Computer Science Department, May."},"#text":"\n","marker":{"#tail":"\n"},"title":{"#tail":"\n","#text":"The SNoW learning architecture."},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"E. Charniak. 1997a. Statistical parsing with a contextfree grammar and word statistics. In Proc. National Conference on Artificial Intelligence."},"#text":"\n","marker":{"#tail":"\n","#text":"Charniak, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"h is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a","@endWordPosition":"351","@position":"2113","annotationId":"T7","@startWordPosition":"350","@citStr":"Charniak, 1997"},{"#tail":"\n","#text":"ing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low ? sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes. Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time (Charniak, 1997b; Charniak, 1997a; Collins, 1997; Ratnaparkhi, 1997). This paper investigates the question of whether work on shallow parsing is worthwhile. That is, we attempt to evaluate quantitatively the intuitions described above ? that learning to perform shallow parsing could be more accurate and more robust than learning to generate full parses. We do that by concentrating on the task of identifying the phrase structure of sentences ? a byproduct of full parsers that can also be produced by shallow parsers. We investigate two instantiations of this task, ?chucking? and identifying atomic phrases. And","@endWordPosition":"695","@position":"4279","annotationId":"T8","@startWordPosition":"693","@citStr":"Charniak, 1997"}]},"title":{"#tail":"\n","#text":"Statistical parsing with a contextfree grammar and word statistics."},"booktitle":{"#tail":"\n","#text":"In Proc. National Conference on Artificial Intelligence."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"E. Charniak. 1997b. Statistical techniques for natural language parsing. The AI Magazine."},"journal":{"#tail":"\n","#text":"The AI Magazine."},"#text":"\n","marker":{"#tail":"\n","#text":"Charniak, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"h is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a","@endWordPosition":"351","@position":"2113","annotationId":"T9","@startWordPosition":"350","@citStr":"Charniak, 1997"},{"#tail":"\n","#text":"ing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low ? sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes. Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time (Charniak, 1997b; Charniak, 1997a; Collins, 1997; Ratnaparkhi, 1997). This paper investigates the question of whether work on shallow parsing is worthwhile. That is, we attempt to evaluate quantitatively the intuitions described above ? that learning to perform shallow parsing could be more accurate and more robust than learning to generate full parses. We do that by concentrating on the task of identifying the phrase structure of sentences ? a byproduct of full parsers that can also be produced by shallow parsers. We investigate two instantiations of this task, ?chucking? and identifying atomic phrases. And","@endWordPosition":"695","@position":"4279","annotationId":"T10","@startWordPosition":"693","@citStr":"Charniak, 1997"}]},"title":{"#tail":"\n","#text":"Statistical techniques for natural language parsing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"E Charniak"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1988"},"rawString":{"#tail":"\n","#text":"Kenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proc. of ACL Conference on Applied Natural Language Processing."},"#text":"\n","marker":{"#tail":"\n","#text":"Church, 1988"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"as been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use","@endWordPosition":"387","@position":"2356","annotationId":"T11","@startWordPosition":"386","@citStr":"Church, 1988"}},"title":{"#tail":"\n","#text":"A stochastic parts program and noun phrase parser for unrestricted text."},"booktitle":{"#tail":"\n","#text":"In Proc. of ACL Conference on Applied Natural Language Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Kenneth W Church"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"M. Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 184?191."},"#text":"\n","pages":{"#tail":"\n","#text":"184--191"},"marker":{"#tail":"\n","#text":"Collins, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"c phrase with no nested sub-phrases. For example, in the parse tree, ( (S (NP (NP Pierre Vinken) , (ADJP (NP 61 years) old) ,) (VP will (VP join (NP the board) (PP as (NP a nonexecutive director)) (NP Nov. 29))) .)) Pierre Vinken, 61 years, the board, a nonexecutive director and Nov. 29 are atomic phrases while other higher-level phrases are not. That is, an atomic phrase denotes a tightly coupled message unit which is just above the level of single words. 2.1 Parsers We perform our comparison using two state-ofthe-art parsers. For the full parser, we use the one developed by Michael Collins (Collins, 1996; Collins, 1997) ? one of the most accurate full parsers around. It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precis","@endWordPosition":"1282","@position":"7788","annotationId":"T12","@startWordPosition":"1281","@citStr":"Collins, 1996"}},"title":{"#tail":"\n","#text":"A new statistical parser based on bigram lexical dependencies."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics."},"#text":"\n","marker":{"#tail":"\n","#text":"Collins, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"  This research is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full ","@endWordPosition":"349","@position":"2097","annotationId":"T13","@startWordPosition":"348","@citStr":"Collins, 1997"},{"#tail":"\n","#text":"rsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low ? sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes. Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time (Charniak, 1997b; Charniak, 1997a; Collins, 1997; Ratnaparkhi, 1997). This paper investigates the question of whether work on shallow parsing is worthwhile. That is, we attempt to evaluate quantitatively the intuitions described above ? that learning to perform shallow parsing could be more accurate and more robust than learning to generate full parses. We do that by concentrating on the task of identifying the phrase structure of sentences ? a byproduct of full parsers that can also be produced by shallow parsers. We investigate two instantiations of this task, ?chucking? and identifying atomic phrases. And, to study robustness, we run our","@endWordPosition":"699","@position":"4312","annotationId":"T14","@startWordPosition":"698","@citStr":"Collins, 1997"},{"#tail":"\n","#text":"o nested sub-phrases. For example, in the parse tree, ( (S (NP (NP Pierre Vinken) , (ADJP (NP 61 years) old) ,) (VP will (VP join (NP the board) (PP as (NP a nonexecutive director)) (NP Nov. 29))) .)) Pierre Vinken, 61 years, the board, a nonexecutive director and Nov. 29 are atomic phrases while other higher-level phrases are not. That is, an atomic phrase denotes a tightly coupled message unit which is just above the level of single words. 2.1 Parsers We perform our comparison using two state-ofthe-art parsers. For the full parser, we use the one developed by Michael Collins (Collins, 1996; Collins, 1997) ? one of the most accurate full parsers around. It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5","@endWordPosition":"1284","@position":"7804","annotationId":"T15","@startWordPosition":"1283","@citStr":"Collins, 1997"}]},"title":{"#tail":"\n","#text":"Three generative, lexicalised models for statistical parsing."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"M Collins"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1983"},"rawString":{"#tail":"\n","#text":"J. P. Gee and F. Grosjean. 1983. Performance structures:a psycholinguistic and linguistic appraisal. Cognitive Psychology, 15:411?458."},"#text":"\n","pages":{"#tail":"\n","#text":"15--411"},"marker":{"#tail":"\n","#text":"Gee, Grosjean, 1983"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (Grishman, 1995; Appelt et al, 1993). Second,","@endWordPosition":"430","@position":"2632","annotationId":"T16","@startWordPosition":"427","@citStr":"Gee and Grosjean, 1983"}},"title":{"#tail":"\n","#text":"Performance structures:a psycholinguistic and linguistic appraisal. Cognitive Psychology,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"J P Gee"},{"#tail":"\n","#text":"F Grosjean"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"G. Greffenstette. 1993. Evaluation techniques for automatic semantic extraction: comparing semantic and window based approaches. In ACL?93 workshop on the Acquisition of Lexical Knowledge from Text."},"#text":"\n","marker":{"#tail":"\n","#text":"Greffenstette, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"orthwhile by comparing a learned shallow parser to one of the best learned full parsers on tasks both can perform ? identifying phrases in sentences. We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts. 1 Introduction Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993). A lot of recent work on shallow parsing has been influenced by Abney?s work (Abney, 1991), who has suggested to ?chunk? sentences to base level phrases. For example, the sentence ?He reckons the current account deficit will narrow to only $ 1.8 billion in September .? would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP  This research is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this directi","@endWordPosition":"169","@position":"1070","annotationId":"T17","@startWordPosition":"168","@citStr":"Greffenstette, 1993"}},"title":{"#tail":"\n","#text":"Evaluation techniques for automatic semantic extraction: comparing semantic and window based approaches."},"booktitle":{"#tail":"\n","#text":"In ACL?93 workshop on the Acquisition of Lexical Knowledge from Text."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"G Greffenstette"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"editor":{"#tail":"\n","#text":"B. Sundheim, editor,"},"rawString":{"#tail":"\n","#text":"R. Grishman. 1995. The NYU system for MUC-6 or where?s syntax? In B. Sundheim, editor, Proceedings of the Sixth Message Understanding Conference. Morgan Kaufmann Publishers."},"#text":"\n","marker":{"#tail":"\n","#text":"Grishman, 1995"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann Publishers."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"nguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (Grishman, 1995; Appelt et al, 1993). Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally. If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis. This can be augmented later if more information is available. Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natu","@endWordPosition":"513","@position":"3202","annotationId":"T18","@startWordPosition":"512","@citStr":"Grishman, 1995"}},"title":{"#tail":"\n","#text":"The NYU system for MUC-6 or where?s syntax? In"},"booktitle":{"#tail":"\n","#text":"Proceedings of the Sixth Message Understanding Conference."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"R Grishman"}}},{"volume":{"#tail":"\n","#text":"33"},"#tail":"\n","date":{"#tail":"\n","#text":"1957"},"rawString":{"#tail":"\n","#text":"Z. S. Harris. 1957. Co-occurrence and transformation in linguistic structure. Language, 33(3):283?340."},"journal":{"#tail":"\n","#text":"Language,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Harris, 1957"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"whether this direction is worthwhile by comparing a learned shallow parser to one of the best learned full parsers on tasks both can perform ? identifying phrases in sentences. We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts. 1 Introduction Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993). A lot of recent work on shallow parsing has been influenced by Abney?s work (Abney, 1991), who has suggested to ?chunk? sentences to base level phrases. For example, the sentence ?He reckons the current account deficit will narrow to only $ 1.8 billion in September .? would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP  This research is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .","@endWordPosition":"165","@position":"1035","annotationId":"T19","@startWordPosition":"164","@citStr":"Harris, 1957"}},"title":{"#tail":"\n","#text":"Co-occurrence and transformation in linguistic structure."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Z S Harris"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL-2000, pages 127?132."},"#text":"\n","pages":{"#tail":"\n","#text":"127--132"},"marker":{"#tail":"\n","#text":"Sang, Buchholz, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ty texts. 1 Introduction Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993). A lot of recent work on shallow parsing has been influenced by Abney?s work (Abney, 1991), who has suggested to ?chunk? sentences to base level phrases. For example, the sentence ?He reckons the current account deficit will narrow to only $ 1.8 billion in September .? would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP  This research is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances ","@endWordPosition":"229","@position":"1404","annotationId":"T20","@startWordPosition":"226","@citStr":"Sang and Buchholz, 2000"},{"#tail":"\n","#text":"sentence. This structure can be easily extracted from the outcome of a full parser and a shallow parser can be trained specifically on this task. There is no agreement on how to define phrases in sentences. The definition could depend on downstream applications and could range from simple syntactic patterns to message units people use in conversations. For the purpose of this study, we chose to use two different definitions. Both can be formally defined and they reflect different levels of shallow parsing patterns. The first is the one used in the chunking competition in CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000). In this case, a full parse tree is represented in a flat form, producing a representation as in the example above. The goal in this case is therefore to accurately predict a collection of  different types of phrases. The chunk types are based on the syntactic category part of the bracket label in the Treebank. Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name. The phrases are: adjective phrase (ADJP), adverb phrase (ADVP), conjunction phrase (CONJP), interjection phrase (INTJ), list marker (LST), noun phrase (NP), prep","@endWordPosition":"1033","@position":"6310","annotationId":"T21","@startWordPosition":"1030","@citStr":"Sang and Buchholz, 2000"},{"#tail":"\n","#text":"me type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints ? non-overlapping constraints in this case ? using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier?s outcomes. Since earlier versions of the SNoW based CSCL were used only to identify single phrases (Punyakanok and Roth, 2001; Munoz et al, 1999) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) to compare it to other shallow parsers. Table 1 shows that it ranks among the top shallow parsers evaluated there 1. Table 1: Rankings of Shallow Parsers in CoNLL-2000. See (Tjong Kim Sang and Buchholz, 2000) for details. Parsers Precision( ) Recall(  )  (  )  KM00 93.45 93.51 93.48  Hal00 93.13 93.51 93.32  CSCL * 93.41 92.64 93.02  TKS00 94.04 91.00 92.50  ZST00 91.99 92.25 92.12  Dej00 91.87 91.31 92.09  Koe00 92.08 91.86 91.97  Osb00 91.65 92.23 91.94  VB00 91.05 92.03 91.54  PMP00 90.63 89.65 90.14  Joh00 86.24 88.25 87.23  VD00 88.82 82.91 85.76 Baseline 72.58 82.14 77","@endWordPosition":"1648","@position":"10035","annotationId":"T22","@startWordPosition":"1645","@citStr":"Sang and Buchholz, 2000"},{"#tail":"\n","#text":"rns fi flffi ff! Recall Precision  fl Precision ffi Recall We have used the evaluation procedure of CoNLL-2000 to produce the results below. Although we do not report significance results here, note that all experiments were done on tens of thousands of instances and clearly all differences and ratios measured are statistically significant. 3 Experimental Results 3.1 Performance We start by reporting the results in which we compare the full parser and the shallow parser on the ?clean? WSJ data. Table 2 shows the results on identifying all phrases ? chunking in CoNLL2000 (Tjong Kim Sang and Buchholz, 2000) terminology. The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser. Table 2: Precision & Recall for phrase identification (chunking) for the full and the shallow parser on the WSJ data. Results are shown for an (weighted) average of 11 types of phrases as well as for two of the most common phrases, NP and VP. Full Parser Shallow Parser P R &quot;$# P R &quot;$# Avr 91.71 92.21 91.96 93.85 95.45 94.64 NP 93.10 92.05 92.57 93.83 95.92 94.87 VP 86.00 90.42 88.15 95.50 95.05 95.28 Next, we compared the perfo","@endWordPosition":"2354","@position":"14144","annotationId":"T23","@startWordPosition":"2351","@citStr":"Sang and Buchholz, 2000"}]},"title":{"#tail":"\n","#text":"Introduction to the CoNLL-2000 shared task: Chunking."},"booktitle":{"#tail":"\n","#text":"In Proceedings of CoNLL-2000 and LLL-2000,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"E F Tjong Kim Sang"},{"#tail":"\n","#text":"S Buchholz"}]}},{"volume":{"#tail":"\n","#text":"19"},"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313?330, June."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Marcus, Santorini, Marcinkiewicz, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s that it ranks among the top shallow parsers evaluated there 1. Table 1: Rankings of Shallow Parsers in CoNLL-2000. See (Tjong Kim Sang and Buchholz, 2000) for details. Parsers Precision( ) Recall(  )  (  )  KM00 93.45 93.51 93.48  Hal00 93.13 93.51 93.32  CSCL * 93.41 92.64 93.02  TKS00 94.04 91.00 92.50  ZST00 91.99 92.25 92.12  Dej00 91.87 91.31 92.09  Koe00 92.08 91.86 91.97  Osb00 91.65 92.23 91.94  VB00 91.05 92.03 91.54  PMP00 90.63 89.65 90.14  Joh00 86.24 88.25 87.23  VD00 88.82 82.91 85.76 Baseline 72.58 82.14 77.07 2.2 Data Training was done on the Penn Treebank (Marcus et al, 1993) Wall Street Journal data, sections 02-21. To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations. This is done using the ?Chunklink? program provided for CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000). Testing was done on two types of data. For the first experiment, we used the WSJ section 00 (which contains about 45,000 tokens and 23,500 phrases). The goal here was simply to evaluate the full parser and the shallow parser on text that is similar to the one they were trained on. 1We note that some of the variations ","@endWordPosition":"1776","@position":"10707","annotationId":"T24","@startWordPosition":"1772","@citStr":"Marcus et al, 1993"}},"title":{"#tail":"\n","#text":"Building a large annotated corpus of English: The Penn Treebank."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M P Marcus"},{"#tail":"\n","#text":"B Santorini"},{"#tail":"\n","#text":"M Marcinkiewicz"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"M. Munoz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. In EMNLP-VLC?99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, June."},"#text":"\n","marker":{"#tail":"\n","#text":"Munoz, Punyakanok, Roth, Zimak, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"d using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic seq","@endWordPosition":"403","@position":"2448","annotationId":"T25","@startWordPosition":"400","@citStr":"Munoz et al, 1999"},{"#tail":"\n","#text":"hrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997). The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001; Munoz et al, 1999). SNoW (Carleson et al, 1999; Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level i","@endWordPosition":"1403","@position":"8522","annotationId":"T26","@startWordPosition":"1400","@citStr":"Munoz et al, 1999"},{"#tail":"\n","#text":" a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers ? each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints ? non-overlapping constraints in this case ? using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier?s outcomes. Since earlier versions of the SNoW based CSCL were used only to identify single phrases (Punyakanok and Roth, 2001; Munoz et al, 1999) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) to compare it to other shallow parsers. Table 1 shows that it ranks among the top shallow parsers evaluated there 1. Table 1: Rankings of Shallow Parsers in CoNLL-2000. See (Tjong Kim Sang and Buchholz, 2000) for details. Parsers Precision( ) Recall(  )  (  )  KM00 93.45 93.51 93.48  Hal00 93.13 93.51 93.32  CSCL * 93.41 92.64 93.02  TKS00 94.04 91.00 92.50  ZST00 91.99 92.25 92.12  Dej00 9","@endWordPosition":"1613","@position":"9841","annotationId":"T27","@startWordPosition":"1610","@citStr":"Munoz et al, 1999"}]},"title":{"#tail":"\n","#text":"A learning approach to shallow parsing."},"booktitle":{"#tail":"\n","#text":"In EMNLP-VLC?99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Munoz"},{"#tail":"\n","#text":"V Punyakanok"},{"#tail":"\n","#text":"D Roth"},{"#tail":"\n","#text":"D Zimak"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In NIPS-13; The 2000 Conference on Advances in Neural Information Processing Systems."},"#text":"\n","marker":{"#tail":"\n","#text":"Punyakanok, Roth, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"mation ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic sequences have been found usef","@endWordPosition":"407","@position":"2475","annotationId":"T28","@startWordPosition":"404","@citStr":"Punyakanok and Roth, 2001"},{"#tail":"\n","#text":"se tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997). The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001; Munoz et al, 1999). SNoW (Carleson et al, 1999; Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliabl","@endWordPosition":"1399","@position":"8502","annotationId":"T29","@startWordPosition":"1396","@citStr":"Punyakanok and Roth, 2001"},{"#tail":"\n","#text":"bines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers ? each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints ? non-overlapping constraints in this case ? using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier?s outcomes. Since earlier versions of the SNoW based CSCL were used only to identify single phrases (Punyakanok and Roth, 2001; Munoz et al, 1999) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) to compare it to other shallow parsers. Table 1 shows that it ranks among the top shallow parsers evaluated there 1. Table 1: Rankings of Shallow Parsers in CoNLL-2000. See (Tjong Kim Sang and Buchholz, 2000) for details. Parsers Precision( ) Recall(  )  (  )  KM00 93.45 93.51 93.48  Hal00 93.13 93.51 93.32  CSCL * 93.41 92.64 93.02  TKS00 94.04 91.00 92.50  ZST00 91.99 9","@endWordPosition":"1609","@position":"9821","annotationId":"T30","@startWordPosition":"1606","@citStr":"Punyakanok and Roth, 2001"}]},"title":{"#tail":"\n","#text":"The use of classifiers in sequential inference."},"booktitle":{"#tail":"\n","#text":"In NIPS-13; The 2000 Conference on Advances in Neural Information Processing Systems."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"V Punyakanok"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Annual Workshop on Very Large Corpora."},"#text":"\n","marker":{"#tail":"\n","#text":"Ramshaw, Marcus, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ted by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing informati","@endWordPosition":"391","@position":"2382","annotationId":"T31","@startWordPosition":"388","@citStr":"Ramshaw and Marcus, 1995"}},"title":{"#tail":"\n","#text":"Text chunking using transformation-based learning."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Third Annual Workshop on Very Large Corpora."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"L A Ramshaw"},{"#tail":"\n","#text":"M P Marcus"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In EMNLP-97, The Second Conference on Empirical Methods in Natural Language Processing, pages 1? 10."},"#text":"\n","pages":{"#tail":"\n","#text":"1--10"},"marker":{"#tail":"\n","#text":"Ratnaparkhi, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al, 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence proce","@endWordPosition":"355","@position":"2151","annotationId":"T32","@startWordPosition":"354","@citStr":"Ratnaparkhi, 1997"},{"#tail":"\n","#text":", especially in cases of spoken language or other cases in which the quality of the natural language inputs is low ? sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes. Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time (Charniak, 1997b; Charniak, 1997a; Collins, 1997; Ratnaparkhi, 1997). This paper investigates the question of whether work on shallow parsing is worthwhile. That is, we attempt to evaluate quantitatively the intuitions described above ? that learning to perform shallow parsing could be more accurate and more robust than learning to generate full parses. We do that by concentrating on the task of identifying the phrase structure of sentences ? a byproduct of full parsers that can also be produced by shallow parsers. We investigate two instantiations of this task, ?chucking? and identifying atomic phrases. And, to study robustness, we run our experiments both on","@endWordPosition":"701","@position":"4332","annotationId":"T33","@startWordPosition":"700","@citStr":"Ratnaparkhi, 1997"}]},"title":{"#tail":"\n","#text":"A linear observed time statistical parser based on maximum entropy models."},"booktitle":{"#tail":"\n","#text":"In EMNLP-97, The Second Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"A Ratnaparkhi"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proc. National Conference on Artificial Intelligence, pages 806? 813."},"#text":"\n","pages":{"#tail":"\n","#text":"806--813"},"marker":{"#tail":"\n","#text":"Roth, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ween them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997). The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001; Munoz et al, 1999). SNoW (Carleson et al, 1999; Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use i","@endWordPosition":"1410","@position":"8563","annotationId":"T34","@startWordPosition":"1409","@citStr":"Roth, 1998"}},"title":{"#tail":"\n","#text":"Learning to resolve natural language ambiguities: A unified approach."},"booktitle":{"#tail":"\n","#text":"In Proc. National Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"D Roth"}}}]}}]}}
