 for Computational Linguistics A Multimodal LDA Model Integrating Textual, Cognitive and Visual Modalities Stephen Roller Department of Computer Science The University of Texas at Austin roller@cs.utexas.edu Sabine Schulte im Walde Institut fu?r Maschinelle Sprachverarbeitung Universita?t Stuttgart schulte@ims.uni-stuttgart.de Abstract Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al, 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal models to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that
dy of work has been devoted to multimodal or ?grounded? models of language where semantic representations of words are extended to include perceptual information. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributiona
ion. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata,
re explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al,
on alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the gen
 for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line 
. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word me
st as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common cr
roaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional
pply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the 
structions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel com
ble actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational model
tion directions (Chen and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multi
en and Mooney, 2011) or robot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model 
ot commands (Tellex et al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The mo
al, 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was 
fforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al, 2011), text illustration (Joshi et al, 2006), or automatic location identification of Twitter users (Eisenstein et al, 2010; Wing and Baldridge, 2011; Roller et al, 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al, 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al, 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al, 2012a; Bruni et al, 2012b; Silberer et al, 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ?meaning of words is entirely given by other words? (Bruni et al, 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by 1146 Andrews et al (
 two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elici
del by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psy
porating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cog
e modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the 
nificant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the
 wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2
 of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the pa
ckr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al, 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulvermu?ller et al, 2005; Tettamanti et al, 2005; Aziz-Zadeh et al, 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-lingu
logic representations (Chen et al, 2010; Chen and Mooney, 2011; Matuszek et al, 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictio
ognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual
n representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new metho
ed to describe typical properties of the cue concept (e.g., McRae et al (2005)). Griffiths et al (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over th
 pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al, 1990) in the prediction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work t
ction of association norms. Andrews et al (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model
roach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of
s from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recent
ison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other wo
create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying 
l based model in word association and word similarity prediction. Bruni et al (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recogni
e using only vector concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We als
 concatenation. Bruni et al (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new ov
tself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text moda
l at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a la
sifiers, which have been immensely successful in object recognition (Farhadi et al, 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al, 2008; Regneri et al, 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al (2010) and Socher et al (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al, 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unpri
ed to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al, 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens. 3.2 Cognitive Modalities Association Norms (AN) is a collection of association norms collected by Schulte im Walde et al (2012). In association norm experiments, subjects are presented with a cue word and asked to list the fi
ality For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al, 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens. 3.2 Cognitive Modalities Association Norms (AN) is a collection of association norms collected by Schulte im Walde et al (2012). In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind. With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words. After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types. Feature Norms (FN) is our new collection of feature norms for a group of 569 German nouns. We present subjects on Amazon Mechanical Turk with a cue noun and ask them to give
final data set contains 11,714 cue-response pairs for 569 nouns and 2,589 response types. Note that the difference between association norms and feature norms is subtle, but important. In AN collection, subjects simply name related words as fast as possible, while in FN collection, subjects must carefully describe the cue. 3.3 Visual Modalities BilderNetle (?little ImageNet? in Swabian German) is our new data set of German noun-to-ImageNet synset mappings. ImageNet is a large-scale and widely used image database, built on top of WordNet, which maps words into groups of images, called synsets (Deng et al, 2009). Multiple synsets exist for each meaning of a word. For example, ImageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral. This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet. Starting with a set of noun compounds and their nominal constituents von der Heide and Borgwaldt (2009), five native German speakers and one native English speaker (including the authors of this paper) work together to map German nouns to ImageNet synsets. With the assistanc
After the collection of all the images, we extracted simple, low-level computer vision features to use as modalities in our experiments. First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (Bay et al, 2008). SURF is a method for selecting points-of-interest within an image. It is faster and more forgiving than the commonly known SIFT algorithm. We compute SURF keypoints for every image in our data set using SimpleCV3 and randomly sample 1% of the keypoints. The keypoints are clustered into 5,000 visual codewords (centroids) using k-means clustering (Sculley, 2010), and images are then quantized over the 5,000 codewords. All images for a given word are summed together to provide an average representation for the word. We refer to this representation as the SURF modality. While this is a standard, basic BoVW model, each individual codeword on its own may not provide a large degree of semantic information; typically a BoVW representation acts predominantly as a feature space for a classifier, and objects can only be recognize using collections of codewords. To test that similar concepts should share similar visual codewords, we cluster the BoVW representa
 feature space for a classifier, and objects can only be recognize using collections of codewords. To test that similar concepts should share similar visual codewords, we cluster the BoVW representations for all our images into 500 clusters with kmeans clustering, and represent each word as membership over the image clusters, forming the SURF Clusters modality. The number of clusters is chosen arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al, 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ?gist? of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modalit
ecognize using collections of codewords. To test that similar concepts should share similar visual codewords, we cluster the BoVW representations for all our images into 500 clusters with kmeans clustering, and represent each word as membership over the image clusters, forming the SURF Clusters modality. The number of clusters is chosen arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al, 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ?gist? of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered
r of clusters is chosen arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al, 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ?gist? of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully 
like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al, 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al, 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimod
ctors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al, 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al, 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al, 2003), or LDA, is an unsuper
 we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al, 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al, 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al, 2003), or LDA, is an unsupervised Bayesian probabilistic model of text documents. It assumes that all documents are probabilistically generated from a shared set ofK common
 et al, 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al, 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al, 2003), or LDA, is an unsupervised Bayesian probabilistic model of text documents. It assumes that all documents are probabilistically generated from a shared set ofK common topics, where each topic is a multinomial distribution over the vocabulary (notated as ?), and documents are modeled as mixtures of these shared topics (notated as ?). LDA assumes every document in the corpus is generated using the fol1149 lowing generative process: 1. A document-specific topic distribution, ?d ? Dir(?) is drawn. 2. For the ith word in the document, (a) A topic assignment zi ? ?d is drawn, (b) and a word wi ? ?z
 document in the corpus is generated using the fol1149 lowing generative process: 1. A document-specific topic distribution, ?d ? Dir(?) is drawn. 2. For the ith word in the document, (a) A topic assignment zi ? ?d is drawn, (b) and a word wi ? ?zi is drawn and observed. The task of Latent Dirichlet Allocation is then to automatically infer the latent document distribution ?d for each document d ? D, and the topic distribution ?k for each of the k = {1, . . . ,K} topics, given the data. The probability that the ith word of document d is p(wi, ?d) = ? k p(wi|?k)p(zi = k|?d). 4.2 Multimodal LDA Andrews et al (2009) extend LDA to allow for the inference of document and topic distributions in a multimodal corpus. In their model, a document consists of a set of (word, feature) pairs,4 rather than just words, and documents are still modeled as mixtures of shared topics. Topics consist of multinomial distributions over words, ?k, but are extended to also include multinomial distributions over features, ?k. The generative process is amended to include these feature distributions: 1. A document-specific topic distribution, ?d ? Dir(?) is drawn. 2. For the ith (word, feature) pair in the document, (a) A topic a
ing sense of feature. and features, and topics become the key link between the text and feature modalities. 4.3 3D Multimodal LDA We can easily extend the bimodal LDA model to incorporate three or more modalities by simply performing inference over n-tuples instead of pairs, and still mandating that each modality is conditionally independent given the topic. We consider the ith tuple (wi, fi, f ?i , . . .) in document d to have a conditional probability of: p(wi, fi, f ? i , . . . , ?d) = ? k p(wi|?i)p(fi|?k)p(f ? i |? ? i) ? ? ? p(zi = k|?d) That is, we simply take the original mLDA model of Andrews et al (2009) and generalize it in the same way they generalize LDA. At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi. 4.4 Hybrid Multimodal LDA 3D Multimodal LDA assumes that all modalities share the same latent topic structure, ?d. It is possible, however, that all modalities do not share some latent structure, but the modalities can still combine in o
oncatenation of the two distributions, ?FN&Sj,w = { ?FNj,w 1 ? j ? K FN , ?Sj?KFN ,w K FN < j ? KFN +KS , where KFN indicates the number of topics for the Feature Norm modality, and likewise for KS . 4.5 Inference Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. To solve these scaling issues, we implement Online Variational Bayesian Inference (Hoffman et al, 2010; Hoffman et al, 2012) for our models. In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is avail
wo distributions, ?FN&Sj,w = { ?FNj,w 1 ? j ? K FN , ?Sj?KFN ,w K FN < j ? KFN +KS , where KFN indicates the number of topics for the Feature Norm modality, and likewise for KS . 4.5 Inference Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. To solve these scaling issues, we implement Online Variational Bayesian Inference (Hoffman et al, 2010; Hoffman et al, 2012) for our models. In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source. 5
thm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source. 5 Experimental Setup 5.1 Generating Multimodal Corpora In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our nontextual modalities. We use the same method as Andrews et al (2009) for generating our multimodal corpora: for each word token in the text corpus, a feature is selected stochastically from the word?s feature distribution, creating a word-feature pair. Words without grounded features are all given the same placeholder feature, also resulting in a wordfeature pair.5 That is, for the feature norm modality, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc. The resulting stochastically generated corpus is used in its corresponding experiments. The 3D text-feature-association norm corpus is generated slightly di
ent. Note that, since KL divergence is a measure of dissimilarity, we use negative symmetric KL divergence so that our ? correlation coefficient is positive. For example, we compute both ?sKL(Ahornblatt, Ahorn) and ?sKL(Ahornblatt, Blatt), and so on for all 488 compound-constituent pairs, and then correlate these values with the human judgments. Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following Andrews et al (2009), we measure association norm prediction as an average of percentile ranks. For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., ?cat? is more similar to ?dog? than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once. 5.3 Model Selection and Hyperparameter Optimization In all settings, we fix all Dirichlet prio
s more similar to ?dog? than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once. 5.3 Model Selection and Hyperparameter Optimization In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents. We do not optimize these hyperparameters or vary them over time. The high Dirichlet priors are chosen to prevent sparsity in topic distributions, while the other parameters are selected as the best from Hoffman et al (2010). In order to optimize the number of topics K, we run five trials of each modality for 2000 iterations for K = {50, 100, 150, 200, 250} (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works usi
f topics K, we run five trials of each modality for 2000 iterations for K = {50, 100, 150, 200, 250} (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features (Andrews et al, 2009; Silberer and Lapata, 2012). We also see that the SURF visual words are able to provide notable, albeit not significant, improvements over the text-only modality. This confirms that the low-level BoVW features do carry semantic information, and are useful to consider individually. The GIST vectors, on the other hand, perform almost exactly the same as the text-only model. These features, which are usually more useful for comparing overall image likeness than object likeness, do not individually contain semantic information useful for compositionality prediction. The performance of the visual 
ve trials of each modality for 2000 iterations for K = {50, 100, 150, 200, 250} (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features (Andrews et al, 2009; Silberer and Lapata, 2012). We also see that the SURF visual words are able to provide notable, albeit not significant, improvements over the text-only modality. This confirms that the low-level BoVW features do carry semantic information, and are useful to consider individually. The GIST vectors, on the other hand, perform almost exactly the same as the text-only model. These features, which are usually more useful for comparing overall image likeness than object likeness, do not individually contain semantic information useful for compositionality prediction. The performance of the visual modalities reverses when we 
 helpful, but disjoint semantic information as association norms. 1153 We see that the image modalities are much more useful than they are in compositionality prediction. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of Bruni et al (2012b)?s suggestion that something like a distributional hypothesis of images is plausible. Once again, the clusters of images using SURF causes a dramatic drop in performance. Combined with the evidence from the compositionality assessment, this shows that the SURF clusters are actively confusing the models and not providing semantic information. GIST clusters, on the other hand, are providing a marginal improvement over the text-only model, but the result is not significant. We take a qualitative look into the GIST clusters in the next section. Once again, we see that the 3D models are ineffecti
ct which tells the time, or o?clock, as in We meet at 2 o?clock (?Wir treffen uns um 2 Uhr.?) The three prototypical pictures are not pictures of clocks, but round, detailed objects similar to clocks. We see GIST has a preference toward clustering images based on the predominant shape of the image. Here we see the clusters of GIST images are not providing a definite semantic relationship, but an overwhelming visual one. 8 Conclusions In this paper, we evaluated the role of low-level image features, SURF and GIST, for their compatibility with the multimodal Latent Dirichlet Allocation model of Andrews et al (2009). We found both fea1154 Most Probable Words Translations Prototypical Images Wasser water Schiff ship See lake Meer sea Meter meter Flu? river @card@ (number) Uhr clock Freitag Friday Sonntag Sunday Samstag Saturday Montag Monday Figure 1: Example topics with prototypical images for the Text + GIST Cluster modality. The first topic shows waterrelated words, as well scenes which often appear with water. The second shows clock-like objects, but not clocks. ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over trad
