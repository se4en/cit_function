{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"date":{"#tail":"\n","#text":"2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"any sentence that did not parse to a top level S containing at least one NP and one VP child. Even with such strong filters, we retained over 140K sentences for use as training data, and provide this exact set of parse trees for use in future work.2 Inspired by the application in language education, for our vocabulary list we use the English Vocabulary Profile (Capel, 2012), which predicts student vocabulary at different stages of learning English as a second language. We take the most basic American English vocabulary (the A1 list), and retrieve all inflections for each word using SimpleNLG (Gatt and Reiter, 2009), yielding a vocabulary of 1226 simple words and punctuation. To mitigate noise in the data, we discard any pair of context and outcome that appears only once in the training data, and estimate the parameters of the unconstrained model using EM. 6.1 Model Comparison We experimented with many generation models before converging on SPINEDEP, described in Figure 2, which we use in these experiments. 1http://simple.wikipedia.org 2data url anon for review We motivate two specfic constraints concernSPINEDEP unsmoothed otaint e m education whe aries d SPINEDEP WordNet a fixed vocabulary sch that only","@endWordPosition":"4720","@position":"26160","annotationId":"T1","@startWordPosition":"4717","@citStr":"Gatt and Reiter, 2009"}},"title":{"#tail":"\n","#text":"Simplenlg: A realisation engine for practical applications."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Albert Gatt and Ehud Reiter. 2009. Simplenlg: A realisation engine for practical applications. In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG \u201909, pages 90\u201393, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"90--93"},"marker":{"#tail":"\n","#text":"Gatt, Reiter, 2009"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG \u201909,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Albert Gatt"},{"#tail":"\n","#text":"Ehud Reiter"}]}},{"date":{"#tail":"\n","#text":"2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cent accuracy without wasting computation on backtracking or rejection. When the word inclusion constraint is introduced, we show clear empirical advantages over the simple solution of searching a large corpus for an appropriate sentence. 2 Related Work The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in its output. Examples such as ¨Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of materi","@endWordPosition":"749","@position":"4882","annotationId":"T2","@startWordPosition":"746","@citStr":"Greene et al., 2010"}},"title":{"#tail":"\n","#text":"Automatic analysis of rhythmic poetry with applications to generation and translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010. Automatic analysis of rhythmic poetry with applications to generation and translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201910, pages 524\u2013533, Stroudsburg, PA, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"524--533"},"marker":{"#tail":"\n","#text":"Greene, Bodrumlu, Knight, 2010"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Stroudsburg, PA, USA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201910,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Erica Greene"},{"#tail":"\n","#text":"Tugba Bodrumlu"},{"#tail":"\n","#text":"Kevin Knight"}]}},{"date":{"#tail":"\n","#text":"2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in its output. Examples such as ¨Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012), which deal with automatic generation of classic fill in the blank questions. Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences. 3 Freeform Generation For clarity in our discussion, we phrase the sentence generation process in the following general terms based around two classes of atomic units : contexts and outcomes. In order to specify a generation system, we must define 1. the set C of contexts c 2. the set O of outcomes o 3. the \u201cImply\u201d function I(c, o) \u2014* List[c E C] 4. M : derivation tree ;-± sentence whe","@endWordPosition":"869","@position":"5578","annotationId":"T3","@startWordPosition":"866","@citStr":"Mostow and Jang (2012)"}},"title":{"#tail":"\n","#text":"Generating diagnostic multiple choice comprehension cloze questions."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Jack Mostow and Hyeju Jang. 2012. Generating diagnostic multiple choice comprehension cloze questions. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 136\u2013146. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"136--146"},"marker":{"#tail":"\n","#text":"Mostow, Jang, 2012"},"publisher":{"#tail":"\n","#text":"Association"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jack Mostow"},{"#tail":"\n","#text":"Hyeju Jang"}]}},{"date":{"#tail":"\n","#text":"2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"roduced, we show clear empirical advantages over the simple solution of searching a large corpus for an appropriate sentence. 2 Related Work The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in its output. Examples such as ¨Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012), which deal","@endWordPosition":"769","@position":"4990","annotationId":"T4","@startWordPosition":"766","@citStr":"A et al., 2009"}},"title":{"#tail":"\n","#text":"Automatic generation of tamil lyrics for melodies."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Ananth Ramakrishnan A, Sankar Kuppan, and Sobha Lalitha Devi. 2009. Automatic generation of tamil lyrics for melodies. In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, pages 40\u201346. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"40--46"},"marker":{"#tail":"\n","#text":"A, Kuppan, Devi, 2009"},"publisher":{"#tail":"\n","#text":"Association"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ananth Ramakrishnan A"},{"#tail":"\n","#text":"Sankar Kuppan"},{"#tail":"\n","#text":"Sobha Lalitha Devi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing With Compositional Vector Grammars. In ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Socher, Bauer, Manning, Ng, 2013"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ists of dependents is a plausible sentence one might as wel pick one ative gals. Othr work sch as ours investig i i d nt nd m ny eg rp or the END symbol. The shaded rectangles contain the resultsuof Ih(c,oo) from thehindicated (c, o) pair. ing 6 Experiments We train our models on sentences drawn from the Simple English Wikipedia1. We obtained these ROOT VBZ NNS sentences from a data dump which we liberally fillikes dogs tered to remove items such as lists and sentences longer than 15 words or shorter then 3 words. We ROOT PRP VZ s ke parsed this data with the recently updated Stanford Parser (Socher et al., 2013) to Penn Treebank constituent form, and removed any sentence that did not parse to a top level S containing at least one NP and one VP child. Even with such strong filters, we retained over 140K sentences for use as training data, and provide this exact set of parse trees for use in future work.2 Inspired by the application in language education, for our vocabulary list we use the English Vocabulary Profile (Capel, 2012), which predicts student vocabulary at different stages of learning English as a second language. We take the most basic American English vocabulary (the A1 list), and retrieve","@endWordPosition":"4600","@position":"25490","annotationId":"T5","@startWordPosition":"4597","@citStr":"Socher et al., 2013"}},"title":{"#tail":"\n","#text":"Parsing With Compositional Vector Grammars."},"booktitle":{"#tail":"\n","#text":"In ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Richard Socher"},{"#tail":"\n","#text":"John Bauer"},{"#tail":"\n","#text":"Christopher D Manning"},{"#tail":"\n","#text":"Andrew Y Ng"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2013"},"rawString":{"#tail":"\n","#text":"Dekai Wu, Karteek Addanki, Markus Saers, and Meriem Beloucif. 2013. Learning to freestyle: Hip hop challenge-response induction via transduction rule segmentation. In EMNLP, pages 102\u2013112."},"#text":"\n","pages":{"#tail":"\n","#text":"102--112"},"marker":{"#tail":"\n","#text":"Wu, Addanki, Saers, Beloucif, 2013"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ord inclusion constraint is introduced, we show clear empirical advantages over the simple solution of searching a large corpus for an appropriate sentence. 2 Related Work The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in its output. Examples such as ¨Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Most","@endWordPosition":"764","@position":"4960","annotationId":"T6","@startWordPosition":"761","@citStr":"Wu et al., 2013"}},"title":{"#tail":"\n","#text":"Learning to freestyle: Hip hop challenge-response induction via transduction rule segmentation."},"booktitle":{"#tail":"\n","#text":"In EMNLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dekai Wu"},{"#tail":"\n","#text":"Karteek Addanki"},{"#tail":"\n","#text":"Markus Saers"},{"#tail":"\n","#text":"Meriem Beloucif"}]}}]}}}}
