er processing. Syntactic-semantic graph matching is used to produce a list of candidate assignments for 63.75% of the pairs analysed, and in 57% of situations the correct relations is one of the system?s suggestions; in 19.6% of situations it suggests only the correct relation. 1 Introduction When analysing texts, it is essential to see how elements of meaning are interconnected. This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1. He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1The sources date his work variously between the 5th and 7th century. role labelling (Baker et al, 1998; Kipper et al, 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al, 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one?s impressions of a text se
d, and in 57% of situations the correct relations is one of the system?s suggestions; in 19.6% of situations it suggests only the correct relation. 1 Introduction When analysing texts, it is essential to see how elements of meaning are interconnected. This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1. He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1The sources date his work variously between the 5th and 7th century. role labelling (Baker et al, 1998; Kipper et al, 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al, 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one?s impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, s
of situations the correct relations is one of the system?s suggestions; in 19.6% of situations it suggests only the correct relation. 1 Introduction When analysing texts, it is essential to see how elements of meaning are interconnected. This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1. He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1The sources date his work variously between the 5th and 7th century. role labelling (Baker et al, 1998; Kipper et al, 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al, 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one?s impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses
s is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1. He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1The sources date his work variously between the 5th and 7th century. role labelling (Baker et al, 1998; Kipper et al, 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al, 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one?s impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses, phrases, words to morphemes. In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning). Anecdotal sup
he first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1. He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1The sources date his work variously between the 5th and 7th century. role labelling (Baker et al, 1998; Kipper et al, 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al, 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one?s impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses, phrases, words to morphemes. In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning). Anecdotal support for this stance
ons between them goes back to the 5th century B.C. and the work of Panini1. He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1The sources date his work variously between the 5th and 7th century. role labelling (Baker et al, 1998; Kipper et al, 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al, 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one?s impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses, phrases, words to morphemes. In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning). Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for sema
back to the 5th century B.C. and the work of Panini1. He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1The sources date his work variously between the 5th and 7th century. role labelling (Baker et al, 1998; Kipper et al, 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al, 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one?s impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses, phrases, words to morphemes. In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning). Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (Misra, 19
ct connections at the semantic level (in other words, syntax carries meaning). Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (Misra, 1966; Gruber, 1965; Fillmore, 1968). Tesnie`re (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants ? for example, agent or instrument ? to such grammatical elements as subject, direct object, indirect object. This idea was expanded to include nouns and their modifiers through verb nominalizations (Chomsky, 1970; Quirk et al, 1985). We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser. Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers. For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun). It then tries to find among the previously processed instances another main element with a matching syntactic graph. If such a graph is found, then the system maps previously assigned semantic
at the semantic level (in other words, syntax carries meaning). Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (Misra, 1966; Gruber, 1965; Fillmore, 1968). Tesnie`re (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants ? for example, agent or instrument ? to such grammatical elements as subject, direct object, indirect object. This idea was expanded to include nouns and their modifiers through verb nominalizations (Chomsky, 1970; Quirk et al, 1985). We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser. Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers. For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun). It then tries to find among the previously processed instances another main element with a matching syntactic graph. If such a graph is found, then the system maps previously assigned semantic relations onto the 
r of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun). It then tries to find among the previously processed instances another main element with a matching syntactic graph. If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph. We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases. The list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent (Barker et al, 1997a). Section 2 overviews research in semantic relation analysis. Section 3 describes the text we used in ex81 periments, and the semantic relation list. Section 4 looks in detail at the graph-matching heuristic. Section 5 describes the experimental setting and shows how often the heuristic was used when processing the input text. We show in detail our findings about syntactic levels (how often graph matching helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6.
ic relation list. Section 4 looks in detail at the graph-matching heuristic. Section 5 describes the experimental setting and shows how often the heuristic was used when processing the input text. We show in detail our findings about syntactic levels (how often graph matching helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al, 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge base
and shows how often the heuristic was used when processing the input text. We show in detail our findings about syntactic levels (how often graph matching helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al, 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system?s interface facilitates the expert
d when processing the input text. We show in detail our findings about syntactic levels (how often graph matching helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al, 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system?s interface facilitates the expert?s task of creating and mani
ten graph matching helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al, 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system?s interface facilitates the expert?s task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary
ssign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al, 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system?s interface facilitates the expert?s task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semant
o clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al, 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system?s interface facilitates the expert?s task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, t
r) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al, 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system?s interface facilitates the expert?s task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles ? relations between verbs and th
ored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system?s interface facilitates the expert?s task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles ? relations between verbs and their arguments. Most approaches rely on VerbNet (Kipper et al, 2000) and FrameNet (Baker et al., 1998) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions (Carreras and Marquez, 
apture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system?s interface facilitates the expert?s task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles ? relations between verbs and their arguments. Most approaches rely on VerbNet (Kipper et al, 2000) and FrameNet (Baker et al., 1998) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions (Carreras and Marquez, 2004; Carreras and Marquez, 2005) and also (Gildea and Jurafsky, 2002; Pradhan et al, 2005; Shi and Mihalcea, 2005). These systems share two ideas which make them different from the approach presented here: they all analyse verb-argument relations, and they all use machine learning or probabilistic approaches (Pradhan et al, 2005) to assign a label to a new in
manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles ? relations between verbs and their arguments. Most approaches rely on VerbNet (Kipper et al, 2000) and FrameNet (Baker et al., 1998) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions (Carreras and Marquez, 2004; Carreras and Marquez, 2005) and also (Gildea and Jurafsky, 2002; Pradhan et al, 2005; Shi and Mihalcea, 2005). These systems share two ideas which make them different from the approach presented here: they all analyse verb-argument relations, and they all use machine learning or probabilistic approaches (Pradhan et al, 2005) to assign a label to a new instance. Labelling every instance relies on the same previously encoded knowledge (see (Carreras and Marquez, 2004; Carreras and Marquez, 2005) for an overview of the systems in the semantic role labelling competitions from 2004 and 2005). Pradhan et al (2005) combine the outputs of multiple parsers to extract reliable syntacti
es which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles ? relations between verbs and their arguments. Most approaches rely on VerbNet (Kipper et al, 2000) and FrameNet (Baker et al., 1998) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions (Carreras and Marquez, 2004; Carreras and Marquez, 2005) and also (Gildea and Jurafsky, 2002; Pradhan et al, 2005; Shi and Mihalcea, 2005). These systems share two ideas which make them different from the approach presented here: they all analyse verb-argument relations, and they all use machine learning or probabilistic approaches (Pradhan et al, 2005) to assign a label to a new instance. Labelling every instance relies on the same previously encoded knowledge (see (Carreras and Marquez, 2004; Carreras and Marquez, 2005) for an overview of the systems in the semantic role labelling competitions from 2004 and 2005). Pradhan et al (2005) combine the outputs of multiple parsers to extract reliable syntactic information, which is t
sing the current system configuration, the effect of syntactic information and incremental learning on semantic analysis. This is described in section 5. Because of these differences in the type of data used, and in the learning approach, the results we obtain cannot be compared to previous approaches. In order to show that the system does learn, we show that the number of examples for which it provides the correct answer increases with the number of examples previously analysed. 3 Input data and semantic relations 3.1 Input data We work with a semi-technical text on meteorological phenomena (Larrick, 1961), meant for primary school students. The text gradually introduces concepts related to precipitation, and explains them. Its nature makes it appropriate for the semantic analysis task in an incremental approach. The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text. The text contains 513 sentences, with an average length of 9.13 words. There are 4686 word tokens and 969 types. The difference between the number of types (2850) and tokens (573) in the extracted pairs (which contain only open-clas
ch. The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text. The text contains 513 sentences, with an average length of 9.13 words. There are 4686 word tokens and 969 types. The difference between the number of types (2850) and tokens (573) in the extracted pairs (which contain only open-class words) shows that the same concepts recur, as expected in a didactic text. The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, DIPETT (Delisle and Szpakowicz, 1995). The parser, written in Prolog, implements a classic constituency English grammar from Quirk et al (1985). Pairs of syntactic units connected by grammatical relations are extracted from the parse trees. A dependency parser would 82 produce a similar output, but DIPETT also provides verb subcategorization information (such as, for example, subject-verb-object or subject-verb-objectindirect object), which we use to select the (best) matching syntactic structures. To find pairs, we use simple structural information. If a unit is directly embedded in another unit, we assume a subordinate relation
rocess ideas introduced later in the text. The text contains 513 sentences, with an average length of 9.13 words. There are 4686 word tokens and 969 types. The difference between the number of types (2850) and tokens (573) in the extracted pairs (which contain only open-class words) shows that the same concepts recur, as expected in a didactic text. The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, DIPETT (Delisle and Szpakowicz, 1995). The parser, written in Prolog, implements a classic constituency English grammar from Quirk et al (1985). Pairs of syntactic units connected by grammatical relations are extracted from the parse trees. A dependency parser would 82 produce a similar output, but DIPETT also provides verb subcategorization information (such as, for example, subject-verb-object or subject-verb-objectindirect object), which we use to select the (best) matching syntactic structures. To find pairs, we use simple structural information. If a unit is directly embedded in another unit, we assume a subordinate relation between them; if the two units are coordinate, we assume a coordinate relation. These assumptions are saf
ystem will present an empty list, and expect the user to input the appropriate relation. When at least one relation is proposed, the user can accept a unique relation, choose among several options, or supply a new one. The system records which action took place, as well as the heuristic that generated the options presented to the user. The pair is also analysed to determine the syntactic level from which it came, to allow for a more detailed analysis of the behaviour of the system. 3.2 Semantic relations The list of semantic relations with which we work is based on extensive literature study (Barker et al, 1997a). Three lists of relations for three syntactic levels ? inter-clause, intra-clause (case) and nounmodifier relations ? were next combined based on syntactic and semantic phenomena. The resulting list is the one used in the experiments we present in this paper. The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier). 1. CAUSAL groups relations enabling or opposing an occurrence. Examples: cause - H causes M: u virus; effect - H is the effect (was caused by) M: exam anxiety; purpose - H is for M: concert hall; 2. CONJUN
consensus in the literature on a list of semantic relations that would work in all situations. This is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts. All the relations in the list we use were necessary, and sufficient, for the analysis of the input text. 4 Syntactic-semantic graph-matching Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text. This design idea was adopted from TANKA (Barker et al, 1997b). The only manually encoded knowledge is a dictionary of markers (subordinators, coordinators, prepositions). This resource does not affect the syntacticsemantic graph-matching heuristic. Because the system gradually accumulates knowledge as it goes through the input text, it uses a form of memory-based learning to make predictions about the semantic relation that fits the current pair. The type of knowledge that it accumulates consists of previously analysed pairs, together with the semantic relation assigned, and a syntactic-semantic graph centered on each word in a sentence which appears 
use the system gradually accumulates knowledge as it goes through the input text, it uses a form of memory-based learning to make predictions about the semantic relation that fits the current pair. The type of knowledge that it accumulates consists of previously analysed pairs, together with the semantic relation assigned, and a syntactic-semantic graph centered on each word in a sentence which appears as the main element in a processed pair. To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P . This idea was inspired by Delisle et al (1993), who used a list of arguments surrounding the main verb together with the verb?s subcategorization information and previously processed examples to analyse semantic roles (case relations). In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004; Carreras and Marquez, 2005). Our system builds a (simple) graph surrounding a head word (which may be a verb ? representing the predicate of a sentence, or repre
