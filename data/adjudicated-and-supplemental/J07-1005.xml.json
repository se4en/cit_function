{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","listItem":[{"#tail":"\n","@confidence":"0.844071285714286","#text":"\n? Department of Computer Science and Institute for Advanced Computer Studies. E-mail:\ndemner@umd.edu.\n? College of Information Studies, Department of Computer Science, and Institute for Advanced Computer\nStudies. E-mail: jimmylin@umd.edu.\nSubmission received: 4 July 2005; revised submission received: 7 January 2006; accepted for publication:\n12 April 2006.\n? 2007 Association for Computational Linguistics\n"},{"#tail":"\n","@confidence":"0.6709594","#text":"\nmentary facets that we attempt to encode in a question-answering system for clinical\ndecision support.\n3. MEDLINE and PubMed\nMEDLINE is a large bibliographic database maintained by the U.S. National Library\nof Medicine (NLM). This database is viewed by medical professionals, biomedical\n"},{"#tail":"\n","@confidence":"0.912447","#text":"\non a variety of other factors (see Section 10 for further discussion). Each citation was\nassigned one of four labels:\n Contains answer: The citation directly contains information that answers\nthe question.\n Relevant: The citation does not directly answer the question, but provides\ntopically relevant information.\n Partially relevant: The citation provides information that is marginally\nrelevant.\n Not relevant: The citation does not provide any topically relevant\n"},{"#tail":"\n","@confidence":"0.95927375","#text":"\n The baseline PubMed results.\n A term-based reranker that computes term overlap between the natural\nlanguage question and the citation (i.e., counted words shared between the\ntwo strings). Each term match was weighted by the outcome score of the\nsentence from which it came (see Section 5.7). This simple algorithm favors\nterm matches that occur in sentences recognized as outcome statements.\n A reranker based on the EBM scorer described in the previous section.\n A reranker that combines normalized scores from the term-based reranker\n"},{"#tail":"\n","@confidence":"0.634601","#text":"\n Precision at ten retrieved documents (P10) measures the fraction of\nrelevant documents in the top ten results.\n Mean Average Precision (MAP) is the average of precision values after\n"},{"#tail":"\n","@confidence":"0.996686","#text":"\n(b) Performance across all clinical tasks\n Significance at the 1% level, depending on direction of change.\n Significance at the 5% level, depending on direction of change.\n"},{"#tail":"\n","@confidence":"0.785403","#text":"\n(b) Performance across all clinical tasks\n"},{"#tail":"\n","@confidence":"0.989875","#text":"\n(b) Performance across all clinical tasks\n Significance at the 1% level, depending on direction of change.\n Significance at the 5% level, depending on direction of change.\n?Difference not statistically significant.\n8. Optimizing Citation Scoring\n"},{"#tail":"\n","@confidence":"0.995177666666667","#text":"\n(b) Performance across all clinical tasks\n Significance at the 1% level, depending on direction of change.\n Significance at the 5% level, depending on direction of change.\n"},{"#tail":"\n","@confidence":"0.953963","#text":"\n(b) Strict Scoring\n Significance at the 1% level, depending on direction of change.\n Significance at the 5% level, depending on direction of change.\n"},{"#tail":"\n","@confidence":"0.8994348","#text":"\nthree-valued judgments:\n A plus (+) indicates that the response directly answers the question.\nNaturally, the physicians would need to follow up and examine the source\ncitation in more detail.\n A check (\n?\n) indicates that the response provides clinically relevant\ninformation that may factor into decisions about patient treatment, and\nthat the source citation was worth examining in more detail.\n A minus (?) indicates that the response does not provide useful\n"}],"figure":[{"#tail":"\n","@confidence":"0.99146547826087","#text":"\n???????????\notherwise\n????????\nhealthy\n?????????\nchildren\n?????\naged\n??\n2\n???\nto\n???\n12\n??????\nyearsPopulation with\n??????\nacute,\n?????????????\nintercurrent,\n??????\nfebrile\n???????\n"},{"#tail":"\n","@confidence":"0.9983796","#text":"\n???\nAll\n?????\nthree\n??????\nactive\n???????????\ntreatments\n??????????\nproduced\n??????????\nsignificant\n???????????\nantipyresis\n???????????\ncompared\n????\nwith\n?????????\nplacebo.Outcome\n??????????\nIbuprofen\n??????????\nprovided\n???????\ngreater\n?????????????\ntemperature\n??????????\ndecrement\n?????\nand\n???????\nlonger\n????????\nduration\n???\nof\n???????????\nantipyresis\n?????\nthan\n???????????????\nacetaminophen\n??????\nwhen\n???\nthe\n????\ntwo\n??????\ndrugs\n?????\nwere\n??????????????\nadministered\n??\nin\n???????????????\napproximately\n??????\nequal\n??????\ndoses.Outcome No adverse effects were observed in any treat-\nment group. CONCLUSION?\n?????????\nIbuprofen\n???\nis\n??\na\n????????\npotent\n???????????\nantipyretic\n???????\nagent\n?????\nand\n??\nis\n???\na\n????\nsafe\n???????????\nalternative\n????\nfor\n????\nthe\n?????????\nselected\n???????\nfebrile\n??????\nchild\n?????\nwho\n?????\nmay\n????????\nbenefit\n?????\nfrom\n????????????\nantipyretic\n???????????\nmedication\n????\nbut\n?????\nwho\n??????\neither\n???????\ncannot\n????\ntake\n???\nor\n?????\ndoes\n????\nnot\n????????\nachieve\n???????????\nsatisfactory\n????????????\nantipyresis\n????\nwith\n????????????????\nacetaminophen.Outcome\n"},{"#tail":"\n","@confidence":"0.6656968","#text":"\n(((?analgesics?[TIAB] NOT Medline[SB]) OR ?analgesics?[MeSH Terms] OR\n?analgesics?[Pharmacological Action] OR analgesic[Text Word]) AND\n((?headache?[TIAB] NOT Medline[SB]) OR ?headache?[MeSH Terms] OR\nheadaches[Text Word]) AND (?adverse effects?[Subheading] OR side effects[Text\nWord])) AND hasabstract[text] AND English[Lang] AND ?humans?[MeSH Terms]\n"}],"author":[{"#tail":"\n","@confidence":"0.970085","#text":"\nDina Demner-Fushman?\n"},{"#tail":"\n","@confidence":"0.979465","#text":"\nJimmy Lin?\n"}],"equation":[{"#tail":"\n","@confidence":"0.9945308","#text":"\nMH - Acetaminophen/*therapeutic use\nMH - Child\nMH - Comparative Study\nMH - Fever/*drug therapy\nMH - Ibuprofen/*therapeutic use\n"},{"#tail":"\n","@confidence":"0.790658","#text":"\nSoutcome = ?1Scues + ?2Sunigram + ?3Sn-gram + ?4Sposition + ?5Slength + ?6Ssemantic type (1)\n"},{"#tail":"\n","@confidence":"0.9995808","#text":"\nLR(x) =\nN\n?\nk=1\n?kPk(X) (2)\n"},{"#tail":"\n","@confidence":"0.957713","#text":"\nSEBM = SPICO + SSoE + Stask (3)\n"},{"#tail":"\n","@confidence":"0.618151","#text":"\nSPICO = Sproblem + Spopulation + Sintervention + Soutcome (4)\n"},{"#tail":"\n","@confidence":"0.400467","#text":"\nSSoE = Sjournal + Sstudy + Sdate (5)\n"},{"#tail":"\n","@confidence":"0.544326","#text":"\nSdate = (yearpublication ? yearcurrent )/100 (6)\n"},{"#tail":"\n","@confidence":"0.99597675","#text":"\nStask =\n?\nt?MeSH\n?(t) (7)\n"},{"#tail":"\n","@confidence":"0.988461","#text":"\nSEBM = ?1SPICO + ?2SSoE + (1 ? ?1 ? ?2)Stask (8)\n"},{"#tail":"\n","@confidence":"0.99396075","#text":"\nStask =\n?\nt?MeSH\n?(t) (9)\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.999162","#text":"\n5.1 Population Extractor\n"},{"#tail":"\n","@confidence":"0.999786","#text":"\n5.2 Evaluation of Population Extractor\n"},{"#tail":"\n","@confidence":"0.999287","#text":"\n5.3 Problem Extractor\n"},{"#tail":"\n","@confidence":"0.998812","#text":"\n5.4 Evaluation of Problem Extractor\n"},{"#tail":"\n","@confidence":"0.987885","#text":"\n5.5 Intervention Extractor\n"},{"#tail":"\n","@confidence":"0.997531","#text":"\n5.6 Evaluation of Intervention Extractor\n"},{"#tail":"\n","@confidence":"0.95863","#text":"\n5.7 Outcome Extractor\n"},{"#tail":"\n","@confidence":"0.989674","#text":"\n5.8 Evaluation of Outcome Extractor\n"},{"#tail":"\n","@confidence":"0.760178","#text":"\n5.9 Determining the Strength of Evidence\n"},{"#tail":"\n","@confidence":"0.98172","#text":"\n6.1 Scores Based on PICO Elements\n"},{"#tail":"\n","@confidence":"0.825669","#text":"\n6.2 Scores Based on Strength of Evidence\n"},{"#tail":"\n","@confidence":"0.963544","#text":"\n6.3 Scores Based on Specific Tasks\n"},{"#tail":"\n","@confidence":"0.881157","#text":"\nMedicine Research Participation Program\n"}],"subsubsectionHeader":[{"#tail":"\n","@confidence":"0.609797","#text":"\n5.10 Sample Output\n"},{"#tail":"\n","@confidence":"0.683925","#text":"\nPubMed?s Clinical Queries filters; examples include drug administration routes and any\n"}],"footnote":[{"#tail":"\n","@confidence":"0.925359666666667","#text":"\nSupplementary Concept Records (additional chemical substance names) within a\n1 http://www.nlm.nih.gov/pubs/factsheets/medline.html\n2 Commonly referred to as MeSH terms or MeSH headings, although technically the latter is redundant.\n"},{"#tail":"\n","@confidence":"0.510413","#text":"\n3 MetaMap does provide alternative mappings, but the current extractor only considers the best candidate.\n"},{"#tail":"\n","@confidence":"0.817545","#text":"\nmethod using confidence values generated by the base classifiers and least squares lin-\n4 http://mallet.cs.umass.edu/\n"},{"#tail":"\n","@confidence":"0.81057","#text":"\n5 http://math.nist.gov/javanumerics/jama/\n"},{"#tail":"\n","@confidence":"0.989578","#text":"\n6 http://www.jfponline.com/\n7 http://www.parkhurstexchange.com/qa/\n"}],"title":{"#tail":"\n","@confidence":"0.873273","#text":"\nAnswering Clinical Questions with\nKnowledge-Based and Statistical Techniques\n"},"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.899473555555556","#text":"\nof the American Medical Informatics\nAssociation (AMIA 2001), pages 17?21,\nPortland, OR.\nAronson, Alan R., James G. Mork,\nClifford W. Gay, Susanne M. Humphrey,\nand Willie J. Rogers. 2004. The NLM\nIndexing Initiative?s Medical Text\nIndexer. In Proceedings of the 11th\nWorld Congress on Medical Informatics\n"},{"#tail":"\n","@confidence":"0.994346462184874","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\n(MEDINFO 2004), pages 268?272,\nSan Francisco, CA.\nAronson, Alan R., Thomas C. Rindflesch,\nand Allen C. Browne. 1994. Exploiting a\nlarge thesaurus for information retrieval.\nIn Proceedings of RIAO 1994: Intelligent\nMultimedia Information Retrieval Systems\nand Management, pages 197?216, New York.\nBaeza-Yates, Ricardo and Berthier\nRibeiro-Neto. 1999. Modern Information\nRetrieval. ACM Press, New York.\nBarry, Carol and Linda Schamber. 1998.\nUsers? criteria for relevance evaluation:\nA cross-situational comparison.\nInformation Processing and Management,\n34(2/3):219?236.\nBooth, Andrew. 2000. Formulating the\nquestion. In Andrew Booth and Graham\nWalton, editors, Managing Knowledge in\nHealth Services. Library Association\nPublishing, London, England.\nChambliss, M. Lee and Jennifer Conley. 1996.\nAnswering clinical questions. The Journal of\nFamily Practice, 43:140?144.\nCogdill, Keith W. and Margaret E. Moore.\n1997. First-year medical students?\ninformation needs and resource selection:\nResponses to a clinical scenario. Bulletin of\nthe Medical Library Association, 85(1):51?54.\nCovell, David G., Gwen C. Uman, and\nPhil R. Manning. 1985. Information needs\nin office practice: Are they being met?\nAnnals of Internal Medicine, 103(4):596?599.\nDe Groote, Sandra L. and Josephine L.\nDorsch. 2003. Measuring use patterns\nof online journals and databases.\nJournal of the Medical Library Association,\n91(2):231?240.\nDemner-Fushman, Dina, Barbara Few,\nSusan E. Hauser, and George Thoma. 2006.\nAutomatically identifying health outcome\ninformation in MEDLINE records. Journal\nof the American Medical Informatics\nAssociation, 13(1):52?60.\nDemner-Fushman, Dina and Jimmy Lin.\n2005. Knowledge extraction for clinical\nquestion answering: Preliminary results.\nIn Proceedings of the AAAI-05 Workshop on\nQuestion Answering in Restricted Domains,\npages 1?10, Pittsburgh, PA.\nDemner-Fushman, Dina and Jimmy Lin.\n2006. Answer extraction, semantic\nclustering, and extractive summarization\nfor clinical question answering. In\nProceedings of the 21st International\nConference on Computational Linguistics and\n44th Annual Meeting of the Association for\nComputational Linguistics (COLING/ACL\n2006), pages 841?848, Sydney, Australia.\nEbell, Mark H., Jay Siwek, Barry D. Weiss,\nSteven H. Woolf, Jeffrey Susman, Bernard\nEwigman, and Marjorie Bowman. 2004.\nStrength of Recommendation Taxonomy\n(SORT): A patient-centered approach to\ngrading evidence in the medical literature.\nThe Journal of the American Board of Family\nPractice, 17(1):59?67.\nElhadad, Noemie, Min-Yen Kan, Judith\nKlavans, and Kathleen McKeown. 2005.\nCustomization in a unified framework for\nsummarizing medical literature. Journal of\nArtificial Intelligence in Medicine,\n33(2):179?198.\nEly, John W., Jerome A. Osheroff, Mark H.\nEbell, George R. Bergus, Barcey T. Levy,\nM. Lee Chambliss, and Eric R. Evans. 1999.\nAnalysis of questions asked by family\ndoctors regarding patient care. BMJ,\n319:358?361.\nEly, John W., Jerome A. Osheroff, M. Lee\nChambliss, Mark H. Ebell, and Marcy E.\nRosenbaum. 2005. Answering physicians?\nclinical questions: Obstacles and potential\nsolutions. Journal of the American Medical\nInformatics Association, 12(2):217?224.\nFiszman, Marcelo, Thomas C. Rindflesch,\nand Halil Kilicoglu. 2004. Abstraction\nsummarization for managing the\nbiomedical research literature. In\nProceedings of the HLT/NAACL 2004\nWorkshop on Computational Lexical\nSemantics, pages 76?83, Boston, MA.\nGorman, Paul N., Joan S. Ash, and\nLeslie W. Wykoff. 1994. Can primary care\nphysicians? questions be answered using\nthe medical journal literature? Bulletin of\nthe Medical Library Association,\n82(2):140?146.\nHaynes, R. Brian, Nancy Wilczynski, K. Ann\nMcKibbon, Cynthia J. Walker, and John C.\nSinclair. 1994. Developing optimal search\nstrategies for detecting clinically sound\nstudies in MEDLINE. Journal of the\nAmerican Medical Informatics Association,\n1(6):447?458.\nHearst, Marti A. 1996. Improving full-text\nprecision on short queries using simple\nconstraints. In Proceedings of the Fifth\nAnnual Symposium on Document Analysis\nand Information Retrieval (SDAIR 1996),\npages 217?232, Las Vegas, NV.\nHersh, William, Ravi Teja Bhupatiraju,\nand Sarah Corley. 2004. Enhancing\naccess to the bibliome: The TREC\ngenomics track. In Proceedings of the\n11th World Congress on Medical Informatics\n(MEDINFO 2004), pages 773?777,\nSan Francisco, CA.\n"},{"#tail":"\n","@confidence":"0.999759117647058","#text":"\nComputational Linguistics Volume 33, Number 1\nHersh, William, Aaron Cohen, Jianji Yang,\nRavi Teja Bhupatiraju1, Phoebe Roberts,\nand Marti Hearst. 2005. TREC 2005\ngenomics track overview. In Proceedings\nof the Fourteenth Text REtrieval Conference\n(TREC 2005), Gaithersburg, MD.\nHildebrandt, Wesley, Boris Katz, and Jimmy\nLin. 2004. Answering definition questions\nwith multiple knowledge sources. In\nProceedings of the 2004 Human Language\nTechnology Conference and the North\nAmerican Chapter of the Association\nfor Computational Linguistics Annual\nMeeting (HLT/NAACL 2004), pages 49?56,\nBoston, MA.\nHirschman, Lynette and Robert Gaizauskas.\n2001. Natural language question\nanswering: The view from here. Natural\nLanguage Engineering, 7(4):275?300.\nHuang, Xiaoli, Jimmy Lin, and Dina\nDemner-Fushman. 2006. Evaluation of\nPICO as a knowledge representation for\nclinical questions. In Proceeding of the 2006\nAnnual Symposium of the American Medical\nInformatics Association (AMIA 2006),\npages 359?363, Washington, D.C.\nIngwersen, Peter. 1999. Cognitive\ninformation retrieval. Annual Review of\nInformation Science and Technology, 34:3?52.\nJacquemart, Pierre and Pierre Zweigenbaum.\n2003. Towards a medical\nquestion-answering system: A feasibility\nstudy. In Robert Baud, Marius Fieschi,\nPierre Le Beux, and Patrick Ruch, editors,\nThe New Navigators: From Professionals to\nPatients, volume 95 of Actes Medical\nInformatics Europe, Studies in Health\nTechnology and Informatics. IOS Press,\nAmsterdam, pages 463?468.\nKauffman, Ralph E., L. A. Sawyer, and\nM. L. Scheinbaum. 1992. Antipyretic\nefficacy of ibuprofen vs acetaminophen.\nAmerican Journal of Diseases of Children,\n146(5):622?625.\nLight, Marc, Xin Ying Qiu, and Padmini\nSrinivasan. 2004. The language of\nbioscience: Facts, speculations, and\nstatements in between. In Proceedings of the\nBioLink 2004 Workshop at HLT/NAACL\n2004, pages 17?24, Boston, MA.\nLin, Jimmy and Dina Demner-Fushman.\n2005a. Automatically evaluating answers\nto definition questions. In Proceedings of the\n2005 Human Language Technology Conference\nand Conference on Empirical Methods in\nNatural Language Processing (HLT/EMNLP\n2005), pages 931?938, Vancouver, Canada.\nLin, Jimmy and Dina Demner-Fushman.\n2005b. Evaluating summaries and\nanswers: Two sides of the same coin? In\nProceedings of the ACL 2005 Workshop on\nIntrinsic and Extrinsic Evaluation Measures\nfor MT and/or Summarization, pages 41?48,\nAnn Arbor, MI.\nLin, Jimmy and Dina Demner-Fushman.\n2006a. The role of knowledge in\nconceptual retrieval: A study in the\ndomain of clinical medicine. In Proceedings\nof the 29th Annual International ACM SIGIR\nConference on Research and Development in\nInformation Retrieval (SIGIR 2006),\npages 99?106, Seattle, WA.\nLin, Jimmy and Dina Demner-Fushman.\n2006b. Will pyramids built of nuggets\ntopple over? In Proceedings of the 2006\nHuman Language Technology Conference\nand North American Chapter of the\nAssociation for Computational Linguistics\nAnnual Meeting (HLT/NAACL 2006),\npages 383?390, New York.\nLin, Jimmy, Damianos Karakos, Dina\nDemner-Fushman, and Sanjeev\nKhudanpur. 2006. Generative content\nmodels for structural analysis of medical\nabstracts. In Proceedings of the HLT/\nNAACL 2006 Workshop on Biomedical\nNatural Language Processing (BioNLP?06),\npages 65?72, New York.\nLin, Jimmy, Dennis Quan, Vineet Sinha,\nKarun Bakshi, David Huynh, Boris Katz,\nand David R. Karger. 2003. What makes a\ngood answer? The role of context in\nquestion answering. In Proceedings of the\nNinth IFIP TC13 International Conference on\nHuman-Computer Interaction (INTERACT\n2003), pages 25?32, Zu?rich, Switzerland.\nLindberg, Donald A., Betsy L. Humphreys,\nand Alexa T. McCray. 1993. The Unified\nMedical Language System. Methods of\nInformation in Medicine, 32(4):281?291.\nMcCray, Alexa T., Anita Burgun, and Olivier\nBodenreider. 2001. Aggregating UMLS\nsemantic types for reducing conceptual\ncomplexity. In Proceedings of 10th World\nCongress on Medical Informatics (MEDINFO\n2001), pages 216?220, London, England.\nMcKeown, Kathleen, Noemie Elhadad,\nand Vasileios Hatzivassiloglou. 2003.\nLeveraging a common representation for\npersonalized search and summarization\nin a medical digital library. In Proceedings\nof the 3rd ACM/IEEE Joint Conference on\nDigital Libraries (JCDL 2003), pages\n159?170, Houston, TX.\nMcKnight, Larry and Padmini Srinivasan.\n2003. Categorization of sentence types\nin medical abstracts. In Proceeding of the\n2003 Annual Symposium of the American\n"},{"#tail":"\n","@confidence":"0.999488660869565","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nMedical Informatics Association (AMIA\n2003), pages 440?444, Washington, D.C.\nMeadow, Charles T., Barbara A. Cerny,\nChristine L. Borgman, and Donald O.\nCase. 1989. Online access to knowledge:\nSystem design. Journal of the American\nSociety for Information Science, 40(2):86?98.\nMendonc?a, Eneida A. and James J. Cimino.\n2001. Building a knowledge base to\nsupport a digital library. In Proceedings\nof 10th World Congress on Medical\nInformatics (MEDINFO 2001),\npages 222?225, London, England.\nMladenic, Dunja and Marko Grobelnik. 1999.\nFeature selection for unbalanced class\ndistribution and Na??ve Bayes. In\nProceedings of the Sixteenth International\nConference on Machine Learning (ICML\n1999), pages 258?267, Bled, Slovenia.\nNenkova, Ani and Rebecca Passonneau.\n2004. Evaluating content selection in\nsummarization: The pyramid method. In\nProceedings of the 2004 Human Language\nTechnology Conference and the North\nAmerican Chapter of the Association for\nComputational Linguistics Annual Meeting\n(HLT/NAACL 2004), pages 145?152,\nBoston, MA.\nNiu, Yun and Graeme Hirst. 2004. Analysis\nof semantic classes in medical text for\nquestion answering. In Proceedings of the\nACL 2004 Workshop on Question Answering\nin Restricted Domains, pages 54?61,\nBarcelona, Spain.\nPratt, Wanda and Meliha Yetisgen-Yildiz.\n2003. A study of biomedical concept\nidentification: MetaMap vs. people.\nIn Proceeding of the 2003 Annual\nSymposium of the American Medical\nInformatics Association (AMIA 2003),\npages 529?533, Washington, D.C.\nRichardson, W. Scott, Mark C. Wilson, James\nNishikawa, and Robert S. Hayward. 1995.\nThe well-built clinical question: A key to\nevidence-based decisions. American College\nof Physicians Journal Club, 123(3):A12?A13.\nRinaldi, Fabio, James Dowdall, Gerold\nSchneider, and Andreas Persidis. 2004.\nAnswering questions in the genomics\ndomain. In Proceedings of the ACL 2004\nWorkshop on Question Answering in\nRestricted Domains, pages 46?53,\nBarcelona, Spain.\nRindflesch, Thomas C. and Marcelo Fiszman.\n2003. The interaction of domain\nknowledge and linguistic structure in\nnatural language processing: Interpreting\nhypernymic propositions in biomedical\ntext. Journal of Biomedical Informatics,\n36(6):462?477.\nSackett, David L., Sharon E. Straus, W. Scott\nRichardson, William Rosenberg, and\nR. Brian Haynes. 2000. Evidence-Based\nMedicine: How to Practice and Teach EBM,\nsecond edition. Churchill Livingstone,\nEdinburgh, Scotland.\nSaracevic, Tefko. 1975. Relevance: A review\nof and a framework for thinking on the\nnotion in information science. Journal of the\nAmerican Society for Information Science,\n26(6):321?343.\nSneiderman, Charles, Dina\nDemner-Fushman, Marcelo Fiszman, and\nThomas C. Rindflesch. 2005. Semantic\ncharacteristics of MEDLINE citations\nuseful for therapeutic decision-making.\nIn Proceeding of the 2005 Annual Symposium\nof the American Medical Informatics\nAssociation (AMIA 2005), page 1117,\nWashington, D.C.\nTbahriti, Imad, Christine Chichester,\nFre?de?rique Lisacek, and Patrick Ruch.\n2006. Using argumentation to retrieve\narticles with similar citations: An inquiry\ninto improving related articles search in\nthe MEDLINE digital library. International\nJournal of Medical Informatics, 75(6):488?495.\nTing, Kai Ming and Ian H. Witten. 1999.\nIssues in stacked generalization. Journal of\nArtificial Intelligence Research, 10:271?289.\nVoorhees, Ellen M. 2003. Overview of the\nTREC 2003 question answering track. In\nProceedings of the Twelfth Text REtrieval\nConference (TREC 2003), pages 54?68,\nGaithersburg, MD.\nVoorhees, Ellen M. and Dawn M. Tice.\n1999. The TREC-8 question answering\ntrack evaluation. In Proceedings of the\nEighth Text REtrieval Conference (TREC-8),\npages 83?106, Gaithersburg, MD.\nWilczynski, Nancy, K. Ann McKibbon, and\nR. Brian Haynes. 2001. Enhancing retrieval\nof best evidence for health care from\nbibliographic databases: Calibration\nof the hand search of the literature. In\nProceedings of 10th World Congress on\nMedical Informatics (MEDINFO 2001),\npages 390?393, London, England.\nYang, Yiming and Jan O. Pedersen. 1997.\nA comparative study on feature selection\nin text categorization. In Proceedings\nof the Fourteenth International Conference\non Machine Learning (ICML 1997),\npages 412?420, Nashville, TN.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.9907455","#text":"\nThe combination of recent developments in question-answering research and the availability of\nunparalleled resources developed specifically for automatic semantic processing of text in the\nmedical domain provides a unique opportunity to explore complex question answering in the\ndomain of clinical medicine. This article presents a system designed to satisfy the information\nneeds of physicians practicing evidence-based medicine. We have developed a series of knowl-\nedge extractors, which employ a combination of knowledge-based and statistical techniques, for\nautomatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted\nelements serve as the input to an algorithm that scores the relevance of citations with respect to\nstructured representations of information needs, in accordance with the principles of evidence-\nbased medicine. Starting with an initial list of citations retrieved by PubMed, our system\ncan bring relevant abstracts into higher ranking positions, and from these abstracts generate\nresponses that directly answer physicians? questions. We describe three separate evaluations: one\nfocused on the accuracy of the knowledge extractors, one conceptualized as a document reranking\ntask, and finally, an evaluation of answers by two physicians. Experiments on a collection\nof real-world clinical questions show that our approach significantly outperforms the already\ncompetitive PubMed baseline.\n"},{"#tail":"\n","@confidence":"0.923806636363636","#text":"\nRecently, the focus of question-answering research has shifted away from simple fact-\nbased questions that can be answered with relatively little linguistic knowledge to\n?harder? questions that require fine-grained text analysis, reasoning capabilities, and\nthe ability to synthesize information from multiple sources. General purpose reasoning\non anything other than superficial lexical relations is exceedingly difficult because there\nis a vast amount of world knowledge that must be encoded, either manually or auto-\nmatically, to overcome the brittleness often associated with long chains of evidence. This\nsituation poses a serious bottleneck to ?advanced? question-answering systems. How-\never, the availability of existing knowledge sources and ontologies in certain domains\nprovides exciting opportunities to experiment with knowledge-rich approaches. How\nmight one go about leveraging these resources effectively? How might one integrate\n"},{"#tail":"\n","@confidence":"0.988833705882353","#text":"\nComputational Linguistics Volume 33, Number 1\nstatistical techniques to overcome the brittleness often associated with knowledge-\nbased approaches?\nWe explore these interesting research questions in the domain of medicine, fo-\ncusing on the information needs of physicians in clinical settings. This domain is\nwell-suited for exploring the posed research questions for several reasons. First,\nsubstantial understanding of the domain has already been codified in the Unified\nMedical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Sec-\nond, software for utilizing this ontology already exists: MetaMap (Aronson 2001)\nidentifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts\nrelations between the concepts. Both systems utilize and propagate semantic infor-\nmation from UMLS knowledge sources: the Metathesaurus, the Semantic Network,\nand the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used\nin this work) contains information about over 1 million biomedical concepts and\n5 million concept names from more than 100 controlled vocabularies. The Seman-\ntic Network provides a consistent categorization of all concepts represented in the\nUMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al\n2000) provides a task-based model of the clinical information-seeking process. The\nPICO framework (Richardson et al 1995) for capturing well-formulated clinical queries\n(described in Section 2) can serve as the basis of a knowledge representation that\nbridges the needs of clinicians and analytical capabilities of a system. The conflu-\nence of these many factors makes clinical question answering a very exciting area of\nresearch.\nFurthermore, the need to answer questions related to patient care at the point of\nservice has been well studied and documented (Covell, Uman, and Manning 1985;\nGorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative\nrepository of abstracts from the medical and biomedical primary literature maintained\nby the National Library of Medicine, provides the clinically relevant sources for answer-\ning physicians? questions, and is commonly used in that capacity (Cogdill and Moore\n1997; De Groote and Dorsch 2003). However, studies have shown that existing systems\nfor searching MEDLINE (such as PubMed, the search service provided by the National\nLibrary of Medicine) are often inadequate and unable to supply clinically relevant\nanswers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley\n1996). Furthermore, it is clear that traditional document retrieval technology applied\nto MEDLINE abstracts is insufficient for satisfactory information access; research and\nexperience point to the need for systems that automatically analyze text and return\nonly the relevant information, appropriately summarizing and fusing segments from\nmultiple texts. Not only is clinical question answering interesting from a research\nperspective, it also represents a potentially high-impact, real-world application of lan-\nguage processing and information retrieval technology?better information systems to\nprovide decision support for physicians have the potential to improve the quality of\nhealth care.\nOur question-answering system supports the practice of evidence-based medi-\ncine (EBM), a widely accepted paradigm for medical practice that stresses the impor-\ntance of evidence from patient-centered clinical research in the health care process.\nEBM prescribes an approach to structuring clinical information needs and identi-\nfies elements (for example, the problem at hand and the interventions under con-\nsideration) that factor into the assessment of clinically relevant studies for medical\npractice. The foundation of our question-answering strategy is built on knowledge\nextractors that automatically identify these elements in MEDLINE abstracts. Using\nthese knowledge extractors, we have developed algorithms for scoring the relevance\n"},{"#tail":"\n","@confidence":"0.99504375862069","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nof MEDLINE citations in accordance with the principles of EBM. Our scorer is em-\nployed to rerank citations retrieved by the PubMed search engine, with the goal of\nbringing as many topically relevant abstracts to higher ranking positions as possible.\nFrom this reranked list of citations, our system is then able to generate textual re-\nsponses that directly address physicians? information needs. We evaluated our system\nwith a collection of real-world clinical questions and demonstrate that our combined\nknowledge-based and statistical approach delivers significantly better document re-\ntrieval and question-answering performance, compared to systems used by physicians\ntoday.\nThis article is organized in the following manner: We start in the next section with\nan overview of evidence-based medicine and its basic principles. Section 3 provides an\noverview of MEDLINE, the bibliographic database used by our system, and PubMed,\nthe public gateway for accessing this database. Section 4 describes our system architec-\nture and outlines our conception of clinical question answering as ?semantic unifica-\ntion? between query frames and knowledge frames derived from MEDLINE citations.\nThe knowledge extractors that underlie our approach are described in Section 5, along\nwith intrinsic evaluations of each component. In Section 6, we detail an algorithm for\nscoring the relevance of MEDLINE citations with respect to structured query represen-\ntations. This scoring algorithm captures the principles of EBM and uses the results of\nthe knowledge extractors as basic features. To evaluate the performance of this citation\nscoring algorithm, we have gathered a corpus of real-world clinical questions. Section 7\npresents results from a document reranking experiment where our EBM scores were\nused to rerank citations retrieved by PubMed. Section 8 provides additional details on\nattempts to optimize the performance of our EBM citation scoring algorithm. Answer\ngeneration, based on reranked results, is described in Section 9. Answers from our\nsystem were manually assessed by two physicians; results are presented in Section 10.\nRelated work is discussed in Section 11, followed by future work in Section 12. Finally,\nwe conclude in Section 13.\n"},{"#tail":"\n","@confidence":"0.999467941176471","#text":"\nEvidence-based medicine (EBM) is a widely accepted paradigm for medical practice\nthat involves the explicit use of current best evidence, that is, high-quality patient-\ncentered clinical research such as reports from randomized controlled trials, in mak-\ning decisions about patient care. Naturally, such evidence, as reported in the primary\nmedical literature, must be suitably integrated with the physician?s own expertise and\npatient-specific factors. It is argued by many that practicing medicine in this manner\nleads to better patient outcomes and higher quality health care. The goal of our work is\nto develop question-answering techniques that complement this paradigm of medical\npractice.\nEBM offers three orthogonal facets that, when taken together, provide a framework\nfor codifying the knowledge involved in answering clinical questions. These three\ncomplementary facets are outlined below.\nThe first facet describes the four main clinical tasks that physicians engage in\n(arranged roughly in order of prevalence):\nTherapy: Selecting treatments to offer a patient, taking into account effectiveness, risk,\ncost, and other relevant factors (includes Prevention?selecting actions to reduce\nthe chance of a disease by identifying and modifying risk factors).\n"},{"#tail":"\n","@confidence":"0.993568369565217","#text":"\nComputational Linguistics Volume 33, Number 1\nDiagnosis: This encompasses two primary types:\nDifferential diagnosis: Identifying and ranking by likelihood potential diseases\nbased on findings observed in a patient.\nDiagnostic test: Selecting and interpreting diagnostic tests for a patient, consid-\nering their precision, accuracy, acceptability, cost, and safety.\nEtiology/Harm: Identifying factors that cause a disease or condition in a patient.\nPrognosis: Estimating a patient?s likely course over time and anticipating likely\ncomplications.\nThese activities represent what Ingwersen (1999) calls ?work tasks.? It is important\nto note that they exist independently of information needs, namely, searching is not\nnecessarily implicated in any of these activities. We are, however, interested in situations\nwhere questions arise during one of these clinical tasks?only then does the physician\nengage in information-seeking behavior. These activities translate into natural ?search\ntasks.? For therapy, the search task is usually therapy selection (for example, determining\nwhich course of action is the best treatment for a disease) or prevention (for example,\nselecting preemptive measures with respect to a particular disease). For diagnosis,\nthere are two different possibilities: in differential diagnosis, a physician is consider-\ning multiple hypotheses regarding what disease a patient has; in diagnostic methods\nselection, the clinician is attempting to ascertain the relative utility of different tests.\nFor etiology, cause determination is the search task, and for prognosis, patient outcome\nprediction.\nTerms and the types of studies relevant to each of the four tasks have been exten-\nsively studied by the Hedges Project at the McMaster University (Haynes et al 1994;\nWilczynski, McKibbon, and Haynes 2001). The results of this research are implemented\nin the PubMed Clinical Queries tools, which can be used to retrieve task-specific cita-\ntions (more about this in the next section).\nThe second facet is independent of the clinical task and pertains to the struc-\nture of a well-built clinical question. The following four components have been iden-\ntified as the key elements of a question related to patient care (Richardson et al\n1995):\n What is the primary problem or disease? What are the characteristics of\nthe patient (e.g., age, gender, or co-existing conditions)?\n What is the main intervention (e.g., a diagnostic test, medication, or\ntherapeutic procedure)?\n What is the main intervention compared to (e.g., no intervention, another\ndrug, another therapeutic procedure, or a placebo)?\n What is the desired effect of the intervention (e.g., cure a disease, relieve\nor eliminate symptoms, reduce side effects, or lower cost)?\nThese four elements are often referenced with the mnemonic PICO, which stands\nfor Patient/Problem, Intervention, Comparison, and Outcome.\nFinally, the third facet serves as a tool for appraising the strength of evidence\npresented in the study, that is, how much confidence should a physician have in the\nresults? Several taxonomies for appraising the strength of evidence based on the type\nand quality of the study have been developed. We chose the Strength of Recommenda-\ntions Taxonomy (SORT) as the basis for determining the potential upper bound on the\n"},{"#tail":"\n","@confidence":"0.987429733333333","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nquality of evidence, due to its emphasis on the use of patient-oriented outcomes and its\nattempt to unify other existing taxonomies (Ebell et al 2004). There are three levels of\nrecommendations according to SORT:\n A-level evidence is based on consistent, good-quality patient\noutcome-oriented evidence presented in systematic reviews, randomized\ncontrolled clinical trials, cohort studies, and meta-analyses.\n B-level evidence is inconsistent, limited-quality, patient-oriented evidence\nin the same types of studies.\n C-level evidence is based on disease-oriented evidence or studies less\nrigorous than randomized controlled clinical trials, cohort studies,\nsystematic reviews, and meta-analyses.\nA question-answering system designed to support the practice of evidence-based\nmedicine must be sensitive to the multifaceted considerations that go into evaluating\nan abstract?s relevance to a clinical information need. It is exactly these three comple-\n"},{"#tail":"\n","@confidence":"0.982283095238095","#text":"\nresearchers, and many other users as the authoritative source of clinical evidence, and\nhence we have adopted it as the target corpus for our clinical question-answering sys-\ntem. MEDLINE contains over 15 million references to articles from approximately 4,800\njournals in 30 languages, dating back to the 1960s. In 2004, over 571,000 new citations\nwere added to the database, and it continues to grow at a steady pace. The subject\nscope of MEDLINE is biomedicine and health, broadly defined to encompass those\nareas of the life sciences, behavioral sciences, chemical sciences, and bioengineering\nneeded by health professionals and others engaged in basic research and clinical care,\npublic health, health policy development, or related educational activities. MEDLINE\nalso covers life sciences vital to biomedical practitioners, researchers, and educators,\nincluding aspects of biology, environmental science, marine biology, plant and animal\nscience, as well as biophysics and chemistry.1\nEach MEDLINE citation includes basic information such as the title of the article,\nname of the authors, name of the publication, publication type, date of publication,\nlanguage, and so on. Of the entries added over the last decade or so, approximately\n76% have English abstracts written by the authors of the articles?these texts provide\nthe source for answers extracted by our system.\nAdditional metadata are associated with each MEDLINE citation. The most impor-\ntant of these is the controlled vocabulary terms assigned by human indexers. NLM?s\ncontrolled vocabulary thesaurus, Medical Subject Headings (MeSH),2 contains approx-\nimately 23,000 descriptors arranged in a hierarchical structure and more than 151,000\n"},{"#tail":"\n","@confidence":"0.979055125","#text":"\nComputational Linguistics Volume 33, Number 1\nseparate thesaurus. Indexing is performed by approximately 100 indexers with at least\nbachelor?s degrees in life sciences and formal training in indexing provided by NLM.\nSince mid-2002, the Library has been employing software that automatically suggests\nMeSH headings based on content (Aronson et al 2004). Nevertheless, the indexing\nprocess remains firmly human-centered.\nAs a concrete example, an abstract titled ?Antipyretic efficacy of ibuprofen vs.\nacetaminophen? might have the following MeSH headings associated with it:\n"},{"#tail":"\n","@confidence":"0.996762588235294","#text":"\nTo represent different aspects of the topic described by a particular MeSH heading,\nup to three subheadings may be assigned, as indicated by the slash notation. In this\nexample, a trained user could interpret from the MeSH terms that the article is about\ndrug therapy for fever and the therapeutic use of ibuprofen and acetaminophen. An\nasterisk placed next to a MeSH heading indicates that the human indexer interprets the\nterm to be the main focus of the article. Multiple MeSH terms can be notated in this\nmanner.\nMEDLINE is publicly accessible on the World Wide Web through PubMed, the Na-\ntional Library of Medicine?s gateway, or through third-party organizations that license\nMEDLINE from NLM. PubMed is a sophisticated boolean search engine that allows\nusers to query not only on abstract text, but also on metadata fields such as MeSH terms.\nIn addition, PubMed provides a number of pre-defined ?search templates? called Clin-\nical Queries (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001) that allow\nusers to narrow the scope of retrieved articles. These filters are implemented as fixed\nboolean query fragments (containing restrictions on MeSH terms, for example) that are\nappended to the original user query. Our experiments involve the use of PubMed to\nretrieve an initial set of candidate citations for subsequent processing.\n"},{"#tail":"\n","@confidence":"0.9952245","#text":"\nWe view clinical question answering as ?semantic unification? between information\nneeds expressed in a PICO-based frame and corresponding structures automatically\nextracted from MEDLINE citations. In accordance with the principles of EBM, this\nmatching process should be sensitive to the nature of the clinical task and the strength\nof evidence of retrieved abstracts.\nAs a concrete example, consider the following clinical question:\nIn children with an acute febrile illness, what is the efficacy of single-medication\ntherapy with acetaminophen or ibuprofen in reducing fever?\nThe information need might be formally encoded in the following manner:\nSearch Task: therapy selection\nProblem/Population: acute febrile illness/in children\nIntervention: acetaminophen\nComparison: ibuprofen\nOutcome: reducing fever\n"},{"#tail":"\n","@confidence":"0.996016041666667","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nThis query representation explicitly encodes the search task and the PICO structure\nof the clinical question. After processing MEDLINE citations, automatically extracting\nPICO elements from the abstracts, and semantically matching these elements with the\nquery, a system might produce the following answer:\nIbuprofen provided greater temperature decrement and longer duration of antipyresis\nthan acetaminophen when the two drugs were administered in approximately equal\ndoses.\nPMID: 1621668\nStrength of Evidence: grade A\nPhysicians are usually most interested in outcome statements that assert a patient-\noriented clinical finding?for example, the relative efficacy of two drugs. Thus, out-\ncomes can serve as the basis for good answers and an entry point into the full text. The\nsystem should automatically evaluate the strength of evidence of the citations supplying\nthe answer, but the decision to adopt the recommendations as suggested ultimately rests\nwith the physician.\nWhat is the best input to a clinical question-answering system? Two possibilities\ninclude a natural language question or a structured PICO query frame. We advocate\nthe latter. With a frame-based query interface, the physician shoulders the burden of\ntranslating an information need into a frame-based representation, but this provides\nseveral advantages. Most importantly, formal representations force physicians to ?think\nthrough? their questions, ensuring that relevant elements are captured. Poorly formu-\nlated queries have been identified by Ely et al (2005) as one of the obstacles to finding\nanswers to clinical questions. Because well-formed questions should have concretely\ninstantiated PICO slots, a frame representation clearly lets the physician see missing\nelements. In addition, a structured query representation obviates the need for linguistic\nanalysis of a natural language question, where ambiguities may negatively impact\noverall performance. We discuss alternative interfaces in Section 12.\nIdeally, we would like to match structured representations derived from the ques-\ntion with those derived from MEDLINE citations (taking into consideration other EBM-\nrelevant factors). However, we do not have access to the computational resources nec-\nessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE\ndatabase and directly index their results. As an alternative, we rely on PubMed to\nretrieve an initial set of hits that we then postprocess in greater detail?this is the\nstandard pipeline architecture commonly employed in other question-answering sys-\ntems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001).\nThe architecture of our system is shown in Figure 1. The query formulator is respon-\nsible for converting a clinical question (in the form of a query frame) into a PubMed\nsearch query. Presently, these queries are already encoded in our test collection (see\nSection 6). PubMed returns an initial list of MEDLINE citations, which is then analyzed\nby our knowledge extractors (see Section 5). The input to the semantic matcher, which\nimplements our EBM citation scoring algorithm, is the query frame and annotated\nMEDLINE citations. The module outputs a ranked list of citations that have been scored\nin accordance with the principles of EBM (see Section 6). Finally, the answer generator\ntakes these citations and extracts appropriate answers (see Section 9).\nIn summary, our conception of clinical question answering as semantic frame\nmatching suggests the need for a number of capabilities, which correspond to the\nbold outlined boxes in Figure 1: knowledge extraction, semantic matching for scoring\n"},{"#tail":"\n","@confidence":"0.949379","#text":"\nComputational Linguistics Volume 33, Number 1\nFigure 1\nArchitecture of our clinical question-answering system.\ncitations, and answer generation. We have realized all three capabilities in an imple-\nmented clinical question-answering system and conducted three separate evaluations\nto assess the effectiveness of our developed capabilities. We do not tackle the query\nformulator, although see discussion in Section 12. Overall, results indicate that our\nimplemented system significantly outperforms the PubMed baseline.\n"},{"#tail":"\n","@confidence":"0.998281451612903","#text":"\nThe automatic extraction of PICO elements from MEDLINE citations represents a key\ncapability integral to clinical question answering. This section, which elaborates on\npreliminary results reported in Demner-Fushman and Lin (2005), describes extraction\nalgorithms for population, problems, interventions, outcomes, and the strength of evi-\ndence. For an example of a completely annotated abstract, see Figure 2. Each individual\nPICO extractor takes as input the abstract text of a MEDLINE citation and identifies the\nrelevant elements: Outcomes are complete sentences, while population, problems, and\ninterventions are short noun phrases.\nOur knowledge extractors rely extensively on MetaMap (Aronson 2001), a system\nfor identifying segments of text that correspond to concepts in the UMLS Metathe-\nsaurus. Many of our algorithms operate at the level of coarser-grained semantic\ntypes called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture\nhigher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional\nfeature we take advantage of (when present) is explicit section markers present in some\nabstracts. These so-called structured abstracts were recommended by the Ad Hoc Work-\ning Group for Critical Appraisal of the Medical Literature (1987) to help humans assess\nthe reliability and content of a publication and to facilitate the indexing and retrieval\nprocesses. These abstracts loosely adhere to the introduction, methods, results, and\nconclusions format common in scientific writing, and delineate a study using explicitly\nmarked sections with variations of the above headings. Although many core clinical\njournals require structured abstracts, there is a great deal of variation in the actual\nheadings. Even when present, the headings are not organized in a manner focused on\npatient care. In addition, abstracts of much high-quality work remain unstructured. For\nthese reasons, explicit section markers are not entirely reliable indicators for the various\nsemantic elements we seek to extract, but must be considered along with other sources\nof evidence.\nThe extraction of each PICO element relies to a different extent on an annotated\ncorpus of MEDLINE abstracts, created through an effort led by the first author at\nthe National Library of Medicine (Demner-Fushman et al 2006). As will be described\nherein, the population, problem, and the intervention extractors are based largely on\nrecognition of semantic types and a few manually constructed rules; the outcome extrac-\n"},{"#tail":"\n","@confidence":"0.99734480952381","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\ntor, in contrast, is implemented as an ensemble of classifiers trained using supervised\nmachine learning techniques (Demner-Fushman et al 2006). These two very different\napproaches can be attributed to differences in the nature of the frame elements: Whereas\nproblems and interventions can be directly mapped to UMLS concepts, and populations\neasily mapped to patterns that include UMLS concepts, outcome statements follow no\npredictable pattern. The initial goal of the annotation effort was to identify outcome\nstatements in abstract text. A physician, two registered nurses, and an engineering\nresearcher manually identified sentences that describe outcomes in 633 MEDLINE\nabstracts; a post hoc analysis demonstrates good agreement (? = 0.77). The annotated\nabstracts were retrieved using PubMed and attempted to model different user behaviors\nranging from naive to expert (where advanced search features were employed). With the\nexception of 50 citations retrieved to answer a question about childhood immunization,\nthe rest of the results were retrieved by querying on a disease, for example, diabetes. Of\nthe 633 citations, 100 abstracts were also fully annotated with population, problems, and\ninterventions. These 100 abstracts were set aside as a held-out test set. Of the remaining\ncitations, 275 were used for training and rule derivation, as described in the following\nsections.\nAfter much exploration, Demner-Fushman et al (2006) discovered that it was not\npractical to annotate PICO entities at the phrase level due to significant unresolvable\ndisagreement and interannotator reliability issues. Consider the following segment:\nThis double-blind, placebo-controlled, randomized, 3-period, complete block, 6-week\ncrossover study examined the efficacy of simvastatin in adult men and women (N =\n151) with stable type 2 DM, low density lipoprotein-cholesterol 100 mg/dL, HDL-C <\n40 mg/dL, and fasting triglyceride level > 150 and < 700 mg/dL.\nAll annotators agreed that the sentence contained the problem, population, and\nintervention. However, they could not agree on the exact phrasal boundaries of each\nelement, and more importantly, general guidelines for ensuring consistent annotations.\nFor example, should the whole clause starting with adult men and women be marked as\npopulation, or should type 2 Diabetes Mellitus (type 2 DM) be marked up only as the\nproblem? How should we indicate that the cholesterol levels description belongs to 151\nsubjects of the study, and so forth? This issue becomes important for evaluation because\nthere is a mismatch between annotated ground truth and the output of our knowledge\nextractors, as we will discuss.\nIn what follows, we describe each of the individual PICO extractors and a series of\ncomponent evaluations that assess their accuracy. This section is organized such that the\ndescription of each extractor and its evaluation are paired together. Results are reported\nin terms of the percentage of correctly identified instances, percentage of instances for\nwhich the extractor had no answer, and percentage of incorrectly identified instances.\nThe baselines and gold standards for each extractor vary, and will be described in-\ndividually. The goal of these component evaluations is a general characterization of\nperformance, as we focused the majority of our efforts on the two other evaluations.\n"},{"#tail":"\n","@confidence":"0.99377","#text":"\nThe PICO framework makes no distinction between the population and the problem,\nwhich is rooted in the concept of the population in clinical studies, as exemplified by\ntext such as POPULATION: Fifty-five postmenopausal women with a urodynamic diagnosis\nof genuine urinary stress incontinence. Although this fragment simultaneously describes\n"},{"#tail":"\n","@confidence":"0.993388296296296","#text":"\nComputational Linguistics Volume 33, Number 1\nthe population (of which a particular patient can be viewed as a sample therefrom) and\nthe problem, we chose to separate the extraction of the two elements because they are\nnot always specified together in abstracts (issues with respect to exact boundaries men-\ntioned previously notwithstanding). Furthermore, many clinical questions ask about a\nparticular problem without specifying a population.\nPopulation elements, which are typically noun phrases, are identified using a series\nof manually crafted rules that codify the following assumptions:\n The concept describing the population belongs to the semantic type\nGROUP or any of its children. In addition, certain nouns are often used to\ndescribe study participants in medical texts; for example, an often\nobserved pattern is ?subjects? or ?cases? followed by a concept from the\nsemantic group DISORDER.\n The number of subjects that participated in the study often precedes or\nfollows a concept identified as a GROUP. In the latter case, the number is\nsometimes given in parentheses using a common pattern n = number,\nwhere ?n = ? is a shorthand for the number of subjects, and number\nprovides the actual number of study participants.\n The confidence that a clause with an identified number and GROUP\ncontains information about the population is inversely proportional to the\ndistance between the two entities.\n The confidence that a clause contains the population is influenced by the\nposition of the clause, with respect to headings in the case of structured\nabstracts and with respect to the beginning of the abstract in the case of\nunstructured abstracts.\nGiven these assumptions, the population extractor searches for the following\npatterns:\n"},{"#tail":"\n","@confidence":"0.8250691","#text":"\nfor example, in 5?6-year-old French children (n = 234), Subjects (n = 54)\n number* GROUP\nfor example, forty-nine infants\n number* DISORDER* GROUP?\nfor example, 44 HIV-infected children\nThe confidence score assigned to a particular pattern match is a function of both its\nposition in the abstract and its position in the clause from which it was extracted. If a\nnumber is followed by a measure, for example, year or percent, the number is discarded,\nand pattern matching continues. After the entire abstract is processed in this manner,\nthe match with the highest confidence value is retained as the population description.\n"},{"#tail":"\n","@confidence":"0.996591","#text":"\nNinety of the 100 fully annotated abstracts in our collection were agreed upon by the\nannotators as being clinical in nature, and were used as test data for our population\nextractor. Because these abstracts were not examined in the process of developing the\nextractor rules, they can be viewed as a blind held-out test set. The output of our popu-\n"},{"#tail":"\n","@confidence":"0.99475271875","#text":"\nas containing the population in the gold standard. Note that this evaluation presents an\nupper bound on the performance of the population extractor, whose outputs are noun\nphrases. We adopted such a lenient evaluation setup because of the boundary issues\npreviously discussed, and also to forestall potential difficulties with scoring partially\noverlapping string matches.\nFor comparison, our baseline simply returned the first three sentences of the ab-\nstract. We considered the baseline correct if any one of the sentences were annotated\nas containing the population in the gold standard (an even more lenient criterion).\nThis baseline was motivated by the observation that the aim and methods sections of\nstructured abstracts are likely to contain the population information?for structured ab-\nstracts, explicit headings provide structural cues; for unstructured abstracts, positional\ninformation serves as a surrogate.\nThe performance of the population extractor is shown in Table 1. A manual error\nanalysis revealed three sources of error: First, not all population descriptions contain\na number explicitly, for example, The medical charts of all patients who were treated with\netanercept for back or neck pain at a single private medical clinic in 2003. Second, not all study\npopulations are population groups, as for example in All primary care trusts in England.\nFinally, tagging and chunking errors propagate to the semantic type assignment level\nand affect the quality of MetaMap output. For example, consider the following sentence:\nWe have compared the LD and recombination patterns defined by single-nucleotide\npolymorphisms in ENCODE region ENm010, chromosome 7p15 2, in Korean, Japanese,\nand Chinese samples.\nBoth Korean and Japanese were mistagged as nouns, which lead to the following\nerroneous chunking:\n[We] [have] [compared] [the LD] [and] [recombination patterns] [defined] [by\nsingle-nucleotide polymorphisms] [in] [ENCODE] [region ENm010,] [chromosome\n7p15 2,] [in Korean,] [Japanese,] [and] [Chinese samples.]\nThis resulted in the tagging of Japanese as a population. Errors of this type affect\nother extractors as well. For example, lead was mistagged as a noun in the phrase\nEchocardiographic findings lead to the right diagnosis, which caused MetaMap to identify\nthe word as a PHARMACOLOGICAL SUBSTANCE (lead is sometimes used as a homeo-\npathic preparation).\n"},{"#tail":"\n","@confidence":"0.999224666666667","#text":"\nThe problem extractor relies on the recognition of concepts belonging to the UMLS\nsemantic group DISORDER. In short, it returns a ranked list of all such concepts within\na given span of text. We evaluate the performance of this simple heuristic on segments\n"},{"#tail":"\n","@confidence":"0.986806666666667","#text":"\nof the abstract varying in length: abstract title only, abstract title and first two sentences,\nand entire abstract text. Concepts in the title, in the introduction section of structured\nabstracts, or in the first two sentences in unstructured abstracts, are given higher confi-\ndence values due to their discourse prominence. Finally, the highest-scoring problem\nis designated as the primary problem in order to differentiate it from co-occurring\nconditions identified in the abstract.\n"},{"#tail":"\n","@confidence":"0.99080575","#text":"\nAlthough our problem extractor returns a list of clinical problems, we only evalu-\nate performance on identification of the primary problem. For some abstracts, MeSH\nheadings can be used as ground truth, because one of the human indexers? tasks in\nassigning terms is to identify the main topic of the article (sometimes a disorder). For\nthis evaluation, we randomly selected 50 abstracts with disorders indexed as the main\ntopic from abstracts retrieved using PubMed on the five clinical questions described\nin Sneiderman et al (2005).\nWe applied our problem extractor on different segments of the abstract: the title\nonly, the title and first two sentences, and the entire abstract. These results are shown in\nTable 2. Here, a problem was considered correctly identified only if it shared the same\nconcept ID as the ground truth problem (from the MeSH heading). The performance of\nour best variant (abstract title and first two sentences) approaches the upper bound on\nMetaMap performance?which is limited by human agreement on the identification of\nsemantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003).\nAlthough problem extraction largely depends on disease coverage in UMLS and\nMetaMap performance, the error rate could be further reduced by more sophisticated\nrecognition of implicitly stated problems. For example, with respect to a question about\nimmunization in children, an abstract about the measles-mumps-rubella vaccination\nnever mentioned the disease without the word vaccination; hence, no concept of the\ntype DISEASE OR SYNDROME was identified.\n"},{"#tail":"\n","@confidence":"0.963919555555556","#text":"\nThe intervention extractor identifies both the intervention and comparison elements in\na PICO frame; processing of these two frame elements can be collapsed because they\nbelong to the same semantic group. In many abstracts, it is unclear which intervention is\nthe primary one and which are the comparisons, and hence our extractor simply returns\na list of all interventions under study.\nFor interventions, we are primarily interested in entities that may participate in the\nUMLS Semantic Network relations associated with each clinical task. Restrictions on\nthe semantic types allowed in these relations prescribe the set of possible clinical in-\nterventions. For therapy these relations include treats, prevents, and carries out; diagnoses\n"},{"#tail":"\n","@confidence":"0.9921245","#text":"\nfor diagnosis; causes and result of for etiology; and prevents for prognosis. At present, the\nidentification of nine semantic types, for example, DIAGNOSTIC PROCEDURE, CLINICAL\nDRUG, and HEALTH CARE ACTIVITY, serves as the foundation for our intervention\nextraction algorithm.\nCandidate scores are further adjusted to reflect a few different factors. In structured\nabstracts, concepts of the relevant semantic type are given additional weight if they\nappear in the title, aims, and methods sections. In unstructured abstracts, concepts\ntowards the beginning of the abstract text are favored. Finally, the intervention extractor\ntakes into account the presence of certain cue phrases that describe the aim and/or\nmethods of the study, such as This study examines or This paper describes.\n"},{"#tail":"\n","@confidence":"0.999899157894737","#text":"\nThe intervention extractor was evaluated in the same manner as the population extrac-\ntor and compared to the same baseline. To iterate, 90 held-out clinical abstracts that\ncontained human-annotated interventions served as ground truth. The output of our\nintervention extractor was judged to be correct if it occurred in a sentence that was\nannotated as containing the intervention in the gold standard. As with the evaluation\nof the population extractor, this represents an upper bound on performance. Results are\nshown in Table 3.\nSome of the errors were caused by ambiguity of terms. For example, in the clause\nserum levels of anti-HBsAg and presence of autoantibodies (ANA, ENA) were evaluated,\nserum is recognized as a TISSUE, levels as INTELLECTUAL PRODUCT, and autoantibodies\nand ANA as IMMUNOLOGIC FACTORS. In this case, however, autoantibodies should\nbe considered a LABORATORY OR TEST RESULT.3 In other cases, extraction errors\nwere caused by summary sentences that were very similar to intervention statements,\nfor example, This study compared the effects of 52 weeks? treatment with pioglitazone, a\nthiazolidinedione that reduces insulin resistance, and glibenclamide, on insulin sensitivity,\nglycaemic control, and lipids in patients with Type 2 diabetes. For this particular abstract,\nthe correct interventions are contained in the sentence Patients with Type 2 diabetes\nwere randomized to receive either pioglitazone (initially 30 mg QD, n = 91) or micronized\nglibenclamide (initially 1.75 mg QD, n = 109) as monotherapy.\n"},{"#tail":"\n","@confidence":"0.99757625","#text":"\nWe approached outcome extraction as a classification problem at the sentence level, that\nis, the outcome extractor assigns a probability of being an outcome to each sentence\nin an abstract. Our preliminary work has led to a strategy based on an ensemble of\nclassifiers, which includes a rule-based classifier, a unigram ?bag of words? classifier,\n"},{"#tail":"\n","@confidence":"0.993225547619048","#text":"\nComputational Linguistics Volume 33, Number 1\nan n-gram classifier, a position classifier, an abstract length classifier, and a semantic\nclassifier. With the exception of the rule-based classifier, all classifiers were trained on\nthe 275 citations from the annotated collection of abstracts described previously.\nKnowledge for the rule-based classifier was hand-coded, prior to the annotation\neffort, by a registered nurse with 20 years of clinical experience. This classifier estimates\nthe likelihood that a sentence states an outcome based on cue phrases such as signif-\nicantly greater, well tolerated, and adverse events. The likelihood of a sentence being an\noutcome as indicated by cue phrases is the ratio of the cumulative score for recognized\nphrases to the maximum possible score. For example, the sentence The dropout rate due to\nadverse events was 12.4% in the moxonidine and 9.8% in the nitrendipine group is segmented\ninto eight phrases by MetaMap, which sets the maximum score to 8. The two phrases\ndropout rate and adverse events contribute one point each to the cumulative score, which\nresults in a likelihood estimate of 0.25 for this sentence.\nThe unigram ?bag of words? classifier is a naive Bayes classifier implemented with\nthe API provided by the MALLET toolkit.4 This classifier outputs the probability of a\nclass assignment.\nThe n-gram based classifier is also a naive Bayes classifier, but it operates on a differ-\nent set of features. We first identified the most informative unigrams and bigrams using\nthe information gain measure (Yang and Pedersen 1997), and then selected only the\npositive outcome predictors using odds ratio (Mladenic and Grobelnik 1999). Disease-\nspecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the\nlist of features was revised by the registered nurse who participated in the annotation\neffort. This classifier also outputs the probability of a class assignment.\nThe position classifier returns the maximum likelihood estimate that a sentence is\nan outcome based on its position in the abstract (for structured abstracts, with respect\nto the results or conclusions sections; for unstructured abstracts, with respect to the end\nof the abstract).\nThe abstract length classifier returns a smoothed (add one smoothing) probability\nthat an abstract of a given length (in the number of sentences) contains an outcome\nstatement. For example, the probability that an abstract four sentences long contains an\noutcome statement is 0.25, and the probability of finding an outcome in a ten sentence?\nlong abstract is 0.92. This feature turns out to be useful because the average length of\nabstracts with and without outcome statements differs: 11.7 sentences for the former,\n7.95 sentences for the latter.\nThe semantic classifier assigns to a sentence an ad hoc score based on the presence\nof UMLS concepts belonging to semantic groups highly associated with outcomes\nsuch as THERAPEUTIC PROCEDURE or PHARMACOLOGICAL SUBSTANCE. The score is\ngiven a boost if the concept has already been identified as the primary problem or an\nintervention.\nThe outputs of our basic classifiers are combined using a simple weighted linear\ninterpolation scheme:\n"},{"#tail":"\n","@confidence":"0.819199","#text":"\nWe attempted two approaches for assigning these weights. The first method relied\non ad hoc weight selection based on intuition. The second involved a more principled\n"},{"#tail":"\n","@confidence":"0.956623666666667","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\near regression adapted for classification (Ting and Witten 1999), which can be described\nby the following equation:\n"},{"#tail":"\n","@confidence":"0.985847909090909","#text":"\nPk is the probability that a sentence specifies an outcome, as determined by classifier\nk (for classifiers that do not return actual probabilities, we normalized the scores and\ntreated them as such). To predict the class of a sentence, the probabilities generated\nby n classifiers are combined using the coefficients (?0, ...,?n). These values are de-\ntermined in the training stage as follows: Probabilities predicted by base classifiers\nfor each sentence are represented in an N ? M matrix A, where M is the number of\nsentences in the training set, and N is the number of classifiers. The gold standard\nclass assignments for each sentence is stored in a vector b, and weights are found by\ncomputing the vector ? that minimizes ||A?? b||. The solution can be found using\nsingular value decomposition, as provided in the JAMA basic linear algebra package\nreleased by NIST.5\n"},{"#tail":"\n","@confidence":"0.994668923076923","#text":"\nBecause outcome statements were annotated in each of the 633 citations in our collec-\ntion, it was possible to evaluate the outcome extractor on a broader set of abstracts. From\nthose not used in training the outcome classifiers, 153 citations pertaining to therapy\nwere selected. Of these, 143 contained outcome statements and were used as the blind\nheld-out test set. In addition, outcome statements in abstracts pertaining to diagnosis\n(57), prognosis (111), and etiology (37) were also used.\nThe output of our outcome extractor is a ranked list of sentences sorted by con-\nfidence. Based on the observation that human annotators typically mark two to three\nsentences in each abstract as outcomes, we evaluated the performance of our extractor\nat cutoffs of two and three sentences. These results are shown in Table 4: The columns\nmarked AH2 and AH3 show performance of the weighted linear interpolation approach\nwith ad hoc weight assignment at two- and three-sentence cutoffs, respectively; the\ncolumns marked LR2 and LR3 show performance of the least squares linear regression\nmodel at the same cutoffs. In the evaluation, our outcome extractor was considered\ncorrect if the returned sentences intersected with sentences judged as outcomes by\nour human annotators. Although this is a lenient criterion, it does roughly capture\nthe performance of our knowledge extractor. Because outcome statements are typically\nfound in the conclusion of a structured abstract (or near the end of the abstract in the\ncase of unstructured abstracts), we compared our answer extractor to the baseline of\nreturning either the final two or final three sentences in the abstract (B2 and B3 in\nTable 4).\nAs can be seen, variants of our outcome extractor performed better than the baseline\nat the two-sentence cutoff, for the most part. Bigger improvements, however, can be\nseen at the three-sentence cutoff level. It is evident that the assignment of weights in\nour ad hoc model is primarily geared towards therapy questions, perhaps overly so.\nBetter overall performance is obtained with the least squares linear regression model.\n"},{"#tail":"\n","@confidence":"0.5771354","#text":"\nComputational Linguistics Volume 33, Number 1\nTable 4\nEvaluation of the outcome extractor. B = baseline, returns last sentences in abstract; AH = ad hoc\nweight assignment; LR = least squares linear regression. Statistically significant improvement\nover the baseline at the 1% level is indicated by  .\n"},{"#tail":"\n","@confidence":"0.7911159","#text":"\nExamples of strength of evidence categories based on Publication Type and MeSH headings.\nStrength of Evidence Publication Type/MeSH\nLevel A(1) Meta-analysis, randomized controlled trials, cohort study,\nfollow-up study\nLevel B(2) Case-control study, case series\nLevel C(3) Case report, in vitro, animal and animal testing,\nalternatives studies\nThe majority of errors made by the outcome extractor were related to inaccurate\nsentence boundary identification, chunking errors, and word sense ambiguity in the\nMetathesaurus.\n"},{"#tail":"\n","@confidence":"0.969277666666666","#text":"\nThe strength of evidence is a classification scheme that helps physicians assess the\nquality of a particular citation for clinical purposes. Metadata associated with most\nMEDLINE citations (MeSH terms) are extensively used to determine the strength of\nevidence and in our EBM citation scoring algorithm (Section 6).\nThe potential highest level of the strength of evidence for a given citation can be\nidentified using the Publication Type (a metadata field) and MeSH terms pertaining\nto the type of the clinical study. Table 5 shows our mapping from publication type\nand MeSH headings to evidence grades based on principles defined in the Strength\nof Recommendations Taxonomy (Ebell et al 2004).\n"},{"#tail":"\n","@confidence":"0.957730428571429","#text":"\nA complete example of our knowledge extractors working in unison is shown in\nFigure 2, which contains an abstract retrieved in response to the following question:\n?In children with an acute febrile illness, what is the efficacy of single-medication\ntherapy with acetaminophen or ibuprofen in reducing fever?? (Kauffman, Sawyer, and\nScheinbaum 1992). Febrile illness is the only concept mapped to DISORDER, and hence\nis identified as the primary problem. 37 otherwise healthy children aged 2 to 12 years is\ncorrectly identified as the population. Acetaminophen, ibuprofen, and placebo are correctly\n"},{"#tail":"\n","@confidence":"0.700818444444444","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nAntipyretic efficacy of ibuprofen vs acetaminophen\nKauffman RE, Sawyer LA, Scheinbaum ML\nAm J Dis Child. 1992 May;146(5):622-5\nOBJECTIVE?To compare the antipyretic efficacy of ibuprofen, placebo, and\nacetaminophen. DESIGN?Double-dummy, double-blind, randomized, placebo-\ncontrolled trial. SETTING?Emergency department and inpatient units of a large,\nmetropolitan, university-based, children?s hospital in Michigan. PARTICIPANTS?\n??\n"},{"#tail":"\n","@confidence":"0.674967692307692","#text":"\nillnessProblem. INTERVENTIONS?Each child was randomly assigned to receive\na single dose of\n???????????????\nacetaminophenIntervention (10 mg/kg),\n??????????\nibuprofenIntervention (10 mg/kg) (7.5\nor 10 mg/kg), or\n????????\nplaceboIntervention (10 mg/kg). MEASUREMENTS/MAIN RESULTS?\nOral temperature was measured before dosing, 30 minutes after dosing, and\nhourly thereafter for 8 hours after the dose. Patients were monitored for ad-\nverse effects during the study and 24 hours after administration of the as-\nsigned drug.\n"},{"#tail":"\n","@confidence":"0.9941278","#text":"\nextracted as the interventions under study. The three outcome sentences are correctly\nclassified; the short sentence concerning adverse effects was ranked lower than the\nother three sentences and hence below the cutoff. The study design, from metadata\nassociated with the citation, allows our strength of evidence extractor to classify this\narticle as grade A.\n"},{"#tail":"\n","@confidence":"0.998773777777778","#text":"\nIn our view of clinical question answering, the knowledge extractors just described sup-\nply the features on which semantic matching occurs. This section describes an algorithm\nthat, when presented with a structured representation of an information need and a\nMEDLINE citation, automatically computes a topical relevance score in accordance with\nthe principles of EBM.\nIn order to develop algorithms that operationalize the three facets of EBM, it is\nnecessary to possess a corpus of clinical questions on which to experiment. Because no\nsuch test collection exists, we had to first manually create one. Fortunately, collections\nof clinical questions (representing real-world information needs of physicians), are\n"},{"#tail":"\n","@confidence":"0.981744272727273","#text":"\navailable on-line. From two sources, the Journal of Family Practice6 and the Parkhurst\nExchange,7 we gathered 50 clinical questions, which capture a realistic sampling of the\nscenarios that a clinical question-answering system would be confronted with. These\nquestions were minimally modified from their original form as downloaded from the\nWorld Wide Web. In a few cases, a single question actually consisted of several smaller\nquestions; such clusters were simplified by removing questions more peripheral to the\ncentral clinical problem. All questions were manually classified into one of the four\nclinical tasks; the distribution of the questions roughly follows the prevalence of each\ntask type as observed in natural settings, noted by Ely et al (1999). The final step in\nthe preparation process was manual translation of the natural language questions into\nPICO query frames.\nOur collection was divided into a development set and a blind held-out test set for\nverification purposes. The breakdown of these questions into the four clinical tasks and\nthe development/test split is shown in Table 6. An example of each question type from\nour development set is presented here, along with its query frame:\nDoes quinine reduce leg cramps for young athletes? (Therapy)\nsearch task: therapy selection\nprimary problem: leg cramps\nco-occurring problems: muscle cramps, cramps\npopulation: young adult\nintervention: quinine\nHow often is coughing the presenting complaint in patients with gastroesophageal\nreflux disease? (Diagnosis)\nsearch task: differential diagnosis\nprimary problem: gastroesophageal reflux disease\nco-occurring problems: cough\nWhat?s the prognosis of lupoid sclerosis? (Prognosis)\nsearch task: patient outcome prediction\nprimary problem: lupus erythematosus\nco-occurring problems: multiple sclerosis\nWhat are the causes of hypomagnesemia? (Etiology)\nsearch task: cause determination\nprimary problem: hypomagnesemia\n"},{"#tail":"\n","@confidence":"0.989050210526316","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nAs discussed earlier, we do not believe that natural language text is the best input\nfor a question-answering system. Instead, a structured PICO-based representation cap-\ntures physicians? information needs in a more perspicuous manner?primarily because\nclinicians are trained to analyze clinical situations with this framework.\nMirroring the organization of our knowledge extractors, we broke up the P in PICO\ninto population, primary problem, and co-occurring problems in the query representa-\ntion. The justification for this will become apparent when we present our algorithm for\nscoring MEDLINE citations, as each of these three facets must be treated differently.\nNote that many elements are specified only to the extent that they were explicit in\nthe original natural language question; for example, if the clinician does not specify\na population, that element will be empty. Finally, outcomes are not directly encoded in\nthe query representation because they are implicit most of the time; for example, in Does\nquinine reduce leg cramps for young athletes?, the desired outcome, naturally, is to reduce\nthe occurrence and severity of leg cramps. Nevertheless, outcome identification is an\nimportant component of the citation scoring algorithm, as we shall see later.\nWhat is the relevance of an abstract with respect to a particular clinical question?\nEvidence-based medicine outlines the need to consider three different facets (see Sec-\ntion 2), which we operationalize in the following manner:\n"},{"#tail":"\n","@confidence":"0.999151111111111","#text":"\nThe relevance of a particular citation, with respect to a structured query, includes\ncontributions from matching PICO structures, the strength of evidence of the citation,\nand factors specifically associated with the search tasks (and indirectly, the clinical\ntasks). In what follows, we describe each of these contributions in detail.\nViewed as a whole, each score component is a heuristic reflection of the factors that\nenter into consideration when a physician examines a MEDLINE citation. Although the\nassignment of numeric scores is based on intuition and may seem ad hoc in many cases,\nevaluation results in the next section demonstrate the effectiveness of our algorithm.\nThis issue will be taken up further in Section 8.\n"},{"#tail":"\n","@confidence":"0.9984605","#text":"\nThe score of an abstract based on extracted PICO elements, SPICO, is broken into individ-\nual components according to the following formula:\n"},{"#tail":"\n","@confidence":"0.99817425","#text":"\nThe first component in the equation, Sproblem, reflects a match between the primary\nproblem in the query frame and the primary problem in the abstract (i.e., the highest-\nscoring problem identified by the problem extractor). A score of 1 is given if the prob-\nlems match exactly based on their unique UMLS concept ID as provided by MetaMap.\nMatching based on concept IDs has the advantage that it abstracts away from termino-\nlogical variation; in essence, MetaMap performs terminological normalization. Failing\nan exact match of concept IDs, a partial string match is given a score of 0.5. If the primary\nproblem in the query has no overlap with the primary problem from the abstract, a score\nof ?1 is given. Finally, if our problem extractor could not identify a problem (but the\nquery frame does contain a problem), a score of ?0.5 is given.\nCo-occurring problems must be taken into consideration in the differential diagnosis\nand cause determination search tasks because knowledge of the problems is typically\n"},{"#tail":"\n","@confidence":"0.989083666666667","#text":"\nComputational Linguistics Volume 33, Number 1\nincomplete in these scenarios. Therefore, physicians would normally be interested in\nany problems mentioned in the abstracts in addition to the primary problem specified\nin the query frame. As an example, consider the question What is the differential diagnosis\nof chronic diarrhea in immunocompetent patients? Although chronic diarrhea is the primary\nproblem, citations that discuss additional related disorders should be favored over those\nthat don?t. In terms of actual scoring, disorders mentioned in the title receive three\npoints, and disorders mentioned anywhere else receive one point (in addition to the\nmatch score based on the primary problem, as discussed).\nScores based on population and intervention, Spopulation and Sintervention respectively, mea-\nsure the overlap between query frame elements and corresponding elements extracted\nfrom abstracts. A point is given to each matching intervention and matching population.\nFor example, finding the population group children from a query frame in the abstract\nincrements the match score; the remaining words in the abstract population are ignored.\nThus, if the query frame contains a population element and an intervention element, the\nscore for an abstract that contains the same UMLS concepts in the corresponding slots\nis incremented by two.\nThe outcome-based score, Soutcome, is simply the value assigned to the highest-scoring\noutcome sentence (we employed the outcome extractor based on the linear regression\nmodel for our experiments). As outcomes are rarely explicitly specified in the original\nquestion, we decided to omit them in the query representation. Our citation scoring\nalgorithm simply considers the inherent quality of the outcome statements in an ab-\nstract, independent of the query. This is justified because, given a match on the primary\nproblem, all clinical outcomes are likely to be of interest to the physician.\n"},{"#tail":"\n","@confidence":"0.625333","#text":"\nThe relevance score component based on the strength of evidence is calculated in the\nfollowing manner:\n"},{"#tail":"\n","@confidence":"0.721830285714286","#text":"\nCitations published in core and high-impact journals such as Journal of the American\nMedical Association (JAMA) get a score of 0.6 for Sjournal, and 0 otherwise. In terms of the\nstudy type, Sstudy, clinical trials, such as randomized controlled trials, receive a score of\n0.5; observational studies, for example, case reports, 0.3; all non-clinical publications,\n?1.5; and 0 otherwise. The study type is directly encoded in the Publication Type field\nof a MEDLINE citation.\nFinally, recency factors into the strength of evidence score according to the formula:\n"},{"#tail":"\n","@confidence":"0.998364","#text":"\nA mild penalty decreases the score of a citation proportionally to the time difference\nbetween the date of the search and the date of publication.\n"},{"#tail":"\n","@confidence":"0.997876666666667","#text":"\nThe final component of our EBM score is based on task-specific considerations, as\nreflected in manually assigned MeSH terms. For search tasks falling into each clinical\ntask, we gathered a list of terms that are positive and negative indicators of relevance.\n"},{"#tail":"\n","@confidence":"0.7189155","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nThe task score, Stask, is given by:\n"},{"#tail":"\n","@confidence":"0.990391866666667","#text":"\nThe function ?(t) maps a MeSH term to a positive score if the term is a positive\nindicator for that particular task type, or a negative score if the term is a negative indi-\ncator for the clinical task. Note that although our current system uses MeSH headings\nassigned by human indexers, manually assigned terms can be replaced with automatic\nprocessing if needed (Aronson et al 2004).\nBelow, we enumerate the relevant indicator terms by clinical task. However, there\nis a set of negative indicators common to all tasks; these were extracted from the set\nof genomics articles provided for the secondary task in the TREC 2004 genomics track\nevaluation (Hersh, Bhupatiraju, and Corley 2004); examples include genetics and cell\nphysiology. The positive and negative weights assigned to each term heuristically encode\nthe relative importance of different MeSH headings and are derived from the Clinical\nQueries filters in PubMed, from the JAMA EBM tutorial series on critical appraisal of\nmedical literature, from MeSH scope notes, and based on a physician?s understanding\nof the domain (the first author).\nIndicators for Therapy Tasks. Positive indicators for therapy were derived from the\n"},{"#tail":"\n","@confidence":"0.995561125","#text":"\nof its children in the MeSH hierarchy. A score of ?1 is given if the MeSH descriptor\nor qualifier is marked as the main theme of the article (indicated via the star notation\nby human indexers), and a score of ?0.5 otherwise. If the question pertains to the\nsearch task of prevention, three additional headings are considered positive indicators:\nprevention and control, prevention measures, and prophylaxis.\nIndicators for Diagnosis Tasks. Positive indicators for therapy are also used as negative\nindicators for diagnosis because the relevant studies are usually disjoint; it is highly\nunlikely that the same clinical trial will study both diagnostic methods and treatment\nmethods. The MeSH term diagnosis and any of its children are considered positive\nindicators. As with therapy questions, terms marked as the major theme get a score of\n?1.0, and ?0.5 otherwise. This general assignment of indicator terms allows a system\nto differentiate between questions such as Does a Short Symptom Checklist accurately\ndiagnose ADHD? and What is the most effective treatment for ADHD in children?, which\nmight retrieve very similar sets of citations.\nIndicators for Prognosis Tasks. Positive indicators for prognosis include the following\nMeSH terms: survival analysis, disease-free survival, treatment outcome, health status, preva-\nlence, risk factors, disability evaluation, quality of life, and recovery of function. For terms\nmarked as the major theme, a score of +2 is given; +1 otherwise. There are no negative\nindicators, other than those common to all tasks previously described.\nIndicators for Etiology Tasks. Negative indicators for etiology include therapy-oriented\nMeSH terms; these terms are given a score of ?0.3. Positive indicators for the diag-\nnosis task are weak positive indicators for etiology, and receive a positive score of\n+0.1. The following MeSH terms are considered highly indicative of citations rele-\nvant to etiology: population at risk, risk factors, etiology, causality, and physiopathology. If\n"},{"#tail":"\n","@confidence":"0.978190666666666","#text":"\nComputational Linguistics Volume 33, Number 1\none of these terms is marked as the major theme, a score of +2 is given; otherwise, a\nscore of +1 is given.\n7. Evaluation of Citation Scoring\nThe previous section describes a relevance-scoring algorithm for MEDLINE citations\nthat attempts to capture the principles of EBM. In this section, we present an evaluation\nof this algorithm.\nIdeally, questions should be answered by directly comparing queries to knowl-\nedge structures derived from MEDLINE citations. However, knowledge extraction on\nsuch large scales is impractical given our computational resources, so we opted for\nan IR-based pipeline approach. Under this strategy, an existing search engine would\nbe employed to generate a candidate list of citations to be rescored, according to our\nalgorithm. PubMed is a logical choice for gathering this initial list of citations because\nit represents one of the most widely used tools employed by physicians and other\nhealth professionals today. The system supports boolean operators and sorts results\nchronologically, most recent citations first.\nThis two-stage retrieval process immediately suggests an evaluation methodology\nfor our citation scoring algorithm?as a document reranking task. Given an initial hit\nlist, can our algorithm automatically re-sort the results such that relevant documents\nare brought to higher ranks? Not only is such a task intuitive to understand, this\nconceptualization also lends itself to an evaluation based on widely accepted practices\nin information retrieval.\nFor each question in our test collection, PubMed queries were manually crafted to\nfetch an initial set of hits. These queries took advantage of existing advanced search\nfeatures to simulate the types of results that would be currently available to a knowl-\nedgeable physician. Specifically, widely accepted tools for narrowing down PubMed\nsearch results such as Clinical Queries were employed whenever appropriate.\nAs a concrete example, consider the following question: What is the best treatment for\nanalgesic rebound headaches? The search started with the initial terms ?analgesic rebound\nheadache? with a ?narrow therapy filter.? In PubMed, this query is:\n"},{"#tail":"\n","@confidence":"0.969838285714286","#text":"\nNote that PubMed automatically identifies concepts and attempts matching both\nin abstract text and MeSH headings. We always restrict searches to articles that have\nabstracts, are published in English, and are assigned the MeSH term humans (as opposed\nto say, experiments on animals)?these are all strategies commonly used by clinicians.\nIn this case, because none of the top 20 results were relevant, the query was ex-\npanded with the term side effects to emphasize the aspect of the problem requiring an\nintervention. The final query for the question became:\n"},{"#tail":"\n","@confidence":"0.976815071428571","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nThe first author, who is a medical doctor, performed the query formulation process\nmanually for every question in our collection; she verified that each hit list contained at\nleast some relevant documents and that the results were as good as could be reasonably\nachieved. The process of generating queries averaged about 40 minutes per question.\nThe top 50 results for each query were retained for our experiments. In total, 2,309\ncitations were gathered because some queries returned fewer than 50 hits. The process\nof generating a ?good? PubMed query is not a trivial task, which we have side-stepped\nin this work by placing a human in the loop. We return to this issue in Section 12.\nAll abstracts gathered by this process were exhaustively examined for relevance by\nthe first author. It is important to note that relevance assessment in the clinical domain\nrequires significant medical knowledge (in short, a medical degree). After careful con-\nsideration, we decided to assess only topical relevance, with the understanding that\nthe applicability of information from a specific citation in real-world settings depends\n"},{"#tail":"\n","@confidence":"0.9259528","#text":"\ninformation.\nBecause all abstracts were judged, we did not have to worry about impartiality\nissues when comparing different systems. In total, the relevance assessment process\ntook approximately 100 hours, or about an average of 2 hours per question.\nOur reranking experiment compared four different systems:\n"},{"#tail":"\n","@confidence":"0.983353375","#text":"\nand the EBM-based reranker (weighted linear interpolation).\nQuestions in the development set were used to debug the EBM-based reranker as\nwe implemented the scoring algorithm. The development questions were also used to\ntune the weight for combining scores from the term-based scorer and EBM-based scorer;\nby simply trying all possible values, we settled on a ? of 0.8, that is, 80% weight to the\nEBM score, and 20% weight to the term-based score. As we shall see later, it is unclear\nif evidence combination in this simple manner helps at all; for one, it is debatable\nwhich metric should be optimized. The test questions were hidden during the system\n"},{"#tail":"\n","@confidence":"0.9096276","#text":"\nComputational Linguistics Volume 33, Number 1\ndevelopment phase and served as a blind held-out test set for assessing the generality\nof our algorithm.\nIn our experiment, we collected the following metrics, all computed automatically\nusing our relevance judgments:\n"},{"#tail":"\n","@confidence":"0.980694794871795","#text":"\neach relevant document is retrieved (Baeza-Yates and Ribeiro-Neto 1999).\nIt is the most widely accepted single-value metric in information retrieval,\nand is seen to balance the need for both precision and recall.\n Mean Reciprocal Rank (MRR) is a measure of how far down a hit list the\nuser must browse before encountering the first relevant result. The score is\nequal to the reciprocal of the rank, that is, a relevant document at rank 1\ngets a score of 1, 1/2 at rank 2, 1/3 at rank 3, and so on. Note that this\nmeasure only captures the appearance of the first relevant document.\nFurthermore, due to its discretization, MRR values are noisy on small\ncollections.\n Total Document Reciprocal Rank (TDRR) is the sum of the reciprocal\nranks of all relevant documents. For example, if relevant documents were\nfound at ranks 2 and 5, the TDRR would be 1/2 + 1/5 = 0.7. TDRR\nprovides an advantage over MRR in that it captures the ranks of all\nrelevant documents?emphasizing their appearance at higher ranks. The\ndownside, however, is that TDRR does not have an intuitive interpretation.\nFor our reranking experiment, we applied the Wilcoxon signed-rank test to deter-\nmine the statistical significance of the results. This test is commonly used in information\nretrieval research because it makes minimal assumptions about the underlying distrib-\nution of differences. For each evaluation metric, significance at the 1% level is indicated\nby either  or , depending on the direction of change; significance at the 5% level\nis indicated by  or , depending on the direction of change. Differences that are not\nstatistically significant are marked with the symbol ?.\nWe report results under two different scoring criteria. Under the lenient condition,\ndocuments marked ?contains answer? and ?relevant? were given credit; these results\nare shown in Table 7 (for the development set) and Table 8 (for the blind held-out test\nset). Across all questions, both the EBM-based reranker and combination reranker sig-\nnificantly outperform the PubMed baseline on all metrics. In many cases, the differences\nare particularly noteworthy?for example, our EBM citation scoring algorithm more\nthan doubles the baseline in terms of MAP and P10 on the test set. There are enough\ntherapy questions to achieve statistical significance in the task-specific results; however,\ndue to the smaller number of questions for the other clinical tasks, those results are\nnot statistically significant. Results also show that the simple term-based reranker out-\nperforms the PubMed baseline, demonstrating the importance of recognizing outcome\nstatements in MEDLINE abstracts.\nAre the differences in performance between the term-based, EBM, and combination\nrerankers statistically significant? Results of Wilcoxon signed-rank tests are shown in\nTable 11. Both the EBM and combination rerankers significantly outperform the term-\nbased reranker (at the 1% level, on all metrics, on both development and test set), with\n"},{"#tail":"\n","@confidence":"0.859089","#text":"\n?Difference not statistically significant.\nthe exception of MRR on the development set. However, for all metrics, on both the\ndevelopment set and test set, there is no significant difference between the EBM and\ncombination reranker (which combines both term-based and EBM-based evidence). In\nthe parameter tuning process, we could not find a weight where performance across all\nmeasures was higher; in the end, we settled on what we felt was a reasonable weight\nthat improved P10 and MRR on the development set.\nUnder the strict condition, only documents marked ?contains answer? were given\ncredit; these results are shown in Table 9 (for the development set) and Table 10\n(for the blind held-out test set). The same trend is observed?in fact, larger relative\ngains were achieved under the strict scoring criteria for our EBM and combination\n"},{"#tail":"\n","@confidence":"0.989523583333333","#text":"\nSignificance at the 1% level, depending on direction of change.\nSignificance at the 5% level, depending on direction of change.\n?Difference not statistically significant.\nrerankers. Results of Wilcoxon signed-rank tests on the term-based, EBM, and com-\nbination rerankers are also shown in Table 11 for the strict scoring condition. In most\ncases, combining term scoring with EBM scoring does not help. In almost all cases,\nthe EBM and combination reranker perform significantly better than the term-based\nreranker.\nHow does better ranking of citations impact end-to-end question answering perfor-\nmance? We shall return to this issue in Sections 9 and 10, which describe and evaluate\nthe answer generation module, respectively. In the next section, we describe more\ndetailed experiments with our EBM citation scoring algorithm.\n"},{"#tail":"\n","@confidence":"0.8893","#text":"\nA potential, and certainly valid, criticism of our EBM citation scoring algorithm is its\nad hoc nature. Weights for various features were assigned based on intuition, reflecting\nour understanding of the domain and our knowledge about the principles of evidence-\nbased medicine. Parameters were fine-tuned during the system implementation process\nby actively working with the development set; however, this was not done in any\nsystematic fashion. Nevertheless, results on the blind held-out test set confirm the\ngenerality of our citation scoring algorithm.\n"},{"#tail":"\n","@confidence":"0.983941416666667","#text":"\n?Difference not statistically significant.\nIn the development of various language technology applications, it is common for\nthe first materialization of a new capability to be rather ad hoc in its implementation.\nThis is a reflection of an initial attempt to understand both the problem and solution\nspaces. Subsequent systems, with a better understanding of the possible technical ap-\nproaches and their limitations, are then able to implement a more principled solution.\nBecause our clinical question-answering system is the first of its type that we are aware\nof, in terms of both depth and scope, it is inevitable that our algorithms suffer from\nsome of these limitations. Similarly, our collection of clinical questions is the first test\ncollection of its type that we are aware of. Typically, construction of formal models is\nonly made possible by the existence of test collections. We hope that our work sheds new\ninsight on question answering in the clinical domain and paves the way for future work.\n"},{"#tail":"\n","@confidence":"0.989339263157895","#text":"\n?Difference not statistically significant.\nIn addition, there are some theoretical obstacles for developing a more formal (say,\ngenerative) model. Most methods for training such models require independently and\nidentically distributed samples from the underlying distribution?which is certainly not\nthe case with our test collection. Moreover, the event space of queries and documents\nis extremely large or even infinite, depending on how it is defined. Our training data,\nassumed to be samples from this underlying distribution, is extremely small compared\nto the event space, and hence it is unlikely that popular methods (e.g., maximum\nlikelihood estimates) would yield an accurate characterization of the true distribution.\nFurthermore, many techniques for automatically setting parameters make use\nof maximum likelihood techniques?which do not maximize the correct objective\nfunction. Maximizing the likelihood of generating the training data does not mean\nthat the evaluation metric under consideration (e.g., mean average precision) is also\nmaximized?this phenomenon is known as metric divergence.\nNevertheless, it is important to better understand the effects of parameter settings\nin our system. This section describes a few experiments aimed at this goal.\nThe EBM score of a MEDLINE citation is the sum of three separate components,\neach representing a facet of evidence-based medicine. This structure naturally suggests\na modification to Equation (3) that weights each score component differently:\n"},{"#tail":"\n","@confidence":"0.994485105263158","#text":"\nThe parameters ?1 and ?2 can be derived from our development set. For therapy\nquestions, we exhaustively searched through the entire parameter space, in increments\nof hundredths, and determined the optimal settings to be ?1 = 0.38, ?2 = 0.34 (which\nwas found to slightly improve all metrics). The performance surface for mean average\nprecision is shown in Figure 3, which plots results for all possible parameter values\non the development set. Numeric results are shown in Table 12. It can be seen that\noptimizing the parameters in this fashion does not lead to a statistically significant\nincrease in any of the metrics. Furthermore, these gains do not carry over to the blind\nheld-out test set. We also tried optimizing the ??s on all questions in the development\nset. These results are shown in Table 13. Once again, differences are not statistically\nsignificant.\nWhy does parameter optimization not help? We believe that there are two factors\nat play here: On the one hand, parameter settings should be specific to the clinical\ntask. This explains why optimizing across all question types at the same time did\nnot improve performance. On the other hand, there are too few questions of any\nparticular type to represent an accurate sampling of all possible questions. This is why\nparameter tuning on therapy questions did not significantly alter performance. These\nexperiments point to the need for larger test collections, which is an area for future\nwork.\n"},{"#tail":"\n","@confidence":"0.9324215","#text":"\nAnother component of our EBM citation scoring algorithm that contains many\nad hoc weights is Stask, defined in Equation (7) and repeated here:\n"},{"#tail":"\n","@confidence":"0.999861416666667","#text":"\nThe function ?(t) maps a particular MeSH term to a weight that quantifies the\ndegree to which it is a positive or negative indicator for the particular clinical task.\nBecause these weights were heuristically assigned, it would be worthwhile to examine\nthe impact they have on performance. As a variant, we modified ?(t) so that all MeSH\nterms were mapped to ?1; in other words, we did not encode granular levels of\n?goodness.? These results are shown in Table 8. Although performance dropped across\nall metrics, none of the differences were statistically significant except for P10 on the\ntest set.\nThe series of experiments described herein help us better understand the effects of\nparameter settings on abstract reranking performance. As can be seen from the results,\nour algorithm is relatively invariant with respect to the choice of parameters, con-\nfirming that our primary contribution is the EBM-based approach to clinical question\n"},{"#tail":"\n","@confidence":"0.952642333333334","#text":"\nComputational Linguistics Volume 33, Number 1\nanswering, and that our performance gains cannot be simply attributed to a fortunate\nchoice of parameters.\n9. From Scoring Citations to Answering Questions\nThe aim of question-answering technology is to move from the ?hit list? paradigm of\ninformation retrieval, where users receive a list of potentially relevant documents that\nthey must then browse through, to a mode of interaction where users directly receive\nresponses that satisfy their information needs. In our current architecture, fetching a\nhigher-quality ranked list is a step towards generating responsive answers.\nThe most important characteristic of answers, as recommended by Ely et al (2005)\nin their study of real-world physicians, is that they focus on bottom-line clinical\nadvice?information that physicians can directly act on. Ideally, answers should in-\ntegrate information from multiple clinical studies, pointing out both similarities and\ndifferences. The system should collate concurrences, that is, if multiple abstracts ar-\nrive at the same conclusion?it need not be repeated unless the physician wishes to\n?drill down?; the system should reconcile contradictions, for example, if two abstracts\ndisagree on a particular treatment because they studied different patient populations.\nWe have noted that many of these desiderata make complex question answering quite\nsimilar to multi-document summarization (Lin and Demner-Fushman 2005b), but these\nfeatures are also beyond the capabilities of current summarization systems.\nIt is clear that the type of answers desired by physicians require a level of semantic\nanalysis that is beyond the current state of the art, even with the aid of existing medical\nontologies. For example, even the seemingly straightforward task of identifying simi-\nlarities and differences in outcome statements is rendered exceedingly complex by the\ntremendous amount of background medical knowledge that must be brought to bear\nin interpreting clinical results and subtle differences in study design, objectives, and\nresults; the closest analogous task in computational linguistics?redundancy detection\nfor multi-document summarization?seems easy by comparison. Furthermore, it is\nunclear if textual strings make ?good answers.? Perhaps a graphical rendering of the\nsemantic predicates present in relevant abstracts might more effectively convey the\ndesired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Per-\nhaps some variation of multi-level bulleted lists, appropriately integrated with interface\nelements for expanding and hiding items, might provide physicians a better overview\nof the information landscape; see, for example, Demner-Fushman and Lin (2006).\nRecognizing this complex set of issues, we decided to take a simple extractive\napproach to answer generation. For each abstract in our reranked list of citations,\nour system produces an answer by combining the title of the abstract and the top\nthree outcome sentences (in the order they appeared in the abstract). We employed the\noutcome scores generated by the regression model. No attempt was made to synthesize\ninformation from multiple citations. A formal evaluation of this simple approach to\nanswer generation is presented in the next section.\n10. Evaluation of Clinical Answers\nEvaluation of answers within a clinical setting involves a complex decision that must\nnot only take into account topical relevance (i.e., ?Does the answer address the infor-\nmation need??), but also situational relevance (e.g., Saracevic 1975, Barry and Schamber\n"},{"#tail":"\n","@confidence":"0.981261388888889","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\n1998). The latter factor includes many issues such as the strength of evidence, recency\nof results, and reputation of the journal. Clinicians need to carefully consider all these\nelements before acting on any information for the purposes of patient care. Within the\nframework of evidence-based medicine, the physician is the final arbiter of how clinical\nanswers are integrated into the broader activities of medical care, but this complicates\nany attempt to evaluate answers generated by our system.\nIn assessing answers produced by our system, we decided to focus only on the\nevaluation of topical relevance?assessors were only presented with answer strings,\ngenerated in the manner described in the previous section. Metadata that would con-\ntribute to judgments about situational relevance, such as the strength of evidence,\nnames of the authors and the journal, and so on, were purposefully suppressed. Our\nevaluation compared the top five answers generated from the original PubMed hit list\nand the top five answers generated from our reranked list of citations. Answers were\nprepared for all 24 questions in our development set.\nWe recruited two medical doctors (one family practitioner, one surgeon) from the\nNational Library of Medicine to evaluate the textual answers. Our instructions clearly\nstated that only topical relevance was to be assessed. We asked the physicians to provide\n"},{"#tail":"\n","@confidence":"0.993999136363636","#text":"\ninformation in answering the clinical question, and that the source citation\nwas not worth examining.\nWe purposely avoided short linguistic labels for the judgments so as to sidestep\nthe question of ?What exactly is an answer to a clinical question?? Informally, an-\nswers marked with a plus can be considered ?actionable? clinical advice. Answers\nmarked with a check provide relevant information that may influence the physician?s\nactions.\nWe adopted a double-blind study design for the actual assessment process: Answers\nfrom both systems were presented in a randomized order without any indication of\nwhich system the response came from (duplicates were suppressed). A paper printout,\ncontaining each question followed by the blinded answers, was presented to each\nassessor. We then coded the relevance judgments in a plain text file manually. During\nthis entire time, the key that maps answers to systems was kept in a separate file and\nhidden from everyone, including the authors. All scores were computed automatically\nwithout human intervention.\nAnswer precision was calculated for two separate conditions: Under the strict\ncondition (Table 15), only ?plus? judgments were considered good; under the lenient\ncondition (Table 16), both ?plus? and ?check? judgments were considered good. As can\nbe seen, our EBM algorithm significantly outperforms the baseline under both the strict\nand lenient conditions, according to both assessors. On average, the length of answers\ngenerated from the original PubMed list of citations was 90 words; answers generated\nfrom the reranked list of citations averaged 87 words. Answers from both sources\n"},{"#tail":"\n","@confidence":"0.999007695652174","#text":"\nwere significantly shorter than the abstracts from which they were extracted (250 word\naverage for original PubMed results and 270 word average for reranked results).\nTo give a feel for the types of responses that are generated by our system, consider\nthe following question:\nWhat is the best treatment for analgesic rebound headaches?\nThe following is an example of a response that received a ?plus? judgment:\nMedication overuse headache from antimigraine therapy: clinical features,\npathogenesis and management: Because of easy availability and low expense, the\ngreatest problem appears to be associated with barbiturate-containing combination\nanalgesics and over-the-counter caffeine-containing combination analgesics. The best\nmanagement advice is to raise awareness and strive for prevention. Reduction in\nheadache risk factors should include behavioural modification approaches to headache\ncontrol earlier in the natural history of migraine.\nThis answer was accepted by both physicians because it clearly states that specific\nanalgesics are most likely to cause the problem, and gives a direct guideline for preven-\ntive treatment.\nIn contrast, the following response to the same question received a ?check?:\nDoes chronic daily headache arise de novo in association with regular use of\nanalgesics? Regular use of analgesics preceded the onset of daily headache in 5 patients\nby a mean of 5.4 years (range, 2 to 10 years). In 1 patient, the onset of daily headache\npreceded regular use of analgesics by almost 30 years. These findings suggest that\nindividuals with primary headache, specifically migraine, are predisposed to\ndeveloping chronic daily headache in association with regular use of analgesics.\n"},{"#tail":"\n","@confidence":"0.976405404255319","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nAlthough this answer provides information about the risks and causes of the\nheadaches, neither prevention nor treatment is explicitly mentioned. For these reasons\nthis response was marked as potentially leading to an answer, but not as containing one.\nTo summarize, we have presented a simple answer generation algorithm that\nis capable of supplying clinically relevant responses to physicians. Compared to\nPubMed, which does not take into account the principles of evidence-based medicine,\nour question-answering system represents a leap forward in information access\ncapabilities.8\n11. Related Work and Discussion\nClinical question answering is an emerging area of research that has only recently begun\nto receive serious attention. As a result, there exist relatively few points of comparison to\nour own work, as the research space is sparsely populated. In this section, however, we\nwill attempt to draw connections to other clinical information systems (although not\nnecessarily for question answering) and related domain-specific question-answering\nsystems. For an overview of systems designed to answer open-domain factoid ques-\ntions, the TREC QA track overview papers are a good place to start (Voorhees and\nTice 1999). In addition, there has been much work on the application of linguistic and\nsemantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for\na brief overview.\nThe idea that clinical information systems should be sensitive to the practice of\nevidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations,\nMendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic\nclinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to auto-\nmatically classify citations for task-specific retrieval, similar in spirit to the Hedges\nProject (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and\nMendonc?a reported good performance for etiology, diagnosis, and in particular therapy,\nbut not prognosis. Although originally developed as a tool to assist in query formu-\nlation, Booth (2000) pointed out that PICO frames can be employed to structure IR\nresults for improving precision. PICO-based querying in information retrieval is merely\nan instance of faceted querying, which has been widely used by librarians since the\nintroduction of automated retrieval systems (e.g., Meadow et al 1989). The work of\nHearst (1996) demonstrates that faceted queries can be converted into simple filtering\nconstraints to boost precision.\nThe feasibility of automatically identifying outcome statements in secondary\nsources has been demonstrated by Niu and Hirst (2004). Their study also illustrates\nthe importance of semantic classes and relations. However, extraction of outcome state-\nments from secondary sources (meta-analyses, in this case) differs from extraction of\noutcomes from MEDLINE citations because secondary sources represent knowledge\nthat has already been distilled by humans (which may limit its scope). Because sec-\nondary sources are often more consistently organized, it is possible to depend on\ncertain surface cues for reliable extraction (which is not possible for MEDLINE ab-\nstracts in general). Our study tackles outcome identification in primary medical sources\nand demonstrates that respectable performance is possible with a feature-combination\napproach.\n8 Although note that answer generation from the PubMed results also requires the use of the outcome\nextractor.\n"},{"#tail":"\n","@confidence":"0.994261489795918","#text":"\nComputational Linguistics Volume 33, Number 1\nThe literature also contains work on sentence-level classification of MEDLINE\nabstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) de-\nscribe a machine learning approach to automatically label sentences as belonging to\nintroduction, methods, results, or conclusion using structured abstracts as training data\n(see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential\nweighting of automatically labeled sections can lead to improved retrieval performance.\nNote, however, that such labels are orthogonal to PICO frame elements, and hence\nare not directly relevant to knowledge extraction for clinical question answering. In a\nsimilar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative\nstatements in MEDLINE abstracts, but once again, this work is not directly applicable\nto clinical question answering.\nIn addition to question answering, multi-document summarization provides a com-\nplementary approach to addressing clinical information needs. The PERSIVAL project,\nthe most comprehensive study of such techniques applied on medical texts to date,\nleverages patient records to generate personalized summaries in response to physicians?\nqueries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although\nthe system incorporates both a user and a task model, it does not explicitly capture\nthe principles of evidence-based medicine. Patient information is no doubt important\nto answering clinical questions, and our work could certainly benefit from experiences\ngained in the PERSIVAL project.\nThe application of domain models and deep semantic knowledge to question\nanswering has been explored by a variety of researchers (e.g., Jacquemart and\nZweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops\non question answering in restricted domains at ACL 2004 and AAAI 2005. Our work\ncontributes to this ongoing discourse by demonstrating a specific application in the\ndomain of clinical medicine.\nFinally, the evaluation of answers to complex questions remains an open research\nproblem. Although it is clear that measures designed for open-domain factoid questions\nare not appropriate, the community has not agreed on a methodology that will allow\nmeaningful comparisons of results from different systems. In Sections 9 and 10, we\nhave discussed many of these issues. Recently, there is a growing consensus that an\nevaluation methodology based on the notion of ?information nuggets? may provide\nan appropriate framework for assessing the quality of answers to complex questions.\nNugget F-score has been employed as a metric in the TREC question-answering track\nsince 2003, to evaluate so-called definition and ?other? questions (Voorhees 2003). A\nnumber of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcom-\nings of the original nugget scoring model, although a number of these issues have been\nrecently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of\nthe nugget evaluation methodology to a domain as specific as clinical medicine is an\nendeavor that has yet to be undertaken.\n12. Future Work\nThe design and implementation of our current system leaves many open avenues for\nfuture exploration, one of which concerns our assumptions about the query interface.\nPreviously, a user study (Lin et al 2003) has shown that people are reluctant to type\nfull natural language questions, even after being told that they were using a question-\nanswering system and that typing complete questions would result in better perform-\nance. We have argued that a query interface based on structured PICO frames will\nyield better-formulated queries, although it is unclear whether physicians would invest\n"},{"#tail":"\n","@confidence":"0.998705568627451","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nthe upfront effort necessary to accomplish this. Issuing extremely short queries appears\nto be an ingrained habit of information seekers today, and the dominance of World Wide\nWeb searches reinforce this behavior. Given these trends, physicians may actually prefer\nthe rapid back-and-forth interaction style that comes with short queries. We believe\nthat if systems can produce noticeably better results with richer queries, users will\nmake more of an effort to formulate them. This, however, presents a chicken-and-egg\nproblem: One possible solution is to develop models that can automatically fill query\nframes given a couple of keywords?this would serve to kick-start the query generation\nprocess.\nThe astute reader will have noticed that the initial retrieval of abstracts in our\nstudy was performed with high-quality manually crafted queries (that were part of\nthe test collection). Although this was intended to demonstrate the performance of our\nEBM citation scoring algorithm with respect to a strong baseline, it also means that we\nhave omitted a component in the automatic question-answering process. Translating a\nclinical question into a good PubMed query is not a trivial task?in our experiments,\nit required an experienced searcher approximately 40 minutes on average per question.\nHowever, it is important to note that query formulation in the clinical domain is not\na problem limited to question-answering systems, but one that users of all retrieval\nsystems must contend with.\nNevertheless, there are three potential solutions to this problem: First, although\nthere is an infinite variety of clinical questions, the number of query types is bounded\nand far smaller in number; see Huang, Lin, and Demner-Fushman (2006) for an analysis.\nIn a query interface based on PICO frames, it is possible to identify a number of proto-\ntypical query frames. From these prototypes, one can generate query templates that ab-\nstract over the actual slot fillers?this is the idea behind Clinical Queries. Although this\nmethod will probably not retrieve citations as high in quality as custom-crafted queries,\nthere is reason to believe that as long as a reasonable set of citations is retrieved, our sys-\ntem will be able to extract relevant answers (given the high accuracy of our knowledge\nextractors and citation scoring algorithm). The second approach to tackling this problem\nis to bypass PubMed altogether and index MEDLINE with another search engine.\nDue to the rapidly changing nature of the entire MEDLINE database, experiments for\npractical purposes would most likely be conducted on a static subset of the collection,\nfor example, the ten-year portion created for the TREC 2004 genomics track (Hersh,\nBhupatiraju, and Corley 2004). Recent results from TREC have demonstrated that high\nperformance ad hoc retrieval is possible in the genomics domain (Hersh et al 2005),\nand it is not a stretch to imagine adopting these technologies for clinical tasks. Using\na separate search engine would provide other benefits as well: Greater control over\nthe document retrieval process would allow one to examine the effects of different\nindexing schemes, different query operators, and techniques such as query expansion;\nsee, for example, Aronson, Rindflesch, and Browne (1994). Finally, yet another way to\nsolve the document retrieval problem is to eliminate that stage completely. Recall that\nour two-stage architecture was a practical expediency, because we did not have access\nto the computing resources necessary to pre-extract PICO elements from the entire\nMEDLINE database and directly index the results. Given access to more resources,\na system could index identified PICO elements and directly match queries against a\nknowledge store.\nFinally, answer generation remains an area that awaits further exploration, although\nwe would have to first define what a good answer should be. We have empirically\nverified that an extractive approach based on outcome sentences is actually quite\nsatisfactory, but our algorithm does not currently integrate evidence from multiple\n"},{"#tail":"\n","@confidence":"0.985539033333333","#text":"\nComputational Linguistics Volume 33, Number 1\nabstracts; although see Demner-Fushman and Lin (2006). Furthermore, the current an-\nswer generator does not handle complex issues such as contradictory and inconsistent\nstatements. To address these very difficult challenges, finer-grained semantic analysis\nof medical texts is required.\n13. Conclusion\nOur experiments in clinical question answering provide some answers to the broader\nresearch question regarding the role of knowledge-based and statistical techniques in\nadvanced question answering. This work demonstrates that the two approaches are\ncomplementary and can be seamlessly integrated into algorithms that draw from the\nbest of both worlds. Explicitly coded semantic knowledge, in the form of UMLS, and\nsoftware for leveraging this resource?for example, MetaMap?combine to simplify\nmany knowledge extraction tasks that would be far more difficult otherwise. The re-\nspectable performance of our population, problem, and intervention extractors, all of\nwhich use relatively simple rules, provides evidence that complex clinical problems\ncan be tackled by appropriate use of ontological knowledge. Explicitly coded semantic\nknowledge is less helpful for outcome identification due to the large variety of possi-\nble ?outcomes;? nevertheless, knowledge-rich features can be combined with simple,\nstatistically derived features to build a good outcome classifier. Overall, this work\ndemonstrates that the application of a semantic domain model yields clinical question\nanswering capabilities that significantly outperform presently available technology,\nespecially when coupled with traditional statistical methods (classification, evidence\ncombination, etc.).\nWe have taken an important step in building a complete question-answering system\nthat assists physicians in the patient care process. Our work demonstrates that the\nprinciples of evidence-based medicine can be computationally captured and imple-\nmented in a system, and although we are still far from operational deployment, these\npositive results are certainly encouraging. Information systems in support of the clinical\ndecision-making process have the potential to improve the quality of health care, which\nis a worthy goal indeed.\n"},{"#tail":"\n","@confidence":"0.9983586","#text":"\nWe would like to thank Dr. Charles\nSneiderman and Dr. Kin Wah Fung for\nthe evaluation of the answers. For this\nwork, D. D-F. was supported by an\nappointment to the National Library of\n"},{"#tail":"\n","@confidence":"0.979267166666667","#text":"\nadministered by the Oak Ridge Institute\nfor Science and Education through an\ninter-agency agreement between the U.S.\nDepartment of Energy and the National\nLibrary of Medicine. For this work, J. L.\nwas supported in part by a grant from the\nNational Library of Medicine, where he\nwas a visiting researcher during the\nsummer of 2005. We would like to thank\nthe anonymous reviewers for their valuable\ncomments. J. L. would like to thank Kiri\nand Esther for their kind support.\n"},{"#tail":"\n","@confidence":"0.884882333333333","#text":"\nAd Hoc Working Group for Critical\nAppraisal of the Medical Literature. 1987.\nA proposal for more informative abstracts\nof clinical articles. Annals of Internal\nMedicine, 106:595?604.\nAronson, Alan R. 2001. Effective mapping\nof biomedical text to the UMLS\nMetathesaurus: The MetaMap program.\nIn Proceeding of the 2001 Annual Symposium\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.99333","#text":"\nUniversity of Maryland, College Park\n"},{"#tail":"\n","@confidence":"0.994504","#text":"\nUniversity of Maryland, College Park\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.944991","@genericHeader":"abstract","#text":"\n1. Introduction\n"},{"#tail":"\n","@confidence":"0.984602","@genericHeader":"method","#text":"\n2. The Framework of Evidence-Based Medicine\n"},{"#tail":"\n","@confidence":"0.934735","@genericHeader":"method","#text":"\n4. System Architecture\n"},{"#tail":"\n","@confidence":"0.800611","@genericHeader":"method","#text":"\n5. Knowledge Extraction for Evidence-Based Medicine\n"},{"#tail":"\n","@confidence":"0.762966","@genericHeader":"method","#text":"\n GROUP ([Nn]=[0?9]+)\n"},{"#tail":"\n","@confidence":"0.911786","@genericHeader":"method","#text":"\n6. Operationalizing Evidence-Based Medicine\n"},{"#tail":"\n","@confidence":"0.966904","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.902759","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.6935175","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nTable 1\n"},{"#tail":"\n","@confidence":"0.4312095","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nTable 3\n"},{"#tail":"\n","@confidence":"0.469302","#text":"\nTable 5\n"},{"#tail":"\n","@confidence":"0.6488185","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nTable 7\n"},{"#tail":"\n","@confidence":"0.662263","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nTable 9\n"},{"#tail":"\n","@confidence":"0.8572445","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nTable 11\n"},{"#tail":"\n","@confidence":"0.8399585","#text":"\nDemner-Fushman and Lin Answering Clinical Questions\nTable 13\n"}],"page":[{"#tail":"\n","@confidence":"0.996964","#text":"\n64\n"},{"#tail":"\n","@confidence":"0.997855","#text":"\n65\n"},{"#tail":"\n","@confidence":"0.981806","#text":"\n66\n"},{"#tail":"\n","@confidence":"0.999611","#text":"\n67\n"},{"#tail":"\n","@confidence":"0.997786","#text":"\n68\n"},{"#tail":"\n","@confidence":"0.997852","#text":"\n69\n"},{"#tail":"\n","@confidence":"0.995156","#text":"\n70\n"},{"#tail":"\n","@confidence":"0.997805","#text":"\n71\n"},{"#tail":"\n","@confidence":"0.998534","#text":"\n72\n"},{"#tail":"\n","@confidence":"0.999333","#text":"\n73\n"},{"#tail":"\n","@confidence":"0.997657","#text":"\n74\n"},{"#tail":"\n","@confidence":"0.995034","#text":"\n75\n"},{"#tail":"\n","@confidence":"0.992852","#text":"\n76\n"},{"#tail":"\n","@confidence":"0.951744","#text":"\n77\n"},{"#tail":"\n","@confidence":"0.993824","#text":"\n78\n"},{"#tail":"\n","@confidence":"0.636905","#text":"\n37\n"},{"#tail":"\n","@confidence":"0.990155","#text":"\n79\n"},{"#tail":"\n","@confidence":"0.991307","#text":"\n80\n"},{"#tail":"\n","@confidence":"0.990801","#text":"\n81\n"},{"#tail":"\n","@confidence":"0.974803","#text":"\n82\n"},{"#tail":"\n","@confidence":"0.99466","#text":"\n83\n"},{"#tail":"\n","@confidence":"0.907428","#text":"\n84\n"},{"#tail":"\n","@confidence":"0.985275","#text":"\n85\n"},{"#tail":"\n","@confidence":"0.960351","#text":"\n86\n"},{"#tail":"\n","@confidence":"0.996318","#text":"\n87\n"},{"#tail":"\n","@confidence":"0.990152","#text":"\n88\n"},{"#tail":"\n","@confidence":"0.993237","#text":"\n89\n"},{"#tail":"\n","@confidence":"0.99098","#text":"\n90\n"},{"#tail":"\n","@confidence":"0.988492","#text":"\n91\n"},{"#tail":"\n","@confidence":"0.993622","#text":"\n92\n"},{"#tail":"\n","@confidence":"0.99281","#text":"\n93\n"},{"#tail":"\n","@confidence":"0.994696","#text":"\n94\n"},{"#tail":"\n","@confidence":"0.989633","#text":"\n95\n"},{"#tail":"\n","@confidence":"0.987312","#text":"\n96\n"},{"#tail":"\n","@confidence":"0.950797","#text":"\n97\n"},{"#tail":"\n","@confidence":"0.995755","#text":"\n98\n"},{"#tail":"\n","@confidence":"0.984145","#text":"\n99\n"},{"#tail":"\n","@confidence":"0.685268","#text":"\n100\n"},{"#tail":"\n","@confidence":"0.972941","#text":"\n101\n"},{"#tail":"\n","@confidence":"0.974691","#text":"\n102\n"},{"#tail":"\n","@confidence":"0.999688","#text":"\n103\n"}],"table":[{"#tail":"\n","@confidence":"0.928979","#text":"\nEvaluation of the population extractor.\nCorrect (%) Unknown (%) Wrong (%)\nBaseline 53 ? 47\nExtractor 80 10 10\nlation extractor was judged to be correct if it occurred in a sentence that was annotated\n"},{"#tail":"\n","@confidence":"0.963602285714286","#text":"\nComputational Linguistics Volume 33, Number 1\nTable 2\nEvaluation of the problem extractor.\nCorrect (%) Unknown (%) Wrong (%)\nAbstract title 85 10 5\nTitle + 1st two sentences 90 5 5\nEntire abstract 86 2 12\n"},{"#tail":"\n","@confidence":"0.829699","#text":"\nEvaluation of the intervention extractor.\nCorrect (%) Unknown (%) Wrong (%)\nBaseline 60 ? 40\nExtractor 80 ? 20\n"},{"#tail":"\n","@confidence":"0.982839666666667","#text":"\n2-sentence cutoff (%) 3-sentence cutoff (%)\nB2 AH2 LR2 B3 AH3 LR3\nTherapy 74 75 77 75 95 93\nDiagnosis 72 70 78 75 78 89\nPrognosis 73 76 79 85 87 89\nEtiology 64 68 74 78 83 88\n"},{"#tail":"\n","@confidence":"0.5456706","#text":"\nPublication Type: Clinical Trial, Randomized Controlled Trial\nPMID: 1621668\nStrength of Evidence: grade A\nFigure 2\nSample output from our PICO extractors.\n"},{"#tail":"\n","@confidence":"0.986531166666667","#text":"\nComputational Linguistics Volume 33, Number 1\nTable 6\nComposition of our clinical questions collection.\nTherapy Diagnosis Prognosis Etiology Total\nDevelopment 10 6 3 5 24\nTest 12 6 3 5 26\n"},{"#tail":"\n","@confidence":"0.7372112","#text":"\n((?headache disorders?[TIAB] NOT Medline[SB]) OR ?headache disorders?[MeSH\nTerms] OR analgesic rebound headache[Text Word]) AND (randomized controlled\ntrial[Publication Type] OR (randomized[Title/Abstract] AND\ncontrolled[Title/Abstract] AND trial[Title/Abstract])) AND hasabstract[text] AND\nEnglish[Lang] AND ?humans?[MeSH Terms]\n"},{"#tail":"\n","@confidence":"0.9810508","#text":"\n(Lenient, Development) Lenient results of reranking experiment on development questions\nfor the baseline PubMed condition, term-based reranker, EBM-based reranker, and combination\nreranker.\nTherapy Diagnosis Prognosis Etiology\nPrecision at 10 (P10)\nPubMed 0.300 0.367 0.400 0.533\nTerm 0.520 (+73%) 0.383 (+4.5%)? 0.433 (+8.3%)? 0.553 (+3.8%)?\nEBM 0.730 (+143%) 0.800 (+118%) 0.633 (+58%)? 0.553 (+3.7%)?\nCombo 0.750 (+150%) 0.783 (+114%) 0.633 (+58%)? 0.573 (+7.5%)?\nMean Average Precision (MAP)\nPubMed 0.354 0.421 0.385 0.608\nTerm 0.622 (+76%) 0.438 (+4.0%)? 0.464 (+21%)? 0.720 (+18%)?\nEBM 0.819 (+131%) 0.794 (+89%) 0.635 (+65%)? 0.649 (+6.7%)?\nCombo 0.813 (+130%) 0.759 (+81%) 0.644 (+67%)? 0.686 (+13%)?\nMean Reciprocal Rank (MRR)\nPubMed 0.428 0.792 0.733 0.900\nTerm 0.853 (+99%) 0.739 (?6.7%)? 0.833 (+14%)? 1.000 (+11%)?\nEBM 0.933 (+118%) 0.917 (+16%)? 0.667 (?9.1%)? 1.000 (+11%)?\nCombo 0.933 (+118%) 0.917 (+16%)? 1.000 (+36%)? 0.900 (+0.0%)?\nTotal Document Reciprocal Rank (TDRR)\nPubMed 1.317 1.805 1.778 2.008\nTerm 2.305 (+75%) 1.887 (+4.6%)? 1.923 (+8.2%)? 2.291 (+14%)?\nEBM 2.869 (+118%) 2.944 (+63%) 2.238 (+26%)? 2.104 (+4.8%)?\nCombo 2.833 (+115%) 2.870 (+59%) 2.487 (+40%)? 2.108 (+5.0%)?\n(a) Breakdown by clinical task\nP10 MAP MRR TDRR\nPubMed 0.378 0.428 0.656 1.640\nTerm 0.482 (+28%)? 0.577 (+35%) 0.853 (+30%)? 2.150 (+31%)\nEBM 0.699 (+85%) 0.754 (+76%) 0.910 (+39%) 2.650 (+62%)\nCombo 0.707 (+87%) 0.752 (+76%) 0.931 (+42%) 2.648 (+61%)\n"},{"#tail":"\n","@confidence":"0.99198884375","#text":"\nComputational Linguistics Volume 33, Number 1\nTable 8\n(Lenient, Test) Lenient results of reranking experiment on blind held-out test questions for\nthe baseline PubMed condition, term-based reranker, EBM-based reranker, and combination\nreranker.\nTherapy Diagnosis Prognosis Etiology\nPrecision at 10 (P10)\nPubMed 0.350 0.150 0.200 0.320\nTerm 0.575 (+64%) 0.383 (+156%)? 0.333 (+67%)? 0.460 (+43%)?\nEBM 0.783 (+124%) 0.583 (+289%) 0.467 (+133%)? 0.660 (+106%)?\nCombo 0.792 (+126%) 0.633 (+322%) 0.433 (+117%)? 0.660 (+106%)?\nMean Average Precision (MAP)\nPubMed 0.421 0.279 0.235 0.364\nTerm 0.563 (+34%) 0.489 (+76%)? 0.415 (+77%)? 0.480 (+32%)?\nEBM 0.765 (+82%) 0.637 (+129%) 0.722 (+207%)? 0.701 (+93%)?\nCombo 0.770 (+83%) 0.653 (+134%) 0.690 (+194%)? 0.687 (+89%)?\nMean Reciprocal Rank (MRR)\nPubMed 0.579 0.443 0.456 0.540\nTerm 0.660 (+14%)? 0.765 (+73%)? 0.611 (+34%)? 0.650 (+20%)?\nEBM 0.917 (+58%) 0.889 (+101%)? 1.000 (+119%)? 1.000 (+85%)?\nCombo 0.958 (+66%) 0.917 (+107%)? 1.000 (+119%)? 1.000 (+85%)?\nTotal Document Reciprocal Rank (TDRR)\nPubMed 1.669 0.926 0.895 1.381\nTerm 2.204 (+32%) 1.880 (+103%)? 1.390 (+55%)? 1.736 (+26%)?\nEBM 2.979 (+79%) 2.341 (+153%) 2.101 (+138%)? 2.671 (+93%)?\nCombo 3.025 (+81%) 2.380 (+157%) 2.048 (+129%)? 2.593 (+88%)?\n(a) Breakdown by clinical task\nP10 MAP MRR TDRR\nPubMed 0.281 0.356 0.526 1.353\nTerm 0.481 (+71%) 0.513 (+44%) 0.677 (+29%)? 1.945 (+44%)\nEBM 0.677 (+141%) 0.718 (+102%) 0.936 (+78%) 2.671 (+98%)\nCombo 0.688 (+145%) 0.718 (+102%) 0.962 (+83%) 2.680 (+98%)\n"},{"#tail":"\n","@confidence":"0.993158266666667","#text":"\n(Strict, Development) Strict results of reranking experiment on development questions for the\nbaseline PubMed condition, term-based reranker, EBM-based reranker, and combination\nreranker.\nTherapy Diagnosis Prognosis Etiology\nPrecision at 10 (P10)\nPubMed 0.130 0.133 0.100 0.253\nTerm 0.230 (+77%)? 0.217 (+63%)? 0.233 (+133%)? 0.293 (+16%)?\nEBM 0.350 (+170%) 0.350 (+163%)? 0.267 (+167%)? 0.293 (+16%)?\nCombo 0.350 (+170%) 0.367 (+175%)? 0.300 (+200%)? 0.313 (+24%)?\nMean Average Precision (MAP)\nPubMed 0.088 0.108 0.058 0.164\nTerm 0.205 (+134%)? 0.142 (+32%)? 0.090 (+54%)? 0.246 (+50%)?\nEBM 0.314 (+260%)? 0.259 (+140%)? 0.105 (+79%)? 0.265 (+62%)?\nCombo 0.301 (+244%)? 0.248 (+130%)? 0.129 (+122%)? 0.273 (+67%)?\nMean Reciprocal Rank (MRR)\nPubMed 0.350 0.453 0.394 0.367\nTerm 0.409 (+17%)? 0.581 (+28%)? 0.528 (+34%)? 0.700 (+91%)?\nEBM 0.675 (+93%) 0.756 (+67%)? 0.444 (+13%)? 0.800 (+118%)?\nCombo 0.569 (+63%) 0.676 (+49%)? 0.833 (+111%)? 0.700 (+91%)?\nTotal Document Reciprocal Rank (TDRR)\nPubMed 0.610 0.711 0.568 0.721\nTerm 0.872 (+43%)? 1.022 (+44%)? 0.804 (+42%)? 1.224 (+70%)?\nEBM 1.434 (+135%) 1.601 (+125%)? 0.824 (+45%)? 1.298 (+80%)?\nCombo 1.282 (+110%) 1.502 (+111%)? 1.173 (+106%)? 1.241 (+72%)?\n(a) Breakdown by clinical task\nP10 MAP MRR TDRR\nPubMed 0.153 0.105 0.385 0.653\nTerm 0.240 (+57%) 0.183 (+75%)? 0.527 (+37%) 0.974 (+49%)\nEBM 0.328 (+115%) 0.264 (+152%) 0.693 (+80%) 1.371 (+110%)\nCombo 0.340 (+123%) 0.260 (+148%) 0.656 (+71%) 1.315 (+101%)\n"},{"#tail":"\n","@confidence":"0.988511125","#text":"\nComputational Linguistics Volume 33, Number 1\nTable 10\n(Strict, Test) Strict results of reranking experiment on blind held-out test questions for the\nbaseline PubMed condition, term-based reranker, EBM-based reranker, and combination\nreranker.\nTherapy Diagnosis Prognosis Etiology\nPrecision at 10 (P10)\nPubMed 0.108 0.017 0.000 0.080\nTerm 0.192 (+77%)? 0.133 (+700%)? 0.033 ? 0.140 (+75%)?\nEBM 0.233 (+115%)? 0.167 (+900%)? 0.100 ? 0.200 (+150%)?\nCombo 0.258 (+139%) 0.200 (+1100%)? 0.100 ? 0.220 (+175%)?\nMean Average Precision (MAP)\nPubMed 0.061 0.024 0.015 0.050\nTerm 0.082 (+36%)? 0.118 (+386%)? 0.086 (+464%)? 0.086 (+74%)?\nEBM 0.109 (+80%)? 0.091 (+276%)? 0.234 (+1442%)? 0.159 (+220%)?\nCombo 0.120 (+99%)? 0.107 (+339%)? 0.224 (+1372%)? 0.165 (+232%)?\nMean Reciprocal Rank (MRR)\nPubMed 0.282 0.073 0.031 0.207\nTerm 0.368 (+31%)? 0.429 (+488%)? 0.146 (+377%)? 0.314 (+52%)?\nEBM 0.397 (+41%)? 0.431 (+490%)? 0.465 (+1422%)? 0.500 (+142%)?\nCombo 0.556 (+97%) 0.422 (+479%)? 0.438 (+1331%)? 0.467 (+126%)?\nTotal Document Reciprocal Rank (TDRR)\nPubMed 0.495 0.137 0.038 0.331\nTerm 0.700 (+41%)? 0.759 (+454%)? 0.171 (+355%)? 0.596 (+80%)?\nEBM 0.807 (+63%)? 0.654 (+377%)? 0.513 (+1262%)? 0.946 (+186%)?\nCombo 0.969 (+96%) 0.698 (+409%)? 0.479 (+1172%)? 0.975 (+195%)?\n(a) Breakdown by clinical task\nP10 MAP MRR TDRR\nPubMed 0.069 0.045 0.190 0.328\nTerm 0.150 (+117%) 0.092 (+105%) 0.346 (+82%) 0.632 (+93%)\nEBM 0.196 (+183%) 0.129 (+187%) 0.433 (+127%) 0.765 (+133%)\nCombo 0.219 (+217%) 0.138 (+207%) 0.494 (+160%) 0.851 (+159%)\n"},{"#tail":"\n","@confidence":"0.99371605","#text":"\nPerformance differences between various rerankers.\nP10 MAP MRR TDRR\nDevelopment Set\nEBM vs. Term +45.0%  +30.8%  +6.7% ? +23.3% \nCombo vs. Term +46.7%  +30.4%  +9.1% ? +23.2% \nCombo vs. EBM +1.2% ? ?0.3% ? +2.3% ? ?0.1% ?\nTest Set\nEBM vs. Term +40.8  +40.1%  +38.3%  +37.3% \nCombo vs. Term +43.2  +40.0%  +42.1%  +37.8% \nCombo vs. EBM +1.7 ? ?0.1% ? +2.7% ? +0.3% ?\n(a) Lenient Scoring\nP10 MAP MRR TDRR\nDevelopment Set\nEBM vs. Term +36.4%  +43.8%  +31.3% ? +40.7% \nCombo vs. Term +41.6%  +41.9%  +24.5% ? +35.0% \nCombo vs. EBM +3.8% ? ?1.3% ? ?5.2% ? ?4.1% ?\nTest Set\nEBM vs. Term +30.8 ? +40.4%  +24.9% ? +20.9% ?\nCombo vs. Term +46.2  +50.0%  +42.8%  +34.6% \nCombo vs. EBM +11.8  +6.8% ? +14.3% ? +11.3% ?\n"},{"#tail":"\n","@confidence":"0.947994769230769","#text":"\nComputational Linguistics Volume 33, Number 1\nFigure 3\nThe MAP performance surface for ?1 and ?2.\nTable 12\nResults of optimizing ?1 and ?2 on therapy questions.\nP10 MAP MRR TDRR\nDevelopment Test\nBaseline 0.730 0.819 0.933 2.869\nOptimized 0.760 (+4.1%)? 0.822 (+0.4%)? 0.933 (+0.0%)? 2.878 (+0.3%)?\nTest Test\nBaseline 0.783 0.765 0.917 2.979\nOptimized 0.783 (+0.0%)? 0.762 (?0.4%)? 0.917 (+0.0%)? 2.972 (?0.2%)?\n?Difference not statistically significant.\n"},{"#tail":"\n","@confidence":"0.93980145","#text":"\nResults of optimizing ?1 and ?2 on all questions.\nP10 MAP MRR TDRR\nDevelopment Test\nBaseline 0.699 0.754 0.910 2.650\nOptimized 0.707 (+1.2%)? 0.755 (+0.1%)? 0.918 (+0.9%)? 2.660 (+0.4%)?\nTest Test\nBaseline 0.677 0.718 0.936 2.671\nOptimized 0.669 (?1.1%)? 0.716 (?0.3%)? 0.936 (+0.0%)? 2.662 (?0.3%)?\n?Difference not statistically significant.\nTable 14\nResults of assigning uniform weights to the EBM score component based on the clinical task.\nP10 MAP MRR TDRR\nDevelopment Test\nBaseline 0.699 0.754 0.910 2.650\n?(t) = ?1 0.690 (?1.2%)? 0.738 (?2.1%)? 0.927 (+1.9%)? 2.646 (?0.2%)?\nTest Test\nBaseline 0.677 0.718 0.936 2.671\n?(t) = ?1 0.627 (?7.4%) 0.681 (?5.2%)? 0.913 (?2.4%)? 2.519 (?5.7%)?\n Significance at the 5% level, depending on direction of change.\n?Difference not statistically significant.\n"},{"#tail":"\n","@confidence":"0.962508789473685","#text":"\nComputational Linguistics Volume 33, Number 1\nTable 15\nStrict answer precision (considering only ?plus? judgments).\nTherapy Diagnosis Prognosis Etiology All\nAssessor 1\nBaseline .160 .233 .333 .480 .267\nEBM .260 (+63%) .367 (+58%) .333 (+0%) .600 (+25%) .367 (+37%)\nAssessor 2\nBaseline .040 .233 .200 .400 .183\nEBM .200 (+400%) .300 (+29%) .266 (+33%) .560 (+40%) .308 (+68%)\nTable 16\nLenient answer precision (considering both ?plus? and ?check? judgments).\nTherapy Diagnosis Prognosis Etiology All\nAssessor 1\nBaseline .400 .300 .533 .520 .417\nEBM .640 (+60%) .567 (+89%) .400 (?25%) .640 (+23%) .592 (+42%)\nAssessor 2\nBaseline .240 .267 .333 .440 .300\nEBM .520 (+117%) .600 (+125%) .400 (+20%) .560 (+27%) .533 (+78%)\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.622100","#tail":"\n","@no":"0","#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.984838","#text":"University of Maryland, College Park"},{"#tail":"\n","@confidence":"0.986797","#text":"University of Maryland, College Park"}],"author":[{"#tail":"\n","@confidence":"0.997124","#text":"Dina Demner-Fushman"},{"#tail":"\n","@confidence":"0.82957","#text":"Jimmy Lin"}],"abstract":{"#tail":"\n","@confidence":"0.984613875","#text":"The combination of recent developments in question-answering research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine. This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine. We have developed a series of knowledge extractors, which employ a combination of knowledge-based and statistical techniques, for automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs, in accordance with the principles of evidencebased medicine. Starting with an initial list of citations retrieved by PubMed, our system can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians? questions. We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking task, and finally, an evaluation of answers by two physicians. Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline."},"title":{"#tail":"\n","@confidence":"0.99967","#text":"Answering Clinical Questions with Knowledge-Based and Statistical Techniques"}}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1987"},"rawString":{"#tail":"\n","#text":"Ad Hoc Working Group for Critical Appraisal of the Medical Literature. 1987. A proposal for more informative abstracts of clinical articles. Annals of Internal Medicine, 106:595?604."},"journal":{"#tail":"\n","#text":"Annals of Internal Medicine,"},"#text":"\n","pages":{"#tail":"\n","#text":"106--595"},"marker":{"#tail":"\n","#text":"Hoc, 1987"},"title":{"#tail":"\n","#text":"Working Group for Critical Appraisal of the Medical Literature."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Ad Hoc"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Aronson, Alan R. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: The MetaMap program."},"#text":"\n","marker":{"#tail":"\n","#text":"Aronson, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"al Linguistics Volume 33, Number 1 statistical techniques to overcome the brittleness often associated with knowledgebased approaches? We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm","@endWordPosition":"488","@position":"3611","annotationId":"T1","@startWordPosition":"487","@citStr":"Aronson 2001"},{"#tail":"\n","#text":"ty integral to clinical question answering. This section, which elaborates on preliminary results reported in Demner-Fushman and Lin (2005), describes extraction algorithms for population, problems, interventions, outcomes, and the strength of evidence. For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases. Our knowledge extractors rely extensively on MetaMap (Aronson 2001), a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus. Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional feature we take advantage of (when present) is explicit section markers present in some abstracts. These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess th","@endWordPosition":"3594","@position":"24905","annotationId":"T2","@startWordPosition":"3593","@citStr":"Aronson 2001"}]},"title":{"#tail":"\n","#text":"Effective mapping of biomedical text to the UMLS Metathesaurus: The MetaMap program."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Alan R Aronson"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"In Proceeding of the 2001 Annual Symposium of the American Medical Informatics Association (AMIA 2001), pages 17?21, Portland, OR."},"#text":"\n","pages":{"#tail":"\n","#text":"17--21"},"marker":{"#tail":"\n","#text":"2001"},"location":{"#tail":"\n","#text":"Portland, OR."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"uestion answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-b","@endWordPosition":"15785","@position":"105131","annotationId":"T3","@startWordPosition":"15785","@citStr":"(2001)"}},"booktitle":{"#tail":"\n","#text":"In Proceeding of the 2001 Annual Symposium of the American Medical Informatics Association (AMIA"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Aronson, Alan R., James G. Mork, Clifford W. Gay, Susanne M. Humphrey, and Willie J. Rogers. 2004. The NLM Indexing Initiative?s Medical Text Indexer. In Proceedings of the 11th World Congress on Medical Informatics Demner-Fushman and Lin Answering Clinical Questions (MEDINFO 2004), pages 268?272, San Francisco, CA."},"#text":"\n","pages":{"#tail":"\n","#text":"268--272"},"marker":{"#tail":"\n","#text":"Aronson, Mork, Gay, Humphrey, Rogers, 2004"},"location":{"#tail":"\n","#text":"San Francisco, CA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a 1 http://www.nlm.nih.gov/pubs/factsheets/medline.html 2 Commonly referred to as MeSH terms or MeSH headings, although technically the latter is redundant. 67 Computational Linguistics Volume 33, Number 1 separate thesaurus. Indexing is performed by approximately 100 indexers with at least bachelor?s degrees in life sciences and formal training in indexing provided by NLM. Since mid-2002, the Library has been employing software that automatically suggests MeSH headings based on content (Aronson et al 2004). Nevertheless, the indexing process remains firmly human-centered. As a concrete example, an abstract titled ?Antipyretic efficacy of ibuprofen vs. acetaminophen? might have the following MeSH headings associated with it: MH - Acetaminophen/*therapeutic use MH - Child MH - Comparative Study MH - Fever/*drug therapy MH - Ibuprofen/*therapeutic use To represent different aspects of the topic described by a particular MeSH heading, up to three subheadings may be assigned, as indicated by the slash notation. In this example, a trained user could interpret from the MeSH terms that the article is a","@endWordPosition":"2500","@position":"17424","annotationId":"T4","@startWordPosition":"2497","@citStr":"Aronson et al 2004"},{"#tail":"\n","#text":"ks falling into each clinical task, we gathered a list of terms that are positive and negative indicators of relevance. 82 Demner-Fushman and Lin Answering Clinical Questions The task score, Stask, is given by: Stask = ? t?MeSH ?(t) (7) The function ?(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task. Note that although our current system uses MeSH headings assigned by human indexers, manually assigned terms can be replaced with automatic processing if needed (Aronson et al 2004). Below, we enumerate the relevant indicator terms by clinical task. However, there is a set of negative indicators common to all tasks; these were extracted from the set of genomics articles provided for the secondary task in the TREC 2004 genomics track evaluation (Hersh, Bhupatiraju, and Corley 2004); examples include genetics and cell physiology. The positive and negative weights assigned to each term heuristically encode the relative importance of different MeSH headings and are derived from the Clinical Queries filters in PubMed, from the JAMA EBM tutorial series on critical appraisal of","@endWordPosition":"9622","@position":"64525","annotationId":"T5","@startWordPosition":"9619","@citStr":"Aronson et al 2004"}]},"title":{"#tail":"\n","#text":"The NLM Indexing Initiative?s Medical Text Indexer."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 11th World Congress on Medical Informatics Demner-Fushman and Lin Answering Clinical Questions (MEDINFO 2004),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alan R Aronson"},{"#tail":"\n","#text":"James G Mork"},{"#tail":"\n","#text":"Clifford W Gay"},{"#tail":"\n","#text":"Susanne M Humphrey"},{"#tail":"\n","#text":"Willie J Rogers"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Aronson, Alan R., Thomas C. Rindflesch, and Allen C. Browne. 1994. Exploiting a large thesaurus for information retrieval."},"#text":"\n","marker":{"#tail":"\n","#text":"Aronson, Rindflesch, Browne, 1994"},"title":{"#tail":"\n","#text":"Exploiting a large thesaurus for information retrieval."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alan R Aronson"},{"#tail":"\n","#text":"Thomas C Rindflesch"},{"#tail":"\n","#text":"Allen C Browne"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In Proceedings of RIAO 1994: Intelligent Multimedia Information Retrieval Systems and Management, pages 197?216, New York."},"#text":"\n","pages":{"#tail":"\n","#text":"197--216"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"New York."},"booktitle":{"#tail":"\n","#text":"In Proceedings of RIAO 1994: Intelligent Multimedia Information Retrieval Systems and Management,"},"@valid":"false"},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Baeza-Yates, Ricardo and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. ACM Press, New York."},"#text":"\n","marker":{"#tail":"\n","#text":"Baeza-Yates, Ribeiro-Neto, 1999"},"publisher":{"#tail":"\n","#text":"ACM Press,"},"location":{"#tail":"\n","#text":"New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"l; for one, it is debatable which metric should be optimized. The test questions were hidden during the system 85 Computational Linguistics Volume 33, Number 1 development phase and served as a blind held-out test set for assessing the generality of our algorithm. In our experiment, we collected the following metrics, all computed automatically using our relevance judgments:  Precision at ten retrieved documents (P10) measures the fraction of relevant documents in the top ten results.  Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved (Baeza-Yates and Ribeiro-Neto 1999). It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.  Mean Reciprocal Rank (MRR) is a measure of how far down a hit list the user must browse before encountering the first relevant result. The score is equal to the reciprocal of the rank, that is, a relevant document at rank 1 gets a score of 1, 1/2 at rank 2, 1/3 at rank 3, and so on. Note that this measure only captures the appearance of the first relevant document. Furthermore, due to its discretization, MRR values are noisy on small collections.  Tot","@endWordPosition":"11121","@position":"74547","annotationId":"T6","@startWordPosition":"11118","@citStr":"Baeza-Yates and Ribeiro-Neto 1999"}},"title":{"#tail":"\n","#text":"Modern Information Retrieval."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ricardo Baeza-Yates"},{"#tail":"\n","#text":"Berthier Ribeiro-Neto"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Barry, Carol and Linda Schamber. 1998. Users? criteria for relevance evaluation: A cross-situational comparison. Information Processing and Management, 34(2/3):219?236."},"#text":"\n","pages":{"#tail":"\n","#text":"34--2"},"marker":{"#tail":"\n","#text":"Barry, Schamber, 1998"},"title":{"#tail":"\n","#text":"Users? criteria for relevance evaluation: A cross-situational comparison."},"booktitle":{"#tail":"\n","#text":"Information Processing and Management,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Carol Barry"},{"#tail":"\n","#text":"Linda Schamber"}]}},{"date":{"#tail":"\n","#text":"2000"},"editor":{"#tail":"\n","#text":"In Andrew Booth and Graham Walton, editors,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the","@endWordPosition":"15859","@position":"105630","annotationId":"T7","@startWordPosition":"15858","@citStr":"Booth (2000)"}},"title":{"#tail":"\n","#text":"Formulating the question."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Booth, Andrew. 2000. Formulating the question. In Andrew Booth and Graham Walton, editors, Managing Knowledge in Health Services. Library Association Publishing, London, England."},"#text":"\n","marker":{"#tail":"\n","#text":"Booth, 2000"},"publisher":{"#tail":"\n","#text":"Library Association Publishing,"},"location":{"#tail":"\n","#text":"London, England."},"booktitle":{"#tail":"\n","#text":"Managing Knowledge in Health Services."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Andrew Booth"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Chambliss, M. Lee and Jennifer Conley. 1996. Answering clinical questions. The Journal of Family Practice, 43:140?144."},"journal":{"#tail":"\n","#text":"The Journal of Family Practice,"},"#text":"\n","pages":{"#tail":"\n","#text":"43--140"},"marker":{"#tail":"\n","#text":"Chambliss, Conley, 1996"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians? questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts. Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology?better information systems to provide d","@endWordPosition":"772","@position":"5516","annotationId":"T8","@startWordPosition":"769","@citStr":"Chambliss and Conley 1996"}},"title":{"#tail":"\n","#text":"Answering clinical questions."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"M Lee Chambliss"},{"#tail":"\n","#text":"Jennifer Conley"}]}},{"volume":{"#tail":"\n","#text":"85"},"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Cogdill, Keith W. and Margaret E. Moore. 1997. First-year medical students? information needs and resource selection: Responses to a clinical scenario. Bulletin of the Medical Library Association, 85(1):51?54."},"journal":{"#tail":"\n","#text":"Bulletin of the Medical Library Association,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Cogdill, Moore, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"s of a system. The confluence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians? questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant","@endWordPosition":"721","@position":"5186","annotationId":"T9","@startWordPosition":"718","@citStr":"Cogdill and Moore 1997"}},"title":{"#tail":"\n","#text":"First-year medical students? information needs and resource selection: Responses to a clinical scenario."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Keith W Cogdill"},{"#tail":"\n","#text":"Margaret E Moore"}]}},{"volume":{"#tail":"\n","#text":"103"},"#tail":"\n","date":{"#tail":"\n","#text":"1985"},"rawString":{"#tail":"\n","#text":"Covell, David G., Gwen C. Uman, and Phil R. Manning. 1985. Information needs in office practice: Are they being met? Annals of Internal Medicine, 103(4):596?599. De Groote, Sandra L. and Josephine L."},"journal":{"#tail":"\n","#text":"Annals of Internal Medicine,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Covell, Uman, Manning, 1985"},"title":{"#tail":"\n","#text":"Information needs in office practice: Are they being met?"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David G Covell"},{"#tail":"\n","#text":"Gwen C Uman"},{"#tail":"\n","#text":"Phil R Manning"}]}},{"volume":{"#tail":"\n","#text":"91"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Dorsch. 2003. Measuring use patterns of online journals and databases. Journal of the Medical Library Association, 91(2):231?240."},"journal":{"#tail":"\n","#text":"Journal of the Medical Library Association,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Dorsch, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians? questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately ","@endWordPosition":"726","@position":"5214","annotationId":"T10","@startWordPosition":"725","@citStr":"Dorsch 2003"}},"title":{"#tail":"\n","#text":"Measuring use patterns of online journals and databases."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dorsch"}}},{"volume":{"#tail":"\n","#text":"13"},"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Demner-Fushman, Dina, Barbara Few, Susan E. Hauser, and George Thoma. 2006. Automatically identifying health outcome information in MEDLINE records. Journal of the American Medical Informatics Association, 13(1):52?60."},"journal":{"#tail":"\n","#text":"Journal of the American Medical Informatics Association,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Demner-Fushman, Few, Hauser, Thoma, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"a great deal of variation in the actual headings. Even when present, the headings are not organized in a manner focused on patient care. In addition, abstracts of much high-quality work remain unstructured. For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence. The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts, created through an effort led by the first author at the National Library of Medicine (Demner-Fushman et al 2006). As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac70 Demner-Fushman and Lin Answering Clinical Questions tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques (Demner-Fushman et al 2006). These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily ","@endWordPosition":"3837","@position":"26523","annotationId":"T11","@startWordPosition":"3834","@citStr":"Demner-Fushman et al 2006"},{"#tail":"\n","#text":" model different user behaviors ranging from naive to expert (where advanced search features were employed). With the exception of 50 citations retrieved to answer a question about childhood immunization, the rest of the results were retrieved by querying on a disease, for example, diabetes. Of the 633 citations, 100 abstracts were also fully annotated with population, problems, and interventions. These 100 abstracts were set aside as a held-out test set. Of the remaining citations, 275 were used for training and rule derivation, as described in the following sections. After much exploration, Demner-Fushman et al (2006) discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues. Consider the following segment: This double-blind, placebo-controlled, randomized, 3-period, complete block, 6-week crossover study examined the efficacy of simvastatin in adult men and women (N = 151) with stable type 2 DM, low density lipoprotein-cholesterol 100 mg/dL, HDL-C < 40 mg/dL, and fasting triglyceride level > 150 and < 700 mg/dL. All annotators agreed that the sentence contained the problem, population, and intervent","@endWordPosition":"4086","@position":"28215","annotationId":"T12","@startWordPosition":"4083","@citStr":"Demner-Fushman et al (2006)"}]},"title":{"#tail":"\n","#text":"Automatically identifying health outcome information in MEDLINE records."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dina Demner-Fushman"},{"#tail":"\n","#text":"Barbara Few"},{"#tail":"\n","#text":"Susan E Hauser"},{"#tail":"\n","#text":"George Thoma"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Demner-Fushman, Dina and Jimmy Lin. 2005. Knowledge extraction for clinical question answering: Preliminary results. In Proceedings of the AAAI-05 Workshop on Question Answering in Restricted Domains, pages 1?10, Pittsburgh, PA."},"#text":"\n","pages":{"#tail":"\n","#text":"1--10"},"marker":{"#tail":"\n","#text":"Demner-Fushman, Lin, 2005"},"location":{"#tail":"\n","#text":"Pittsburgh, PA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ee capabilities in an implemented clinical question-answering system and conducted three separate evaluations to assess the effectiveness of our developed capabilities. We do not tackle the query formulator, although see discussion in Section 12. Overall, results indicate that our implemented system significantly outperforms the PubMed baseline. 5. Knowledge Extraction for Evidence-Based Medicine The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section, which elaborates on preliminary results reported in Demner-Fushman and Lin (2005), describes extraction algorithms for population, problems, interventions, outcomes, and the strength of evidence. For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases. Our knowledge extractors rely extensively on MetaMap (Aronson 2001), a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus. Many of our algorithms oper","@endWordPosition":"3528","@position":"24431","annotationId":"T13","@startWordPosition":"3525","@citStr":"Demner-Fushman and Lin (2005)"}},"title":{"#tail":"\n","#text":"Knowledge extraction for clinical question answering: Preliminary results."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the AAAI-05 Workshop on Question Answering in Restricted Domains,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dina Demner-Fushman"},{"#tail":"\n","#text":"Jimmy Lin"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Demner-Fushman, Dina and Jimmy Lin. 2006. Answer extraction, semantic clustering, and extractive summarization for clinical question answering. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006), pages 841?848, Sydney, Australia."},"#text":"\n","pages":{"#tail":"\n","#text":"841--848"},"marker":{"#tail":"\n","#text":"Demner-Fushman, Lin, 2006"},"location":{"#tail":"\n","#text":"Sydney, Australia."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"mputational linguistics?redundancy detection for multi-document summarization?seems easy by comparison. Furthermore, it is unclear if textual strings make ?good answers.? Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Perhaps some variation of multi-level bulleted lists, appropriately integrated with interface elements for expanding and hiding items, might provide physicians a better overview of the information landscape; see, for example, Demner-Fushman and Lin (2006). Recognizing this complex set of issues, we decided to take a simple extractive approach to answer generation. For each abstract in our reranked list of citations, our system produces an answer by combining the title of the abstract and the top three outcome sentences (in the order they appeared in the abstract). We employed the outcome scores generated by the regression model. No attempt was made to synthesize information from multiple citations. A formal evaluation of this simple approach to answer generation is presented in the next section. 10. Evaluation of Clinical Answers Evaluation of","@endWordPosition":"14497","@position":"96650","annotationId":"T14","@startWordPosition":"14494","@citStr":"Demner-Fushman and Lin (2006)"},{"#tail":"\n","#text":"rom the entire MEDLINE database and directly index the results. Given access to more resources, a system could index identified PICO elements and directly match queries against a knowledge store. Finally, answer generation remains an area that awaits further exploration, although we would have to first define what a good answer should be. We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple 99 Computational Linguistics Volume 33, Number 1 abstracts; although see Demner-Fushman and Lin (2006). Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements. To address these very difficult challenges, finer-grained semantic analysis of medical texts is required. 13. Conclusion Our experiments in clinical question answering provide some answers to the broader research question regarding the role of knowledge-based and statistical techniques in advanced question answering. This work demonstrates that the two approaches are complementary and can be seamlessly integrated into algorithms that draw from the best of both worlds. E","@endWordPosition":"17271","@position":"115044","annotationId":"T15","@startWordPosition":"17268","@citStr":"Demner-Fushman and Lin (2006)"}]},"title":{"#tail":"\n","#text":"Answer extraction, semantic clustering, and extractive summarization for clinical question answering."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dina Demner-Fushman"},{"#tail":"\n","#text":"Jimmy Lin"}]}},{"volume":{"#tail":"\n","#text":"17"},"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Ebell, Mark H., Jay Siwek, Barry D. Weiss, Steven H. Woolf, Jeffrey Susman, Bernard Ewigman, and Marjorie Bowman. 2004. Strength of Recommendation Taxonomy (SORT): A patient-centered approach to grading evidence in the medical literature. The Journal of the American Board of Family Practice, 17(1):59?67."},"journal":{"#tail":"\n","#text":"The Journal of the American Board of Family Practice,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Ebell, Siwek, Weiss, Woolf, Susman, Ewigman, Bowman, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ly, the third facet serves as a tool for appraising the strength of evidence presented in the study, that is, how much confidence should a physician have in the results? Several taxonomies for appraising the strength of evidence based on the type and quality of the study have been developed. We chose the Strength of Recommendations Taxonomy (SORT) as the basis for determining the potential upper bound on the 66 Demner-Fushman and Lin Answering Clinical Questions quality of evidence, due to its emphasis on the use of patient-oriented outcomes and its attempt to unify other existing taxonomies (Ebell et al 2004). There are three levels of recommendations according to SORT:  A-level evidence is based on consistent, good-quality patient outcome-oriented evidence presented in systematic reviews, randomized controlled clinical trials, cohort studies, and meta-analyses.  B-level evidence is inconsistent, limited-quality, patient-oriented evidence in the same types of studies.  C-level evidence is based on disease-oriented evidence or studies less rigorous than randomized controlled clinical trials, cohort studies, systematic reviews, and meta-analyses. A question-answering system designed to support th","@endWordPosition":"2022","@position":"14043","annotationId":"T16","@startWordPosition":"2019","@citStr":"Ebell et al 2004"},{"#tail":"\n","#text":"sess the quality of a particular citation for clinical purposes. Metadata associated with most MEDLINE citations (MeSH terms) are extensively used to determine the strength of evidence and in our EBM citation scoring algorithm (Section 6). The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study. Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy (Ebell et al 2004). 5.10 Sample Output A complete example of our knowledge extractors working in unison is shown in Figure 2, which contains an abstract retrieved in response to the following question: ?In children with an acute febrile illness, what is the efficacy of single-medication therapy with acetaminophen or ibuprofen in reducing fever?? (Kauffman, Sawyer, and Scheinbaum 1992). Febrile illness is the only concept mapped to DISORDER, and hence is identified as the primary problem. 37 otherwise healthy children aged 2 to 12 years is correctly identified as the population. Acetaminophen, ibuprofen, and pla","@endWordPosition":"7600","@position":"50901","annotationId":"T17","@startWordPosition":"7597","@citStr":"Ebell et al 2004"}]},"title":{"#tail":"\n","#text":"Strength of Recommendation Taxonomy (SORT): A patient-centered approach to grading evidence in the medical literature."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mark H Ebell"},{"#tail":"\n","#text":"Jay Siwek"},{"#tail":"\n","#text":"Barry D Weiss"},{"#tail":"\n","#text":"Steven H Woolf"},{"#tail":"\n","#text":"Jeffrey Susman"},{"#tail":"\n","#text":"Bernard Ewigman"},{"#tail":"\n","#text":"Marjorie Bowman"}]}},{"volume":{"#tail":"\n","#text":"33"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Elhadad, Noemie, Min-Yen Kan, Judith Klavans, and Kathleen McKeown. 2005. Customization in a unified framework for summarizing medical literature. Journal of Artificial Intelligence in Medicine, 33(2):179?198."},"journal":{"#tail":"\n","#text":"Journal of Artificial Intelligence in Medicine,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Elhadad, Kan, Klavans, McKeown, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"n a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering. In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs. The PERSIVAL project, the most comprehensive study of such techniques applied on medical texts to date, leverages patient records to generate personalized summaries in response to physicians? queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project. The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers (e.g., Jacquemart and Zweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 20","@endWordPosition":"16246","@position":"108359","annotationId":"T18","@startWordPosition":"16243","@citStr":"Elhadad et al 2005"}},"title":{"#tail":"\n","#text":"Customization in a unified framework for summarizing medical literature."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Noemie Elhadad"},{"#tail":"\n","#text":"Min-Yen Kan"},{"#tail":"\n","#text":"Judith Klavans"},{"#tail":"\n","#text":"Kathleen McKeown"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Ely, John W., Jerome A. Osheroff, Mark H. Ebell, George R. Bergus, Barcey T. Levy, M. Lee Chambliss, and Eric R. Evans. 1999. Analysis of questions asked by family doctors regarding patient care. BMJ, 319:358?361. Ely, John W., Jerome A. Osheroff, M. Lee Chambliss, Mark H. Ebell, and Marcy E."},"#text":"\n","marker":{"#tail":"\n","#text":"Ely, Osheroff, Ebell, Bergus, Levy, Chambliss, Evans, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"k-based model of the clinical information-seeking process. The PICO framework (Richardson et al 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians? questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994","@endWordPosition":"679","@position":"4888","annotationId":"T19","@startWordPosition":"676","@citStr":"Ely et al 1999"},{"#tail":"\n","#text":"ture a realistic sampling of the scenarios that a clinical question-answering system would be confronted with. These questions were minimally modified from their original form as downloaded from the World Wide Web. In a few cases, a single question actually consisted of several smaller questions; such clusters were simplified by removing questions more peripheral to the central clinical problem. All questions were manually classified into one of the four clinical tasks; the distribution of the questions roughly follows the prevalence of each task type as observed in natural settings, noted by Ely et al (1999). The final step in the preparation process was manual translation of the natural language questions into PICO query frames. Our collection was divided into a development set and a blind held-out test set for verification purposes. The breakdown of these questions into the four clinical tasks and the development/test split is shown in Table 6. An example of each question type from our development set is presented here, along with its query frame: Does quinine reduce leg cramps for young athletes? (Therapy) search task: therapy selection primary problem: leg cramps co-occurring problems: muscle","@endWordPosition":"8322","@position":"56051","annotationId":"T20","@startWordPosition":"8319","@citStr":"Ely et al (1999)"}]},"title":{"#tail":"\n","#text":"Analysis of questions asked by family doctors regarding patient"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"John W Ely"},{"#tail":"\n","#text":"Jerome A Osheroff"},{"#tail":"\n","#text":"Mark H Ebell"},{"#tail":"\n","#text":"George R Bergus"},{"#tail":"\n","#text":"Barcey T Levy"},{"#tail":"\n","#text":"M Lee Chambliss"},{"#tail":"\n","#text":"Eric R Evans"}]}},{"volume":{"#tail":"\n","#text":"12"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Rosenbaum. 2005. Answering physicians? clinical questions: Obstacles and potential solutions. Journal of the American Medical Informatics Association, 12(2):217?224."},"journal":{"#tail":"\n","#text":"Journal of the American Medical Informatics Association,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Rosenbaum, 2005"},"title":{"#tail":"\n","#text":"Answering physicians? clinical questions: Obstacles and potential solutions."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Rosenbaum"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Fiszman, Marcelo, Thomas C. Rindflesch, and Halil Kilicoglu. 2004. Abstraction summarization for managing the biomedical research literature. In Proceedings of the HLT/NAACL 2004 Workshop on Computational Lexical Semantics, pages 76?83, Boston, MA."},"#text":"\n","pages":{"#tail":"\n","#text":"76--83"},"marker":{"#tail":"\n","#text":"Fiszman, Rindflesch, Kilicoglu, 2004"},"location":{"#tail":"\n","#text":"Boston, MA."},"title":{"#tail":"\n","#text":"Abstraction summarization for managing the biomedical research literature."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the HLT/NAACL 2004 Workshop on Computational Lexical Semantics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Marcelo Fiszman"},{"#tail":"\n","#text":"Thomas C Rindflesch"},{"#tail":"\n","#text":"Halil Kilicoglu"}]}},{"volume":{"#tail":"\n","#text":"82"},"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Gorman, Paul N., Joan S. Ash, and Leslie W. Wykoff. 1994. Can primary care physicians? questions be answered using the medical journal literature? Bulletin of the Medical Library Association, 82(2):140?146. Haynes, R. Brian, Nancy Wilczynski, K. Ann McKibbon, Cynthia J. Walker, and John C."},"journal":{"#tail":"\n","#text":"Bulletin of the Medical Library Association,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Gorman, Ash, Wykoff, 1994"},"title":{"#tail":"\n","#text":"Can primary care physicians? questions be answered using the medical journal literature?"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Paul N Gorman"},{"#tail":"\n","#text":"Joan S Ash"},{"#tail":"\n","#text":"Leslie W Wykoff"}]}},{"volume":{"#tail":"\n","#text":"1"},"#tail":"\n","date":{"#tail":"\n","#text":"1994"},"rawString":{"#tail":"\n","#text":"Sinclair. 1994. Developing optimal search strategies for detecting clinically sound studies in MEDLINE. Journal of the American Medical Informatics Association, 1(6):447?458."},"journal":{"#tail":"\n","#text":"Journal of the American Medical Informatics Association,"},"#text":"\n","issue":{"#tail":"\n","#text":"6"},"marker":{"#tail":"\n","#text":"Sinclair, 1994"},"title":{"#tail":"\n","#text":"Developing optimal search strategies for detecting clinically sound studies in MEDLINE."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Sinclair"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1996"},"rawString":{"#tail":"\n","#text":"Hearst, Marti A. 1996. Improving full-text precision on short queries using simple constraints. In Proceedings of the Fifth Annual Symposium on Document Analysis and Information Retrieval (SDAIR 1996), pages 217?232, Las Vegas, NV."},"#text":"\n","pages":{"#tail":"\n","#text":"217--232"},"marker":{"#tail":"\n","#text":"Hearst, 1996"},"location":{"#tail":"\n","#text":"Las Vegas, NV."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"o the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because seco","@endWordPosition":"15910","@position":"105960","annotationId":"T21","@startWordPosition":"15909","@citStr":"Hearst (1996)"}},"title":{"#tail":"\n","#text":"Improving full-text precision on short queries using simple constraints."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth Annual Symposium on Document Analysis and Information Retrieval (SDAIR"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Marti A Hearst"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Hersh, William, Ravi Teja Bhupatiraju, and Sarah Corley. 2004. Enhancing access to the bibliome: The TREC genomics track. In Proceedings of the 11th World Congress on Medical Informatics (MEDINFO 2004), pages 773?777, San Francisco, CA."},"#text":"\n","pages":{"#tail":"\n","#text":"773--777"},"marker":{"#tail":"\n","#text":"Hersh, Bhupatiraju, Corley, 2004"},"location":{"#tail":"\n","#text":"San Francisco, CA."},"title":{"#tail":"\n","#text":"Enhancing access to the bibliome: The TREC genomics track."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 11th World Congress on Medical Informatics (MEDINFO 2004),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"William Hersh"},{"#tail":"\n","#text":"Ravi Teja Bhupatiraju"},{"#tail":"\n","#text":"Sarah Corley"}]}},{"date":{"#tail":"\n","#text":"2005"},"title":{"#tail":"\n","#text":"genomics track overview."},"volume":{"#tail":"\n","#text":"33"},"#tail":"\n","institution":{"#tail":"\n","#text":"Computational Linguistics"},"rawString":{"#tail":"\n","#text":"Computational Linguistics Volume 33, Number 1 Hersh, William, Aaron Cohen, Jianji Yang, Ravi Teja Bhupatiraju1, Phoebe Roberts, and Marti Hearst. 2005. TREC 2005 genomics track overview. In Proceedings of the Fourteenth Text REtrieval Conference (TREC 2005), Gaithersburg, MD."},"journal":{"#tail":"\n","#text":"TREC"},"#text":"\n","marker":{"#tail":"\n","#text":"Hersh, Cohen, Yang, Bhupatiraju1, Roberts, Hearst, 2005"},"location":{"#tail":"\n","#text":"Gaithersburg, MD."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourteenth Text REtrieval Conference (TREC"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"William Hersh"},{"#tail":"\n","#text":"Aaron Cohen"},{"#tail":"\n","#text":"Jianji Yang"},{"#tail":"\n","#text":"Ravi Teja Bhupatiraju1"},{"#tail":"\n","#text":"Phoebe Roberts"},{"#tail":"\n","#text":"Marti Hearst"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Hildebrandt, Wesley, Boris Katz, and Jimmy Lin. 2004. Answering definition questions with multiple knowledge sources. In Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004), pages 49?56, Boston, MA."},"#text":"\n","pages":{"#tail":"\n","#text":"49--56"},"marker":{"#tail":"\n","#text":"Hildebrandt, Katz, Lin, 2004"},"location":{"#tail":"\n","#text":"Boston, MA."},"title":{"#tail":"\n","#text":"Answering definition questions with multiple knowledge sources."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Wesley Hildebrandt"},{"#tail":"\n","#text":"Boris Katz"},{"#tail":"\n","#text":"Jimmy Lin"}]}},{"volume":{"#tail":"\n","#text":"7"},"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Hirschman, Lynette and Robert Gaizauskas. 2001. Natural language question answering: The view from here. Natural Language Engineering, 7(4):275?300."},"journal":{"#tail":"\n","#text":"Natural Language Engineering,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Hirschman, Gaizauskas, 2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative, we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail?this is the standard pipeline architecture commonly employed in other question-answering systems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001). The architecture of our system is shown in Figure 1. The query formulator is responsible for converting a clinical question (in the form of a query frame) into a PubMed search query. Presently, these queries are already encoded in our test collection (see Section 6). PubMed returns an initial list of MEDLINE citations, which is then analyzed by our knowledge extractors (see Section 5). The input to the semantic matcher, which implements our EBM citation scoring algorithm, is the query frame and annotated MEDLINE citations. The module outputs a ranked list of citations that have been scored i","@endWordPosition":"3263","@position":"22629","annotationId":"T22","@startWordPosition":"3260","@citStr":"Hirschman and Gaizauskas 2001"}},"title":{"#tail":"\n","#text":"Natural language question answering: The view from here."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Lynette Hirschman"},{"#tail":"\n","#text":"Robert Gaizauskas"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Huang, Xiaoli, Jimmy Lin, and Dina Demner-Fushman. 2006. Evaluation of PICO as a knowledge representation for clinical questions. In Proceeding of the 2006 Annual Symposium of the American Medical Informatics Association (AMIA 2006), pages 359?363, Washington, D.C. Ingwersen, Peter. 1999. Cognitive information retrieval. Annual Review of Information Science and Technology, 34:3?52."},"#text":"\n","pages":{"#tail":"\n","#text":"359--363"},"marker":{"#tail":"\n","#text":"Huang, Lin, Demner-Fushman, 2006"},"location":{"#tail":"\n","#text":"Washington, D.C. Ingwersen, Peter."},"title":{"#tail":"\n","#text":"Evaluation of PICO as a knowledge representation for clinical questions."},"booktitle":{"#tail":"\n","#text":"In Proceeding of the 2006 Annual Symposium of the American Medical Informatics Association (AMIA"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Xiaoli Huang"},{"#tail":"\n","#text":"Jimmy Lin"},{"#tail":"\n","#text":"Dina Demner-Fushman"}]}},{"date":{"#tail":"\n","#text":"2003"},"editor":{"#tail":"\n","#text":"In Robert Baud, Marius Fieschi, Pierre Le Beux, and Patrick Ruch, editors,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"e, leverages patient records to generate personalized summaries in response to physicians? queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project. The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers (e.g., Jacquemart and Zweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005. Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine. Finally, the evaluation of answers to complex questions remains an open research problem. Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems. In Sections 9 and 10, we have d","@endWordPosition":"16315","@position":"108826","annotationId":"T23","@startWordPosition":"16312","@citStr":"Jacquemart and Zweigenbaum 2003"}},"title":{"#tail":"\n","#text":"Towards a medical question-answering system: A feasibility study."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Jacquemart, Pierre and Pierre Zweigenbaum. 2003. Towards a medical question-answering system: A feasibility study. In Robert Baud, Marius Fieschi, Pierre Le Beux, and Patrick Ruch, editors, The New Navigators: From Professionals to Patients, volume 95 of Actes Medical Informatics Europe, Studies in Health Technology and Informatics. IOS Press, Amsterdam, pages 463?468."},"#text":"\n","pages":{"#tail":"\n","#text":"463--468"},"marker":{"#tail":"\n","#text":"Jacquemart, Zweigenbaum, 2003"},"publisher":{"#tail":"\n","#text":"IOS Press,"},"location":{"#tail":"\n","#text":"Amsterdam,"},"booktitle":{"#tail":"\n","#text":"The New Navigators: From Professionals to Patients, volume 95 of Actes Medical Informatics Europe, Studies in Health Technology and Informatics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Pierre Jacquemart"},{"#tail":"\n","#text":"Pierre Zweigenbaum"}]}},{"volume":{"#tail":"\n","#text":"146"},"#tail":"\n","date":{"#tail":"\n","#text":"1992"},"rawString":{"#tail":"\n","#text":"Kauffman, Ralph E., L. A. Sawyer, and M. L. Scheinbaum. 1992. Antipyretic efficacy of ibuprofen vs acetaminophen. American Journal of Diseases of Children, 146(5):622?625."},"journal":{"#tail":"\n","#text":"American Journal of Diseases of Children,"},"#text":"\n","issue":{"#tail":"\n","#text":"5"},"marker":{"#tail":"\n","#text":"Kauffman, Sawyer, Scheinbaum, 1992"},"title":{"#tail":"\n","#text":"Antipyretic efficacy of ibuprofen vs acetaminophen."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ralph E Kauffman"},{"#tail":"\n","#text":"L A Sawyer"},{"#tail":"\n","#text":"M L Scheinbaum"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Light, Marc, Xin Ying Qiu, and Padmini Srinivasan. 2004. The language of bioscience: Facts, speculations, and statements in between. In Proceedings of the BioLink 2004 Workshop at HLT/NAACL 2004, pages 17?24, Boston, MA."},"#text":"\n","pages":{"#tail":"\n","#text":"17--24"},"marker":{"#tail":"\n","#text":"Light, Qiu, Srinivasan, 2004"},"location":{"#tail":"\n","#text":"Boston, MA."},"title":{"#tail":"\n","#text":"The language of bioscience: Facts, speculations, and statements in between."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the BioLink 2004 Workshop at HLT/NAACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Marc Light"},{"#tail":"\n","#text":"Xin Ying Qiu"},{"#tail":"\n","#text":"Padmini Srinivasan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Lin, Jimmy and Dina Demner-Fushman. 2005a. Automatically evaluating answers to definition questions. In Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), pages 931?938, Vancouver, Canada."},"#text":"\n","pages":{"#tail":"\n","#text":"931--938"},"marker":{"#tail":"\n","#text":"Lin, Demner-Fushman, 2005"},"location":{"#tail":"\n","#text":"Vancouver, Canada."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"an directly act on. Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion?it need not be repeated unless the physician wishes to ?drill down?; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these features are also beyond the capabilities of current summarization systems. It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objective","@endWordPosition":"14318","@position":"95373","annotationId":"T24","@startWordPosition":"14315","@citStr":"Lin and Demner-Fushman 2005"},{"#tail":"\n","#text":"iscussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of ?information nuggets? may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ?other? questions (Voorhees 2003). A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken. 12. Future Work The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously, a user study (Lin et al 2003) has shown that people are reluctant to type full natural language questions, even after being told that they were using a questionanswering system and that typing complete questions would result in better performance.","@endWordPosition":"16504","@position":"110053","annotationId":"T25","@startWordPosition":"16501","@citStr":"Lin and Demner-Fushman 2005"}]},"title":{"#tail":"\n","#text":"Automatically evaluating answers to definition questions."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jimmy Lin"},{"#tail":"\n","#text":"Dina Demner-Fushman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Lin, Jimmy and Dina Demner-Fushman. 2005b. Evaluating summaries and answers: Two sides of the same coin? In Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 41?48, Ann Arbor, MI."},"#text":"\n","pages":{"#tail":"\n","#text":"41--48"},"marker":{"#tail":"\n","#text":"Lin, Demner-Fushman, 2005"},"location":{"#tail":"\n","#text":"Ann Arbor, MI."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"an directly act on. Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion?it need not be repeated unless the physician wishes to ?drill down?; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these features are also beyond the capabilities of current summarization systems. It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objective","@endWordPosition":"14318","@position":"95373","annotationId":"T26","@startWordPosition":"14315","@citStr":"Lin and Demner-Fushman 2005"},{"#tail":"\n","#text":"iscussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of ?information nuggets? may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ?other? questions (Voorhees 2003). A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken. 12. Future Work The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously, a user study (Lin et al 2003) has shown that people are reluctant to type full natural language questions, even after being told that they were using a questionanswering system and that typing complete questions would result in better performance.","@endWordPosition":"16504","@position":"110053","annotationId":"T27","@startWordPosition":"16501","@citStr":"Lin and Demner-Fushman 2005"}]},"title":{"#tail":"\n","#text":"Evaluating summaries and answers: Two sides of the same coin?"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jimmy Lin"},{"#tail":"\n","#text":"Dina Demner-Fushman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Lin, Jimmy and Dina Demner-Fushman. 2006a. The role of knowledge in conceptual retrieval: A study in the domain of clinical medicine. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006), pages 99?106, Seattle, WA."},"#text":"\n","pages":{"#tail":"\n","#text":"99--106"},"marker":{"#tail":"\n","#text":"Lin, Demner-Fushman, 2006"},"location":{"#tail":"\n","#text":"Seattle, WA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"re exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particul","@endWordPosition":"15752","@position":"104915","annotationId":"T28","@startWordPosition":"15749","@citStr":"Lin and Demner-Fushman (2006"}},"title":{"#tail":"\n","#text":"The role of knowledge in conceptual retrieval: A study in the domain of clinical medicine."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jimmy Lin"},{"#tail":"\n","#text":"Dina Demner-Fushman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Lin, Jimmy and Dina Demner-Fushman. 2006b. Will pyramids built of nuggets topple over? In Proceedings of the 2006 Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2006), pages 383?390, New York."},"#text":"\n","pages":{"#tail":"\n","#text":"383--390"},"marker":{"#tail":"\n","#text":"Lin, Demner-Fushman, 2006"},"location":{"#tail":"\n","#text":"New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"re exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc?a reported good performance for etiology, diagnosis, and in particul","@endWordPosition":"15752","@position":"104915","annotationId":"T29","@startWordPosition":"15749","@citStr":"Lin and Demner-Fushman (2006"}},"title":{"#tail":"\n","#text":"Will pyramids built of nuggets topple over?"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2006 Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jimmy Lin"},{"#tail":"\n","#text":"Dina Demner-Fushman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Lin, Jimmy, Damianos Karakos, Dina Demner-Fushman, and Sanjeev Khudanpur. 2006. Generative content models for structural analysis of medical abstracts. In Proceedings of the HLT/ NAACL 2006 Workshop on Biomedical Natural Language Processing (BioNLP?06), pages 65?72, New York."},"#text":"\n","pages":{"#tail":"\n","#text":"65--72"},"marker":{"#tail":"\n","#text":"Lin, Karakos, Demner-Fushman, Khudanpur, 2006"},"location":{"#tail":"\n","#text":"New York."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"and demonstrates that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering. In addition to question answering, multi-document summarization provides ","@endWordPosition":"16117","@position":"107427","annotationId":"T30","@startWordPosition":"16114","@citStr":"Lin et al 2006"}},"title":{"#tail":"\n","#text":"Generative content models for structural analysis of medical abstracts."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the HLT/ NAACL 2006 Workshop on Biomedical Natural Language Processing (BioNLP?06),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jimmy Lin"},{"#tail":"\n","#text":"Damianos Karakos"},{"#tail":"\n","#text":"Dina Demner-Fushman"},{"#tail":"\n","#text":"Sanjeev Khudanpur"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Lin, Jimmy, Dennis Quan, Vineet Sinha, Karun Bakshi, David Huynh, Boris Katz, and David R. Karger. 2003. What makes a good answer? The role of context in question answering. In Proceedings of the Ninth IFIP TC13 International Conference on Human-Computer Interaction (INTERACT 2003), pages 25?32, Zu?rich, Switzerland."},"#text":"\n","pages":{"#tail":"\n","#text":"25--32"},"marker":{"#tail":"\n","#text":"Lin, Quan, Sinha, Bakshi, Huynh, Katz, Karger, 2003"},"location":{"#tail":"\n","#text":"Zu?rich, Switzerland."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"orhees 2003). A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken. 12. Future Work The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously, a user study (Lin et al 2003) has shown that people are reluctant to type full natural language questions, even after being told that they were using a questionanswering system and that typing complete questions would result in better performance. We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest 98 Demner-Fushman and Lin Answering Clinical Questions the upfront effort necessary to accomplish this. Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance o","@endWordPosition":"16565","@position":"110435","annotationId":"T31","@startWordPosition":"16562","@citStr":"Lin et al 2003"}},"title":{"#tail":"\n","#text":"What makes a good answer? The role of context in question answering."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Ninth IFIP TC13 International Conference on Human-Computer Interaction (INTERACT"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jimmy Lin"},{"#tail":"\n","#text":"Dennis Quan"},{"#tail":"\n","#text":"Vineet Sinha"},{"#tail":"\n","#text":"Karun Bakshi"},{"#tail":"\n","#text":"David Huynh"},{"#tail":"\n","#text":"Boris Katz"},{"#tail":"\n","#text":"David R Karger"}]}},{"volume":{"#tail":"\n","#text":"32"},"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Lindberg, Donald A., Betsy L. Humphreys, and Alexa T. McCray. 1993. The Unified Medical Language System. Methods of Information in Medicine, 32(4):281?291."},"journal":{"#tail":"\n","#text":"Methods of Information in Medicine,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Lindberg, Humphreys, McCray, 1993"},"title":{"#tail":"\n","#text":"The Unified Medical Language System."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Donald A Lindberg"},{"#tail":"\n","#text":"Betsy L Humphreys"},{"#tail":"\n","#text":"Alexa T McCray"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"McCray, Alexa T., Anita Burgun, and Olivier Bodenreider. 2001. Aggregating UMLS semantic types for reducing conceptual complexity. In Proceedings of 10th World Congress on Medical Informatics (MEDINFO 2001), pages 216?220, London, England."},"#text":"\n","pages":{"#tail":"\n","#text":"216--220"},"marker":{"#tail":"\n","#text":"McCray, Burgun, Bodenreider, 2001"},"location":{"#tail":"\n","#text":"London, England."},"title":{"#tail":"\n","#text":"Aggregating UMLS semantic types for reducing conceptual complexity."},"booktitle":{"#tail":"\n","#text":"In Proceedings of 10th World Congress on Medical Informatics (MEDINFO"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alexa T McCray"},{"#tail":"\n","#text":"Anita Burgun"},{"#tail":"\n","#text":"Olivier Bodenreider"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"McKeown, Kathleen, Noemie Elhadad, and Vasileios Hatzivassiloglou. 2003."},"#text":"\n","marker":{"#tail":"\n","#text":"McKeown, 2003"},"title":{"#tail":"\n","#text":"Noemie Elhadad, and Vasileios Hatzivassiloglou."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Kathleen McKeown"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Leveraging a common representation for personalized search and summarization in a medical digital library. In Proceedings of the 3rd ACM/IEEE Joint Conference on Digital Libraries (JCDL 2003), pages 159?170, Houston, TX."},"#text":"\n","pages":{"#tail":"\n","#text":"159--170"},"marker":{"#tail":"\n","#text":"2003"},"location":{"#tail":"\n","#text":"Houston, TX."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"05). We applied our problem extractor on different segments of the abstract: the title only, the title and first two sentences, and the entire abstract. These results are shown in Table 2. Here, a problem was considered correctly identified only if it shared the same concept ID as the ground truth problem (from the MeSH heading). The performance of our best variant (abstract title and first two sentences) approaches the upper bound on MetaMap performance?which is limited by human agreement on the identification of semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003). Although problem extraction largely depends on disease coverage in UMLS and MetaMap performance, the error rate could be further reduced by more sophisticated recognition of implicitly stated problems. For example, with respect to a question about immunization in children, an abstract about the measles-mumps-rubella vaccination never mentioned the disease without the word vaccination; hence, no concept of the type DISEASE OR SYNDROME was identified. 5.5 Intervention Extractor The intervention extractor identifies both the intervention and comparison elements in a PICO frame; processing of th","@endWordPosition":"5613","@position":"38068","annotationId":"T32","@startWordPosition":"5613","@citStr":"(2003)"},{"#tail":"\n","#text":"possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of s","@endWordPosition":"16087","@position":"107225","annotationId":"T33","@startWordPosition":"16087","@citStr":"(2003)"}]},"title":{"#tail":"\n","#text":"Leveraging a common representation for personalized search and summarization in a medical digital library."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 3rd ACM/IEEE Joint Conference on Digital Libraries (JCDL"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"McKnight, Larry and Padmini Srinivasan. 2003. Categorization of sentence types in medical abstracts. In Proceeding of the 2003 Annual Symposium of the American Demner-Fushman and Lin Answering Clinical Questions Medical Informatics Association (AMIA 2003), pages 440?444, Washington, D.C. Meadow, Charles T., Barbara A. Cerny, Christine L. Borgman, and Donald O."},"#text":"\n","pages":{"#tail":"\n","#text":"440--444"},"marker":{"#tail":"\n","#text":"McKnight, Srinivasan, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of s","@endWordPosition":"16087","@position":"107225","annotationId":"T34","@startWordPosition":"16084","@citStr":"McKnight and Srinivasan (2003)"}},"title":{"#tail":"\n","#text":"Categorization of sentence types in medical abstracts."},"booktitle":{"#tail":"\n","#text":"In Proceeding of the 2003 Annual Symposium of the American Demner-Fushman and Lin Answering Clinical Questions Medical Informatics Association (AMIA"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Larry McKnight"},{"#tail":"\n","#text":"Padmini Srinivasan"}]}},{"volume":{"#tail":"\n","#text":"40"},"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Case. 1989. Online access to knowledge: System design. Journal of the American Society for Information Science, 40(2):86?98."},"journal":{"#tail":"\n","#text":"Journal of the American Society for Information Science,"},"#text":"\n","issue":{"#tail":"\n","#text":"2"},"marker":{"#tail":"\n","#text":"Case, 1989"},"title":{"#tail":"\n","#text":"Online access to knowledge: System design."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Case"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Mendonc?a, Eneida A. and James J. Cimino. 2001. Building a knowledge base to support a digital library. In Proceedings of 10th World Congress on Medical Informatics (MEDINFO 2001), pages 222?225, London, England."},"#text":"\n","pages":{"#tail":"\n","#text":"222--225"},"marker":{"#tail":"\n","#text":"Mendonca, Cimino, 2001"},"location":{"#tail":"\n","#text":"London, England."},"title":{"#tail":"\n","#text":"Building a knowledge base to support a digital library."},"booktitle":{"#tail":"\n","#text":"In Proceedings of 10th World Congress on Medical Informatics (MEDINFO"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Eneida A Mendonca"},{"#tail":"\n","#text":"James J Cimino"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Mladenic, Dunja and Marko Grobelnik. 1999."},"#text":"\n","marker":{"#tail":"\n","#text":"Mladenic, Grobelnik, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" one point each to the cumulative score, which results in a likelihood estimate of 0.25 for this sentence. The unigram ?bag of words? classifier is a naive Bayes classifier implemented with the API provided by the MALLET toolkit.4 This classifier outputs the probability of a class assignment. The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure (Yang and Pedersen 1997), and then selected only the positive outcome predictors using odds ratio (Mladenic and Grobelnik 1999). Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment. The position classifier returns the maximum likelihood estimate that a sentence is an outcome based on its position in the abstract (for structured abstracts, with respect to the results or conclusions sections; for unstructured abstracts, with respect to the end of the abstract). The abstract length classifier returns a smoothed (add one","@endWordPosition":"6511","@position":"43987","annotationId":"T35","@startWordPosition":"6508","@citStr":"Mladenic and Grobelnik 1999"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Dunja Mladenic"},{"#tail":"\n","#text":"Marko Grobelnik"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Feature selection for unbalanced class distribution and Na??ve Bayes. In Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999), pages 258?267, Bled, Slovenia."},"#text":"\n","pages":{"#tail":"\n","#text":"258--267"},"marker":{"#tail":"\n","#text":"1999"},"location":{"#tail":"\n","#text":"Bled, Slovenia."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"k factors). 65 Computational Linguistics Volume 33, Number 1 Diagnosis: This encompasses two primary types: Differential diagnosis: Identifying and ranking by likelihood potential diseases based on findings observed in a patient. Diagnostic test: Selecting and interpreting diagnostic tests for a patient, considering their precision, accuracy, acceptability, cost, and safety. Etiology/Harm: Identifying factors that cause a disease or condition in a patient. Prognosis: Estimating a patient?s likely course over time and anticipating likely complications. These activities represent what Ingwersen (1999) calls ?work tasks.? It is important to note that they exist independently of information needs, namely, searching is not necessarily implicated in any of these activities. We are, however, interested in situations where questions arise during one of these clinical tasks?only then does the physician engage in information-seeking behavior. These activities translate into natural ?search tasks.? For therapy, the search task is usually therapy selection (for example, determining which course of action is the best treatment for a disease) or prevention (for example, selecting preemptive measures w","@endWordPosition":"1573","@position":"11131","annotationId":"T36","@startWordPosition":"1573","@citStr":"(1999)"},{"#tail":"\n","#text":"listic sampling of the scenarios that a clinical question-answering system would be confronted with. These questions were minimally modified from their original form as downloaded from the World Wide Web. In a few cases, a single question actually consisted of several smaller questions; such clusters were simplified by removing questions more peripheral to the central clinical problem. All questions were manually classified into one of the four clinical tasks; the distribution of the questions roughly follows the prevalence of each task type as observed in natural settings, noted by Ely et al (1999). The final step in the preparation process was manual translation of the natural language questions into PICO query frames. Our collection was divided into a development set and a blind held-out test set for verification purposes. The breakdown of these questions into the four clinical tasks and the development/test split is shown in Table 6. An example of each question type from our development set is presented here, along with its query frame: Does quinine reduce leg cramps for young athletes? (Therapy) search task: therapy selection primary problem: leg cramps co-occurring problems: muscle","@endWordPosition":"8322","@position":"56051","annotationId":"T37","@startWordPosition":"8322","@citStr":"(1999)"}]},"title":{"#tail":"\n","#text":"Feature selection for unbalanced class distribution and Na??ve Bayes."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixteenth International Conference on Machine Learning (ICML"},"@valid":"true"},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Nenkova, Ani and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004), pages 145?152, Boston, MA."},"#text":"\n","pages":{"#tail":"\n","#text":"145--152"},"marker":{"#tail":"\n","#text":"Nenkova, Passonneau, 2004"},"location":{"#tail":"\n","#text":"Boston, MA."},"title":{"#tail":"\n","#text":"Evaluating content selection in summarization: The pyramid method."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ani Nenkova"},{"#tail":"\n","#text":"Rebecca Passonneau"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Niu, Yun and Graeme Hirst. 2004. Analysis of semantic classes in medical text for question answering. In Proceedings of the ACL 2004 Workshop on Question Answering in Restricted Domains, pages 54?61, Barcelona, Spain."},"#text":"\n","pages":{"#tail":"\n","#text":"54--61"},"marker":{"#tail":"\n","#text":"Niu, Hirst, 2004"},"location":{"#tail":"\n","#text":"Barcelona,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary ","@endWordPosition":"15942","@position":"106196","annotationId":"T38","@startWordPosition":"15939","@citStr":"Niu and Hirst (2004)"}},"title":{"#tail":"\n","#text":"Analysis of semantic classes in medical text for question answering."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL 2004 Workshop on Question Answering in Restricted Domains,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yun Niu"},{"#tail":"\n","#text":"Graeme Hirst"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Pratt, Wanda and Meliha Yetisgen-Yildiz. 2003. A study of biomedical concept identification: MetaMap vs. people."},"#text":"\n","pages":{"#tail":"\n","#text":"people."},"marker":{"#tail":"\n","#text":"Pratt, Yetisgen-Yildiz, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ed in Sneiderman et al (2005). We applied our problem extractor on different segments of the abstract: the title only, the title and first two sentences, and the entire abstract. These results are shown in Table 2. Here, a problem was considered correctly identified only if it shared the same concept ID as the ground truth problem (from the MeSH heading). The performance of our best variant (abstract title and first two sentences) approaches the upper bound on MetaMap performance?which is limited by human agreement on the identification of semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003). Although problem extraction largely depends on disease coverage in UMLS and MetaMap performance, the error rate could be further reduced by more sophisticated recognition of implicitly stated problems. For example, with respect to a question about immunization in children, an abstract about the measles-mumps-rubella vaccination never mentioned the disease without the word vaccination; hence, no concept of the type DISEASE OR SYNDROME was identified. 5.5 Intervention Extractor The intervention extractor identifies both the intervention and comparison elements in a PICO frame; processing of th","@endWordPosition":"5613","@position":"38068","annotationId":"T39","@startWordPosition":"5610","@citStr":"Pratt and Yetisgen-Yildiz (2003)"}},"title":{"#tail":"\n","#text":"A study of biomedical concept identification: MetaMap vs."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Wanda Pratt"},{"#tail":"\n","#text":"Meliha Yetisgen-Yildiz"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"In Proceeding of the 2003 Annual Symposium of the American Medical Informatics Association (AMIA 2003), pages 529?533, Washington, D.C."},"#text":"\n","pages":{"#tail":"\n","#text":"529--533"},"marker":{"#tail":"\n","#text":"2003"},"location":{"#tail":"\n","#text":"Washington, D.C."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"05). We applied our problem extractor on different segments of the abstract: the title only, the title and first two sentences, and the entire abstract. These results are shown in Table 2. Here, a problem was considered correctly identified only if it shared the same concept ID as the ground truth problem (from the MeSH heading). The performance of our best variant (abstract title and first two sentences) approaches the upper bound on MetaMap performance?which is limited by human agreement on the identification of semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003). Although problem extraction largely depends on disease coverage in UMLS and MetaMap performance, the error rate could be further reduced by more sophisticated recognition of implicitly stated problems. For example, with respect to a question about immunization in children, an abstract about the measles-mumps-rubella vaccination never mentioned the disease without the word vaccination; hence, no concept of the type DISEASE OR SYNDROME was identified. 5.5 Intervention Extractor The intervention extractor identifies both the intervention and comparison elements in a PICO frame; processing of th","@endWordPosition":"5613","@position":"38068","annotationId":"T40","@startWordPosition":"5613","@citStr":"(2003)"},{"#tail":"\n","#text":"possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of s","@endWordPosition":"16087","@position":"107225","annotationId":"T41","@startWordPosition":"16087","@citStr":"(2003)"}]},"booktitle":{"#tail":"\n","#text":"In Proceeding of the 2003 Annual Symposium of the American Medical Informatics Association (AMIA"},"@valid":"true"},{"volume":{"#tail":"\n","#text":"123"},"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Richardson, W. Scott, Mark C. Wilson, James Nishikawa, and Robert S. Hayward. 1995. The well-built clinical question: A key to evidence-based decisions. American College of Physicians Journal Club, 123(3):A12?A13."},"journal":{"#tail":"\n","#text":"American College of Physicians Journal Club,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Richardson, Wilson, Nishikawa, Hayward, 1995"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al 2000) provides a task-based model of the clinical information-seeking process. The PICO framework (Richardson et al 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomed","@endWordPosition":"597","@position":"4375","annotationId":"T42","@startWordPosition":"594","@citStr":"Richardson et al 1995"},{"#tail":"\n","#text":" the types of studies relevant to each of the four tasks have been extensively studied by the Hedges Project at the McMaster University (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). The results of this research are implemented in the PubMed Clinical Queries tools, which can be used to retrieve task-specific citations (more about this in the next section). The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question. The following four components have been identified as the key elements of a question related to patient care (Richardson et al 1995):  What is the primary problem or disease? What are the characteristics of the patient (e.g., age, gender, or co-existing conditions)?  What is the main intervention (e.g., a diagnostic test, medication, or therapeutic procedure)?  What is the main intervention compared to (e.g., no intervention, another drug, another therapeutic procedure, or a placebo)?  What is the desired effect of the intervention (e.g., cure a disease, relieve or eliminate symptoms, reduce side effects, or lower cost)? These four elements are often referenced with the mnemonic PICO, which stands for Patient/Problem, ","@endWordPosition":"1825","@position":"12782","annotationId":"T43","@startWordPosition":"1822","@citStr":"Richardson et al 1995"}]},"title":{"#tail":"\n","#text":"The well-built clinical question: A key to evidence-based decisions."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"W Scott Richardson"},{"#tail":"\n","#text":"Mark C Wilson"},{"#tail":"\n","#text":"James Nishikawa"},{"#tail":"\n","#text":"Robert S Hayward"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Rinaldi, Fabio, James Dowdall, Gerold Schneider, and Andreas Persidis. 2004."},"#text":"\n","marker":{"#tail":"\n","#text":"Rinaldi, Dowdall, Schneider, Persidis, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"enerate personalized summaries in response to physicians? queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project. The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers (e.g., Jacquemart and Zweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005. Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine. Finally, the evaluation of answers to complex questions remains an open research problem. Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems. In Sections 9 and 10, we have discussed many of thes","@endWordPosition":"16319","@position":"108847","annotationId":"T44","@startWordPosition":"16316","@citStr":"Rinaldi et al 2004"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Fabio Rinaldi"},{"#tail":"\n","#text":"James Dowdall"},{"#tail":"\n","#text":"Gerold Schneider"},{"#tail":"\n","#text":"Andreas Persidis"}]}},{"#tail":"\n","date":{"#tail":"\n"},"rawString":{"#tail":"\n","#text":"Answering questions in the genomics domain. In Proceedings of the ACL 2004 Workshop on Question Answering in Restricted Domains, pages 46?53, Barcelona, Spain."},"#text":"\n","pages":{"#tail":"\n","#text":"46--53"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Barcelona,"},"title":{"#tail":"\n","#text":"Answering questions in the genomics domain."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL 2004 Workshop on Question Answering in Restricted Domains,"},"@valid":"true"},{"volume":{"#tail":"\n","#text":"36"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Rindflesch, Thomas C. and Marcelo Fiszman. 2003. The interaction of domain knowledge and linguistic structure in natural language processing: Interpreting hypernymic propositions in biomedical text. Journal of Biomedical Informatics, 36(6):462?477."},"journal":{"#tail":"\n","#text":"Journal of Biomedical Informatics,"},"#text":"\n","issue":{"#tail":"\n","#text":"6"},"marker":{"#tail":"\n","#text":"Rindflesch, Fiszman, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" overcome the brittleness often associated with knowledgebased approaches? We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al 2000) provides a task-based mode","@endWordPosition":"499","@position":"3686","annotationId":"T45","@startWordPosition":"496","@citStr":"Rindflesch and Fiszman 2003"}},"title":{"#tail":"\n","#text":"The interaction of domain knowledge and linguistic structure in natural language processing: Interpreting hypernymic propositions in biomedical text."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Thomas C Rindflesch"},{"#tail":"\n","#text":"Marcelo Fiszman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Sackett, David L., Sharon E. Straus, W. Scott Richardson, William Rosenberg, and R. Brian Haynes. 2000. Evidence-Based Medicine: How to Practice and Teach EBM, second edition. Churchill Livingstone, Edinburgh, Scotland."},"#text":"\n","marker":{"#tail":"\n","#text":"Sackett, Straus, Richardson, Rosenberg, Haynes, 2000"},"location":{"#tail":"\n","#text":"Edinburgh, Scotland."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al 2000) provides a task-based model of the clinical information-seeking process. The PICO framework (Richardson et al 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, an","@endWordPosition":"581","@position":"4259","annotationId":"T46","@startWordPosition":"578","@citStr":"Sackett et al 2000"}},"title":{"#tail":"\n","#text":"Evidence-Based Medicine: How to Practice and Teach EBM, second edition. Churchill Livingstone,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"David L Sackett"},{"#tail":"\n","#text":"Sharon E Straus"},{"#tail":"\n","#text":"W Scott Richardson"},{"#tail":"\n","#text":"William Rosenberg"},{"#tail":"\n","#text":"R Brian Haynes"}]}},{"volume":{"#tail":"\n","#text":"26"},"#tail":"\n","date":{"#tail":"\n","#text":"1975"},"rawString":{"#tail":"\n","#text":"Saracevic, Tefko. 1975. Relevance: A review of and a framework for thinking on the notion in information science. Journal of the American Society for Information Science, 26(6):321?343."},"journal":{"#tail":"\n","#text":"Journal of the American Society for Information Science,"},"#text":"\n","issue":{"#tail":"\n","#text":"6"},"marker":{"#tail":"\n","#text":"Saracevic, 1975"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"itle of the abstract and the top three outcome sentences (in the order they appeared in the abstract). We employed the outcome scores generated by the regression model. No attempt was made to synthesize information from multiple citations. A formal evaluation of this simple approach to answer generation is presented in the next section. 10. Evaluation of Clinical Answers Evaluation of answers within a clinical setting involves a complex decision that must not only take into account topical relevance (i.e., ?Does the answer address the information need??), but also situational relevance (e.g., Saracevic 1975, Barry and Schamber 94 Demner-Fushman and Lin Answering Clinical Questions 1998). The latter factor includes many issues such as the strength of evidence, recency of results, and reputation of the journal. Clinicians need to carefully consider all these elements before acting on any information for the purposes of patient care. Within the framework of evidence-based medicine, the physician is the final arbiter of how clinical answers are integrated into the broader activities of medical care, but this complicates any attempt to evaluate answers generated by our system. In assessing answers pr","@endWordPosition":"14626","@position":"97477","annotationId":"T47","@startWordPosition":"14625","@citStr":"Saracevic 1975"}},"title":{"#tail":"\n","#text":"Relevance: A review of and a framework for thinking on the notion in information science."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Tefko Saracevic"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Sneiderman, Charles, Dina Demner-Fushman, Marcelo Fiszman, and Thomas C. Rindflesch. 2005. Semantic characteristics of MEDLINE citations useful for therapeutic decision-making."},"#text":"\n","marker":{"#tail":"\n","#text":"Sneiderman, Demner-Fushman, Fiszman, Rindflesch, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"om co-occurring conditions identified in the abstract. 5.4 Evaluation of Problem Extractor Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem. For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers? tasks in assigning terms is to identify the main topic of the article (sometimes a disorder). For this evaluation, we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in Sneiderman et al (2005). We applied our problem extractor on different segments of the abstract: the title only, the title and first two sentences, and the entire abstract. These results are shown in Table 2. Here, a problem was considered correctly identified only if it shared the same concept ID as the ground truth problem (from the MeSH heading). The performance of our best variant (abstract title and first two sentences) approaches the upper bound on MetaMap performance?which is limited by human agreement on the identification of semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (20","@endWordPosition":"5518","@position":"37465","annotationId":"T48","@startWordPosition":"5515","@citStr":"Sneiderman et al (2005)"}},"title":{"#tail":"\n","#text":"Semantic characteristics of MEDLINE citations useful for therapeutic decision-making."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Charles Sneiderman"},{"#tail":"\n","#text":"Dina Demner-Fushman"},{"#tail":"\n","#text":"Marcelo Fiszman"},{"#tail":"\n","#text":"Thomas C Rindflesch"}]}},{"#tail":"\n","rawString":{"#tail":"\n","#text":"In Proceeding of the 2005 Annual Symposium of the American Medical Informatics Association (AMIA 2005), page 1117, Washington, D.C."},"#text":"\n","pages":{"#tail":"\n","#text":"1117"},"marker":{"#tail":"\n"},"location":{"#tail":"\n","#text":"Washington, D.C."},"booktitle":{"#tail":"\n","#text":"In Proceeding of the 2005 Annual Symposium of the American Medical Informatics Association (AMIA 2005),"},"@valid":"false"},{"volume":{"#tail":"\n","#text":"75"},"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Tbahriti, Imad, Christine Chichester, Fre?de?rique Lisacek, and Patrick Ruch. 2006. Using argumentation to retrieve articles with similar citations: An inquiry into improving related articles search in the MEDLINE digital library. International Journal of Medical Informatics, 75(6):488?495."},"journal":{"#tail":"\n","#text":"International Journal of Medical Informatics,"},"#text":"\n","issue":{"#tail":"\n","#text":"6"},"marker":{"#tail":"\n","#text":"Tbahriti, Chichester, Lisacek, Ruch, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering. In addition to question answering, multi-document summarization provides a complementary approac","@endWordPosition":"16121","@position":"107450","annotationId":"T49","@startWordPosition":"16118","@citStr":"Tbahriti et al (2006)"}},"title":{"#tail":"\n","#text":"Using argumentation to retrieve articles with similar citations: An inquiry into improving related articles search in the MEDLINE digital library."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Imad Tbahriti"},{"#tail":"\n","#text":"Christine Chichester"},{"#tail":"\n","#text":"Frederique Lisacek"},{"#tail":"\n","#text":"Patrick Ruch"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Ting, Kai Ming and Ian H. Witten. 1999. Issues in stacked generalization. Journal of Artificial Intelligence Research, 10:271?289."},"journal":{"#tail":"\n","#text":"Journal of Artificial Intelligence Research,"},"#text":"\n","pages":{"#tail":"\n","#text":"10--271"},"marker":{"#tail":"\n","#text":"Ting, Witten, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"lem or an intervention. The outputs of our basic classifiers are combined using a simple weighted linear interpolation scheme: Soutcome = ?1Scues + ?2Sunigram + ?3Sn-gram + ?4Sposition + ?5Slength + ?6Ssemantic type (1) We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition. The second involved a more principled method using confidence values generated by the base classifiers and least squares lin4 http://mallet.cs.umass.edu/ 76 Demner-Fushman and Lin Answering Clinical Questions ear regression adapted for classification (Ting and Witten 1999), which can be described by the following equation: LR(x) = N ? k=1 ?kPk(X) (2) Pk is the probability that a sentence specifies an outcome, as determined by classifier k (for classifiers that do not return actual probabilities, we normalized the scores and treated them as such). To predict the class of a sentence, the probabilities generated by n classifiers are combined using the coefficients (?0, ...,?n). These values are determined in the training stage as follows: Probabilities predicted by base classifiers for each sentence are represented in an N ? M matrix A, where M is the number of se","@endWordPosition":"6819","@position":"46010","annotationId":"T50","@startWordPosition":"6816","@citStr":"Ting and Witten 1999"}},"title":{"#tail":"\n","#text":"Issues in stacked generalization."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kai Ming Ting"},{"#tail":"\n","#text":"Ian H Witten"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Voorhees, Ellen M. 2003. Overview of the TREC 2003 question answering track. In Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), pages 54?68, Gaithersburg, MD."},"#text":"\n","pages":{"#tail":"\n","#text":"54--68"},"marker":{"#tail":"\n","#text":"Voorhees, 2003"},"location":{"#tail":"\n","#text":"Gaithersburg, MD."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"d for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems. In Sections 9 and 10, we have discussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of ?information nuggets? may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ?other? questions (Voorhees 2003). A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken. 12. Future Work The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously, a user study (Lin et al 20","@endWordPosition":"16469","@position":"109832","annotationId":"T51","@startWordPosition":"16468","@citStr":"Voorhees 2003"}},"title":{"#tail":"\n","#text":"Overview of the TREC"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Twelfth Text REtrieval Conference (TREC"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Ellen M Voorhees"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Voorhees, Ellen M. and Dawn M. Tice. 1999. The TREC-8 question answering track evaluation. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 83?106, Gaithersburg, MD."},"#text":"\n","pages":{"#tail":"\n","#text":"83--106"},"marker":{"#tail":"\n","#text":"Voorhees, Tice, 1999"},"location":{"#tail":"\n","#text":"Gaithersburg, MD."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"Section 12. Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative, we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail?this is the standard pipeline architecture commonly employed in other question-answering systems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001). The architecture of our system is shown in Figure 1. The query formulator is responsible for converting a clinical question (in the form of a query frame) into a PubMed search query. Presently, these queries are already encoded in our test collection (see Section 6). PubMed returns an initial list of MEDLINE citations, which is then analyzed by our knowledge extractors (see Section 5). The input to the semantic matcher, which implements our EBM citation scoring algorithm, is the query frame and annotated MEDLINE citations. The module outputs a ranked list of c","@endWordPosition":"3259","@position":"22597","annotationId":"T52","@startWordPosition":"3256","@citStr":"Voorhees and Tice 1999"},{"#tail":"\n","#text":"nd Discussion Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges P","@endWordPosition":"15729","@position":"104761","annotationId":"T53","@startWordPosition":"15726","@citStr":"Voorhees and Tice 1999"}]},"title":{"#tail":"\n","#text":"The TREC-8 question answering track evaluation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Eighth Text REtrieval Conference (TREC-8),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ellen M Voorhees"},{"#tail":"\n","#text":"Dawn M Tice"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Wilczynski, Nancy, K. Ann McKibbon, and R. Brian Haynes. 2001. Enhancing retrieval of best evidence for health care from bibliographic databases: Calibration of the hand search of the literature. In Proceedings of 10th World Congress on Medical Informatics (MEDINFO 2001), pages 390?393, London, England."},"#text":"\n","pages":{"#tail":"\n","#text":"390--393"},"marker":{"#tail":"\n","#text":"Wilczynski, McKibbon, Haynes, 2001"},"location":{"#tail":"\n","#text":"London, England."},"title":{"#tail":"\n","#text":"Enhancing retrieval of best evidence for health care from bibliographic databases: Calibration of the hand search of the literature."},"booktitle":{"#tail":"\n","#text":"In Proceedings of 10th World Congress on Medical Informatics (MEDINFO"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Nancy Wilczynski"},{"#tail":"\n","#text":"K Ann McKibbon"},{"#tail":"\n","#text":"R Brian Haynes"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Yang, Yiming and Jan O. Pedersen. 1997."},"#text":"\n","marker":{"#tail":"\n","#text":"Yang, Pedersen, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"Map, which sets the maximum score to 8. The two phrases dropout rate and adverse events contribute one point each to the cumulative score, which results in a likelihood estimate of 0.25 for this sentence. The unigram ?bag of words? classifier is a naive Bayes classifier implemented with the API provided by the MALLET toolkit.4 This classifier outputs the probability of a class assignment. The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure (Yang and Pedersen 1997), and then selected only the positive outcome predictors using odds ratio (Mladenic and Grobelnik 1999). Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment. The position classifier returns the maximum likelihood estimate that a sentence is an outcome based on its position in the abstract (for structured abstracts, with respect to the results or conclusions sections; for unstructured abstract","@endWordPosition":"6496","@position":"43884","annotationId":"T54","@startWordPosition":"6493","@citStr":"Yang and Pedersen 1997"}},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Yiming Yang"},{"#tail":"\n","#text":"Jan O Pedersen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"A comparative study on feature selection in text categorization. In Proceedings of the Fourteenth International Conference on Machine Learning (ICML 1997), pages 412?420, Nashville, TN."},"#text":"\n","pages":{"#tail":"\n","#text":"412--420"},"marker":{"#tail":"\n","#text":"1997"},"location":{"#tail":"\n","#text":"Nashville, TN."},"title":{"#tail":"\n","#text":"A comparative study on feature selection in text categorization."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourteenth International Conference on Machine Learning (ICML"},"@valid":"true"}]}}]}}
