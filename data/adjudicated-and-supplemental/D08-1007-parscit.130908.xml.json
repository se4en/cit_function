{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"volume":{"#tail":"\n","#text":"16"},"#tail":"\n","date":{"#tail":"\n","#text":"1990"},"rawString":{"#tail":"\n","#text":"Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22\u201329."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Church, Hanks, 1990"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ce word order) to create a neighborhood of implicit negative evidence. We create negatives by substitution rather than perturbation, and use corpuswide statistics to choose our negative instances. 3 Methodology 3.1 Creating Examples To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text. To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data. We measure this association using pointwise Mutual Information (MI) (Church and Hanks, 1990). The MI between a 60 verb predicate, v, and its object argument, n, is: Pr(v, n) Pr(n|v) MI(v, n) =log Pr(v)Pr(n) =log Pr(n) (2) If MI>0, the probability v and n occur together is greater than if they were independently distributed. We create sets of positive and negative examples separately for each predicate, v. First, we extract all pairs where MI(v, n)>T as positives. For each positive, we create pseudo-negative examples, (v, n\u2032), by pairing v with a new argument, n\u2032, that either has MI below the threshold or did not occur with v in the corpus. We require each negative n\u2032 to have a simila","@endWordPosition":"1224","@position":"8146","annotationId":"T1","@startWordPosition":"1221","@citStr":"Church and Hanks, 1990"}},"title":{"#tail":"\n","#text":"Word association norms, mutual information, and lexicography."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kenneth Ward Church"},{"#tail":"\n","#text":"Patrick Hanks"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Michael Fleischman, Eduard Hovy, and Abdessamad Echihabi. 2003. Offline strategies for online question answering: answering questions before they are asked. In ACL, pages 1\u20137."},"#text":"\n","pages":{"#tail":"\n","#text":"1--7"},"marker":{"#tail":"\n","#text":"Fleischman, Hovy, Echihabi, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":", 2002) on a 10 GB corpus, giving 3620 clusters. If a noun belongs in a cluster, a corresponding feature fires. If a noun is in none of the clusters, a no-class feature fires. As an example, CBC cluster 1891 contains: sidewalk, driveway, roadway, footpath, bridge, highway, road, runway, street, alley, path, Interstate, ... In our training data, we have examples like widen highway, widen road and widen motorway. If we 62 see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).2 This data provides counts for pairs such as \u201cEdwin Moses, hurdler\u201d and \u201cWilliam Farley, industrialist.\u201d We have features for all concepts and therefore learn their association with each verb. 4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set t","@endWordPosition":"2773","@position":"17455","annotationId":"T2","@startWordPosition":"2770","@citStr":"Fleischman et al. (2003)"}},"title":{"#tail":"\n","#text":"Offline strategies for online question answering: answering questions before they are asked."},"booktitle":{"#tail":"\n","#text":"In ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Michael Fleischman"},{"#tail":"\n","#text":"Eduard Hovy"},{"#tail":"\n","#text":"Abdessamad Echihabi"}]}},{"volume":{"#tail":"\n","#text":"29"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459\u2013484."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Keller, Lapata, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" they rarely contain digits, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n\u2032) where v, n is a predicateargument pair observed in the corpus and v, n\u2032 has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not ","@endWordPosition":"1673","@position":"10830","annotationId":"T3","@startWordPosition":"1669","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"m of attribute-value features. Let every feature Oi be of the form Oi(v, n) = (v = v� n f(n)). That is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f(n). For example, a feature for a verb-object pair might be, \u201cthe verb is eat and the object is lower-case.\u201d In this representation, features for one predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)\u2019s conditional probability scores for pseudodisambiguation of (v, n, n\u2032) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007). 61 classifier for each predicate independently. The prediction becomes yv = Av · 4)v(n), where Av are the learned weights corresponding to predicate v and all features 4)v(n)=f(n) depend on the argument only. Some predicate partitions ","@endWordPosition":"1970","@position":"12556","annotationId":"T4","@startWordPosition":"1967","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"eriments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a l","@endWordPosition":"2902","@position":"18278","annotationId":"T5","@startWordPosition":"2899","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"igh for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations. 4.3 Pseudodisambiguation We first evaluate DSP on disambiguating positives from pseudo-negatives, comparing to recently3Which all correspond to nouns occurring in the object position of the verb (e.g. Probj(n|lead)), except \u201claunch (subj)\u201d which corresponds to Pr3ubj(n|launch). 63 System MacroAvg MicroAvg F Pairwise P R F P R Acc Cov Dagan et al. (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98 Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83 Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57 DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00 DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)\u2019s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Er","@endWordPosition":"3461","@position":"21758","annotationId":"T6","@startWordPosition":"3458","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":" it. We tested both and adopt the latter because it resulted in better performance on our development set. 5Available from the LDC as LDC2006T13. This collection was generated from approximately 1 trillion tokens of online text. Unfortunately, tokens appearing less than 200 times have been mapped to the (UNK) symbol, and only N-grams appearing more than 40 times are included. Unlike results from search engines, however, experiments with this corpus are replicable. not be able to provide a score for each example. The similarity-smoothed examples will be undefined if SIMS(w) is empty. Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. As a reasonable default for these cases, we assign them a negative decision. We evaluate disambiguation using precision (P), recall (R), and their harmonic mean, F-Score (F). Table 1 gives the results of our comparison. In the MacroAvg results, we weight each example equally. For MicroAvg, we weight each example by the frequency of the noun. To more directly compare with previous work, we also reproduced Pairwise Disambiguation by randomly pairing each positive with one of the negatives and then evaluating each system by the per","@endWordPosition":"3850","@position":"24123","annotationId":"T7","@startWordPosition":"3847","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"accepts far more pairs than MI (73% vs. 44%), even far more than a system that accepts any previously observed verb-object combination as plausible (57%). Recall is higher on more frequent verb-object pairs, but 70% of the pairs occurred only once in the corpus. Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modified KN-smoothing (Chen and Goodman, 1998), the recall of MI>0 on SJM only increases from 44.1% to 44.9%, still far below DSP. Frequency-based models have fundamentally low coverage. As furimportant than the property of being an entity\u201d (Resnik, 1996). DSPall Erk (2007) Keller and Lapata (2003) F-Score 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 65 Verb Plaus./Implaus. Resnik Dagan et al. Erk MI DSP see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02 read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/\u2014 2.12/-0.65 find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81 hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67 write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31 urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/\u2014 -0.34/-0.62 warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/\u2014 2.00/-0.99 ","@endWordPosition":"4782","@position":"29752","annotationId":"T8","@startWordPosition":"4779","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"d with a dash (\u2014). Only DSP is completely defined and completely correct. 0 0.2 0.4 0.6 0.8 1 Recall Figure 2: Pronoun resolution precision-recall on MUC. ther evidence, if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation experiment (Section 4.3), MI>0 gets a MacroAvg precision of 86% but a MacroAvg recall of only 12%.9 4.6 Pronoun Resolution Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004). We study the cases where a 9Recall that even the Keller and Lapata (2003) system, built on the world\u2019s largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). pronoun is the direct object of a verb predicate, v. A pronoun\u2019s antecedent must obey v\u2019s selectional preferences. If we have a better model of SP, we should be able to better select pronoun antecedents. We parsed the MUC-7 (1997) coreference corpus and extracted all pronouns in a direct object relation. For each pronoun, p, modified by a verb, v, we extracted all preceding nouns within the current or previous sentence. Thir","@endWordPosition":"5047","@position":"31788","annotationId":"T9","@startWordPosition":"5044","@citStr":"Keller and Lapata (2003)"}]},"title":{"#tail":"\n","#text":"Using the web to obtain frequencies for unseen bigrams."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Frank Keller"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"volume":{"#tail":"\n","#text":"29"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459\u2013484."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Keller, Lapata, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" they rarely contain digits, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n\u2032) where v, n is a predicateargument pair observed in the corpus and v, n\u2032 has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not ","@endWordPosition":"1673","@position":"10830","annotationId":"T10","@startWordPosition":"1669","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"m of attribute-value features. Let every feature Oi be of the form Oi(v, n) = (v = v� n f(n)). That is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f(n). For example, a feature for a verb-object pair might be, \u201cthe verb is eat and the object is lower-case.\u201d In this representation, features for one predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)\u2019s conditional probability scores for pseudodisambiguation of (v, n, n\u2032) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007). 61 classifier for each predicate independently. The prediction becomes yv = Av · 4)v(n), where Av are the learned weights corresponding to predicate v and all features 4)v(n)=f(n) depend on the argument only. Some predicate partitions ","@endWordPosition":"1970","@position":"12556","annotationId":"T11","@startWordPosition":"1967","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"eriments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a l","@endWordPosition":"2902","@position":"18278","annotationId":"T12","@startWordPosition":"2899","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"igh for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations. 4.3 Pseudodisambiguation We first evaluate DSP on disambiguating positives from pseudo-negatives, comparing to recently3Which all correspond to nouns occurring in the object position of the verb (e.g. Probj(n|lead)), except \u201claunch (subj)\u201d which corresponds to Pr3ubj(n|launch). 63 System MacroAvg MicroAvg F Pairwise P R F P R Acc Cov Dagan et al. (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98 Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83 Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57 DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00 DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)\u2019s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Er","@endWordPosition":"3461","@position":"21758","annotationId":"T13","@startWordPosition":"3458","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":" it. We tested both and adopt the latter because it resulted in better performance on our development set. 5Available from the LDC as LDC2006T13. This collection was generated from approximately 1 trillion tokens of online text. Unfortunately, tokens appearing less than 200 times have been mapped to the (UNK) symbol, and only N-grams appearing more than 40 times are included. Unlike results from search engines, however, experiments with this corpus are replicable. not be able to provide a score for each example. The similarity-smoothed examples will be undefined if SIMS(w) is empty. Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. As a reasonable default for these cases, we assign them a negative decision. We evaluate disambiguation using precision (P), recall (R), and their harmonic mean, F-Score (F). Table 1 gives the results of our comparison. In the MacroAvg results, we weight each example equally. For MicroAvg, we weight each example by the frequency of the noun. To more directly compare with previous work, we also reproduced Pairwise Disambiguation by randomly pairing each positive with one of the negatives and then evaluating each system by the per","@endWordPosition":"3850","@position":"24123","annotationId":"T14","@startWordPosition":"3847","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"accepts far more pairs than MI (73% vs. 44%), even far more than a system that accepts any previously observed verb-object combination as plausible (57%). Recall is higher on more frequent verb-object pairs, but 70% of the pairs occurred only once in the corpus. Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modified KN-smoothing (Chen and Goodman, 1998), the recall of MI>0 on SJM only increases from 44.1% to 44.9%, still far below DSP. Frequency-based models have fundamentally low coverage. As furimportant than the property of being an entity\u201d (Resnik, 1996). DSPall Erk (2007) Keller and Lapata (2003) F-Score 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 65 Verb Plaus./Implaus. Resnik Dagan et al. Erk MI DSP see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02 read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/\u2014 2.12/-0.65 find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81 hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67 write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31 urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/\u2014 -0.34/-0.62 warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/\u2014 2.00/-0.99 ","@endWordPosition":"4782","@position":"29752","annotationId":"T15","@startWordPosition":"4779","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"d with a dash (\u2014). Only DSP is completely defined and completely correct. 0 0.2 0.4 0.6 0.8 1 Recall Figure 2: Pronoun resolution precision-recall on MUC. ther evidence, if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation experiment (Section 4.3), MI>0 gets a MacroAvg precision of 86% but a MacroAvg recall of only 12%.9 4.6 Pronoun Resolution Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004). We study the cases where a 9Recall that even the Keller and Lapata (2003) system, built on the world\u2019s largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). pronoun is the direct object of a verb predicate, v. A pronoun\u2019s antecedent must obey v\u2019s selectional preferences. If we have a better model of SP, we should be able to better select pronoun antecedents. We parsed the MUC-7 (1997) coreference corpus and extracted all pronouns in a direct object relation. For each pronoun, p, modified by a verb, v, we extracted all preceding nouns within the current or previous sentence. Thir","@endWordPosition":"5047","@position":"31788","annotationId":"T16","@startWordPosition":"5044","@citStr":"Keller and Lapata (2003)"}]},"title":{"#tail":"\n","#text":"Using the web to obtain frequencies for unseen bigrams."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Frank Keller"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"volume":{"#tail":"\n","#text":"29"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459\u2013484."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Keller, Lapata, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" they rarely contain digits, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n\u2032) where v, n is a predicateargument pair observed in the corpus and v, n\u2032 has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not ","@endWordPosition":"1673","@position":"10830","annotationId":"T17","@startWordPosition":"1669","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"m of attribute-value features. Let every feature Oi be of the form Oi(v, n) = (v = v� n f(n)). That is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f(n). For example, a feature for a verb-object pair might be, \u201cthe verb is eat and the object is lower-case.\u201d In this representation, features for one predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)\u2019s conditional probability scores for pseudodisambiguation of (v, n, n\u2032) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007). 61 classifier for each predicate independently. The prediction becomes yv = Av · 4)v(n), where Av are the learned weights corresponding to predicate v and all features 4)v(n)=f(n) depend on the argument only. Some predicate partitions ","@endWordPosition":"1970","@position":"12556","annotationId":"T18","@startWordPosition":"1967","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"eriments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a l","@endWordPosition":"2902","@position":"18278","annotationId":"T19","@startWordPosition":"2899","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"igh for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations. 4.3 Pseudodisambiguation We first evaluate DSP on disambiguating positives from pseudo-negatives, comparing to recently3Which all correspond to nouns occurring in the object position of the verb (e.g. Probj(n|lead)), except \u201claunch (subj)\u201d which corresponds to Pr3ubj(n|launch). 63 System MacroAvg MicroAvg F Pairwise P R F P R Acc Cov Dagan et al. (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98 Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83 Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57 DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00 DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)\u2019s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Er","@endWordPosition":"3461","@position":"21758","annotationId":"T20","@startWordPosition":"3458","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":" it. We tested both and adopt the latter because it resulted in better performance on our development set. 5Available from the LDC as LDC2006T13. This collection was generated from approximately 1 trillion tokens of online text. Unfortunately, tokens appearing less than 200 times have been mapped to the (UNK) symbol, and only N-grams appearing more than 40 times are included. Unlike results from search engines, however, experiments with this corpus are replicable. not be able to provide a score for each example. The similarity-smoothed examples will be undefined if SIMS(w) is empty. Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. As a reasonable default for these cases, we assign them a negative decision. We evaluate disambiguation using precision (P), recall (R), and their harmonic mean, F-Score (F). Table 1 gives the results of our comparison. In the MacroAvg results, we weight each example equally. For MicroAvg, we weight each example by the frequency of the noun. To more directly compare with previous work, we also reproduced Pairwise Disambiguation by randomly pairing each positive with one of the negatives and then evaluating each system by the per","@endWordPosition":"3850","@position":"24123","annotationId":"T21","@startWordPosition":"3847","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"accepts far more pairs than MI (73% vs. 44%), even far more than a system that accepts any previously observed verb-object combination as plausible (57%). Recall is higher on more frequent verb-object pairs, but 70% of the pairs occurred only once in the corpus. Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modified KN-smoothing (Chen and Goodman, 1998), the recall of MI>0 on SJM only increases from 44.1% to 44.9%, still far below DSP. Frequency-based models have fundamentally low coverage. As furimportant than the property of being an entity\u201d (Resnik, 1996). DSPall Erk (2007) Keller and Lapata (2003) F-Score 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 65 Verb Plaus./Implaus. Resnik Dagan et al. Erk MI DSP see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02 read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/\u2014 2.12/-0.65 find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81 hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67 write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31 urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/\u2014 -0.34/-0.62 warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/\u2014 2.00/-0.99 ","@endWordPosition":"4782","@position":"29752","annotationId":"T22","@startWordPosition":"4779","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"d with a dash (\u2014). Only DSP is completely defined and completely correct. 0 0.2 0.4 0.6 0.8 1 Recall Figure 2: Pronoun resolution precision-recall on MUC. ther evidence, if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation experiment (Section 4.3), MI>0 gets a MacroAvg precision of 86% but a MacroAvg recall of only 12%.9 4.6 Pronoun Resolution Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004). We study the cases where a 9Recall that even the Keller and Lapata (2003) system, built on the world\u2019s largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). pronoun is the direct object of a verb predicate, v. A pronoun\u2019s antecedent must obey v\u2019s selectional preferences. If we have a better model of SP, we should be able to better select pronoun antecedents. We parsed the MUC-7 (1997) coreference corpus and extracted all pronouns in a direct object relation. For each pronoun, p, modified by a verb, v, we extracted all preceding nouns within the current or previous sentence. Thir","@endWordPosition":"5047","@position":"31788","annotationId":"T23","@startWordPosition":"5044","@citStr":"Keller and Lapata (2003)"}]},"title":{"#tail":"\n","#text":"Using the web to obtain frequencies for unseen bigrams."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Frank Keller"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"volume":{"#tail":"\n","#text":"29"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459\u2013484."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Keller, Lapata, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" they rarely contain digits, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n\u2032) where v, n is a predicateargument pair observed in the corpus and v, n\u2032 has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not ","@endWordPosition":"1673","@position":"10830","annotationId":"T24","@startWordPosition":"1669","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"m of attribute-value features. Let every feature Oi be of the form Oi(v, n) = (v = v� n f(n)). That is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f(n). For example, a feature for a verb-object pair might be, \u201cthe verb is eat and the object is lower-case.\u201d In this representation, features for one predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)\u2019s conditional probability scores for pseudodisambiguation of (v, n, n\u2032) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007). 61 classifier for each predicate independently. The prediction becomes yv = Av · 4)v(n), where Av are the learned weights corresponding to predicate v and all features 4)v(n)=f(n) depend on the argument only. Some predicate partitions ","@endWordPosition":"1970","@position":"12556","annotationId":"T25","@startWordPosition":"1967","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"eriments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a l","@endWordPosition":"2902","@position":"18278","annotationId":"T26","@startWordPosition":"2899","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"igh for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations. 4.3 Pseudodisambiguation We first evaluate DSP on disambiguating positives from pseudo-negatives, comparing to recently3Which all correspond to nouns occurring in the object position of the verb (e.g. Probj(n|lead)), except \u201claunch (subj)\u201d which corresponds to Pr3ubj(n|launch). 63 System MacroAvg MicroAvg F Pairwise P R F P R Acc Cov Dagan et al. (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98 Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83 Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57 DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00 DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)\u2019s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Er","@endWordPosition":"3461","@position":"21758","annotationId":"T27","@startWordPosition":"3458","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":" it. We tested both and adopt the latter because it resulted in better performance on our development set. 5Available from the LDC as LDC2006T13. This collection was generated from approximately 1 trillion tokens of online text. Unfortunately, tokens appearing less than 200 times have been mapped to the (UNK) symbol, and only N-grams appearing more than 40 times are included. Unlike results from search engines, however, experiments with this corpus are replicable. not be able to provide a score for each example. The similarity-smoothed examples will be undefined if SIMS(w) is empty. Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. As a reasonable default for these cases, we assign them a negative decision. We evaluate disambiguation using precision (P), recall (R), and their harmonic mean, F-Score (F). Table 1 gives the results of our comparison. In the MacroAvg results, we weight each example equally. For MicroAvg, we weight each example by the frequency of the noun. To more directly compare with previous work, we also reproduced Pairwise Disambiguation by randomly pairing each positive with one of the negatives and then evaluating each system by the per","@endWordPosition":"3850","@position":"24123","annotationId":"T28","@startWordPosition":"3847","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"accepts far more pairs than MI (73% vs. 44%), even far more than a system that accepts any previously observed verb-object combination as plausible (57%). Recall is higher on more frequent verb-object pairs, but 70% of the pairs occurred only once in the corpus. Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modified KN-smoothing (Chen and Goodman, 1998), the recall of MI>0 on SJM only increases from 44.1% to 44.9%, still far below DSP. Frequency-based models have fundamentally low coverage. As furimportant than the property of being an entity\u201d (Resnik, 1996). DSPall Erk (2007) Keller and Lapata (2003) F-Score 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 65 Verb Plaus./Implaus. Resnik Dagan et al. Erk MI DSP see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02 read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/\u2014 2.12/-0.65 find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81 hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67 write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31 urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/\u2014 -0.34/-0.62 warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/\u2014 2.00/-0.99 ","@endWordPosition":"4782","@position":"29752","annotationId":"T29","@startWordPosition":"4779","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"d with a dash (\u2014). Only DSP is completely defined and completely correct. 0 0.2 0.4 0.6 0.8 1 Recall Figure 2: Pronoun resolution precision-recall on MUC. ther evidence, if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation experiment (Section 4.3), MI>0 gets a MacroAvg precision of 86% but a MacroAvg recall of only 12%.9 4.6 Pronoun Resolution Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004). We study the cases where a 9Recall that even the Keller and Lapata (2003) system, built on the world\u2019s largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). pronoun is the direct object of a verb predicate, v. A pronoun\u2019s antecedent must obey v\u2019s selectional preferences. If we have a better model of SP, we should be able to better select pronoun antecedents. We parsed the MUC-7 (1997) coreference corpus and extracted all pronouns in a direct object relation. For each pronoun, p, modified by a verb, v, we extracted all preceding nouns within the current or previous sentence. Thir","@endWordPosition":"5047","@position":"31788","annotationId":"T30","@startWordPosition":"5044","@citStr":"Keller and Lapata (2003)"}]},"title":{"#tail":"\n","#text":"Using the web to obtain frequencies for unseen bigrams."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Frank Keller"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"volume":{"#tail":"\n","#text":"29"},"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459\u2013484."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"3"},"marker":{"#tail":"\n","#text":"Keller, Lapata, 2003"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" they rarely contain digits, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n\u2032) where v, n is a predicateargument pair observed in the corpus and v, n\u2032 has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not ","@endWordPosition":"1673","@position":"10830","annotationId":"T31","@startWordPosition":"1669","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"m of attribute-value features. Let every feature Oi be of the form Oi(v, n) = (v = v� n f(n)). That is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f(n). For example, a feature for a verb-object pair might be, \u201cthe verb is eat and the object is lower-case.\u201d In this representation, features for one predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)\u2019s conditional probability scores for pseudodisambiguation of (v, n, n\u2032) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007). 61 classifier for each predicate independently. The prediction becomes yv = Av · 4)v(n), where Av are the learned weights corresponding to predicate v and all features 4)v(n)=f(n) depend on the argument only. Some predicate partitions ","@endWordPosition":"1970","@position":"12556","annotationId":"T32","@startWordPosition":"1967","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"eriments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a l","@endWordPosition":"2902","@position":"18278","annotationId":"T33","@startWordPosition":"2899","@citStr":"Keller and Lapata, 2003"},{"#tail":"\n","#text":"igh for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations. 4.3 Pseudodisambiguation We first evaluate DSP on disambiguating positives from pseudo-negatives, comparing to recently3Which all correspond to nouns occurring in the object position of the verb (e.g. Probj(n|lead)), except \u201claunch (subj)\u201d which corresponds to Pr3ubj(n|launch). 63 System MacroAvg MicroAvg F Pairwise P R F P R Acc Cov Dagan et al. (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98 Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83 Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57 DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00 DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)\u2019s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Er","@endWordPosition":"3461","@position":"21758","annotationId":"T34","@startWordPosition":"3458","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":" it. We tested both and adopt the latter because it resulted in better performance on our development set. 5Available from the LDC as LDC2006T13. This collection was generated from approximately 1 trillion tokens of online text. Unfortunately, tokens appearing less than 200 times have been mapped to the (UNK) symbol, and only N-grams appearing more than 40 times are included. Unlike results from search engines, however, experiments with this corpus are replicable. not be able to provide a score for each example. The similarity-smoothed examples will be undefined if SIMS(w) is empty. Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. As a reasonable default for these cases, we assign them a negative decision. We evaluate disambiguation using precision (P), recall (R), and their harmonic mean, F-Score (F). Table 1 gives the results of our comparison. In the MacroAvg results, we weight each example equally. For MicroAvg, we weight each example by the frequency of the noun. To more directly compare with previous work, we also reproduced Pairwise Disambiguation by randomly pairing each positive with one of the negatives and then evaluating each system by the per","@endWordPosition":"3850","@position":"24123","annotationId":"T35","@startWordPosition":"3847","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"accepts far more pairs than MI (73% vs. 44%), even far more than a system that accepts any previously observed verb-object combination as plausible (57%). Recall is higher on more frequent verb-object pairs, but 70% of the pairs occurred only once in the corpus. Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modified KN-smoothing (Chen and Goodman, 1998), the recall of MI>0 on SJM only increases from 44.1% to 44.9%, still far below DSP. Frequency-based models have fundamentally low coverage. As furimportant than the property of being an entity\u201d (Resnik, 1996). DSPall Erk (2007) Keller and Lapata (2003) F-Score 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 65 Verb Plaus./Implaus. Resnik Dagan et al. Erk MI DSP see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02 read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/\u2014 2.12/-0.65 find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81 hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67 write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31 urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/\u2014 -0.34/-0.62 warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/\u2014 2.00/-0.99 ","@endWordPosition":"4782","@position":"29752","annotationId":"T36","@startWordPosition":"4779","@citStr":"Keller and Lapata (2003)"},{"#tail":"\n","#text":"d with a dash (\u2014). Only DSP is completely defined and completely correct. 0 0.2 0.4 0.6 0.8 1 Recall Figure 2: Pronoun resolution precision-recall on MUC. ther evidence, if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation experiment (Section 4.3), MI>0 gets a MacroAvg precision of 86% but a MacroAvg recall of only 12%.9 4.6 Pronoun Resolution Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004). We study the cases where a 9Recall that even the Keller and Lapata (2003) system, built on the world\u2019s largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). pronoun is the direct object of a verb predicate, v. A pronoun\u2019s antecedent must obey v\u2019s selectional preferences. If we have a better model of SP, we should be able to better select pronoun antecedents. We parsed the MUC-7 (1997) coreference corpus and extracted all pronouns in a direct object relation. For each pronoun, p, modified by a verb, v, we extracted all preceding nouns within the current or previous sentence. Thir","@endWordPosition":"5047","@position":"31788","annotationId":"T37","@startWordPosition":"5044","@citStr":"Keller and Lapata (2003)"}]},"title":{"#tail":"\n","#text":"Using the web to obtain frequencies for unseen bigrams."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Frank Keller"},{"#tail":"\n","#text":"Mirella Lapata"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Dekang Lin. 1998a. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768\u2013773."},"#text":"\n","pages":{"#tail":"\n","#text":"768--773"},"marker":{"#tail":"\n","#text":"Lin, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" from observed text. Dagan et al. (1999) define the similarity-weighted probability, PrSIM, to be: �PrSIM(n|v) � Sim(v\u2032, v)Pr(n|v\u2032) (1) v\u2032∈SIMS(v) where Sim(v\u2032, v) returns a real-valued similarity between two verbs v\u2032 and v (normalized over all pair similarities in the sum). In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, SIMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)\u2019s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as \u201c[X wins Y] ⇒ [X plays Y]\u201d are only valid for certain arguments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminative t","@endWordPosition":"817","@position":"5476","annotationId":"T38","@startWordPosition":"816","@citStr":"Lin (1998"},{"#tail":"\n","#text":".. In our training data, we have examples like widen highway, widen road and widen motorway. If we 62 see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).2 This data provides counts for pairs such as \u201cEdwin Moses, hurdler\u201d and \u201cWilliam Farley, industrialist.\u201d We have features for all concepts and therefore learn their association with each verb. 4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative arg","@endWordPosition":"2823","@position":"17762","annotationId":"T39","@startWordPosition":"2822","@citStr":"Lin, 1998"},{"#tail":"\n","#text":"cations: max at j=2) on the macroaveraged score across all development partitions. Note that we can not use the development set to optimize T and K because the development examples are obtained after setting these values. 4.2 Feature weights It is interesting to inspect the feature weights returned by our system. In particular, the weights on the verb co-occurrence features (Section 3.3.1) provide a high-quality, argument-specific similarityranking of other verb contexts. The DSP parameters for eat, for example, place high weight on features like Pr(nlbraise), Pr(nlration), and Pr(nlgarnish). Lin (1998a)\u2019s similar word list for eat misses these but includes sleep (ranked 6) and sit (ranked 14), because these have similar subjects to eat. Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP\u201eoo, on the verb join,3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a): participate 0.164, lead 0.150, return to 0.148, say 0.143, rejoin 0.142, sign ","@endWordPosition":"3228","@position":"20326","annotationId":"T40","@startWordPosition":"3227","@citStr":"Lin (1998"},{"#tail":"\n","#text":"71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)\u2019s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Erk (2007): �MISIM(n, v) = log Sim(n\u2032, n) Pr(v, n\u2032) n\u2032∈SIMS(n) Pr(v)Pr(n\u2032) We gather similar words using Lin (1998a), mining similar verbs from a comparable-sized parsed corpus, and collecting similar nouns from a broader 10 GB corpus of English text.4 We also use Keller and Lapata (2003)\u2019s approach to obtaining web-counts. Rather than mining parse trees, this technique retrieves counts for the pattern \u201cV Det N\u201d in raw online text, where V is any inflection of the verb, Det is the, a, or the empty string, and N is the singular or plural form of the noun. We compute a web-based MI by collecting Pr(n, v), Pr(n), and Pr(v) using all inflections, except we only use the root form of the noun. Rather than using","@endWordPosition":"3575","@position":"22470","annotationId":"T41","@startWordPosition":"3574","@citStr":"Lin (1998"}]},"title":{"#tail":"\n","#text":"Automatic retrieval and clustering of similar words."},"booktitle":{"#tail":"\n","#text":"In COLING-ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dekang Lin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Dekang Lin. 1998a. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768\u2013773."},"#text":"\n","pages":{"#tail":"\n","#text":"768--773"},"marker":{"#tail":"\n","#text":"Lin, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" from observed text. Dagan et al. (1999) define the similarity-weighted probability, PrSIM, to be: �PrSIM(n|v) � Sim(v\u2032, v)Pr(n|v\u2032) (1) v\u2032∈SIMS(v) where Sim(v\u2032, v) returns a real-valued similarity between two verbs v\u2032 and v (normalized over all pair similarities in the sum). In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, SIMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)\u2019s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as \u201c[X wins Y] ⇒ [X plays Y]\u201d are only valid for certain arguments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminative t","@endWordPosition":"817","@position":"5476","annotationId":"T42","@startWordPosition":"816","@citStr":"Lin (1998"},{"#tail":"\n","#text":".. In our training data, we have examples like widen highway, widen road and widen motorway. If we 62 see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).2 This data provides counts for pairs such as \u201cEdwin Moses, hurdler\u201d and \u201cWilliam Farley, industrialist.\u201d We have features for all concepts and therefore learn their association with each verb. 4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative arg","@endWordPosition":"2823","@position":"17762","annotationId":"T43","@startWordPosition":"2822","@citStr":"Lin, 1998"},{"#tail":"\n","#text":"cations: max at j=2) on the macroaveraged score across all development partitions. Note that we can not use the development set to optimize T and K because the development examples are obtained after setting these values. 4.2 Feature weights It is interesting to inspect the feature weights returned by our system. In particular, the weights on the verb co-occurrence features (Section 3.3.1) provide a high-quality, argument-specific similarityranking of other verb contexts. The DSP parameters for eat, for example, place high weight on features like Pr(nlbraise), Pr(nlration), and Pr(nlgarnish). Lin (1998a)\u2019s similar word list for eat misses these but includes sleep (ranked 6) and sit (ranked 14), because these have similar subjects to eat. Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP\u201eoo, on the verb join,3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a): participate 0.164, lead 0.150, return to 0.148, say 0.143, rejoin 0.142, sign ","@endWordPosition":"3228","@position":"20326","annotationId":"T44","@startWordPosition":"3227","@citStr":"Lin (1998"},{"#tail":"\n","#text":"71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)\u2019s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Erk (2007): �MISIM(n, v) = log Sim(n\u2032, n) Pr(v, n\u2032) n\u2032∈SIMS(n) Pr(v)Pr(n\u2032) We gather similar words using Lin (1998a), mining similar verbs from a comparable-sized parsed corpus, and collecting similar nouns from a broader 10 GB corpus of English text.4 We also use Keller and Lapata (2003)\u2019s approach to obtaining web-counts. Rather than mining parse trees, this technique retrieves counts for the pattern \u201cV Det N\u201d in raw online text, where V is any inflection of the verb, Det is the, a, or the empty string, and N is the singular or plural form of the noun. We compute a web-based MI by collecting Pr(n, v), Pr(n), and Pr(v) using all inflections, except we only use the root form of the noun. Rather than using","@endWordPosition":"3575","@position":"22470","annotationId":"T45","@startWordPosition":"3574","@citStr":"Lin (1998"}]},"title":{"#tail":"\n","#text":"Automatic retrieval and clustering of similar words."},"booktitle":{"#tail":"\n","#text":"In COLING-ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dekang Lin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Dekang Lin. 1998a. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768\u2013773."},"#text":"\n","pages":{"#tail":"\n","#text":"768--773"},"marker":{"#tail":"\n","#text":"Lin, 1998"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" from observed text. Dagan et al. (1999) define the similarity-weighted probability, PrSIM, to be: �PrSIM(n|v) � Sim(v\u2032, v)Pr(n|v\u2032) (1) v\u2032∈SIMS(v) where Sim(v\u2032, v) returns a real-valued similarity between two verbs v\u2032 and v (normalized over all pair similarities in the sum). In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, SIMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)\u2019s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as \u201c[X wins Y] ⇒ [X plays Y]\u201d are only valid for certain arguments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminative t","@endWordPosition":"817","@position":"5476","annotationId":"T46","@startWordPosition":"816","@citStr":"Lin (1998"},{"#tail":"\n","#text":".. In our training data, we have examples like widen highway, widen road and widen motorway. If we 62 see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).2 This data provides counts for pairs such as \u201cEdwin Moses, hurdler\u201d and \u201cWilliam Farley, industrialist.\u201d We have features for all concepts and therefore learn their association with each verb. 4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative arg","@endWordPosition":"2823","@position":"17762","annotationId":"T47","@startWordPosition":"2822","@citStr":"Lin, 1998"},{"#tail":"\n","#text":"cations: max at j=2) on the macroaveraged score across all development partitions. Note that we can not use the development set to optimize T and K because the development examples are obtained after setting these values. 4.2 Feature weights It is interesting to inspect the feature weights returned by our system. In particular, the weights on the verb co-occurrence features (Section 3.3.1) provide a high-quality, argument-specific similarityranking of other verb contexts. The DSP parameters for eat, for example, place high weight on features like Pr(nlbraise), Pr(nlration), and Pr(nlgarnish). Lin (1998a)\u2019s similar word list for eat misses these but includes sleep (ranked 6) and sit (ranked 14), because these have similar subjects to eat. Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP\u201eoo, on the verb join,3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a): participate 0.164, lead 0.150, return to 0.148, say 0.143, rejoin 0.142, sign ","@endWordPosition":"3228","@position":"20326","annotationId":"T48","@startWordPosition":"3227","@citStr":"Lin (1998"},{"#tail":"\n","#text":"71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise). proposed systems that also require no manuallycompiled resources like WordNet. We convert Dagan et al. (1999)\u2019s similarity-smoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by Erk (2007): �MISIM(n, v) = log Sim(n\u2032, n) Pr(v, n\u2032) n\u2032∈SIMS(n) Pr(v)Pr(n\u2032) We gather similar words using Lin (1998a), mining similar verbs from a comparable-sized parsed corpus, and collecting similar nouns from a broader 10 GB corpus of English text.4 We also use Keller and Lapata (2003)\u2019s approach to obtaining web-counts. Rather than mining parse trees, this technique retrieves counts for the pattern \u201cV Det N\u201d in raw online text, where V is any inflection of the verb, Det is the, a, or the empty string, and N is the singular or plural form of the noun. We compute a web-based MI by collecting Pr(n, v), Pr(n), and Pr(v) using all inflections, except we only use the root form of the noun. Rather than using","@endWordPosition":"3575","@position":"22470","annotationId":"T49","@startWordPosition":"3574","@citStr":"Lin (1998"}]},"title":{"#tail":"\n","#text":"Automatic retrieval and clustering of similar words."},"booktitle":{"#tail":"\n","#text":"In COLING-ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dekang Lin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In NAACL-HLT, pages 564\u2013571."},"#text":"\n","pages":{"#tail":"\n","#text":"564--571"},"marker":{"#tail":"\n","#text":"Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"r arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, SIMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)\u2019s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as \u201c[X wins Y] ⇒ [X plays Y]\u201d are only valid for certain arguments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminative techniques are widely used in NLP and have been applied to the related tasks of word prediction and language modeling. Even-Zohar and Roth (2000) use a classifier to predict the most likely word to fill a position in a sentence (in their experiments: a verb) from a set of candidates (sets of verbs), by inspecting the context of the target toke","@endWordPosition":"862","@position":"5820","annotationId":"T50","@startWordPosition":"859","@citStr":"Pantel et al., 2007"},{"#tail":"\n","#text":"e predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)\u2019s conditional probability scores for pseudodisambiguation of (v, n, n\u2032) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007). 61 classifier for each predicate independently. The prediction becomes yv = Av · 4)v(n), where Av are the learned weights corresponding to predicate v and all features 4)v(n)=f(n) depend on the argument only. Some predicate partitions may have insufficient examples for training. Also, a predicate may occur in test data that was unseen during training. To handle these instances, we decided to cluster lowfrequency predicates. In our experiments assigning SP to verb-object pairs, we cluster all verbs that have less than 250 positive examples, using clusters generated by the CBC algorithm (Pante","@endWordPosition":"2028","@position":"12919","annotationId":"T51","@startWordPosition":"2025","@citStr":"Pantel et al. (2007)"}]},"title":{"#tail":"\n","#text":"ISP: Learning inferential selectional preferences."},"booktitle":{"#tail":"\n","#text":"In NAACL-HLT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Patrick Pantel"},{"#tail":"\n","#text":"Rahul Bhagat"},{"#tail":"\n","#text":"Bonaventura Coppola"},{"#tail":"\n","#text":"Timothy Chklovski"},{"#tail":"\n","#text":"Eduard Hovy"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In ACL, pages 104\u2013111."},"#text":"\n","pages":{"#tail":"\n","#text":"104--111"},"marker":{"#tail":"\n","#text":"Rooth, Riezler, Prescher, Carroll, Beil, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"r 2008. c�2008 Association for Computational Linguistics occurrence model on two tasks: identifying objects of verbs in an unseen corpus and finding pronominal antecedents in coreference data. 2 Related Work Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996). For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Usually, the classes are from WordNet (Miller et al., 1990), although they can also be inferred from clustering (Rooth et al., 1999). Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models. Another line of research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, PrSI","@endWordPosition":"642","@position":"4356","annotationId":"T52","@startWordPosition":"639","@citStr":"Rooth et al., 1999"},{"#tail":"\n","#text":"ts, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n\u2032) where v, n is a predicateargument pair observed in the corpus and v, n\u2032 has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of ","@endWordPosition":"1677","@position":"10851","annotationId":"T53","@startWordPosition":"1674","@citStr":"Rooth et al., 1999"},{"#tail":"\n","#text":"Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature. Whe","@endWordPosition":"2906","@position":"18299","annotationId":"T54","@startWordPosition":"2903","@citStr":"Rooth et al., 1999"}]},"title":{"#tail":"\n","#text":"Inducing a semantically annotated lexicon via EM-based clustering."},"booktitle":{"#tail":"\n","#text":"In ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mats Rooth"},{"#tail":"\n","#text":"Stefan Riezler"},{"#tail":"\n","#text":"Detlef Prescher"},{"#tail":"\n","#text":"Glenn Carroll"},{"#tail":"\n","#text":"Franz Beil"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In ACL, pages 104\u2013111."},"#text":"\n","pages":{"#tail":"\n","#text":"104--111"},"marker":{"#tail":"\n","#text":"Rooth, Riezler, Prescher, Carroll, Beil, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"r 2008. c�2008 Association for Computational Linguistics occurrence model on two tasks: identifying objects of verbs in an unseen corpus and finding pronominal antecedents in coreference data. 2 Related Work Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996). For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Usually, the classes are from WordNet (Miller et al., 1990), although they can also be inferred from clustering (Rooth et al., 1999). Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models. Another line of research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, PrSI","@endWordPosition":"642","@position":"4356","annotationId":"T55","@startWordPosition":"639","@citStr":"Rooth et al., 1999"},{"#tail":"\n","#text":"ts, hyphens, or begin with a human first name like Bob. DSP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n\u2032) where v, n is a predicateargument pair observed in the corpus and v, n\u2032 has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of ","@endWordPosition":"1677","@position":"10851","annotationId":"T56","@startWordPosition":"1674","@citStr":"Rooth et al., 1999"},{"#tail":"\n","#text":"Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data. Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved. Passive subjects (the car was bought) were converted to objects (bought car). We set the MI-threshold, T, to be 0, and the negative-to-positive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature. Whe","@endWordPosition":"2906","@position":"18299","annotationId":"T57","@startWordPosition":"2903","@citStr":"Rooth et al., 1999"}]},"title":{"#tail":"\n","#text":"Inducing a semantically annotated lexicon via EM-based clustering."},"booktitle":{"#tail":"\n","#text":"In ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Mats Rooth"},{"#tail":"\n","#text":"Stefan Riezler"},{"#tail":"\n","#text":"Detlef Prescher"},{"#tail":"\n","#text":"Glenn Carroll"},{"#tail":"\n","#text":"Franz Beil"}]}},{"volume":{"#tail":"\n","#text":"31"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Julie Weeds and David Weir. 2005. Co-occurrence retrieval: a flexible framework for lexical distributional similarity. Computational Linguistics, 31(4):439\u2013475."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Weeds, Weir, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ude Probj(n|v\u2032) features for every verb that occurs more than 10 times in our corpus. Avv\u2032 may be positive or negative, depending on the relation between v\u2032 and v. We also include features for the probability of the noun occurring as the subject of other verbs, Prs,,bj(n|v\u2032). For example, nouns that can be the object of eat will also occur as the subject of taste and contain. Other contexts, such as adjectival and nominal predicates, could also aid the prediction, but have not yet been investigated. The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir (2005). They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation. Our approach, on the other hand, discriminatively sets millions of individual similarity values. Like Weeds and Weir (2005), our similarity values are asymmetric. 3.3.2 String-based We include several simple character-based features of the noun string: the number of tokens, the case, and whether it contains digits, hyphens, an apostrophe, or other punctuation. We also include a feature for the first and last token, and fire indicator features if any token in the noun occurs on in-hou","@endWordPosition":"2515","@position":"15873","annotationId":"T58","@startWordPosition":"2512","@citStr":"Weeds and Weir (2005)"}},"title":{"#tail":"\n","#text":"Co-occurrence retrieval: a flexible framework for lexical distributional similarity."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Julie Weeds"},{"#tail":"\n","#text":"David Weir"}]}}]}}}}
