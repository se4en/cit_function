{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Katz, Graham and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using Latent Semantic Analysis. In Proceedings of the ACL\u201906 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12\u201319, Sydney."},"#text":"\n","pages":{"#tail":"\n","#text":"12--19"},"marker":{"#tail":"\n","#text":"Katz, Giesbrecht, 2006"},"location":{"#tail":"\n","#text":"Sydney."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"hown otherwise. We found that close to half of these also have a clear literal meaning; and of those with a literal meaning, on average around 40% of their usages are literal. Distinguishing token phrases as idiomatic or literal combinations of words is thus essential for NLP tasks, such as semantic parsing and machine translation, which require the identification of multiword semantic units. Most recent studies focusing on the identification of idiomatic and non-idiomatic tokens either assume the existence of manually annotated data for a supervised classification (Patrick and Fletcher 2005; Katz and Giesbrecht 2006), or rely on manually encoded linguistic knowledge about idioms (Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006), or even ignore the specific properties of nonliteral language and rely mainly on general purpose methods for the task (Birke and Sarkar 2006). We propose unsupervised methods that rely on automatically acquired knowledge about idiom types to identify their token occurrences as idiomatic or literal (Section 6). More specifically, we explore the hypothesis that the type-based knowledge we automatically acquire about an idiomatic expression can be used to deter","@endWordPosition":"1210","@position":"8019","annotationId":"T1","@startWordPosition":"1207","@citStr":"Katz and Giesbrecht 2006"},{"#tail":"\n","#text":"ngs in the BNC). Identification of idiomatic tokens in context is thus necessary for a full understanding of text, and this will be the focus of Sections 6 and 7. Recent studies addressing token identification for idiomatic expressions mainly perform the task as one of word sense disambiguation, and draw on the local context of 82 Fazly, Cook, and Stevenson Unsupervised Idiom Identification a token to disambiguate it. Such techniques either do not use any information regarding the linguistic properties of idioms (Birke and Sarkar 2006), or mainly focus on the property of non-compositionality (Katz and Giesbrecht 2006). Studies that do make use of deep linguistic information often handcode the knowledge into the systems (Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006). Our goal is to develop techniques that draw on the specific linguistic properties of idioms for their identification, without the need for handcoded knowledge or manually labelled training data. Such unsupervised techniques can also help provide automatically labelled (noisy) training data to bootstrap (semi-)supervised methods. In Sections 3 and 4, we showed that the lexical and syntactic fixedness of idioms is especi","@endWordPosition":"11822","@position":"74768","annotationId":"T2","@startWordPosition":"11819","@citStr":"Katz and Giesbrecht 2006"},{"#tail":"\n","#text":"r idiomatic. CONTEXT. Recall our assumption that the idiomatic and literal usages of an idiom correspond to two coarse-grained meanings of the expression. It is natural to further assume that the literal and idiomatic usages have more in common semantically within each group than between the two groups. Adopting a distributional approach to meaning\u2014 where the meaning of an expression is approximated by the words with which it cooccurs (Firth 1957)\u2014we would expect the literal and idiomatic usages of an expression to typically occur with different sets of words. Indeed, in a supervised setting, Katz and Giesbrecht (2006) show that the local context of an idiom usage is useful in identifying its sense. Inspired by this work, we propose an unsupervised method that incorporates distributional information about the local context of the usages of an idiom, in addition to the (syntactic) knowledge about its canonical forms, in order to determine if its token usages are literal or idiomatic. To achieve this, the method compares the context surrounding a test instance of an expression to \u201cgold-standard\u201d contexts for the idiomatic and literal usages of the expression, which are taken from noisy training data automatic","@endWordPosition":"12664","@position":"80118","annotationId":"T3","@startWordPosition":"12661","@citStr":"Katz and Giesbrecht (2006)"},{"#tail":"\n","#text":" automatically extracted from online resources such as WordNet. The similarity between the context of a target token and that of each seed set determines the class of the token. The approach is general in that it uses a slightly modified version of an existing word sense disambiguation algorithm. This is both an advantage and a drawback: The algorithm can be easily extended to other parts of speech and other languages; however, such a general method ignores the specific properties of non-literal (metaphorical and/or idiomatic) language. Similarly, the supervised token classification method of Katz and Giesbrecht (2006) relies primarily on the local context of a token, and fails to exploit specific linguistic 95 Computational Linguistics Volume 35, Number 1 properties of non-literal language. Our results suggest that such properties are often more informative than the local context, in determining the class of an MWE token. The supervised classifier of Patrick and Fletcher (2005) distinguishes between compositional and non-compositional usages of English verb-particle constructions. Their classifier incorporates linguistically motivated features, such as the degree of separation between the verb and particle","@endWordPosition":"19036","@position":"120241","annotationId":"T4","@startWordPosition":"19033","@citStr":"Katz and Giesbrecht (2006)"}]},"title":{"#tail":"\n","#text":"Automatic identification of non-compositional multi-word expressions using Latent Semantic Analysis."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL\u201906 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Graham Katz"},{"#tail":"\n","#text":"Eugenie Giesbrecht"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Katz, Graham and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using Latent Semantic Analysis. In Proceedings of the ACL\u201906 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12\u201319, Sydney."},"#text":"\n","pages":{"#tail":"\n","#text":"12--19"},"marker":{"#tail":"\n","#text":"Katz, Giesbrecht, 2006"},"location":{"#tail":"\n","#text":"Sydney."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"hown otherwise. We found that close to half of these also have a clear literal meaning; and of those with a literal meaning, on average around 40% of their usages are literal. Distinguishing token phrases as idiomatic or literal combinations of words is thus essential for NLP tasks, such as semantic parsing and machine translation, which require the identification of multiword semantic units. Most recent studies focusing on the identification of idiomatic and non-idiomatic tokens either assume the existence of manually annotated data for a supervised classification (Patrick and Fletcher 2005; Katz and Giesbrecht 2006), or rely on manually encoded linguistic knowledge about idioms (Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006), or even ignore the specific properties of nonliteral language and rely mainly on general purpose methods for the task (Birke and Sarkar 2006). We propose unsupervised methods that rely on automatically acquired knowledge about idiom types to identify their token occurrences as idiomatic or literal (Section 6). More specifically, we explore the hypothesis that the type-based knowledge we automatically acquire about an idiomatic expression can be used to deter","@endWordPosition":"1210","@position":"8019","annotationId":"T5","@startWordPosition":"1207","@citStr":"Katz and Giesbrecht 2006"},{"#tail":"\n","#text":"ngs in the BNC). Identification of idiomatic tokens in context is thus necessary for a full understanding of text, and this will be the focus of Sections 6 and 7. Recent studies addressing token identification for idiomatic expressions mainly perform the task as one of word sense disambiguation, and draw on the local context of 82 Fazly, Cook, and Stevenson Unsupervised Idiom Identification a token to disambiguate it. Such techniques either do not use any information regarding the linguistic properties of idioms (Birke and Sarkar 2006), or mainly focus on the property of non-compositionality (Katz and Giesbrecht 2006). Studies that do make use of deep linguistic information often handcode the knowledge into the systems (Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006). Our goal is to develop techniques that draw on the specific linguistic properties of idioms for their identification, without the need for handcoded knowledge or manually labelled training data. Such unsupervised techniques can also help provide automatically labelled (noisy) training data to bootstrap (semi-)supervised methods. In Sections 3 and 4, we showed that the lexical and syntactic fixedness of idioms is especi","@endWordPosition":"11822","@position":"74768","annotationId":"T6","@startWordPosition":"11819","@citStr":"Katz and Giesbrecht 2006"},{"#tail":"\n","#text":"r idiomatic. CONTEXT. Recall our assumption that the idiomatic and literal usages of an idiom correspond to two coarse-grained meanings of the expression. It is natural to further assume that the literal and idiomatic usages have more in common semantically within each group than between the two groups. Adopting a distributional approach to meaning\u2014 where the meaning of an expression is approximated by the words with which it cooccurs (Firth 1957)\u2014we would expect the literal and idiomatic usages of an expression to typically occur with different sets of words. Indeed, in a supervised setting, Katz and Giesbrecht (2006) show that the local context of an idiom usage is useful in identifying its sense. Inspired by this work, we propose an unsupervised method that incorporates distributional information about the local context of the usages of an idiom, in addition to the (syntactic) knowledge about its canonical forms, in order to determine if its token usages are literal or idiomatic. To achieve this, the method compares the context surrounding a test instance of an expression to \u201cgold-standard\u201d contexts for the idiomatic and literal usages of the expression, which are taken from noisy training data automatic","@endWordPosition":"12664","@position":"80118","annotationId":"T7","@startWordPosition":"12661","@citStr":"Katz and Giesbrecht (2006)"},{"#tail":"\n","#text":" automatically extracted from online resources such as WordNet. The similarity between the context of a target token and that of each seed set determines the class of the token. The approach is general in that it uses a slightly modified version of an existing word sense disambiguation algorithm. This is both an advantage and a drawback: The algorithm can be easily extended to other parts of speech and other languages; however, such a general method ignores the specific properties of non-literal (metaphorical and/or idiomatic) language. Similarly, the supervised token classification method of Katz and Giesbrecht (2006) relies primarily on the local context of a token, and fails to exploit specific linguistic 95 Computational Linguistics Volume 35, Number 1 properties of non-literal language. Our results suggest that such properties are often more informative than the local context, in determining the class of an MWE token. The supervised classifier of Patrick and Fletcher (2005) distinguishes between compositional and non-compositional usages of English verb-particle constructions. Their classifier incorporates linguistically motivated features, such as the degree of separation between the verb and particle","@endWordPosition":"19036","@position":"120241","annotationId":"T8","@startWordPosition":"19033","@citStr":"Katz and Giesbrecht (2006)"}]},"title":{"#tail":"\n","#text":"Automatic identification of non-compositional multi-word expressions using Latent Semantic Analysis."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the ACL\u201906 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Graham Katz"},{"#tail":"\n","#text":"Eugenie Giesbrecht"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Lin, Dekang. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL\u201998), pages 768\u2013774, Montreal."},"#text":"\n","pages":{"#tail":"\n","#text":"768--774"},"marker":{"#tail":"\n","#text":"Lin, 1998"},"location":{"#tail":"\n","#text":"Montreal."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ure to determine); (ii) it can only measure the lexical fixedness of idiomatic combinations, and so could not apply to literal combinations. We thus interpret this property statistically in the following way: We expect a lexically fixed verb+noun combination to appear much more frequently than its variants in general. Specifically, we examine the strength of association between the verb and the noun constituent of a combination (the target expression or its lexical variants) as an indirect cue to its idiomaticity, an approach inspired by Lin (1999). We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.2 Variants are generated by replacing either 2 We also replicated our experiments with an automatically built thesaurus created from the British National Corpus (BNC) in a similar fashion, and kindly provided to us by Diana McCarthy. Results were similar, hence we do not report them here. 66 Fazly, Cook, and Stevenson Unsupervised Idiom Identification the noun or the verb constituent of a pair with a semantically (and syntactically) similar word.3 Examples of automatically generated variants for the pair (s","@endWordPosition":"3148","@position":"20690","annotationId":"T9","@startWordPosition":"3147","@citStr":"Lin (1998)"}},"title":{"#tail":"\n","#text":"Automatic retrieval and clustering of similar words."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL\u201998),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dekang Lin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"rawString":{"#tail":"\n","#text":"Lin, Dekang. 1999. Automatic identification of non-compositional phrases. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL\u201999), pages 317\u2013324, College Park, Maryland."},"#text":"\n","pages":{"#tail":"\n","#text":"317--324"},"marker":{"#tail":"\n","#text":"Lin, 1999"},"location":{"#tail":"\n","#text":"College Park, Maryland."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ties, such as their semantic idiosyncrasy or their restricted flexibility, pointed out earlier. Some researchers have relied on a manual encoding of idiom-specific knowledge in a lexicon (Copestake et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid, and Spranger 2004). Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic expressions. Specifically, we focus on a cross-linguistically prominent class of phrasal idioms which are commonly and productively formed from the combination of a frequent verb and a noun in its direct object position (Cowie, Mackin, and McCaig 1983; Nunberg, Sag, and Wasow 1994; Fellbaum 2002), for example, shoot the breeze, make a face, and push one\u2019s luck. We refer to these as verb+noun idioma","@endWordPosition":"791","@position":"5317","annotationId":"T10","@startWordPosition":"790","@citStr":"Lin 1999"},{"#tail":"\n","#text":"of expressions (which is what we are developing our measure to determine); (ii) it can only measure the lexical fixedness of idiomatic combinations, and so could not apply to literal combinations. We thus interpret this property statistically in the following way: We expect a lexically fixed verb+noun combination to appear much more frequently than its variants in general. Specifically, we examine the strength of association between the verb and the noun constituent of a combination (the target expression or its lexical variants) as an indirect cue to its idiomaticity, an approach inspired by Lin (1999). We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.2 Variants are generated by replacing either 2 We also replicated our experiments with an automatically built thesaurus created from the British National Corpus (BNC) in a similar fashion, and kindly provided to us by Diana McCarthy. Results were similar, hence we do not report them here. 66 Fazly, Cook, and Stevenson Unsupervised Idiom Identification the noun or the verb constituent of a pair with a semantically (and syntactically) similar word.3 Exam","@endWordPosition":"3139","@position":"20634","annotationId":"T11","@startWordPosition":"3138","@citStr":"Lin (1999)"},{"#tail":"\n","#text":" variants using an information-theoretic measure called pointwise mutual information or PMI (Church et al. 1991): P(vr, nt) PMI(vr, nt) = log P(vr) P(nt) Nv+n f (vr, nt) = log (1) f(vr, *) f (*, nt) where (vr, nt) E {(v, n)} U Ssim(v, n); Nv+n is the total number of verb\u2013object pairs in the corpus; f (vr, nt) is the frequency of vr and nt co-occurring as a verb\u2013object pair; f (vr, *) is the total frequency of the target (transitive) verb with any noun as its direct object; and f (*, nt) is the total frequency of the noun nt in the direct object position of any verb in the corpus. In his work, Lin (1999) assumes that a target expression is non-compositional if and only if its PMI value is significantly different from that of all the variants. Instead, we propose a novel technique that brings together the association strengths (PMI values) of the target and the variant expressions into a single measure reflecting the degree of lexical fixedness for the target pair. We assume that the target pair is lexically fixed to the extent that its PMI deviates from the average PMI of its variants. By our measure, the target pair is considered lexically fixed (i.e., is given a high fixedness score) only i","@endWordPosition":"3460","@position":"22391","annotationId":"T12","@startWordPosition":"3459","@citStr":"Lin (1999)"},{"#tail":"\n","#text":"ir to the direct object relation. Smadja (1993) proposes a collocation extraction method which measures the fixedness of a word sequence (e.g., a verb\u2013noun pair) by examining the relative position of the component words across their occurrences together. We replicate Smadja\u2019s method, where we measure fixedness of a target verb\u2013noun pair as the spread (variance) of the co-occurrence frequency of the verb and the noun over 10 relative positions within a five-word window.8 Recall from Section 3.1 that our Fixednesslex measure is intended as an improvement over the non-compositionality measure of Lin (1999). For the sake of completeness, we also compare the classification performance of our Fixednesslex with that of Lin\u2019s (1999) measure, which we refer to as Lin.9 We first elaborate on the methodological aspects of our experiments in Section 4.1, and then present a discussion of the experimental results in Section 4.2. 4.1 Experimental Setup 4.1.1 Corpus and Data Extraction. We use the British National Corpus (BNC; Burnard 2000); to extract verb\u2013noun pairs, along with information on the syntactic patterns they appear in. We automatically parse the BNC using the Collins parser (Collins 1999), and","@endWordPosition":"5894","@position":"37532","annotationId":"T13","@startWordPosition":"5893","@citStr":"Lin (1999)"},{"#tail":"\n","#text":"ciation measures (Inkpen 2003; Mohammad and Hirst, submitted). In our experiments, we also found that PMI consistently performs better than two other association measures, the Dice coefficient and the log-likelihood measure. Experiments by Krenn and Evert (2001) showed contradicting results for PMI; however, these experiments were performed on small-sized corpora, and on data which contained items with very low frequency. 8 We implement the method as explained in Smadja (1993), taking into account the part-of-speech tags of the target component words. 9 We implement the method as explained in Lin (1999), using 95% confidence intervals. We thus need to ignore variants with frequency lower than 4 for which no confidence interval can be formed. 72 Fazly, Cook, and Stevenson Unsupervised Idiom Identification verb already in the list; for example, lose is added in analogy with find. Here is the final list of the 28 verbs in alphabetical order: blow, bring, catch, cut, find, get, give, have, hear, hit, hold, keep, kick, lay, lose, make, move, place, pull, push, put, see, set, shoot, smell, take, throw, touch From the corpus, we extract all the verb\u2013noun pairs (lemmas) that contain any of these lis","@endWordPosition":"6301","@position":"40156","annotationId":"T14","@startWordPosition":"6300","@citStr":"Lin (1999)"},{"#tail":"\n","#text":"ms and Other Multiword Expressions Our work relates to previous studies on determining the compositionality (the inverse of idiomaticity) of idioms and other multiword expressions (MWEs). Most previous work on the compositionality of MWEs either treats them as collocations (Smadja 1993), or examines the distributional similarity between the expression and its constituents (Baldwin et al. 2003; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003). Others have identified MWEs by looking into specific linguistic cues, such as the lexical fixedness of non-compositional MWEs (Lin 1999; Wermter and Hahn 2005), or the lexical flexibility of productive noun compounds (Lapata and Lascarides 2003). Venkatapathy and Joshi (2005) combine aspects of this work, by incorporating lexical fixedness, distributional similarity, and collocation-based measures into a set of features which are used to rank verb+noun combinations according to their compositionality. Our work differs from such studies in that it considers various kinds of fixedness as surface behaviors that are tightly related to the underlying semantic idiosyncrasy (idiomaticity) of expressions. Accordingly, we propose nove","@endWordPosition":"18027","@position":"113446","annotationId":"T15","@startWordPosition":"18026","@citStr":"Lin 1999"}]},"title":{"#tail":"\n","#text":"Automatic identification of non-compositional phrases."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL\u201999),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dekang Lin"}}}]}}}}
