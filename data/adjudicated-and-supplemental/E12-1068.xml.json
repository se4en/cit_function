{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","note":{"#tail":"\n","@confidence":"0.6027095","#text":"\nProceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?674,\nAvignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics\n"},"listItem":[{"#tail":"\n","@confidence":"0.997932","#text":"\n1. Surface with no features is presented with an\nunderspecified input (a sequence of stems), and\nreturns the most likely inflected sequence.\n2. Surface with case, number, gender is a hybrid\n"},{"#tail":"\n","@confidence":"0.6102936","#text":"\nprepositions are replaced by their POS tags in the input. Verbs are inflected in the input (?haben?, meaning\n?have? as in ?they have?, in the example). Prepositions are lexicalized (?zu? in the example) and indicate which\ncase value they mark (?Dat?, i.e., Dative in the example).\ncase=nominative, in-weak-context=true).5\n3. Single joint sequence model on features. We\n"}],"figure":[{"#tail":"\n","@confidence":"0.6080841","#text":"\nin\nin<APPR><Dat> in\nim\ndie<+ART><Def> dem\ncontrast Gegensatz<+NN><Masc><Sg>Gegensatz Gegensatz\nto zu<APPR><Dat> zu\nzur\nthe die<+ART><Def> der\nanimated lebhaft<+ADJ><Pos> lebhaften lebhaften\ndebate Debatte<+NN><Fem><Sg> Debatte Debatte\n"},{"#tail":"\n","@confidence":"0.6088476","#text":"\nCommon lemmawi?5...wi+5 , tagwi?7...wi+7\nCase casewi?5...wi+5\nGender genderwi?5...wi+5\nNumber numberwi?5...wi+5\nin-weak-context in-weak-contextwi?5...wi+5\n"},{"#tail":"\n","@confidence":"0.884196428571429","#text":"\n1 baseline 14.16\n2 unigram surface (no features) 9.97\n3 surface (no features) 14.26\n4 surface (with case, number, gender features) 14.58\n5 1 JSM morphological features 14.53\n6 4 JSMs morphological features 14.29\n7 4 CRFs morphological features, lexical information 14.72\n"}],"address":{"#tail":"\n","@confidence":"0.941198","#text":"\nD?70174 Stuttgart, Germany USA\n"},"author":{"#tail":"\n","@confidence":"0.978739","#text":"\nAlexander Fraser? Marion Weller? Aoife Cahill? Fabienne Cap?\n"},"subsectionHeader":[{"#tail":"\n","@confidence":"0.985593","#text":"\n2.1 Issues of inflection prediction\n"},{"#tail":"\n","@confidence":"0.994134","#text":"\n2.2 Procedure\n"},{"#tail":"\n","@confidence":"0.996189","#text":"\n2.3 German Stem Markup\n"},{"#tail":"\n","@confidence":"0.999852","#text":"\n8.1 Details of Splitting Process\n"},{"#tail":"\n","@confidence":"0.961416","#text":"\n8.2 Model for Compound Merging\n"},{"#tail":"\n","@confidence":"0.966494","#text":"\n8.3 Experiments\n"}],"footnote":[{"#tail":"\n","@confidence":"0.757462","#text":"\n4This is the reason for which the preposition + article in\n"},{"#tail":"\n","@confidence":"0.6838425","#text":"\n7http://www.statmt.org/wmt09/translation-task.html\n8However, we reduced the monolingual data (only) by\n"},{"#tail":"\n","@confidence":"0.983037","#text":"\n15We found it most effective to merge word parts during\nMERT (so MERT uses the same stem references as before).\n1 1 JSM morphological features 13.94\n2 4 CRFs morphological features, lexical information 14.04\n"}],"construct":{"#tail":"\n","@confidence":"0.756867857142857","#text":"\noutput decoder input prediction output prediction inflected forms gloss\nhaben<VAFIN> haben-V haben-V haben have\nZugang<+NN><Masc><Sg> NN-Sg-Masc NN-Masc.Acc.Sg.in-weak-context=false Zugang access\nzu<APPR><Dat> APPR-zu-Dat APPR-zu-Dat zu to\ndie<+ART><Def> ART-in-weak-context=true ART-Neut.Dat.Pl.in-weak-context=true den the\nbetreffend<+ADJ><Pos> ADJA ADJA-Neut.Dat.Pl.in-weak-context=true betreffenden respective\nLand<+NN><Neut><Pl> NN-Pl-Neut NN-Neut.Dat.Pl.in-weak-context=true La?ndern countries\n"},"title":{"#tail":"\n","@confidence":"0.892443","#text":"\nModeling Inflection and Word-Formation in SMT\n"},"@confidence":"0.000000","reference":[{"#tail":"\n","@confidence":"0.9852","#text":"\nEleftherios Avramidis and Philipp Koehn. 2008. En-\nriching Morphologically Poor Languages for Statis-\ntical Machine Translation. In Proceedings of ACL-\n"},{"#tail":"\n","@confidence":"0.998970175438597","#text":"\n08: HLT, pages 763?770, Columbus, Ohio, June.\nAssociation for Computational Linguistics.\nIbrahim Badr, Rabih Zbib, and James Glass. 2008.\nSegmentation for English-to-Arabic statistical ma-\nchine translation. In Proceedings of ACL-08: HLT,\nShort Papers, pages 153?156, Columbus, Ohio,\nJune. Association for Computational Linguistics.\nOndr?ej Bojar and Kamil Kos. 2010. 2010 Failures in\nEnglish-Czech Phrase-Based MT. In Proceedings\nof the Joint Fifth Workshop on Statistical Machine\nTranslation and MetricsMATR, pages 60?66, Upp-\nsala, Sweden, July. Association for Computational\nLinguistics.\nAnn Clifton and Anoop Sarkar. 2011. Combin-\ning morpheme-based machine translation with post-\nprocessing morpheme prediction. In Proceed-\nings of the 49th Annual Meeting of the Associa-\ntion for Computational Linguistics: Human Lan-\nguage Technologies, pages 32?42, Portland, Ore-\ngon, USA, June. Association for Computational\nLinguistics.\nAdria` de Gispert and Jose? B. Marin?o. 2008. On the\nimpact of morphology in English to Spanish statisti-\ncal MT. Speech Communication, 50(11-12):1034?\n1046.\nAlexander Fraser. 2009. Experiments in Morphosyn-\ntactic Processing for Translating to and from Ger-\nman. In Proceedings of the Fourth Workshop on\nStatistical Machine Translation, pages 115?119,\nAthens, Greece, March. Association for Computa-\ntional Linguistics.\nFabienne Fritzinger and Alexander Fraser. 2010. How\nto Avoid Burning Ducks: Combining Linguistic\nAnalysis and Corpus Statistics for German Com-\npound Processing. In Proceedings of the Fifth\nWorkshop on Statistical Machine Translation, pages\n224?234. Association for Computational Linguis-\ntics.\nPhilipp Koehn and Hieu Hoang. 2007. Factored\nTranslation Models. In Proceedings of the 2007\nJoint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural\nLanguage Learning (EMNLP-CoNLL), pages 868?\n876, Prague, Czech Republic, June. Association for\nComputational Linguistics.\nPhilipp Koehn and Kevin Knight. 2003. Empirical\nmethods for compound splitting. In EACL ?03:\nProceedings of the 10th conference of the European\nchapter of the Association for Computational Lin-\nguistics, pages 187?193, Morristown, NJ, USA. As-\nsociation for Computational Linguistics.\nJohn Lafferty, Andrew McCallum, and Fernando\nPereira. 2001. Conditional random fields: Prob-\nabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the International\nConference on Machine Learning, pages 282?289.\nMorgan Kaufmann, San Francisco, CA.\nThomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.\n2010. Practical very large scale CRFs. In Proceed-\nings the 48th Annual Meeting of the Association for\nComputational Linguistics (ACL), pages 504?513.\nAssociation for Computational Linguistics, July.\nMinh-Thang Luong, Preslav Nakov, and Min-Yen\nKan. 2010. A Hybrid Morpheme-Word Represen-\ntation for Machine Translation of Morphologically\nRich Languages. In Proceedings of the 2010 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 148?157, Cambridge, MA, Octo-\nber. Association for Computational Linguistics.\nMaja Popovic, Daniel Stein, and Hermann Ney. 2006.\nStatistical Machine Translation of German Com-\npound Words. In Proceedings of FINTAL-06, pages\n616?624, Turku, Finland. Springer Verlag, LNCS.\nHelmut Schmid, Arne Fitschen, and Ulrich Heid.\n2004. SMOR: A German Computational Morphol-\nogy Covering Derivation, Composition, and Inflec-\ntion. In 4th International Conference on Language\nResources and Evaluation.\nHelmut Schmid. 2004. Efficient Parsing of Highly\nAmbiguous Context-Free Grammars with Bit Vec-\ntors. In Proceedings of Coling 2004, pages 162?\n168, Geneva, Switzerland, Aug 23?Aug 27. COL-\nING.\nAndreas Stolcke. 2002. SRILM - An Extensible Lan-\nguage Modeling Toolkit. In International Confer-\nence on Spoken Language Processing.\nSara Stymne and Nicola Cancedda. 2011. Produc-\ntive Generation of Compound Words in Statistical\nMachine Translation. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n250?260, Edinburgh, Scotland UK, July. Associa-\ntion for Computational Linguistics.\nSara Stymne. 2008. German Compounds in Factored\nStatistical Machine Translation. In Proceedings of\nGOTAL-08, pages 464?475, Gothenburg, Sweden.\nSpringer Verlag, LNCS/LNAI.\nKristina Toutanova, Hisami Suzuki, and Achim\nRuopp. 2008. Applying Morphology Generation\nModels to Machine Translation. In Proceedings of\nACL-08: HLT, pages 514?522, Columbus, Ohio,\nJune. Association for Computational Linguistics.\nSami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,\nand Markus Sadeniemi. 2007. Morphology-aware\nstatistical machine translation based on morphs in-\nduced in an unsupervised manner. In PROC. OF\nMT SUMMIT XI, pages 491?498.\nPhilip Williams and Philipp Koehn. 2011. Agree-\nment constraints for statistical machine translation\ninto German. In Proceedings of the Sixth Workshop\non Statistical Machine Translation, pages 217?226,\nEdinburgh, Scotland, July. Association for Compu-\ntational Linguistics.\nReyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-\nto-Morphology Mapping in Factored Phrase-Based\n"},{"#tail":"\n","@confidence":"0.9982256","#text":"\nStatistical Machine Translation from English to\nTurkish. In Proceedings of the 48th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 454?464, Uppsala, Sweden, July. Asso-\nciation for Computational Linguistics.\n"}],"#tail":"\n","bodyText":[{"#tail":"\n","@confidence":"0.999763769230769","#text":"\nThe current state-of-the-art in statistical\nmachine translation (SMT) suffers from is-\nsues of sparsity and inadequate modeling\npower when translating into morphologi-\ncally rich languages. We model both in-\nflection and word-formation for the task\nof translating into German. We translate\nfrom English words to an underspecified\nGerman representation and then use linear-\nchain CRFs to predict the fully specified\nGerman representation. We show that im-\nproved modeling of inflection and word-\nformation leads to improved SMT.\n"},{"#tail":"\n","@confidence":"0.993759947368421","#text":"\nPhrase-based statistical machine translation\n(SMT) suffers from problems of data sparsity\nwith respect to inflection and word-formation\nwhich are particularly strong when translating to\na morphologically rich target language, such as\nGerman. We address the problem of inflection\nby first translating to a stem-based representation,\nand then using a second process to inflect these\nstems. We study several models for doing\nthis, including: strongly lexicalized models,\nunlexicalized models using linguistic features,\nand models combining the strengths of both of\nthese approaches. We address the problem of\nword-formation for compounds in German, by\ntranslating from English into German word parts,\nand then determining whether to merge these\nparts to form compounds.\nWe make the following new contributions: (i)\nwe introduce the first SMT system combining\ninflection prediction with synthesis of portman-\nteaus and compounds. (ii) For inflection, we com-\npare the mostly unlexicalized prediction of lin-\nguistic features (with a subsequent surface form\ngeneration step) versus the direct prediction of\nsurface forms, and show that both approaches\nhave complementary strengths. (iii) We com-\nbine the advantages of the prediction of linguis-\ntic features with the prediction of surface forms.\nWe implement this in a CRF framework which\nimproves on a standard phrase-based SMT base-\nline. (iv) We develop separate (but related) pro-\ncedures for inflection prediction and dealing with\nword-formation (compounds and portmanteaus),\nin contrast with most previous work which usu-\nally either approaches both problems as inflec-\ntional problems, or approaches both problems as\nword-formation problems.\nWe evaluate on the end-to-end SMT task of\ntranslating from English to German of the 2009\nACL workshop on SMT. We achieve BLEU score\nincreases on both the test set and the blind test set.\n2 Overview of the translation process for\ninflection prediction\nThe work we describe is focused on generaliz-\ning phrase-based statistical machine translation to\nbetter model German NPs and PPs. We particu-\nlarly want to ensure that we can generate novel\nGerman NPs, where what we mean by novel is\nthat the (inflected) realization is not present in the\nparallel German training data used to build the\nSMT system, and hence cannot be produced by\nour baseline (a standard phrase-based SMT sys-\ntem). We first present our system for dealing with\nthe difficult problem of inflection in German, in-\ncluding the inflection-dependent phenomenon of\nportmanteaus. Later, after performing an exten-\nsive analysis of this system, we will extend it\n"},{"#tail":"\n","@confidence":"0.993533","#text":"\nto model compounds, a highly productive phe-\nnomenon in German (see Section 8).\nThe key linguistic knowledge sources that we\nuse are morphological analysis and generation of\nGerman based on SMOR, a morphological ana-\nlyzer/generator of German (Schmid et al 2004)\nand the BitPar parser, which is a state-of-the-art\nparser of German (Schmid, 2004).\n"},{"#tail":"\n","@confidence":"0.998985703703704","#text":"\nIn order to ensure coherent German NPs, we\nmodel linguistic features of each word in an NP.\nWe model case, gender, and number agreement\nand whether or not the word is in the scope of\na determiner (such as a definite article), which\nwe label in-weak-context (this linguistic feature\nis necessary to determine the type of inflection of\nadjectives and other words: strong, weak, mixed).\nThis is a diverse group of features. The number\nof a German noun can often be determined given\nonly the English source word. The gender of a\nGerman noun is innate and often difficult to deter-\nmine given only the English source word. Case\nis a function of the slot in the subcategorization\nframe of the verb (or preposition). There is agree-\nment in all of these features in an NP. For instance\nthe number of an article or adjective is determined\nby the head noun, while the type of inflection of an\nadjective is determined by the choice of article.\nWe can have a large number of surface forms.\nFor instance, English blue can be translated as\nGerman blau, blaue, blauer, blaues, blauen. We\npredict which form is correct given the context.\nOur system can generate forms not seen in the\ntraining data. We follow a two-step process: in\nstep-1 we translate to blau (the stem), in step-2 we\npredict features and generate the inflected form.1\n"},{"#tail":"\n","@confidence":"0.99923325","#text":"\nWe begin building an SMT system by parsing the\nGerman training data with BitPar. We then extract\nmorphological features from the parse. Next, we\nlookup the surface forms in the SMOR morpholog-\nical analyzer. We use the morphological features\nin the parse to disambiguate the set of possible\nSMOR analyses. Finally, we output the ?stems?\nof the German text, with the addition of markup\ntaken from the parse (discussed in Section 2.3).\n1E.g., case=nominative, gender=masculine, num-\nber=singular, in-weak-context=true; inflected: blaue.\nWe then build a standard Moses system trans-\nlating from English to German stems. We obtain\na sequence of stems and POS2 from this system,\nand then predict the correct inflection using a se-\nquence model. Finally we generate surface forms.\n"},{"#tail":"\n","@confidence":"0.99903825","#text":"\nThe translation process consists of two major\nsteps. The first step is translation of English\nwords to German stems, which are enriched with\nsome inflectional markup. The second step is\nthe full inflection of these stems (plus markup)\nto obtain the final sequence of inflected words.\nThe purpose of the additional German inflectional\nmarkup is to strongly improve prediction of in-\nflection in the second step through the addition of\nmarkup to the stems in the first step.\nIn general, all features to be predicted are\nstripped from the stemmed representation because\nthey are subject to agreement restrictions of a\nnoun or prepositional phrase (such as case of\nnouns or all features of adjectives). However, we\nneed to keep all morphological features that are\nnot dependent on, and thus not predictable from,\nthe (German) context. They will serve as known\ninput for the inflection prediction model. We now\ndescribe this markup in detail.\nNouns are marked with gender and number: we\nconsider the gender of a noun as part of its stem,\nwhereas number is a feature which we can obtain\nfrom English nouns.\nPersonal pronouns have number and gender an-\nnotation, and are additionally marked with nom-\ninative and not-nominative, because English pro-\nnouns are marked for this (except for you).\nPrepositions are marked with the case their ob-\nject takes: this moves some of the difficulty in pre-\ndicting case from the inflection prediction step to\nthe stem translation step. Since the choice of case\nin a PP is often determined by the PP?s meaning\n(and there are often different meanings possible\ngiven different case choices), it seems reasonable\nto make this decision during stem translation.\nVerbs are represented using their inflected surface\nform. Having access to inflected verb forms has a\npositive influence on case prediction in the second\n2We use an additional target factor to obtain the coarse\nPOS for each stem, applying a 7-gram POS model. Koehn\nand Hoang (2007) showed that the use of a POS factor only\nresults in negligible BLEU improvements, but we need ac-\ncess to the POS in our inflection prediction models.\n"},{"#tail":"\n","@confidence":"0.977649","#text":"\ninput decoder output inflected merged\n"},{"#tail":"\n","@confidence":"0.968968777777778","#text":"\ninflection to form portmanteaus, in dem means in the.\nstep through subject-verb agreement.\nArticles are reduced to their stems (the stem itself\nmakes clear the definite or indefinite distinction,\nbut lemmatizing involves removing markings of\ncase, gender and number features).\nOther words are also represented by their stems\n(except for words not covered by SMOR, where\nsurface forms are used instead).\n"},{"#tail":"\n","@confidence":"0.986544206896552","#text":"\nPortmanteaus are a word-formation phenomenon\ndependent on inflection. As we have discussed,\nstandard phrase-based systems have problems\nwith picking a definite article with the correct\ncase, gender and number (typically due to spar-\nsity in the language model, e.g., a noun which\nwas never before seen in dative case will often\nnot receive the correct article). In German, port-\nmanteaus increase this sparsity further, as they\nare compounds of prepositions and articles which\nmust agree with a noun.\nWe adopt the linguistically strict definition of\nthe term portmanteau: the merging of two func-\ntion words.3 We treat this phenomena by split-\nting the component parts during training and re-\nmerging during generation. Specifically for\nGerman, this requires splitting the words which\nhave German POS tag APPRART into an APPR\n(preposition) and an ART (article). Merging is re-\nstricted, the article must be definite, singular4 and\nthe preposition can only take accusative or dative\ncase. Some prepositions allow for merging with\nan article only for certain noun genders, for exam-\nple the preposition inDative is only merged with\nthe following article if the following noun is of\nmasculine or neuter gender. The definite article\n3Some examples are: zum (to the) = zu (to) + dem (the)\n[German], du (from the) = de (from) + le (the) [French] or al\n(to the) = a (to) + el (the) [Spanish].\n"},{"#tail":"\n","@confidence":"0.998120666666667","#text":"\nmust be inflected before making a decision about\nwhether to merge a preposition and the article into\na portmanteau. See Table 1 for examples.\n"},{"#tail":"\n","@confidence":"0.9186635","#text":"\nWe present 5 procedures for inflectional predic-\ntion using supervised sequence models. The first\ntwo procedures use simple N-gram models over\nfully inflected surface forms.\n"},{"#tail":"\n","@confidence":"0.98229425","#text":"\nsystem giving the surface model access to linguis-\ntic features. In this system prepositions have addi-\ntionally been labeled with the case they mark (in\nboth the underspecified input and the fully spec-\nified output the sequence model is built on) and\ngender and number markup is also available.\nThe rest of the procedures predict morpholog-\nical features (which are input to a morphological\ngenerator) rather than surface words. We have de-\nveloped a two-stage process for predicting fully\ninflected surface forms. The first stage takes a\nstem and predicts morphological features for that\nstem, based on the surrounding context. The aim\nof the first stage is to take a stem and predict\nfour morphological features: case, gender, num-\nber and type of inflection. We experiment with\na number of models for doing this. The sec-\nond stage takes the stems marked with morpho-\nlogical features (predicted in the first stage) and\nuses a morphological generator to generate the\nfull surface form. For the second stage, a modified\nversion of SMOR (Schmid et al 2004) is used,\nwhich, given a stem annotated with morphologi-\ncal features, generates exactly one surface form.\nWe now introduce our first linguistic feature\nprediction systems, which we call joint sequence\nmodels (JSMs). These are standard language\nmodels, where the ?word? tokens are not repre-\nsented as surface forms, but instead using POS\nand features. In testing, we supply the input as a\nsequence in underspecified form, where some of\nthe features are specified in the stem markup (for\ninstance, POS=Noun, gender=masculine, num-\nber=plural), and then use Viterbi search to find the\nmost probable fully specified form (for instance,\nPOS=Noun, gender=masculine, number=plural,\n"},{"#tail":"\n","@confidence":"0.962748402597403","#text":"\nillustrate the different stages of the inflection pre-\ndiction when using a joint sequence model. The\nstemmed input sequence (cf. Section 2.3) contains\nseveral features that will be part of the input to\nthe inflection prediction. With the exception of\nverbs and prepositions, the representation for fea-\nture prediction is based on POS-tags.\nAs gender and number are given by the heads\nof noun phrases and prepositional phrases, and\nthe expected type of inflection is set by articles,\nthe model has sufficient information to compute\nvalues for these features and there is no need to\nknow the actual words. In contrast, the prediction\nof case is more difficult as it largely depends on\nthe content of the sentence (e.g. which phrase is\nobject, which phrase is subject). Assuming that\nverbs and prepositions indicate subcategorization\nframes, the model is provided crucial information\nfor the prediction of case by keeping verbs (recall\nthat verbs are produced by the stem translation\nsystem in their inflected form) and prepositions\n(the prepositions also have case markup) instead\nof replacing them with their tags.\nAfter having predicted a single label with val-\nues for all features, an inflected word form for the\nstem and the features is generated. The prediction\nsteps are illustrated in Table 2.\n4. Using four joint sequence models (one for\neach linguistic feature). Here the four linguistic\nfeature values are predicted separately. The as-\nsumption that the different linguistic features can\nbe predicted independently of one another is a rea-\n5Joint sequence models are a particularly simple HMM.\nUnlike the HMMs used for POS-tagging, an HMM as used\nhere only has a single emission possibility for each state,\nwith probability 1. The states in the HMM are the fully\nspecified representation. The emissions of the HMM are the\nstems+markup (the underspecified representation).\nsonable linguistic assumption to make given the\nadditional German markup that we use. By split-\nting the inflection prediction problem into 4 com-\nponent parts, we end up with 4 simpler models\nwhich are less sensitive to data sparseness.\nEach linguistic feature is modeled indepen-\ndently (by a JSM) and has a different input rep-\nresentation based on the previously described\nmarkup. The input consists of a sequence of\ncoarse POS tags, and for those stems that are\nmarked up with the relevant feature, this feature\nvalue. Finally, we combine the predicted fea-\ntures together to produce the same final output as\nthe single joint sequence model, and then generate\neach surface form using SMOR.\n5. Using four CRFs (one for each linguistic fea-\nture). The sequence models already presented are\nlimited to the n-gram feature space, and those that\npredict linguistic features are not strongly lexi-\ncalized. Toutanova et al(2008) uses an MEMM\nwhich allows the integration of a wide variety of\nfeature functions. We also wanted to experiment\nwith additional feature functions, and so we train\n4 separate linear chain CRF6 models on our data\n(one for each linguistic feature we want to pre-\ndict). We chose CRFs over MEMMs to avoid the\nlabel bias problem (Lafferty et al 2001).\nThe CRF feature functions, for each German\nword wi, are in Table 3. The common feature\nfunctions are used in all models, while each of the\n4 separate models (one for each linguistic feature)\nincludes the context of only that linguistic feature.\nWe use L1 regularization to eliminate irrelevant\nfeature functions, the regularization parameter is\noptimized on held out data.\n6We use the Wapiti Toolkit (Lavergne et al 2010) on 4\nx 12-Core Opteron 6176 2.3 GHz with 256GB RAM to train\nour CRF models. Training a single CRF model on our data\nwas not tractable, so we use one for each linguistic feature.\n"},{"#tail":"\n","@confidence":"0.75503","#text":"\nture functions are binary indicators of the pattern).\n"},{"#tail":"\n","@confidence":"0.986914222222222","#text":"\nTo evaluate our end-to-end system, we perform\nthe well-studied task of news translation, us-\ning the Moses SMT package. We use the En-\nglish/German data released for the 2009 ACL\nWorkshop on Machine Translation shared task on\ntranslation.7 There are 82,740 parallel sentences\nfrom news-commentary09.de-en and 1,418,115\nparallel sentences from europarl-v4.de-en. The\nmonolingual data contains 9.8 M sentences.8\nTo build the baseline, the data was tokenized\nusing the Moses tokenizer and lowercased. We\nuse GIZA++ to generate alignments, by running\n5 iterations of Model 1, 5 iterations of the HMM\nModel, and 4 iterations of Model 4. We sym-\nmetrize using the ?grow-diag-final-and? heuris-\ntic. Our Moses systems use default settings. The\nLM uses the monolingual data and is trained as\na five-gram9 using the SRILM-Toolkit (Stolcke,\n2002). We run MERT separately for each sys-\ntem. The recaser used is the same for all systems.\nIt is the standard recaser supplied with Moses,\ntrained on all German training data. The dev set\nis wmt-2009-a and the test set is wmt-2009-b, and\nwe report end-to-end case sensitive BLEU scores\nagainst the unmodified reference SGML file. The\nblind test set used is wmt-2009-blind (all lines).\nIn developing our inflection prediction sys-\ntems (and making such decisions as n-gram order\nused), we worked on the so-called ?clean data?\ntask, predicting the inflection on stemmed refer-\nence sentences (rather than MT output). We used\nthe 2000 sentence dev-2006 corpus for this task.\nOur contrastive systems consist of two steps,\nthe first is a translation step using a similar\nMoses system (except that the German side is\nstemmed, with the markup indicated in Sec-\n"},{"#tail":"\n","@confidence":"0.982418625","#text":"\nretaining only one copy of each unique line, which resulted\nin 7.55 M sentences.\n9Add-1 smoothing for unigrams and Kneser-Ney\nsmoothing for higher order n-grams, pruning defaults.\ntion 2.3), and the second is inflection prediction\nas described previously in the paper. To derive\nthe stem+markup representation we first parse\nthe German training data and then produce the\nstemmed representation. We then build a sys-\ntem for translating from English words to Ger-\nman stems (the stem+markup representation), on\nthe same data (so the German side of the parallel\ndata, and the German language modeling uses the\nstem+markup representation). Likewise, MERT\nis performed using references which are in the\nstem+markup representation.\nTo train the inflection prediction systems, we\nuse the monolingual data. The basic surface form\nmodel is trained on lowercased surface forms,\nthe hybrid surface form model with features is\ntrained on lowercased surface forms annotated\nwith markup. The linguistic feature prediction\nsystems are trained on the monolingual data pro-\ncessed as described previously (see Table 2).\nOur JSMs are trained using the SRILM Toolkit.\nWe use the SRILM disambig tool for predicting\ninflection, which takes a ?map? that specifies the\nset of fully specified representations that each un-\nderspecified stem can map to. For surface form\nmodels, it specifies the mapping from stems to\nlowercased surface forms (or surface forms with\nmarkup for the hybrid surface model).\n"},{"#tail":"\n","@confidence":"0.847281857142857","#text":"\nWe build two different kinds of translation sys-\ntem, the baseline and the stem translation system\n(where MERT is used to train the system to pro-\nduce a stem+markup sequence which agrees with\nthe stemmed reference of the dev set). In this sec-\ntion we present the end-to-end translation results\nfor the different inflection prediction models de-\nfined in Section 4, see Table 4.\nIf we translate from English into a stemmed\nGerman representation and then apply a unigram\nstem-to-surface-form model to predict the surface\nform, we achieve a BLEU score of 9.97 (line 2).\nThis is only presented for comparison.\nThe baseline10 is 14.16, line 1. We compare\nthis with a 5-gram sequence model11 that predicts\n10This is a better case-sensitive score than the baselines\non wmt-2009-b in experiments by top-performers Edinburgh\nand Karlsruhe at the shared task. We use Moses with default\nsettings.\n11Note that we use a different set, the ?clean data? set, to\ndetermine the choice of n-gram order, see Section 7. We use\n"},{"#tail":"\n","@confidence":"0.969283411764706","#text":"\nsurface forms without access to morphological\nfeatures, resulting in a BLEU score of 14.26. In-\ntroducing morphological features (case on prepo-\nsitions, number and gender on nouns) increases\nthe BLEU score to 14.58, which is in the same\nrange as the single JSM system predicting all lin-\nguistic features at once.\nThis result shows that the mostly unlexicalized\nsingle JSM can produce competitive results with\ndirect surface form prediction, despite not having\naccess to a model of inflected forms, which is the\ndesired final output. This strongly suggests that\nthe prediction of morphological features can be\nused to achieve additional generalization over di-\nrect surface form prediction. When comparing the\nsimple direct surface form prediction (line 3) with\nthe hybrid system enriched with number, gender\nand case (line 4), it becomes evident that feature\nmarkup can also aid surface form prediction.\nSince the single JSM has no access to lexical\ninformation, we used a language model to score\ndifferent feature predictions: for each sentence of\nthe development set, the 100 best feature predic-\ntions were inflected and scored with a language\nmodel. We then optimized weights for the two\nscores LM (language model on surface forms)\nand FP (feature prediction, the score assigned by\nthe JSM). This method disprefers feature predic-\ntions with a top FP-score if the inflected sen-\ntence obtains a bad LM score and likewise dis-\nfavors low-ranked feature prediction with a high\nLM score. The prediction of case is the most\ndifficult given no lexical information, thus scor-\ning different prediction possibilities on inflected\nwords is helpful. An example is when the case of\na noun phrase leads to an inflected phrase which\nnever occurs in the (inflected) language model\n(e.g., case=genitive vs. case=other). Applying\nthis method to the single JSM leads to a negligible\nimprovement (14.53 vs. 14.56). Using the n-best\noutput of the stem translation system did not lead\nto any improvement.\nThe comparison between different feature pre-\ndiction models is also illustrative. Performance\ndecreases somewhat when using individual joint\nsequence models (one for each linguistic feature)\ncompared to one single model (14.29, line 6).\nThe framework using the individual CRFs for\na 5-gram for surface forms and a 4-gram for JSMs, and the\nsame smoothing (Kneser-Ney, add-1 for unigrams, default\npruning).\n"},{"#tail":"\n","@confidence":"0.975659782608696","#text":"\nthe development test set wmt-2009-b\neach linguistic feature performs best (14.72, line\n7). The CRF framework combines the advantages\nof surface form prediction and linguistic feature\nprediction by using feature functions that effec-\ntively cover the feature function spaces used by\nboth forms of prediction. The performance of the\nCRF models results in a statistically significant\nimprovement12 (p < 0.05) over the baseline. We\nalso tried CRFs with bilingual features (projected\nfrom English parses via the alignment output by\nMoses), but obtained only a small improvement of\n0.03, probably because the required information\nis transferred in our stem markup (also a poor im-\nprovement beyond monolingual features is con-\nsistent with previous work, see Section 8.3). De-\ntails are omitted due to space.\nWe further validated our results by translating\nthe blind test set from wmt-2009, which we have\nnever looked at in any way. Here we also had\na statistically significant difference between the\nbaseline and the CRF-based prediction, the scores\nwere 13.68 and 14.18.\n"},{"#tail":"\n","@confidence":"0.868394470588235","#text":"\nStem Markup. The first step of translating\nfrom English to German stems (with the markup\nwe previously discussed) is substantially easier\nthan translating directly to inflected German (we\nsee BLEU scores on stems+markup that are over\n2.0 BLEU higher than the BLEU scores on in-\nflected forms when running MERT). The addition\nof case to prepositions only lowered the BLEU\nscore reached by MERT by about 0.2, but is very\nhelpful for prediction of the case feature.\nInflection Prediction Task. Clean data task re-\nsults13 are given in Table 5. The 4 CRFs outper-\nform the 4 JSMs by more than 2%.\n12We used Kevin Gimpel?s implementation of pairwise\nbootstrap resampling with 1000 samples.\n1326,061 of 55,057 tokens in our test set are ambiguous.\nWe report % surface form matches for ambiguous tokens.\n"},{"#tail":"\n","@confidence":"0.99128035","#text":"\nthe single and the four separate joint sequence models.\nAs we mentioned in Section 4, there is a spar-\nsity issue at small training data sizes for the sin-\ngle joint sequence model. This is shown in Ta-\nble 6. At the largest training data sizes, model-\ning all 4 features together results in the best pre-\ndictions of inflection. However using 4 separate\nmodels is worth this minimal decrease in perfor-\nmance, since it facilitates experimentation with\nthe CRF framework for which the training of a\nsingle model is not currently tractable.\nOverall, the inflection prediction works well for\ngender, number and type of inflection, which are\nlocal features to the NP that normally agree with\nthe explicit markup output by the stem transla-\ntion system (for example, the gender of a com-\nmon noun, which is marked in the stem markup,\nis usually successfully propagated to the rest of\nthe NP). Prediction of case does not always work\nwell, and could maybe be improved through hier-\narchical labeled-syntax stem translation.\nPortmanteaus. An example of where the sys-\ntem is improved because of the new handling of\nportmanteaus can be seen in the dative phrase\nim internationalen Rampenlicht (in the interna-\ntional spotlight), which does not occur in the par-\nallel data. The accusative phrase in das interna-\ntionale Rampenlicht does occur, however in this\ncase there is no portmanteau, but a one-to-one\nmapping between in the and in das. For a given\ncontext, only one of accusative or dative case is\nvalid, and a strongly disfluent sentence results\nfrom the incorrect choice. In our system, these\ntwo cases are handled in the same way (def-article\ninternational Rampenlicht). This allows us to\ngeneralize from the accusative example with no\nportmanteau and take advantage of longer phrase\npairs, even when translating to something that will\nbe inflected as dative and should be realized as a\nportmanteau. The baseline does not have this ca-\npability. It should be noted that the portmanteau\nmerging method described in Section 3 remerges\nall occurrences of APPR and ART that can techni-\ncally form a portmanteau. There are a few cases\nwhere merging, despite being grammatical, does\nnot lead to a good result. Such exceptions require\nsemantic interpretation and are difficult to capture\nwith a fixed set of rules.\n8 Adding Compounds to the System\nCompounds are highly productive in German and\nlead to data sparsity. We split the German com-\npounds in the training data, so that our stem trans-\nlation system can now work with the individual\nwords in the compounds. After we have trans-\nlated to a split/stemmed representation, we deter-\nmine whether to merge words together to form a\ncompound. Then we merge them to create stems\nin the same representation as before and we per-\nform inflection and portmanteau merging exactly\nas previously discussed.\n"},{"#tail":"\n","@confidence":"0.999502555555555","#text":"\nWe prepare the training data by splitting com-\npounds in two steps, following the technique of\nFritzinger and Fraser (2010). First, possible split\npoints are extracted using SMOR, and second, the\nbest split points are selected using the geometric\nmean of word part frequencies.\ncompound word parts gloss\nInflationsrate Inflation Rate inflation rate\nauszubrechen aus zu brechen out to break (to break out)\nTraining data is then stemmed as described in\nSection 2.3. The formerly modifying words of the\ncompound (in our example the words to the left\nof the rightmost word) do not have a stem markup\nassigned, except for two cases: i) they are nouns\nthemselves or ii) they are particles separated from\na verb. In these cases, former modifiers are rep-\nresented identically to their individual occurring\ncounterparts, which helps generalization.\n"},{"#tail":"\n","@confidence":"0.997888666666667","#text":"\nAfter translation, compound parts have to be\nresynthesized into compounds before inflection.\nTwo decisions have to be taken: i) where to\n"},{"#tail":"\n","@confidence":"0.98744625","#text":"\nmerge and ii) how to merge. Following the work\nof Stymne and Cancedda (2011), we implement\na linear-chain CRF merging system using the\nfollowing features: stemmed (separated) surface\nform, part-of-speech14 and frequencies from the\ntraining corpus for bigrams/merging of word and\nword+1, word as true prefix, word+1 as true suf-\nfix, plus frequency comparisons of these. The\nCRF is trained on the split monolingual data. It\nonly proposes merging decisions, merging itself\nuses a list extracted from the monolingual data\n(Popovic et al 2006).\n"},{"#tail":"\n","@confidence":"0.9572928","#text":"\nWe evaluated the end-to-end inflection system\nwith the addition of compounds.15 As in the in-\nflection experiments described in Section 5, we\nuse a 5-gram surface LM and a 7-gram POS\nLM, but for this experiment, they are trained on\nstemmed, split data. The POS LM helps com-\npound parts and heads appear in correct order.\nThe results are in Table 7. The BLEU score of the\nCRF on test is 14.04, which is low. However the\nsystem produces 19 compound types which are\nin the reference but not in the parallel data, and\ntherefore not accessible to other systems. We also\nobserve many more compounds in general. The\n100-best inflection rescoring technique previously\ndiscussed reached 14.07 on the test set. Blind\ntest results with CRF prediction are much better,\n14.08, which is a statistically significant improve-\nment over the baseline (13.68) and approaches the\nresult we obtained without compounds (14.18).\nCorrectly generated compounds are single words\nwhich usually carry the same information as mul-\ntiple words in English, and are hence likely un-\nderweighted by BLEU. We again see many in-\nteresting generalizations. For instance, take the\ncase of translating English miniature cameras to\nthe German compound Miniaturkameras. minia-\nture camera or miniature cameras does not occur\nin the training data, and so there is no appropri-\nate phrase pair in any system (baseline, inflec-\ntion, or inflection&compound-splitting). How-\never, our system with compound splitting has\nlearned from split composita that English minia-\n14Compound modifiers get assigned a special tag based on\nthe POS of their former heads, e.g., Inflation in the example\nis marked as a non-head of a noun.\n"},{"#tail":"\n","@confidence":"0.536009","#text":"\nture can be translated as German Miniatur- and\ngets the correct output.\n"},{"#tail":"\n","@confidence":"0.999875804878049","#text":"\nThere has been a large amount of work on trans-\nlating from a morphologically rich language to\nEnglish, we omit a literature review here due to\nspace considerations. Our work is in the opposite\ndirection, which primarily involves problems of\ngeneration, rather than problems of analysis.\nThe idea of translating to stems and then in-\nflecting is not novel. We adapted the work of\nToutanova et al(2008), which is effective but lim-\nited by the conflation of two separate issues: word\nformation and inflection.\nGiven a stem such as brother, Toutanova et. al?s\nsystem might generate the ?stem and inflection?\ncorresponding to and his brother. Viewing and\nand his as inflection is problematic since a map-\nping from the English phrase and his brother to\nthe Arabic stem for brother is required. The situ-\nation is worse if there are English words (e.g., ad-\njectives) separating his and brother. This required\nmapping is a significant problem for generaliza-\ntion. We view this issue as a different sort of prob-\nlem entirely, one of word-formation (rather than\ninflection). We apply a ?split in preprocessing and\nresynthesize in postprocessing? approach to these\nphenomena, combined with inflection prediction\nthat is similar to that of Toutanova et. al. The\nonly work that we are aware of which deals with\nboth issues is the work of de Gispert and Marin?o\n(2008), which deals with verbal morphology and\nattached pronouns. There has been other work\non solving inflection. Koehn and Hoang (2007)\nintroduced factored SMT. We use more complex\ncontext features. Fraser (2009) tried to solve the\ninflection prediction problem by simply building\nan SMT system for translating from stems to in-\nflected forms. Bojar and Kos (2010) improved on\nthis by marking prepositions with the case they\nmark (one of the most important markups in our\nsystem). Both efforts were ineffective on large\ndata sets. Williams and Koehn (2011) used uni-\nfication in an SMT system to model some of the\n"},{"#tail":"\n","@confidence":"0.999748044776119","#text":"\nagreement phenomena that we model. Our CRF\nframework allows us to use more complex con-\ntext features.\nWe have directly addressed the question as to\nwhether inflection should be predicted using sur-\nface forms as the target of the prediction, or\nwhether linguistic features should be predicted,\nalong with the use of a subsequent generation\nstep. The direct prediction of surface forms is\nlimited to those forms observed in the training\ndata, which is a significant limitation. How-\never, it is reasonable to expect that the use of\nfeatures (and morphological generation) could\nalso be problematic as this requires the use of\nmorphologically-aware syntactic parsers to anno-\ntate the training data with such features, and addi-\ntionally depends on the coverage of morpholog-\nical analysis and generation. Despite this, our\nresearch clearly shows that the feature-based ap-\nproach is superior for English-to-German SMT.\nThis is a striking result considering state-of-the-\nart performance of German parsing is poor com-\npared with the best performance on English pars-\ning. As parsing performance improves, the per-\nformance of linguistic-feature-based approaches\nwill increase.\nVirpioja et al(2007), Badr et al(2008), Luong\net al(2010), Clifton and Sarkar (2011), and oth-\ners are primarily concerned with using morpheme\nsegmentation in SMT, which is a useful approach\nfor dealing with issues of word-formation. How-\never, this does not deal directly with linguistic fea-\ntures marked by inflection. In German these lin-\nguistic features are marked very irregularly and\nthere is widespread syncretism, making it difficult\nto split off morphemes specifying these features.\nSo it is questionable as to whether morpheme seg-\nmentation techniques are sufficient to solve the in-\nflectional problem we are addressing.\nMuch previous work looks at the impact of us-\ning source side information (i.e., feature func-\ntions on the aligned English), such as those\nof Avramidis and Koehn (2008), Yeniterzi and\nOflazer (2010) and others. Toutanova et. al.?s\nwork showed that it is most important to model\ntarget side coherence and our stem markup also\nallows us to access source side information. Us-\ning additional source side information beyond the\nmarkup did not produce a gain in performance.\nFor compound splitting, we follow Fritzinger\nand Fraser (2010), using linguistic knowledge en-\ncoded in a rule-based morphological analyser and\nthen selecting the best analysis based on the ge-\nometric mean of word part frequencies. Other\napproaches use less deep linguistic resources\n(e.g., POS-tags Stymne (2008)) or are (almost)\nknowledge-free (e.g., Koehn and Knight (2003)).\nCompound merging is less well studied. Popovic\net al(2006) used a simple, list-based merging ap-\nproach, merging all consecutive words included\nin a merging list. This approach resulted in too\nmany compounds. We follow Stymne and Can-\ncedda (2011), for compound merging. We trained\na CRF using (nearly all) of the features they used\nand found their approach to be effective (when\ncombined with inflection and portmanteau merg-\ning) on one of our two test sets.\n"},{"#tail":"\n","@confidence":"0.99976975","#text":"\nWe have shown that both the prediction of sur-\nface forms and the prediction of linguistic features\nare of interest for improving SMT. We have ob-\ntained the advantages of both in our CRF frame-\nwork, and also integrated handling of compounds,\nand an inflection-dependent word formation phe-\nnomenon, portmanteaus. We validated our work\non a well-studied large corpora translation task.\n"},{"#tail":"\n","@confidence":"0.994274375","#text":"\nThe authors wish to thank the anonymous review-\ners for their comments. Aoife Cahill was partly\nsupported by Deutsche Forschungsgemeinschaft\ngrant SFB 732. Alexander Fraser, Marion Weller\nand Fabienne Cap were funded by Deutsche\nForschungsgemeinschaft grant Models of Mor-\nphosyntax for Statistical Machine Translation.\nThe research leading to these results has received\nfunding from the European Community?s Seventh\nFramework Programme (FP7/2007-2013) under\ngrant agreement Nr. 248005. This work was sup-\nported in part by the IST Programme of the Euro-\npean Community, under the PASCAL2 Network\nof Excellence, IST-2007-216886. This publica-\ntion only reflects the authors? views. We thank\nThomas Lavergne and Helmut Schmid.\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.836226","#text":"\n?Institut fu?r Maschinelle Sprachverarbeitung ?Educational Testing Service\nUniversita?t Stuttgart Princeton, NJ 08541\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.990196","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.997289","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.996829","@genericHeader":"introduction","#text":"\n3 Portmanteaus\n"},{"#tail":"\n","@confidence":"0.996784","@genericHeader":"method","#text":"\n4 Models for Inflection Prediction\n"},{"#tail":"\n","@confidence":"0.995527","@genericHeader":"method","#text":"\n5 Experimental Setup\n"},{"#tail":"\n","@confidence":"0.999919","@genericHeader":"method","#text":"\n6 Results for Inflection Prediction\n"},{"#tail":"\n","@confidence":"0.648511","@genericHeader":"method","#text":"\n7 Analysis of Inflection-based System\n"},{"#tail":"\n","@confidence":"0.995915","@genericHeader":"method","#text":"\n9 Related Work\n"},{"#tail":"\n","@confidence":"0.986518","@genericHeader":"conclusions","#text":"\n10 Conclusion\n"},{"#tail":"\n","@confidence":"0.966924","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.976318","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.965772","#text":"\nTable 1: Re-merging of prepositions and articles after\n"},{"#tail":"\n","@confidence":"0.62305","#text":"\nTable 2 remain unmerged.\n"},{"#tail":"\n","@confidence":"0.942739","#text":"\nTable 2: Overview: inflection prediction steps using a single joint sequence model. All words except verbs and\n"},{"#tail":"\n","@confidence":"0.992016","#text":"\nTable 3: Feature functions used in CRF models (fea-\n"},{"#tail":"\n","@confidence":"0.969248","#text":"\nTable 4: BLEU scores (detokenized, case sensitive) on\n"},{"#tail":"\n","@confidence":"0.7768765","#text":"\nTable 5: Comparing predicting surface forms directly\nwith predicting morphological features.\n"},{"#tail":"\n","@confidence":"0.980331","#text":"\nTable 6: Accuracy for different training data sizes of\n"},{"#tail":"\n","@confidence":"0.962342","#text":"\nTable 7: Results with Compounds on the test set\n"}],"page":[{"#tail":"\n","@confidence":"0.997122","#text":"\n664\n"},{"#tail":"\n","@confidence":"0.99583","#text":"\n665\n"},{"#tail":"\n","@confidence":"0.989385","#text":"\n666\n"},{"#tail":"\n","@confidence":"0.841384","#text":"\n667\n"},{"#tail":"\n","@confidence":"0.991005","#text":"\n668\n"},{"#tail":"\n","@confidence":"0.75647","#text":"\n669\n"},{"#tail":"\n","@confidence":"0.982854","#text":"\n670\n"},{"#tail":"\n","@confidence":"0.982368","#text":"\n671\n"},{"#tail":"\n","@confidence":"0.966318","#text":"\n672\n"},{"#tail":"\n","@confidence":"0.98093","#text":"\n673\n"},{"#tail":"\n","@confidence":"0.999657","#text":"\n674\n"}],"table":[{"#tail":"\n","@confidence":"0.807203285714286","#text":"\nModel Accuracy\nunigram surface (no features) 55.98\nsurface (no features) 86.65\nsurface (with case, number, gender features) 91.24\n1 JSM morphological features 92.45\n4 JSMs morphological features 92.01\n4 CRFs morphological features, lexical information 94.29\n"},{"#tail":"\n","@confidence":"0.9897018","#text":"\ntraining data 1 model 4 models\n7.3 M sentences 92.41 91.88\n1.5 M sentences 92.45 92.01\n100000 sentences 90.20 90.64\n1000 sentences 83.72 86.94\n"}],"email":{"#tail":"\n","@confidence":"0.980238","#text":"\n{fraser,wellermn,cap}@ims.uni-stuttgart.de acahill@ets.org\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.786361","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.99376","#text":"Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?674, Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics"},"address":{"#tail":"\n","@confidence":"0.950804","#text":"Universita?t Stuttgart Princeton, NJ 08541 D?70174 Stuttgart, Germany USA"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.992162","#text":"Institut fu?r Maschinelle Sprachverarbeitung ?Educational Testing Service"},"author":{"#tail":"\n","@confidence":"0.993687","#text":"Alexander Fraser Marion Weller Aoife Cahill Fabienne Cap"},"abstract":{"#tail":"\n","@confidence":"0.994877","#text":"The current state-of-the-art in statistical machine translation (SMT) suffers from issues of sparsity and inadequate modeling power when translating into morphologically rich languages. We model both inflection and word-formation for the task of translating into German. We translate from English words to an underspecified German representation and then use linearchain CRFs to predict the fully specified German representation. We show that improved modeling of inflection and wordformation leads to improved SMT."},"title":{"#tail":"\n","@confidence":"0.998963","#text":"Modeling Inflection and Word-Formation in SMT"},"email":[{"#tail":"\n","@confidence":"0.956692","#text":"fraser@ims.uni-stuttgart.deacahill@ets.org"},{"#tail":"\n","@confidence":"0.956692","#text":"wellermn@ims.uni-stuttgart.deacahill@ets.org"},{"#tail":"\n","@confidence":"0.956692","#text":"cap@ims.uni-stuttgart.deacahill@ets.org"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"date":{"#tail":"\n","#text":"2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"which is a useful approach for dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the impact of using source side information (i.e., feature functions on the aligned English), such as those of Avramidis and Koehn (2008), Yeniterzi and Oflazer (2010) and others. Toutanova et. al.?s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Sty","@endWordPosition":"5956","@position":"37459","annotationId":"T1","@startWordPosition":"5953","@citStr":"Avramidis and Koehn (2008)"}},"title":{"#tail":"\n","#text":"Enriching Morphologically Poor Languages for Statistical Machine Translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Eleftherios Avramidis and Philipp Koehn. 2008. Enriching Morphologically Poor Languages for Statistical Machine Translation. In Proceedings of ACL672 08: HLT, pages 763?770, Columbus, Ohio, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"763--770"},"marker":{"#tail":"\n","#text":"Avramidis, Koehn, 2008"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Columbus, Ohio,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL672 08: HLT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Eleftherios Avramidis"},{"#tail":"\n","#text":"Philipp Koehn"}]}},{"date":{"#tail":"\n","#text":"2008"},"title":{"#tail":"\n","#text":"Segmentation for English-to-Arabic statistical machine translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Segmentation for English-to-Arabic statistical machine translation. In Proceedings of ACL-08: HLT, Short Papers, pages 153?156, Columbus, Ohio, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"153--156"},"marker":{"#tail":"\n","#text":"Badr, Zbib, Glass, 2008"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Columbus, Ohio,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL-08: HLT, Short Papers,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ibrahim Badr"},{"#tail":"\n","#text":"Rabih Zbib"},{"#tail":"\n","#text":"James Glass"}]}},{"date":{"#tail":"\n","#text":"2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cessing and resynthesize in postprocessing? approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Marin?o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the 671 agreement phenomena that we model. Our CRF framework allows us to use more complex context features. We have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step. The d","@endWordPosition":"5606","@position":"35264","annotationId":"T2","@startWordPosition":"5603","@citStr":"Bojar and Kos (2010)"}},"title":{"#tail":"\n","#text":"Failures in English-Czech Phrase-Based MT."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in English-Czech Phrase-Based MT. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 60?66, Uppsala, Sweden, July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"60--66"},"marker":{"#tail":"\n","#text":"Bojar, Kos, 2010"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Uppsala, Sweden,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ondrej Bojar"},{"#tail":"\n","#text":"Kamil Kos"}]}},{"date":{"#tail":"\n","#text":"2011"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"se of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation. Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT. This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al(2007), Badr et al(2008), Luong et al(2010), Clifton and Sarkar (2011), and others are primarily concerned with using morpheme segmentation in SMT, which is a useful approach for dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the impact of using source side i","@endWordPosition":"5843","@position":"36755","annotationId":"T3","@startWordPosition":"5840","@citStr":"Clifton and Sarkar (2011)"}},"title":{"#tail":"\n","#text":"Combining morpheme-based machine translation with postprocessing morpheme prediction."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Ann Clifton and Anoop Sarkar. 2011. Combining morpheme-based machine translation with postprocessing morpheme prediction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 32?42, Portland, Oregon, USA, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"32--42"},"marker":{"#tail":"\n","#text":"Clifton, Sarkar, 2011"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Portland, Oregon, USA,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ann Clifton"},{"#tail":"\n","#text":"Anoop Sarkar"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Adria` de Gispert and Jose? B. Marin?o. 2008. On the impact of morphology in English to Spanish statistical MT. Speech Communication, 50(11-12):1034? 1046."},"journal":{"#tail":"\n","#text":"Speech Communication,"},"#text":"\n","pages":{"#tail":"\n","#text":"50--11"},"marker":{"#tail":"\n","#text":"de Gispert, Marino, 2008"},"title":{"#tail":"\n","#text":"On the impact of morphology in English to Spanish statistical MT."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Adria` de Gispert"},{"#tail":"\n","#text":"Jose B Marino"}]}},{"date":{"#tail":"\n","#text":"2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"zation. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a ?split in preprocessing and resynthesize in postprocessing? approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Marin?o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the 671 agreement phenomena that we model. Our CRF framework allows us to use more complex context features. We have directly addressed the question as to whether inflection should be predicted using surfac","@endWordPosition":"5581","@position":"35114","annotationId":"T4","@startWordPosition":"5580","@citStr":"Fraser (2009)"}},"title":{"#tail":"\n","#text":"Experiments in Morphosyntactic Processing for Translating to and from German."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Alexander Fraser. 2009. Experiments in Morphosyntactic Processing for Translating to and from German. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115?119, Athens, Greece, March. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"115--119"},"marker":{"#tail":"\n","#text":"Fraser, 2009"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Athens, Greece,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourth Workshop on Statistical Machine Translation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Alexander Fraser"}}},{"date":{"#tail":"\n","#text":"2010"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" productive in German and lead to data sparsity. We split the German compounds in the training data, so that our stem translation system can now work with the individual words in the compounds. After we have translated to a split/stemmed representation, we determine whether to merge words together to form a compound. Then we merge them to create stems in the same representation as before and we perform inflection and portmanteau merging exactly as previously discussed. 8.1 Details of Splitting Process We prepare the training data by splitting compounds in two steps, following the technique of Fritzinger and Fraser (2010). First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies. compound word parts gloss Inflationsrate Inflation Rate inflation rate auszubrechen aus zu brechen out to break (to break out) Training data is then stemmed as described in Section 2.3. The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb. In these cases, f","@endWordPosition":"4767","@position":"30118","annotationId":"T5","@startWordPosition":"4764","@citStr":"Fritzinger and Fraser (2010)"},{"#tail":"\n","#text":"morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the impact of using source side information (i.e., feature functions on the aligned English), such as those of Avramidis and Koehn (2008), Yeniterzi and Oflazer (2010) and others. Toutanova et. al.?s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied. Popovic et al(2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow Stymne and Cancedda (2011), for compound merging. We trained a CRF us","@endWordPosition":"6014","@position":"37821","annotationId":"T6","@startWordPosition":"6011","@citStr":"Fritzinger and Fraser (2010)"}]},"title":{"#tail":"\n","#text":"How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing. In Proceedings of the Fifth Workshop on Statistical Machine Translation, pages 224?234. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"224--234"},"marker":{"#tail":"\n","#text":"Fritzinger, Fraser, 2010"},"publisher":{"#tail":"\n","#text":"Association"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fifth Workshop on Statistical Machine Translation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Fabienne Fritzinger"},{"#tail":"\n","#text":"Alexander Fraser"}]}},{"date":{"#tail":"\n","#text":"2007"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":": this moves some of the difficulty in predicting case from the inflection prediction step to the stem translation step. Since the choice of case in a PP is often determined by the PP?s meaning (and there are often different meanings possible given different case choices), it seems reasonable to make this decision during stem translation. Verbs are represented using their inflected surface form. Having access to inflected verb forms has a positive influence on case prediction in the second 2We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model. Koehn and Hoang (2007) showed that the use of a POS factor only results in negligible BLEU improvements, but we need access to the POS in our inflection prediction models. 665 input decoder output inflected merged in in<APPR><Dat> in im die<+ART><Def> dem contrast Gegensatz<+NN><Masc><Sg>Gegensatz Gegensatz to zu<APPR><Dat> zu zur the die<+ART><Def> der animated lebhaft<+ADJ><Pos> lebhaften lebhaften debate Debatte<+NN><Fem><Sg> Debatte Debatte Table 1: Re-merging of prepositions and articles after inflection to form portmanteaus, in dem means in the. step through subject-verb agreement. Articles are reduced to the","@endWordPosition":"1283","@position":"8117","annotationId":"T7","@startWordPosition":"1280","@citStr":"Koehn and Hoang (2007)"},{"#tail":"\n","#text":"eparating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a ?split in preprocessing and resynthesize in postprocessing? approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Marin?o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the 671 agreement phenomena that we model. Our CRF framework allows us to use more complex context features. We have directly add","@endWordPosition":"5570","@position":"35037","annotationId":"T8","@startWordPosition":"5567","@citStr":"Koehn and Hoang (2007)"}]},"title":{"#tail":"\n","#text":"Factored Translation Models."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Philipp Koehn and Hieu Hoang. 2007. Factored Translation Models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868? 876, Prague, Czech Republic, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"868--876"},"marker":{"#tail":"\n","#text":"Koehn, Hoang, 2007"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Prague, Czech Republic,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Hieu Hoang"}]}},{"date":{"#tail":"\n","#text":"2003"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"va et. al.?s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied. Popovic et al(2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow Stymne and Cancedda (2011), for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets. 10 Conclusion We have shown that both the prediction of surface forms and the prediction of linguistic features are of interest for improving ","@endWordPosition":"6060","@position":"38132","annotationId":"T9","@startWordPosition":"6057","@citStr":"Koehn and Knight (2003)"}},"title":{"#tail":"\n","#text":"Empirical methods for compound splitting."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In EACL ?03: Proceedings of the 10th conference of the European chapter of the Association for Computational Linguistics, pages 187?193, Morristown, NJ, USA. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"187--193"},"marker":{"#tail":"\n","#text":"Koehn, Knight, 2003"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Morristown, NJ, USA."},"booktitle":{"#tail":"\n","#text":"In EACL ?03: Proceedings of the 10th conference of the European chapter of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philipp Koehn"},{"#tail":"\n","#text":"Kevin Knight"}]}},{"date":{"#tail":"\n","#text":"2001"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":", and then generate each surface form using SMOR. 5. Using four CRFs (one for each linguistic feature). The sequence models already presented are limited to the n-gram feature space, and those that predict linguistic features are not strongly lexicalized. Toutanova et al(2008) uses an MEMM which allows the integration of a wide variety of feature functions. We also wanted to experiment with additional feature functions, and so we train 4 separate linear chain CRF6 models on our data (one for each linguistic feature we want to predict). We chose CRFs over MEMMs to avoid the label bias problem (Lafferty et al 2001). The CRF feature functions, for each German word wi, are in Table 3. The common feature functions are used in all models, while each of the 4 separate models (one for each linguistic feature) includes the context of only that linguistic feature. We use L1 regularization to eliminate irrelevant feature functions, the regularization parameter is optimized on held out data. 6We use the Wapiti Toolkit (Lavergne et al 2010) on 4 x 12-Core Opteron 6176 2.3 GHz with 256GB RAM to train our CRF models. Training a single CRF model on our data was not tractable, so we use one for each linguistic feature","@endWordPosition":"2639","@position":"16838","annotationId":"T10","@startWordPosition":"2636","@citStr":"Lafferty et al 2001"}},"title":{"#tail":"\n","#text":"Conditional random fields: Probabilistic models for segmenting and labeling sequence data."},"#tail":"\n","rawString":{"#tail":"\n","#text":"John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning, pages 282?289. Morgan Kaufmann, San Francisco, CA."},"#text":"\n","pages":{"#tail":"\n","#text":"282--289"},"marker":{"#tail":"\n","#text":"Lafferty, McCallum, Pereira, 2001"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann,"},"location":{"#tail":"\n","#text":"San Francisco, CA."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the International Conference on Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"John Lafferty"},{"#tail":"\n","#text":"Andrew McCallum"},{"#tail":"\n","#text":"Fernando Pereira"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513. Association for Computational Linguistics, July."},"#text":"\n","pages":{"#tail":"\n","#text":"504--513"},"marker":{"#tail":"\n","#text":"Lavergne, Cappe, Yvon, 2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" and so we train 4 separate linear chain CRF6 models on our data (one for each linguistic feature we want to predict). We chose CRFs over MEMMs to avoid the label bias problem (Lafferty et al 2001). The CRF feature functions, for each German word wi, are in Table 3. The common feature functions are used in all models, while each of the 4 separate models (one for each linguistic feature) includes the context of only that linguistic feature. We use L1 regularization to eliminate irrelevant feature functions, the regularization parameter is optimized on held out data. 6We use the Wapiti Toolkit (Lavergne et al 2010) on 4 x 12-Core Opteron 6176 2.3 GHz with 256GB RAM to train our CRF models. Training a single CRF model on our data was not tractable, so we use one for each linguistic feature. 667 Common lemmawi?5...wi+5 , tagwi?7...wi+7 Case casewi?5...wi+5 Gender genderwi?5...wi+5 Number numberwi?5...wi+5 in-weak-context in-weak-contextwi?5...wi+5 Table 3: Feature functions used in CRF models (feature functions are binary indicators of the pattern). 5 Experimental Setup To evaluate our end-to-end system, we perform the well-studied task of news translation, using the Moses SMT package. We use the English/","@endWordPosition":"2708","@position":"17261","annotationId":"T11","@startWordPosition":"2705","@citStr":"Lavergne et al 2010"}},"title":{"#tail":"\n","#text":"Practical very large scale CRFs."},"booktitle":{"#tail":"\n","#text":"In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Thomas Lavergne"},{"#tail":"\n","#text":"Olivier Cappe"},{"#tail":"\n","#text":"Francois Yvon"}]}},{"date":{"#tail":"\n","#text":"2010"},"title":{"#tail":"\n","#text":"A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan. 2010. A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 148?157, Cambridge, MA, October. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"148--157"},"marker":{"#tail":"\n","#text":"Luong, Nakov, Kan, 2010"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Cambridge, MA,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Minh-Thang Luong"},{"#tail":"\n","#text":"Preslav Nakov"},{"#tail":"\n","#text":"Min-Yen Kan"}]}},{"date":{"#tail":"\n","#text":"2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"into compounds before inflection. Two decisions have to be taken: i) where to 670 merge and ii) how to merge. Following the work of Stymne and Cancedda (2011), we implement a linear-chain CRF merging system using the following features: stemmed (separated) surface form, part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word+1, word as true prefix, word+1 as true suffix, plus frequency comparisons of these. The CRF is trained on the split monolingual data. It only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al 2006). 8.3 Experiments We evaluated the end-to-end inflection system with the addition of compounds.15 As in the inflection experiments described in Section 5, we use a 5-gram surface LM and a 7-gram POS LM, but for this experiment, they are trained on stemmed, split data. The POS LM helps compound parts and heads appear in correct order. The results are in Table 7. The BLEU score of the CRF on test is 14.04, which is low. However the system produces 19 compound types which are in the reference but not in the parallel data, and therefore not accessible to other systems. We also observe many more co","@endWordPosition":"4992","@position":"31543","annotationId":"T12","@startWordPosition":"4989","@citStr":"Popovic et al 2006"}},"title":{"#tail":"\n","#text":"Statistical Machine Translation of German Compound Words."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Maja Popovic, Daniel Stein, and Hermann Ney. 2006. Statistical Machine Translation of German Compound Words. In Proceedings of FINTAL-06, pages 616?624, Turku, Finland. Springer Verlag, LNCS."},"#text":"\n","pages":{"#tail":"\n","#text":"616--624"},"marker":{"#tail":"\n","#text":"Popovic, Stein, Ney, 2006"},"publisher":{"#tail":"\n","#text":"Springer Verlag, LNCS."},"location":{"#tail":"\n","#text":"Turku, Finland."},"booktitle":{"#tail":"\n","#text":"In Proceedings of FINTAL-06,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Maja Popovic"},{"#tail":"\n","#text":"Daniel Stein"},{"#tail":"\n","#text":"Hermann Ney"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. SMOR: A German Computational Morphology Covering Derivation, Composition, and Inflection. In 4th International Conference on Language Resources and Evaluation."},"#text":"\n","marker":{"#tail":"\n","#text":"Schmid, Fitschen, Heid, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ng data used to build the SMT system, and hence cannot be produced by our baseline (a standard phrase-based SMT system). We first present our system for dealing with the difficult problem of inflection in German, including the inflection-dependent phenomenon of portmanteaus. Later, after performing an extensive analysis of this system, we will extend it 664 to model compounds, a highly productive phenomenon in German (see Section 8). The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR, a morphological analyzer/generator of German (Schmid et al 2004) and the BitPar parser, which is a state-of-the-art parser of German (Schmid, 2004). 2.1 Issues of inflection prediction In order to ensure coherent German NPs, we model linguistic features of each word in an NP. We model case, gender, and number agreement and whether or not the word is in the scope of a determiner (such as a definite article), which we label in-weak-context (this linguistic feature is necessary to determine the type of inflection of adjectives and other words: strong, weak, mixed). This is a diverse group of features. The number of a German noun can often be determined given ","@endWordPosition":"581","@position":"3918","annotationId":"T13","@startWordPosition":"578","@citStr":"Schmid et al 2004"},{"#tail":"\n","#text":" have developed a two-stage process for predicting fully inflected surface forms. The first stage takes a stem and predicts morphological features for that stem, based on the surrounding context. The aim of the first stage is to take a stem and predict four morphological features: case, gender, number and type of inflection. We experiment with a number of models for doing this. The second stage takes the stems marked with morphological features (predicted in the first stage) and uses a morphological generator to generate the full surface form. For the second stage, a modified version of SMOR (Schmid et al 2004) is used, which, given a stem annotated with morphological features, generates exactly one surface form. We now introduce our first linguistic feature prediction systems, which we call joint sequence models (JSMs). These are standard language models, where the ?word? tokens are not represented as surface forms, but instead using POS and features. In testing, we supply the input as a sequence in underspecified form, where some of the features are specified in the stem markup (for instance, POS=Noun, gender=masculine, number=plural), and then use Viterbi search to find the most probable fully sp","@endWordPosition":"1913","@position":"12055","annotationId":"T14","@startWordPosition":"1910","@citStr":"Schmid et al 2004"}]},"title":{"#tail":"\n","#text":"SMOR: A German Computational Morphology Covering Derivation, Composition, and Inflection."},"booktitle":{"#tail":"\n","#text":"In 4th International Conference on Language Resources and Evaluation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Helmut Schmid"},{"#tail":"\n","#text":"Arne Fitschen"},{"#tail":"\n","#text":"Ulrich Heid"}]}},{"date":{"#tail":"\n","#text":"2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"andard phrase-based SMT system). We first present our system for dealing with the difficult problem of inflection in German, including the inflection-dependent phenomenon of portmanteaus. Later, after performing an extensive analysis of this system, we will extend it 664 to model compounds, a highly productive phenomenon in German (see Section 8). The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR, a morphological analyzer/generator of German (Schmid et al 2004) and the BitPar parser, which is a state-of-the-art parser of German (Schmid, 2004). 2.1 Issues of inflection prediction In order to ensure coherent German NPs, we model linguistic features of each word in an NP. We model case, gender, and number agreement and whether or not the word is in the scope of a determiner (such as a definite article), which we label in-weak-context (this linguistic feature is necessary to determine the type of inflection of adjectives and other words: strong, weak, mixed). This is a diverse group of features. The number of a German noun can often be determined given only the English source word. The gender of a German noun is innate and often diffi","@endWordPosition":"594","@position":"4001","annotationId":"T15","@startWordPosition":"593","@citStr":"Schmid, 2004"}},"title":{"#tail":"\n","#text":"Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Helmut Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors. In Proceedings of Coling 2004, pages 162? 168, Geneva, Switzerland, Aug 23?Aug 27. COLING."},"#text":"\n","pages":{"#tail":"\n","#text":"162--168"},"marker":{"#tail":"\n","#text":"Schmid, 2004"},"publisher":{"#tail":"\n","#text":"COLING."},"location":{"#tail":"\n","#text":"Geneva, Switzerland,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of Coling"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Helmut Schmid"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In International Conference on Spoken Language Processing."},"#text":"\n","marker":{"#tail":"\n","#text":"Stolcke, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"task on translation.7 There are 82,740 parallel sentences from news-commentary09.de-en and 1,418,115 parallel sentences from europarl-v4.de-en. The monolingual data contains 9.8 M sentences.8 To build the baseline, the data was tokenized using the Moses tokenizer and lowercased. We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the ?grow-diag-final-and? heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit (Stolcke, 2002). We run MERT separately for each system. The recaser used is the same for all systems. It is the standard recaser supplied with Moses, trained on all German training data. The dev set is wmt-2009-a and the test set is wmt-2009-b, and we report end-to-end case sensitive BLEU scores against the unmodified reference SGML file. The blind test set used is wmt-2009-blind (all lines). In developing our inflection prediction systems (and making such decisions as n-gram order used), we worked on the so-called ?clean data? task, predicting the inflection on stemmed reference sentences (rather than MT o","@endWordPosition":"2904","@position":"18552","annotationId":"T16","@startWordPosition":"2903","@citStr":"Stolcke, 2002"}},"title":{"#tail":"\n","#text":"SRILM - An Extensible Language Modeling Toolkit."},"booktitle":{"#tail":"\n","#text":"In International Conference on Spoken Language Processing."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Andreas Stolcke"}}},{"date":{"#tail":"\n","#text":"2011"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"3. The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb. In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization. 8.2 Model for Compound Merging After translation, compound parts have to be resynthesized into compounds before inflection. Two decisions have to be taken: i) where to 670 merge and ii) how to merge. Following the work of Stymne and Cancedda (2011), we implement a linear-chain CRF merging system using the following features: stemmed (separated) surface form, part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word+1, word as true prefix, word+1 as true suffix, plus frequency comparisons of these. The CRF is trained on the split monolingual data. It only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al 2006). 8.3 Experiments We evaluated the end-to-end inflection system with the addition of compounds.15 As in the inflection experiments describe","@endWordPosition":"4922","@position":"31082","annotationId":"T17","@startWordPosition":"4919","@citStr":"Stymne and Cancedda (2011)"},{"#tail":"\n","#text":"mance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied. Popovic et al(2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow Stymne and Cancedda (2011), for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets. 10 Conclusion We have shown that both the prediction of surface forms and the prediction of linguistic features are of interest for improving SMT. We have obtained the advantages of both in our CRF framework, and also integrated handling of compounds, and an inflection-dependent word formation phenomenon, portmanteaus. We validated our work on a well-studied large corpora translation t","@endWordPosition":"6099","@position":"38378","annotationId":"T18","@startWordPosition":"6095","@citStr":"Stymne and Cancedda (2011)"}]},"title":{"#tail":"\n","#text":"Productive Generation of Compound Words in Statistical Machine Translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Sara Stymne and Nicola Cancedda. 2011. Productive Generation of Compound Words in Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 250?260, Edinburgh, Scotland UK, July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"250--260"},"marker":{"#tail":"\n","#text":"Stymne, Cancedda, 2011"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Edinburgh, Scotland UK,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixth Workshop on Statistical Machine Translation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sara Stymne"},{"#tail":"\n","#text":"Nicola Cancedda"}]}},{"date":{"#tail":"\n","#text":"2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"08), Yeniterzi and Oflazer (2010) and others. Toutanova et. al.?s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied. Popovic et al(2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow Stymne and Cancedda (2011), for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets. 10 Conclusion We have shown that both the prediction of surface forms and the p","@endWordPosition":"6051","@position":"38069","annotationId":"T19","@startWordPosition":"6050","@citStr":"Stymne (2008)"}},"title":{"#tail":"\n","#text":"German Compounds in Factored Statistical Machine Translation."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Sara Stymne. 2008. German Compounds in Factored Statistical Machine Translation. In Proceedings of GOTAL-08, pages 464?475, Gothenburg, Sweden. Springer Verlag, LNCS/LNAI."},"#text":"\n","pages":{"#tail":"\n","#text":"464--475"},"marker":{"#tail":"\n","#text":"Stymne, 2008"},"publisher":{"#tail":"\n","#text":"Springer Verlag, LNCS/LNAI."},"location":{"#tail":"\n","#text":"Gothenburg,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of GOTAL-08,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Sara Stymne"}}},{"date":{"#tail":"\n","#text":"2008"},"title":{"#tail":"\n","#text":"Applying Morphology Generation Models to Machine Translation."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying Morphology Generation Models to Machine Translation. In Proceedings of ACL-08: HLT, pages 514?522, Columbus, Ohio, June. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"514--522"},"marker":{"#tail":"\n","#text":"Toutanova, Suzuki, Ruopp, 2008"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Columbus, Ohio,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of ACL-08: HLT,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kristina Toutanova"},{"#tail":"\n","#text":"Hisami Suzuki"},{"#tail":"\n","#text":"Achim Ruopp"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz, and Markus Sadeniemi. 2007. Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner. In PROC. OF MT SUMMIT XI, pages 491?498."},"#text":"\n","pages":{"#tail":"\n","#text":"491--498"},"marker":{"#tail":"\n","#text":"Virpioja, Vayrynen, Creutz, Sadeniemi, 2007"},"title":{"#tail":"\n","#text":"Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner."},"booktitle":{"#tail":"\n","#text":"In PROC. OF MT SUMMIT XI,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sami Virpioja"},{"#tail":"\n","#text":"Jaakko J Vayrynen"},{"#tail":"\n","#text":"Mathias Creutz"},{"#tail":"\n","#text":"Markus Sadeniemi"}]}},{"date":{"#tail":"\n","#text":"2011"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"f which deals with both issues is the work of de Gispert and Marin?o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the 671 agreement phenomena that we model. Our CRF framework allows us to use more complex context features. We have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step. The direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation. However, it is reasonable to expect that the use of features (and ","@endWordPosition":"5638","@position":"35456","annotationId":"T20","@startWordPosition":"5635","@citStr":"Williams and Koehn (2011)"}},"title":{"#tail":"\n","#text":"Agreement constraints for statistical machine translation into German."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Philip Williams and Philipp Koehn. 2011. Agreement constraints for statistical machine translation into German. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 217?226, Edinburgh, Scotland, July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"217--226"},"marker":{"#tail":"\n","#text":"Williams, Koehn, 2011"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Edinburgh, Scotland,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Sixth Workshop on Statistical Machine Translation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philip Williams"},{"#tail":"\n","#text":"Philipp Koehn"}]}},{"date":{"#tail":"\n","#text":"2010"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"or dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the impact of using source side information (i.e., feature functions on the aligned English), such as those of Avramidis and Koehn (2008), Yeniterzi and Oflazer (2010) and others. Toutanova et. al.?s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) kn","@endWordPosition":"5960","@position":"37489","annotationId":"T21","@startWordPosition":"5957","@citStr":"Yeniterzi and Oflazer (2010)"}},"title":{"#tail":"\n","#text":"Syntaxto-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish."},"#tail":"\n","institution":{"#tail":"\n","#text":"for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntaxto-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 454?464, Uppsala, Sweden, July. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"454--464"},"marker":{"#tail":"\n","#text":"Yeniterzi, Oflazer, 2010"},"publisher":{"#tail":"\n","#text":"Association"},"location":{"#tail":"\n","#text":"Uppsala, Sweden,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Reyyan Yeniterzi"},{"#tail":"\n","#text":"Kemal Oflazer"}]}}]}}]}}
