{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","figure":[{"#tail":"\n","@confidence":"0.986827511627907","#text":"\nCo\nrpu\ns\nTok\neni\nzat\nion\nPO\nS-t\nagg\ning\nLem\nma\ntiza\ntion\nTer\nm\nwe\nigh\ntin\ng\nWo\nrd-\ncon\ncep\nt\nma\nppi\nng\ncon\ncep\nt p\nairs\nwit\nh g\nlos\nses\nWo\nrd\npai\nr\ngen\nera\ntor\nPre\npro\nces\nsin\ng\nWo\nrd\npai\nr\nfilte\nr\ntf.id\nf\nWo\nrd\nsen\nse\ndic\ntion\nary\nAb\nbre\nvia\ntion\ns\nSto\nplis\nt\noth\ner u\nser\ndef\nine\nd fi\nlter\ns\nPO\nS\ncom\nbin\natio\nns\n"},{"#tail":"\n","@confidence":"0.4477334","#text":"\nCORPUS # DOCS # TOKENS DOMAIN\nBN 9,022 7,728,501\ndescriptions\nof professions\nGIRT 151,319 19,645,417\nabstracts of social\nscience papers\nSPP 106 144,074\nscientific .ppt\npresentations\n"},{"#tail":"\n","@confidence":"0.954080692307692","#text":"\nusing a Fisher Z-value transformation.\n01234 0\n50\n100\n150\n200\n250\n300\n350\nConc\nept p\nair\nSemantic relatedness score\n"},{"#tail":"\n","@confidence":"0.9906048","#text":"\n01234 0\n10\n20\n30\n40\n50\nConc\nept p\nair\nSemantic relatedness scores\n"},{"#tail":"\n","@confidence":"0.994643090909091","#text":"\n00.30.60.91.21.51.8 0\n1\n2\n3\n4\nAver\naged\njudg\nmen\nt\nStandard deviation\n"}],"address":{"#tail":"\n","@confidence":"0.915146","#text":"\nD-64289 Darmstadt, Germany\n"},"author":{"#tail":"\n","@confidence":"0.518821","#text":"\nTorsten Zesch and Iryna Gurevych\n"},"subsectionHeader":[{"#tail":"\n","@confidence":"0.990754","#text":"\n4.1 System architecture\n"},{"#tail":"\n","@confidence":"0.903893","#text":"\n4.2 Experimental setup\n4.2.1 Extraction of concept pairs\n"}],"subsubsectionHeader":{"#tail":"\n","@confidence":"0.951824","#text":"\n4.2.2 Graphical User Interface\n"},"footnote":[{"#tail":"\n","@confidence":"0.905754333333333","#text":"\n1In the near future, we are planning to make the software\navailable to interested researchers.\n2In this paper, word denotes the graphemic form of a to-\n"},{"#tail":"\n","@confidence":"0.989176","#text":"\n7http://berufenet.arbeitsagentur.de\n8l=logarithmic term frequency, t=logarithmic inverse doc-\nument frequency, c=cosine normalization.\n"},{"#tail":"\n","@confidence":"0.96855","#text":"\n9http://www.wikipedia.de\n"}],"@confidence":"0.000000","#tail":"\n","reference":[{"#tail":"\n","@confidence":"0.999597243902439","#text":"\nAlexander Budanitsky and Graeme Hirst. 2006. Evaluating\nWordNet-based Measures of Semantic Distance. Compu-\ntational Linguistics, 32(1).\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud\nRivlin, Zach Solan, and Gadi Wolfman. 2002. Placing\nSearch in Context: The Concept Revisited. ACM Trans-\nactions on Information Systems, 20(1):116?131.\nIryna Gurevych. 2005. Using the Structure of a Conceptual\nNetwork in Computing Semantic Relatedness. In Pro-\nceedings of the 2nd International Joint Conference on Nat-\nural Language Processing, pages 767?778, Jeju Island,\nRepublic of Korea.\nIryna Gurevych. 2006. Computing Semantic Relatedness\nAcross Parts of Speech. Technical report, Darmstadt Uni-\nversity of Technology, Germany, Department of Computer\nScience, Telecooperation.\nJay J. Jiang and David W. Conrath. 1997. Semantic Similar-\nity Based on Corpus Statistics and Lexical Taxonomy. In\nProceedings of the 10th International Conference on Re-\nsearch in Computational Linguistics.\nMichael Kluck. 2004. The GIRT Data in the Evaluation of\nCLIR Systems - from 1997 Until 2003. Lecture Notes in\nComputer Science, 3237:376?390, January.\nClaudia Kunze, 2004. Lexikalisch-semantische Wortnetze,\nchapter Computerlinguistik und Sprachtechnologie, pages\n423?431. Spektrum Akademischer Verlag.\nClaudia Leacock and Martin Chodorow, 1998. WordNet: An\nElectronic Lexical Database, chapter Combining Local\nContext and WordNet Similarity for Word Sense Identi-\nfication, pages 265?283. Cambridge: MIT Press.\nLudovic Lebart and Martin Rajman. 2000. Computing Sim-\nilarity. In Robert Dale, editor, Handbook of NLP. Dekker:\nBasel.\nMichael Lesk. 1986. Automatic Sense Disambiguation Us-\ning Machine Readable Dictionaries: How to tell a pine\ncone from an ice cream cone. In Proceedings of the 5th\nAnnual International Conference on Systems Documenta-\ntion, pages 24?26, Toronto, Ontario, Canada.\nDekang Lin. 1998. An Information-Theoretic Definition of\nSimilarity. In Proceedings of International Conference on\nMachine Learning, Madison, Wisconsin.\n"},{"#tail":"\n","@confidence":"0.999228195652175","#text":"\nRada Mihalcea and Dan Moldovan. 2001. Automatic Gen-\neration of a Coarse Grained WordNet. In Proceedings\nof NAACL Workshop on WordNet and Other Lexical Re-\nsources, Pittsburgh, PA, June.\nGeorge A. Miller and Walter G. Charles. 1991. Contextual\nCorrelates of Semantic Similarity. Language and Cogni-\ntive Processes, 6(1):1?28.\nJane Morris and Graeme Hirst. 2004. Non-Classical Lexical\nSemantic Relations. In Workshop on Computational Lex-\nical Semantics, Human Language Technology Conference\nof the North American Chapter of the ACL, Boston.\nSiddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-\nsen. 2003. Using Measures of Semantic Relatedness\nfor Word Sense Disambiguation. In Proceedings of the\nFourth International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, Mexico City.\nPhilip Resnik. 1995. Using Information Content to Evalu-\nate Semantic Similarity. In Proceedings of the 14th Inter-\nnational Joint Conference on Artificial Intelligence, pages\n448?453, Montreal, Canada.\nHerbert Rubenstein and John B. Goodenough. 1965. Con-\ntextual Correlates of Synonymy. Communications of the\nACM, 8(10):627?633.\nGerard Salton. 1989. Automatic Text Processing: the Trans-\nformation, Analysis, and Retrieval of Information by Com-\nputer. Addison-Wesley Longman Publishing, Boston,\nMA, USA.\nHelmut Schmid. 1995. Probabilistic Part-of-Speech Tagging\nUsing Decision Trees. In International Conference on\nNew Methods in Language Processing, Manchester, UK.\nSabine Schulte im Walde and Alissa Melinger. 2005. Iden-\ntifying Semantic Relations and Functional Properties of\nHuman Verb Associations. In Proceedings of the Joint\nConference on Human Language Technology and Empiri-\ncal Methods in NLP, pages 612?619, Vancouver, Canada.\nSIR Project. 2006. Project ?Semantic Information\nRetrieval?. URL http://www.cre-elearning.\ntu-darmstadt.de/elearning/sir/.\nJulie Weeds and David Weir. 2005. Co-occurrence Retrieval:\nA Flexible Framework For Lexical Distributional Similar-\nity. Computational Linguistics, 31(4):439?475, Decem-\nber.\nZhibiao Wu and Martha Palmer. 1994. Verb Semantics and\nLexical Selection. In 32nd Annual Meeting of the ACL,\npages 133?138, NewMexico State University, Las Cruces,\nNew Mexico.\n"}],"bodyText":[{"#tail":"\n","@confidence":"0.810681333333333","#text":"\nProceedings of the Workshop on Linguistic Distances, pages 16?24,\nSydney, July 2006. c?2006 Association for Computational Linguistics\nAutomatically creating datasets for measures of semantic relatedness\n"},{"#tail":"\n","@confidence":"0.997647","#text":"\nSemantic relatedness is a special form of\nlinguistic distance between words. Eval-\nuating semantic relatedness measures is\nusually performed by comparison with hu-\nman judgments. Previous test datasets had\nbeen created analytically and were limited\nin size. We propose a corpus-based system\nfor automatically creating test datasets.1\nExperiments with human subjects show\nthat the resulting datasets cover all de-\ngrees of relatedness. As a result of the\ncorpus-based approach, test datasets cover\nall types of lexical-semantic relations and\ncontain domain-specific words naturally\noccurring in texts.\n"},{"#tail":"\n","@confidence":"0.9623015","#text":"\nLinguistic distance plays an important role in\nmany applications like information retrieval, word\nsense disambiguation, text summarization or\nspelling correction. It is defined on different kinds\nof textual units, e.g. documents, parts of a docu-\nment (e.g. words and their surrounding context),\nwords or concepts (Lebart and Rajman, 2000).2\nLinguistic distance between words is inverse to\ntheir semantic similarity or relatedness.\nSemantic similarity is typically defined via the\nlexical relations of synonymy (automobile ? car)\nand hypernymy (vehicle ? car), while semantic\nrelatedness (SR) is defined to cover any kind of\nlexical or functional association that may exist be-\n"},{"#tail":"\n","@confidence":"0.977972794871795","#text":"\nken and concept refers to a particular sense of a word.\ntween two words (Gurevych, 2005).3 Dissimilar\nwords can be semantically related, e.g. via func-\ntional relationships (night ? dark) or when they\nare antonyms (high ? low). Many NLP applica-\ntions require knowledge about semantic related-\nness rather than just similarity (Budanitsky and\nHirst, 2006).\nA number of competing approaches for comput-\ning semantic relatedness of words have been de-\nveloped (see Section 2). A commonly accepted\nmethod for evaluating these approaches is to com-\npare their results with a gold standard based on\nhuman judgments on word pairs. For that pur-\npose, relatedness scores for each word pair have\nto be determined experimentally. Creating test\ndatasets for such experiments has so far been a\nlabor-intensive manual process.\nWe propose a corpus-based system to automat-\nically create test datasets for semantic relatedness\nexperiments. Previous datasets were created ana-\nlytically, preventing their use to gain insights into\nthe nature of SR and also not necessarily reflecting\nthe reality found in a corpus. They were also lim-\nited in size. We provide a larger annotated test set\nthat is used to better analyze the connections and\ndifferences between the approaches for computing\nsemantic relatedness.\nThe remainder of this paper is organized as fol-\nlows: we first focus on the notion of semantic re-\nlatedness and how it can be evaluated. Section 3\nreviews related work. Section 4 describes our sys-\ntem for automatically extracting word pairs from a\ncorpus. Furthermore, the experimental setup lead-\ning to human judgments of semantic relatedness\n3Nevertheless the two terms are often (mis)used inter-\nchangeably. We will use semantic relatedness in the remain-\nder of this paper, as it is the more general term that subsumes\nsemantic similarity.\n"},{"#tail":"\n","@confidence":"0.9991525","#text":"\nis presented. Section 5 discusses the results, and\nfinally we draw some conclusions in Section 6.\n"},{"#tail":"\n","@confidence":"0.964675137931034","#text":"\nVarious approaches for computing semantic re-\nlatedness of words or concepts have been pro-\nposed, e.g. dictionary-based (Lesk, 1986),\nontology-based (Wu and Palmer, 1994; Leacock\nand Chodorow, 1998), information-based (Resnik,\n1995; Jiang and Conrath, 1997) or distributional\n(Weeds and Weir, 2005). The knowledge sources\nused for computing relatedness can be as different\nas dictionaries, ontologies or large corpora.\nAccording to Budanitsky and Hirst (2006),\nthere are three prevalent approaches for evaluating\nSR measures: mathematical analysis, application-\nspecific evaluation and comparison with human\njudgments.\nMathematical analysis can assess a measure\nwith respect to some formal properties, e.g.\nwhether a measure is a metric (Lin, 1998).4 How-\never, mathematical analysis cannot tell us whether\na measure closely resembles human judgments or\nwhether it performs best when used in a certain\napplication.\nThe latter question is tackled by application-\nspecific evaluation, where a measure is tested\nwithin the framework of a certain application,\ne.g. word sense disambiguation (Patwardhan et\nal., 2003) or malapropism detection (Budanitsky\nand Hirst, 2006). Lebart and Rajman (2000) ar-\ngue for application-specific evaluation of similar-\nity measures, because measures are always used\nfor some task. But they also note that evaluating\na measure as part of a usually complex applica-\ntion only indirectly assesses its quality. A certain\nmeasure may work well in one application, but not\nin another. Application-based evaluation can only\nstate the fact, but give little explanation about the\nreasons.\nThe remaining approach - comparison with hu-\nman judgments - is best suited for application\nindependent evaluation of relatedness measures.\nHuman annotators are asked to judge the related-\nness of presented word pairs. Results from these\nexperiments are used as a gold standard for eval-\nuation. A further advantage of comparison with\nhuman judgments is the possibility to gain deeper\n4That means, whether it fulfills some mathematical crite-\nria: d(x, y) ? 0; d(x, y) = 0 ? x = y; d(x, y) = d(y, x);\nd(x, z) ? d(x, y) + d(y, z).\ninsights into the nature of semantic relatedness.\nHowever, creating datasets for evaluation has so\nfar been limited in a number of respects. Only\na small number of word pairs was manually se-\nlected, with semantic similarity instead of related-\nness in mind. Word pairs consisted only of noun-\nnoun combinations and only general terms were\nincluded. Polysemous and homonymous words\nwere not disambiguated to concepts, i.e. humans\nannotated semantic relatedness of words rather\nthan concepts.\n"},{"#tail":"\n","@confidence":"0.999150026315789","#text":"\nIn the seminal work by Rubenstein and Goode-\nnough (1965), similarity judgments were obtained\nfrom 51 test subjects on 65 noun pairs written on\npaper cards. Test subjects were instructed to order\nthe cards according to the ?similarity of meaning?\nand then assign a continuous similarity value (0.0 -\n4.0) to each card. Miller and Charles (1991) repli-\ncated the experiment with 38 test subjects judg-\ning on a subset of 30 pairs taken from the original\n65 pairs. This experiment was again replicated by\nResnik (1995) with 10 subjects. Table 1 summa-\nrizes previous experiments.\nA comprehensive evaluation of SR measures re-\nquires a higher number of word pairs. However,\nthe original experimental setup is not scalable as\nordering several hundred paper cards is a cum-\nbersome task. Furthermore, semantic relatedness\nis an intuitive concept and being forced to assign\nfine-grained continuous values is felt to overstrain\nthe test subjects. Gurevych (2005) replicated the\nexperiment of Rubenstein and Goodenough with\nthe original 65 word pairs translated into German.\nShe used an adapted experimental setup where test\nsubjects had to assign discrete values {0,1,2,3,4}\nand word pairs were presented in isolation. This\nsetup is also scalable to a higher number of word\npairs (350) as was shown in Gurevych (2006).\nFinkelstein et al (2002) annotated a larger set of\nword pairs (353), too. They used a 0-10 range of\nrelatedness scores, but did not give further details\nabout their experimental setup. In psycholinguis-\ntics, relatedness of words can also be determined\nthrough association tests (Schulte im Walde and\nMelinger, 2005). Results of such experiments are\nhard to quantify and cannot easily serve as the ba-\nsis for evaluating SR measures.\nRubenstein and Goodenough selected word\npairs analytically to cover the whole spectrum of\n"},{"#tail":"\n","@confidence":"0.994622704918033","#text":"\nsimilarity from ?not similar? to ?synonymous?.\nThis elaborate process is not feasible for a larger\ndataset or if domain-specific test sets should be\ncompiled quickly. Therefore, we automatically\ncreate word pairs using a corpus-based approach.\nWe assume that due to lexical-semantic cohesion,\ntexts contain a sufficient number of words re-\nlated by means of different lexical and semantic\nrelations. Resulting from our corpus-based ap-\nproach, test sets will also contain domain-specific\nterms. Previous studies only included general\nterms as opposed to domain-specific vocabularies\nand therefore failed to produce datasets that can\nbe used to evaluate the ability of a measure to cope\nwith domain-specific or technical terms. This is an\nimportant property if semantic relatedness is used\nin information retrieval where users tend to use\nspecific search terms (Porsche) rather than general\nones (car).\nFurthermore, manually selected word pairs\nare often biased towards highly related pairs\n(Gurevych, 2006), because human annotators tend\nto select only highly related pairs connected by re-\nlations they are aware of. Automatic corpus-based\nselection of word pairs is more objective, leading\nto a balanced dataset with pairs connected by all\nkinds of lexical-semantic relations. Morris and\nHirst (2004) pointed out that many relations be-\ntween words in a text are non-classical (i.e. other\nthan typical taxonomic relations like synonymy or\nhypernymy) and therefore not covered by seman-\ntic similarity.\nPrevious studies only considered semantic re-\nlatedness (or similarity) of words rather than con-\ncepts. However, polysemous or homonymous\nwords should be annotated on the level of con-\ncepts. If we assume that bank has two meanings\n(?financial institution? vs. ?river bank?)5 and it is\npaired with money, the result is two sense quali-\n5WordNet lists 10 meanings.\nfied pairs (bankfinancial ? money) and (bankriver\n? money). It is obvious that the judgments on the\ntwo concept pairs should differ considerably. Con-\ncept annotated datasets can be used to test the abil-\nity of a measure to differentiate between senses\nwhen determining the relatedness of polysemous\nwords. To our knowledge, this study is the first to\ninclude concept pairs and to automatically gener-\nate the test dataset.\nIn our experiment, we annotated a high number\nof pairs similar in size to the test sets by Finkel-\nstein (2002) and Gurevych (2006). We used the re-\nvised experimental setup (Gurevych, 2005), based\non discrete relatedness scores and presentation of\nword pairs in isolation, that is scalable to the\nhigher number of pairs. We annotated semantic\nrelatedness instead of similarity and included also\nnon noun-noun pairs. Additionally, our corpus-\nbased approach includes domain-specific techni-\ncal terms and enables evaluation of the robustness\nof a measure.\n"},{"#tail":"\n","@confidence":"0.9915814375","#text":"\nFigure 1 gives an overview of our automatic\ncorpus-based system for creating test datasets for\nevaluating SR measures.\nIn the first step, a source corpus is preprocessed\nusing tokenization, POS-tagging and lemmatiza-\ntion resulting in a list of POS-tagged lemmas.\nRandomly generating word pairs from this list\nwould result in too many unrelated pairs, yielding\nan unbalanced dataset. Thus, we assign weights to\neach word (e.g. using tf.idf-weighting). The most\nimportant document-specific words get the high-\nest weights and due to lexical cohesion of the doc-\numents many related words can be found among\nthe top rated. Therefore, we randomly generate\na user-defined number of word pairs from the r\nwords with the highest weights for each document.\n"},{"#tail":"\n","@confidence":"0.996802428571429","#text":"\nIn the next step, user defined filters are applied\nto the initial list of word pairs. For example, a fil-\nter can remove all pairs containing only uppercase\nletters (mostly acronyms). Another filter can en-\nforce a certain fraction of POS combinations to be\npresent in the result set.\nAs we want to obtain judgment scores for se-\nmantic relatedness of concepts instead of words,\nwe have to include all word sense combinations of\na pair in the list. An external dictionary of word\nsenses is necessary for this step. It is also used to\nadd a gloss for each word sense that enables test\nsubjects to distinguish between senses.\nIf differences in meaning between senses are\nvery fine-grained, distinguishing between them is\nhard even for humans (Mihalcea and Moldovan,\n2001).6 Pairs containing such words are not suit-\nable for evaluation. To limit their impact on the\nexperiment, a threshold for the maximal number\nof senses can be defined. Words with a number of\nsenses above the threshold are removed from the\nlist.\nThe result of the extraction process is a list of\nsense disambiguated, POS-tagged pairs of con-\ncepts.\n6E.g. the German verb ?halten? that can be translated as\nhold, maintain, present, sustain, etc. has 26 senses in Ger-\nmaNet.\n"},{"#tail":"\n","@confidence":"0.989258369565217","#text":"\nWe extracted word pairs from three different\ndomain-specific corpora (see Table 2). This is\nmotivated by the aim to enable research in infor-\nmation retrieval incorporating SR measures. In\nparticular, the ?Semantic Information Retrieval?\nproject (SIR Project, 2006) systematically investi-\ngates the use of lexical-semantic relations between\nwords or concepts for improving the performance\nof information retrieval systems.\nThe BERUFEnet (BN) corpus7 consists of de-\nscriptions of 5,800 professions in Germany and\ntherefore contains many terms specific to profes-\nsional training. Evaluating semantic relatedness\non a test set based on this corpus may reveal the\nability of a measure to adapt to a very special do-\nmain. The GIRT (German Indexing and Retrieval\nTestdatabase) corpus (Kluck, 2004) is a collec-\ntion of abstracts of social science papers. It is a\nstandard corpus for evaluating German informa-\ntion retrieval systems. The third corpus is com-\npiled from 106 arbitrarily selected scientific Pow-\nerPoint presentations (SPP). They cover a wide\nrange of topics from bio genetics to computer sci-\nence and contain many technical terms. Due to\nthe special structure of presentations, this corpus\nwill be particularly demanding with respect to the\nrequired preprocessing components of an informa-\ntion retrieval system.\nThe three preprocessing steps (tokenization,\nPOS-tagging, lemmatization) are performed us-\ning TreeTagger (Schmid, 1995). The resulting\nlist of POS-tagged lemmas is weighted using the\nSMART ?ltc?8 tf.idf-weighting scheme (Salton,\n1989).\nWe implemented a set of filters for word pairs.\nOne group of filters removed unwanted word\npairs. Word pairs are filtered if they contain at\nleast one word that a) has less than three letters b)\ncontains only uppercase letters (mostly acronyms)\nor c) can be found in a stoplist. Another fil-\nter enforced a specified fraction of combinations\nof nouns (N), verbs (V) and adjectives (A) to be\npresent in the result set. We used the following pa-\nrameters: NN = 0.5, NV = 0.15, NA = 0.15,\nV V = 0.1, V A = 0.05, AA = 0.05. That means\n50% of the resulting word pairs for each corpus\n"},{"#tail":"\n","@confidence":"0.985148653846154","#text":"\nwere noun-noun pairs, 15% noun-verb pairs and\nso on.\nWord pairs containing polysemous words\nare expanded to concept pairs using Ger-\nmaNet (Kunze, 2004), the German equivalent to\nWordNet, as a sense inventory for each word. It\nis the most complete resource of this type for Ger-\nman.\nGermaNet contains only a few conceptual\nglosses. As they are required to enable test sub-\njects to distinguish between senses, we use artifi-\ncial glosses composed from synonyms and hyper-\nnyms as a surrogate, e.g. for brother: ?brother,\nmale sibling? vs. ?brother, comrade, friend?\n(Gurevych, 2005). We removed words which had\nmore than three senses.\nMarginal manual post-processing was neces-\nsary, since the lemmatization process introduced\nsome errors. Foreign words were translated into\nGerman, unless they are common technical termi-\nnology. We initially selected 100 word pairs from\neach corpus. 11 word pairs were removed be-\ncause they comprised non-words. Expanding the\nword list to a concept list increased the size of the\nlist. Thus, the final dataset contained 328 automat-\nically created concept pairs.\n"},{"#tail":"\n","@confidence":"0.974308142857143","#text":"\nWe developed a web-based interface to obtain\nhuman judgments of semantic relatedness for each\nautomatically generated concept pair. Test sub-\njects were invited via email to participate in the\nexperiment. Thus, they were not supervised dur-\ning the experiment.\nGurevych (2006) observed that some annotators\nwere not familiar with the exact definition of se-\nmantic relatedness. Their results differed particu-\nlarly in cases of antonymy or distributionally re-\nlated pairs. We created a manual with a detailed\nintroduction to SR stressing the crucial points.\nThe manual was presented to the subjects before\nthe experiment and could be re-accessed at any\n"},{"#tail":"\n","@confidence":"0.964133038461539","#text":"\nwords are defined by means of synonyms and re-\nlated words.\nDuring the experiment, one concept pair at a\ntime was presented to the test subjects in random\nordering. Subjects had to assign a discrete related-\nness value {0,1,2,3,4} to each pair. Figure 2 shows\nthe system?s GUI.\nIn case of a polysemous word, synonyms or\nrelated words were presented to enable test sub-\njects to understand the sense of a presented con-\ncept. Because this additional information can lead\nto undesirable priming effects, test subjects were\ninstructed to deliberately decide only about the re-\nlatedness of a concept pair and use the gloss solely\nto understand the sense of the presented concept.\nSince our corpus-based approach includes\ndomain-specific vocabulary, we could not assume\nthat the subjects were familiar with all words.\nThus, they were instructed to look up unknown\nwords in the German Wikipedia.9\nSeveral test subjects were asked to repeat the\nexperiment with a minimum break of one day. Re-\nsults from the repetition can be used to measure\nintra-subject correlation. They can also be used\nto obtain some hints on varying difficulty of judg-\nment for special concept pairs or parts-of-speech.\n"},{"#tail":"\n","@confidence":"0.951322875","#text":"\n21 test subjects (13 males, 8 females) participated\nin the experiment, two of them repeated it. The\naverage age of the subjects was 26 years. Most\nsubjects had an IT background. The experiment\ntook 39 minutes on average, leaving about 7 sec-\nonds for rating each concept pair.\nThe summarized inter-subject correlation be-\ntween 21 subjects was r=.478 (cf. Table 3), which\n"},{"#tail":"\n","@confidence":"0.968576692307692","#text":"\nall pairs, grouped by corpus and grouped by POS\ncombinations.\nis statistically significant at p < .05. This correla-\ntion coefficient is an upper bound of performance\nfor automatic SR measures applied on the same\ndataset.\nResnik (1995) reported a correlation of\nr=.9026.10 The results are not directly compara-\nble, because he only used noun-noun pairs, words\ninstead of concepts, a much smaller dataset, and\nmeasured semantic similarity instead of semantic\nrelatedness. Finkelstein et al (2002) did not\nreport inter-subject correlation for their larger\ndataset. Gurevych (2006) reported a correlation\nof r=.69. Test subjects were trained students of\ncomputational linguistics, and word pairs were\nselected analytically.\nEvaluating the influence of using concept pairs\ninstead of word pairs is complicated because word\nlevel judgments are not directly available. There-\nfore, we computed a lower and an upper bound\nfor correlation coefficients. For the lower bound,\nwe always selected the concept pair with highest\nstandard deviation from each set of corresponding\nconcept pairs. The upper bound is computed by\nselecting the concept pair with the lowest standard\ndeviation. The differences between correlation co-\nefficient for concepts and words are not signifi-\ncant. Table 3 shows only the lower bounds.\nCorrelation coefficients for experiments mea-\nsuring semantic relatedness are expected to be\nlower than results for semantic similarity, since the\nformer also includes additional relations (like co-\noccurrence of words) and is thus a more compli-\ncated task. Judgments for such relations strongly\ndepend on experience and cultural background of\nthe test subjects. While most people may agree\n10Note that Resnik used the averaged correlation coeffi-\ncient. We computed the summarized correlation coefficient\n"},{"#tail":"\n","@confidence":"0.99747936","#text":"\nthat (car ? vehicle) are highly related, a strong\nconnection between (parts ? speech) may only be\nestablished by a certain group. Due to the corpus-\nbased approach, many domain-specific concept\npairs are introduced into the test set. Therefore,\ninter-subject correlation is lower than the results\nobtained by Gurevych (2006).\nIn our experiment, intra-subject correlation was\nr=.670 for the first and r=.623 for the second in-\ndividual who repeated the experiment, yielding\na summarized intra-subject correlation of r=.647.\nRubenstein and Goodenough (1965) reported an\nintra-subject correlation of r=.85 for 15 subjects\njudging the similarity of a subset (36) of the orig-\ninal 65 word pairs. The values may again not be\ncompared directly. Furthermore, we cannot gen-\neralize from these results, because the number of\nparticipants which repeated our experiment was\ntoo low.\nThe distribution of averaged human judgments\non the whole test set (see Figure 3) is almost bal-\nanced with a slight underrepresentation of highly\nrelated concepts. To create more highly re-\nlated concept pairs, more sophisticated weighting\nschemes or selection on the basis of lexical chain-\n"},{"#tail":"\n","@confidence":"0.997321770491803","#text":"\nobserved for low or high judgments.\ning could be used. However, even with the present\nsetup, automatic extraction of concept pairs per-\nforms remarkably well and can be used to quickly\ncreate balanced test datasets.\nBudanitsky and Hirst (2006) pointed out that\ndistribution plots of judgments for the word pairs\nused by Rubenstein and Goodenough display an\nempty horizontal band that could be used to sepa-\nrate related and unrelated pairs. This empty band\nis not observed here. However, Figure 4 shows the\ndistribution of averaged judgments with the high-\nest agreement between annotators (standard devi-\nation < 0.8). The plot clearly shows an empty hor-\nizontal band with no judgments. The connection\nbetween averaged judgments and standard devia-\ntion is plotted in Figure 5.\nWhen analyzing the concept pairs with lowest\ndeviation there is a clear tendency for particularly\nhighly related pairs, e.g. hypernymy: Universit?t\n? Bildungseinrichtung (university ? educational\ninstitution); functional relation: T?tigkeit ? aus-\nf?hren (task ? perform); or pairs that are obviously\nnot connected, e.g. logisch ? Juni (logical ? June).\nTable 4 lists some example concept pairs along\nwith averaged judgments and standard deviation.\nConcept pairs with high deviations between\njudgments often contain polysemous words. For\nexample, Quelle (source) was disambiguated to\nWasserquelle (spring) and paired with Text\n(text). The data shows a clear distinction be-\ntween one group that rated the pair low (0) and\nanother group that rated the pair high (3 or 4). The\nlatter group obviously missed the point that tex-\ntual source was not an option here. High devia-\ntions were also common among special technical\nterms like (Mips ?Core), proper names (Georg ?\nAugust ? two common first names in German) or\nfunctionally related pairs (agieren ? mobil). Hu-\nman experience and cultural background clearly\ninfluence the judgment of such pairs.\nThe effect observed here and the effect noted\nby Budanitsky and Hirst is probably caused by the\nsame underlying principle. Human agreement on\nsemantic relatedness is only reliable if two words\nor concepts are highly related or almost unrelated.\nIntuitively, this means that classifying word pairs\nas related or unrelated is much easier than numeri-\ncally rating semantic relatedness. For an informa-\ntion retrieval task, such a classification might be\nsufficient.\nDifferences in correlation coefficients for the\nthree corpora are not significant indicating that the\nphenomenon is not domain-specific. Differences\nin correlation coefficients for different parts-of-\nspeech are significant (see Table 3). Verb-verb and\nverb-adjective pairs have the lowest correlation.\nA high fraction of these pairs is in the problem-\natic medium relatedness area. Adjective-adjective\npairs have the highest correlation. Most of these\npairs are either highly related or not related at all.\n"},{"#tail":"\n","@confidence":"0.999897769230769","#text":"\nWe proposed a system for automatically creating\ndatasets for evaluating semantic relatedness mea-\nsures. We have shown that our corpus-based ap-\nproach enables fast development of large domain-\nspecific datasets that cover all types of lexical and\nsemantic relations. We conducted an experiment\nto obtain human judgments of semantic related-\nness on concept pairs. Results show that averaged\nhuman judgments cover all degrees of relatedness\nwith a slight underrepresentation of highly related\nconcept pairs. More highly related concept pairs\ncould be generated by using more sophisticated\nweighting schemes or selecting concept pairs on\nthe basis of lexical chaining.\nInter-subject correlation in this experiment is\nlower than the results from previous studies due\nto several reasons. We measured semantic relat-\nedness instead of semantic similarity. The for-\nmer is a more complicated task for annotators be-\ncause its definition includes all kinds of lexical-\nsemantic relations not just synonymy. In addition,\nconcept pairs were automatically selected elimi-\nnating the bias towards strong classical relations\nwith high agreement that is introduced into the\ndataset by a manual selection process. Further-\nmore, our dataset contains many domain-specific\n"},{"#tail":"\n","@confidence":"0.908716666666667","#text":"\nUniversit?t ? Bildungseinrichtung university ? educational institution GIRT 3.90 0.30\nT?tigkeit ? ausf?hren task ? to perform BN 3.67 0.58\nstrafen ? Paragraph to punish ? paragraph GIRT 3.00 1.18\nQuelle ? Text spring ? text GIRT 2.43 1.57\nMips ? Core mips ? core SPP 2.10 1.55\nelektronisch ? neu electronic ? new GIRT 1.71 1.15\nverarbeiten ? dichten to manipulate ? to caulk BN 1.29 1.42\nLeopold ? Institut Leopold ? institute SPP 0.81 1.25\nOutfit ? Strom outfit ? electricity GIRT 0.24 0.44\n"},{"#tail":"\n","@confidence":"0.983323033333333","#text":"\nlisted for polysemous words. Conceptual glosses are omitted due to space limitations.\nconcept pairs which have been rated very differ-\nently by test subjects depending on their expe-\nrience. Future experiments should ensure that\ndomain-specific pairs are judged by domain ex-\nperts to reduce disagreement between annotators\ncaused by varying degrees of familiarity with the\ndomain.\nAn analysis of the data shows that test sub-\njects more often agreed on highly related or unre-\nlated concept pairs, while they often disagreed on\npairs with a medium relatedness value. This result\nraises the question whether human judgments of\nsemantic relatedness with medium scores are re-\nliable and should be used for evaluating seman-\ntic relatedness measures. We plan to investigate\nthe impact of this outcome on the evaluation of\nsemantic relatedness measures. Additionally, for\nsome applications like information retrieval it may\nbe sufficient to detect highly related pairs rather\nthan accurately rating word pairs with medium\nvalues.\nThere is also a significant difference between\nthe correlation coefficient for different POS com-\nbinations. Further investigations are needed to elu-\ncidate whether these differences are caused by the\nnew procedure for corpus-based selection of word\npairs proposed in this paper or are due to inherent\nproperties of semantic relations existing between\nword classes.\n"},{"#tail":"\n","@confidence":"0.990358428571429","#text":"\nWe would like to thank Sabine Schulte im Walde\nfor her remarks on experimental setups. We are\ngrateful to the Bundesagentur f?r Arbeit for pro-\nviding the BERUFEnet corpus. This work was\ncarried out as part of the ?Semantic Information\nRetrieval? (SIR) project funded by the German\nResearch Foundation.\n"}],"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.952884","#text":"\nDepartment of Telecooperation\nDarmstadt University of Technology\n"},"sectionHeader":[{"#tail":"\n","@confidence":"0.975798","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.997502","@genericHeader":"keywords","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.97142","@genericHeader":"introduction","#text":"\n2 Evaluating SR measures\n"},{"#tail":"\n","@confidence":"0.999669","@genericHeader":"method","#text":"\n3 Related work\n"},{"#tail":"\n","@confidence":"0.67912","@genericHeader":"method","#text":"\nCORRELATION\nPAPER LANGUAGE PAIRS POS REL-TYPE SCORES # SUBJECTS INTER INTRA\n"},{"#tail":"\n","@confidence":"0.992334","@genericHeader":"method","#text":"\n4 Experiment\n"},{"#tail":"\n","@confidence":"0.997718","@genericHeader":"method","#text":"\n5 Results and discussion\n"},{"#tail":"\n","@confidence":"0.614509","@genericHeader":"method","#text":"\nCONCEPTS WORDS\nINTER INTRA INTER INTRA\n"},{"#tail":"\n","@confidence":"0.997875","@genericHeader":"method","#text":"\n6 Conclusion\n"},{"#tail":"\n","@confidence":"0.9652595","@genericHeader":"method","#text":"\nPAIR\nGERMAN ENGLISH CORPUS AVG ST-DEV\n"},{"#tail":"\n","@confidence":"0.978407","@genericHeader":"acknowledgments","#text":"\nAcknowledgments\n"},{"#tail":"\n","@confidence":"0.985108","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.999562","#text":"\nTable 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and\n"},{"#tail":"\n","@confidence":"0.99346","#text":"\nTable 2: Corpus statistics.\n"},{"#tail":"\n","@confidence":"0.997096","#text":"\nTable 3: Summarized correlation coefficients for\n"},{"#tail":"\n","@confidence":"0.8509705","#text":"\nlogisch ? Juni logical ? June SPP 0.14 0.48\nTable 4: Example concept pairs with averaged judgments and standard deviation. Only one sense is\n"}],"page":[{"#tail":"\n","@confidence":"0.997031","#text":"\n16\n"},{"#tail":"\n","@confidence":"0.999414","#text":"\n17\n"},{"#tail":"\n","@confidence":"0.907192","#text":"\n18\n"},{"#tail":"\n","@confidence":"0.980838","#text":"\n19\n"},{"#tail":"\n","@confidence":"0.975272","#text":"\n20\n"},{"#tail":"\n","@confidence":"0.960777","#text":"\n21\n"},{"#tail":"\n","@confidence":"0.988108","#text":"\n22\n"},{"#tail":"\n","@confidence":"0.976653","#text":"\n23\n"},{"#tail":"\n","@confidence":"0.998458","#text":"\n24\n"}],"keyword":{"#tail":"\n","@confidence":"0.480633","#text":"\nCharles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych\n"},"figureCaption":[{"#tail":"\n","@confidence":"0.8759425","#text":"\nFigure 1: System architecture for extraction of\nconcept pairs.\n"},{"#tail":"\n","@confidence":"0.8690095","#text":"\ntime.\nFigure 2: Screenshot of the GUI. Polysemous\n"},{"#tail":"\n","@confidence":"0.9820885","#text":"\nFigure 3: Distribution of averaged human judg-\nments.\n"},{"#tail":"\n","@confidence":"0.967368","#text":"\nFigure 4: Distribution of averaged human judg-\nments with standard deviation < 0.8.\n"},{"#tail":"\n","@confidence":"0.8346655","#text":"\nFigure 5: Averaged judgments and standard devia-\ntion for all concept pairs. Low deviations are only\n"}],"table":[{"#tail":"\n","@confidence":"0.878616285714286","#text":"\nR/G (1965) English 65 N sim continuous 0?4 51 - .850\nM/C (1991) English 30 N sim continuous 0?4 38 - -\nRes (1995) English 30 N sim continuous 0?4 10 .903 -\nFin (2002) English 353 N, V, A relat continuous 0?10 16 - -\nGur (2005) German 65 N sim discrete {0,1,2,3,4} 24 .810 -\nGur (2006) German 350 N, V, A relat discrete {0,1,2,3,4} 8 .690 -\nZ/G (2006) German 328 N, V, A relat discrete {0,1,2,3,4} 21 .478 .647\n"},{"#tail":"\n","@confidence":"0.9998791","#text":"\nall .478 .647 .490 .675\nBN .469 .695 .501 .718\nGIRT .451 .598 .463 .625\nSPP .535 .649 .523 .679\nAA .556 .890 .597 .887\nNA .547 .773 .511 .758\nNV .510 .658 .540 .647\nNN .463 .620 .476 .661\nVA .317 .318 .391 .212\nVV .278 .494 .301 .476\n"}],"email":{"#tail":"\n","@confidence":"0.30854","#text":"\n{zesch,gurevych} (at) tk.informatik.tu-darmstadt.de\n"}}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.811366","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.931271","#text":"Proceedings of the Workshop on Linguistic Distances, pages 16?24, Sydney, July 2006. c?2006 Association for Computational Linguistics"},"address":{"#tail":"\n","@confidence":"0.999983","#text":"D-64289 Darmstadt, Germany"},"#text":"\n","affiliation":{"#tail":"\n","@confidence":"0.999942","#text":"Department of Telecooperation Darmstadt University of Technology"},"author":[{"#tail":"\n","@confidence":"0.996788","#text":"Torsten Zesch"},{"#tail":"\n","@confidence":"0.996788","#text":"Iryna Gurevych"}],"abstract":{"#tail":"\n","@confidence":"0.998932","#text":"Semantic relatedness is a special form of linguistic distance between words. Evaluating semantic relatedness measures is usually performed by comparison with human judgments. Previous test datasets had been created analytically and were limited in size. We propose a corpus-based system for automatically creating test datasets.1 Experiments with human subjects show that the resulting datasets cover all degrees of relatedness. As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts."},"title":{"#tail":"\n","@confidence":"0.965035","#text":"Automatically creating datasets for measures of semantic relatedness"},"email":[{"#tail":"\n","@confidence":"0.993965","#text":"zesch(at)tk.informatik.tu-darmstadt.de"},{"#tail":"\n","@confidence":"0.993965","#text":"gurevych(at)tk.informatik.tu-darmstadt.de"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"volume":{"#tail":"\n","#text":"32"},"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"rawString":{"#tail":"\n","#text":"Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based Measures of Semantic Distance. Computational Linguistics, 32(1)."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Budanitsky, Hirst, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"icle ? car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be1In the near future, we are planning to make the software available to interested researchers. 2In this paper, word denotes the graphemic form of a token and concept refers to a particular sense of a word. tween two words (Gurevych, 2005).3 Dissimilar words can be semantically related, e.g. via functional relationships (night ? dark) or when they are antonyms (high ? low). Many NLP applications require knowledge about semantic relatedness rather than just similarity (Budanitsky and Hirst, 2006). A number of competing approaches for computing semantic relatedness of words have been developed (see Section 2). A commonly accepted method for evaluating these approaches is to compare their results with a gold standard based on human judgments on word pairs. For that purpose, relatedness scores for each word pair have to be determined experimentally. Creating test datasets for such experiments has so far been a labor-intensive manual process. We propose a corpus-based system to automatically create test datasets for semantic relatedness experiments. Previous datasets were created analytic","@endWordPosition":"311","@position":"2172","annotationId":"T1","@startWordPosition":"308","@citStr":"Budanitsky and Hirst, 2006"},{"#tail":"\n","#text":"eral term that subsumes semantic similarity. 16 is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word","@endWordPosition":"632","@position":"4216","annotationId":"T2","@startWordPosition":"629","@citStr":"Budanitsky and Hirst (2006)"},{"#tail":"\n","#text":"igure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain21 00.30.60.91.21.51.8 0 1 2 3 4 Aver aged judg men t Standard deviation Figure 5: Averaged judgments and standard deviation for all concept pairs. Low deviations are only observed for low or high judgments. ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. Budanitsky and Hirst (2006) pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs. This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8). The plot clearly shows an empty horizontal band with no judgments. The connection between averaged judgments and standard deviation is plotted in Figure 5. When analyzing the concept pairs with lowest deviation there is a clear t","@endWordPosition":"3824","@position":"23982","annotationId":"T3","@startWordPosition":"3821","@citStr":"Budanitsky and Hirst (2006)"}]},"title":{"#tail":"\n","#text":"Evaluating WordNet-based Measures of Semantic Distance."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Alexander Budanitsky"},{"#tail":"\n","#text":"Graeme Hirst"}]}},{"volume":{"#tail":"\n","#text":"20"},"#tail":"\n","date":{"#tail":"\n","#text":"2002"},"rawString":{"#tail":"\n","#text":"Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, and Gadi Wolfman. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116?131."},"journal":{"#tail":"\n","#text":"ACM Transactions on Information Systems,"},"#text":"\n","issue":{"#tail":"\n","#text":"1"},"marker":{"#tail":"\n","#text":"Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, 2002"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005). Results of such experiments are hard to quantify and cannot easily serve as the basis for evaluating SR measures. Rubenstein and Goodenough selected word pairs analytically to cover the whole spectrum of 17 CORRELATION PAPER LANGUAGE PAIRS POS REL-TYPE SCORES # SUBJECTS INTER INTRA R/G (1965) English 65 N s","@endWordPosition":"1188","@position":"7702","annotationId":"T4","@startWordPosition":"1185","@citStr":"Finkelstein et al (2002)"},{"#tail":"\n","#text":".658 .540 .647 NN .463 .620 .476 .661 VA .317 .318 .391 .212 VV .278 .494 .301 .476 Table 3: Summarized correlation coefficients for all pairs, grouped by corpus and grouped by POS combinations. is statistically significant at p < .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=.9026.10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically. Evaluating the influence of using concept pairs instead of word pairs is complicated because word level judgments are not directly available. Therefore, we computed a lower and an upper bound for correlation coefficients. For the lower bound, we always selected the concept pair with highest standard deviation from each set of corresponding concept pairs. The upper bound i","@endWordPosition":"3326","@position":"20796","annotationId":"T5","@startWordPosition":"3323","@citStr":"Finkelstein et al (2002)"}]},"title":{"#tail":"\n","#text":"Placing Search in Context: The Concept Revisited."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Lev Finkelstein"},{"#tail":"\n","#text":"Evgeniy Gabrilovich"},{"#tail":"\n","#text":"Yossi Matias"},{"#tail":"\n","#text":"Ehud Rivlin"},{"#tail":"\n","#text":"Zach Solan"},{"#tail":"\n","#text":"Gadi Wolfman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Iryna Gurevych. 2005. Using the Structure of a Conceptual Network in Computing Semantic Relatedness. In Proceedings of the 2nd International Joint Conference on Natural Language Processing, pages 767?778, Jeju Island, Republic of Korea."},"#text":"\n","pages":{"#tail":"\n","#text":"767--778"},"marker":{"#tail":"\n","#text":"Gurevych, 2005"},"location":{"#tail":"\n","#text":"Jeju Island, Republic of"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ds or concepts (Lebart and Rajman, 2000).2 Linguistic distance between words is inverse to their semantic similarity or relatedness. Semantic similarity is typically defined via the lexical relations of synonymy (automobile ? car) and hypernymy (vehicle ? car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be1In the near future, we are planning to make the software available to interested researchers. 2In this paper, word denotes the graphemic form of a token and concept refers to a particular sense of a word. tween two words (Gurevych, 2005).3 Dissimilar words can be semantically related, e.g. via functional relationships (night ? dark) or when they are antonyms (high ? low). Many NLP applications require knowledge about semantic relatedness rather than just similarity (Budanitsky and Hirst, 2006). A number of competing approaches for computing semantic relatedness of words have been developed (see Section 2). A commonly accepted method for evaluating these approaches is to compare their results with a gold standard based on human judgments on word pairs. For that purpose, relatedness scores for each word pair have to be determin","@endWordPosition":"271","@position":"1911","annotationId":"T6","@startWordPosition":"270","@citStr":"Gurevych, 2005"},{"#tail":"\n","#text":"Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects. Table 1 summarizes previous experiments. A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be det","@endWordPosition":"1127","@position":"7321","annotationId":"T7","@startWordPosition":"1126","@citStr":"Gurevych (2005)"},{"#tail":"\n","#text":"meanings. fied pairs (bankfinancial ? money) and (bankriver ? money). It is obvious that the judgments on the two concept pairs should differ considerably. Concept annotated datasets can be used to test the ability of a measure to differentiate between senses when determining the relatedness of polysemous words. To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset. In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006). We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. We annotated semantic relatedness instead of similarity and included also non noun-noun pairs. Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure. 4 Experiment 4.1 System architecture Figure 1 gives an overview of our automatic corpus-based system for creating test datasets for evaluating SR measures. In the first step, a source corpus is preprocessed using tokenization, POS-tagging","@endWordPosition":"1764","@position":"11309","annotationId":"T8","@startWordPosition":"1763","@citStr":"Gurevych, 2005"},{"#tail":"\n","#text":"presentations Table 2: Corpus statistics. were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet (Kunze, 2004), the German equivalent to WordNet, as a sense inventory for each word. It is the most complete resource of this type for German. GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e.g. for brother: ?brother, male sibling? vs. ?brother, comrade, friend? (Gurevych, 2005). We removed words which had more than three senses. Marginal manual post-processing was necessary, since the lemmatization process introduced some errors. Foreign words were translated into German, unless they are common technical terminology. We initially selected 100 word pairs from each corpus. 11 word pairs were removed because they comprised non-words. Expanding the word list to a concept list increased the size of the list. Thus, the final dataset contained 328 automatically created concept pairs. 4.2.2 Graphical User Interface We developed a web-based interface to obtain human judgment","@endWordPosition":"2735","@position":"17153","annotationId":"T9","@startWordPosition":"2734","@citStr":"Gurevych, 2005"}]},"title":{"#tail":"\n","#text":"Using the Structure of a Conceptual Network in Computing Semantic Relatedness."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 2nd International Joint Conference on Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Iryna Gurevych"}}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Technical report,"},"date":{"#tail":"\n","#text":"2006"},"institution":{"#tail":"\n","#text":"Darmstadt University of Technology, Germany, Department of Computer Science, Telecooperation."},"rawString":{"#tail":"\n","#text":"Iryna Gurevych. 2006. Computing Semantic Relatedness Across Parts of Speech. Technical report, Darmstadt University of Technology, Germany, Department of Computer Science, Telecooperation."},"#text":"\n","marker":{"#tail":"\n","#text":"Gurevych, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"tal setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005). Results of such experiments are hard to quantify and cannot easily serve as the basis for evaluating SR measures. Rubenstein and Goodenough selected word pairs analytically to cover the whole spectrum of 17 CORRELATION PAPER LANGUAGE PAIRS POS REL-TYPE SCORES # SUBJECTS INTER INTRA","@endWordPosition":"1184","@position":"7676","annotationId":"T10","@startWordPosition":"1183","@citStr":"Gurevych (2006)"},{"#tail":"\n","#text":"relations. Resulting from our corpus-based approach, test sets will also contain domain-specific terms. Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms. This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of. Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations. Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e. other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity. Previous studies only considered semantic relatedness (or similarity) of words rather than concepts. However","@endWordPosition":"1528","@position":"9855","annotationId":"T11","@startWordPosition":"1527","@citStr":"Gurevych, 2006"},{"#tail":"\n","#text":"th money, the result is two sense quali5WordNet lists 10 meanings. fied pairs (bankfinancial ? money) and (bankriver ? money). It is obvious that the judgments on the two concept pairs should differ considerably. Concept annotated datasets can be used to test the ability of a measure to differentiate between senses when determining the relatedness of polysemous words. To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset. In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006). We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. We annotated semantic relatedness instead of similarity and included also non noun-noun pairs. Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure. 4 Experiment 4.1 System architecture Figure 1 gives an overview of our automatic corpus-based system for creating test datasets for evaluating SR measures. In the first step, a sour","@endWordPosition":"1755","@position":"11252","annotationId":"T12","@startWordPosition":"1754","@citStr":"Gurevych (2006)"},{"#tail":"\n","#text":" unless they are common technical terminology. We initially selected 100 word pairs from each corpus. 11 word pairs were removed because they comprised non-words. Expanding the word list to a concept list increased the size of the list. Thus, the final dataset contained 328 automatically created concept pairs. 4.2.2 Graphical User Interface We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair. Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. Gurevych (2006) observed that some annotators were not familiar with the exact definition of semantic relatedness. Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and could be re-accessed at any time. Figure 2: Screenshot of the GUI. Polysemous words are defined by means of synonyms and related words. During the experiment, one concept pair at a time was presented to the test subjects in random ordering. Subjects had to","@endWordPosition":"2859","@position":"17966","annotationId":"T13","@startWordPosition":"2858","@citStr":"Gurevych (2006)"},{"#tail":"\n","#text":" Summarized correlation coefficients for all pairs, grouped by corpus and grouped by POS combinations. is statistically significant at p < .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=.9026.10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically. Evaluating the influence of using concept pairs instead of word pairs is complicated because word level judgments are not directly available. Therefore, we computed a lower and an upper bound for correlation coefficients. For the lower bound, we always selected the concept pair with highest standard deviation from each set of corresponding concept pairs. The upper bound is computed by selecting the concept pair with the lowest standard deviation. The di","@endWordPosition":"3337","@position":"20879","annotationId":"T14","@startWordPosition":"3336","@citStr":"Gurevych (2006)"},{"#tail":"\n","#text":"transformation. 01234 0 50 100 150 200 250 300 350 Conc ept p air Semantic relatedness score Figure 3: Distribution of averaged human judgments. 01234 0 10 20 30 40 50 Conc ept p air Semantic relatedness scores Figure 4: Distribution of averaged human judgments with standard deviation < 0.8. that (car ? vehicle) are highly related, a strong connection between (parts ? speech) may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore, inter-subject correlation is lower than the results obtained by Gurevych (2006). In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. Rubenstein and Goodenough (1965) reported an intra-subject correlation of r=.85 for 15 subjects judging the similarity of a subset (36) of the original 65 word pairs. The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low. The distribution of averaged human judgments on the whole","@endWordPosition":"3626","@position":"22739","annotationId":"T15","@startWordPosition":"3625","@citStr":"Gurevych (2006)"}]},"title":{"#tail":"\n","#text":"Computing Semantic Relatedness Across Parts of Speech."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Iryna Gurevych"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Jay J. Jiang and David W. Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics."},"#text":"\n","marker":{"#tail":"\n","#text":"Jiang, Conrath, 1997"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ding to human judgments of semantic relatedness 3Nevertheless the two terms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether","@endWordPosition":"603","@position":"4014","annotationId":"T16","@startWordPosition":"600","@citStr":"Jiang and Conrath, 1997"}},"title":{"#tail":"\n","#text":"Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 10th International Conference on Research in Computational Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jay J Jiang"},{"#tail":"\n","#text":"David W Conrath"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Michael Kluck. 2004. The GIRT Data in the Evaluation of CLIR Systems - from 1997 Until 2003. Lecture Notes in Computer Science, 3237:376?390, January."},"#text":"\n","pages":{"#tail":"\n","#text":"3237--376"},"marker":{"#tail":"\n","#text":"Kluck, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" measures. In particular, the ?Semantic Information Retrieval? project (SIR Project, 2006) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems. The BERUFEnet (BN) corpus7 consists of descriptions of 5,800 professions in Germany and therefore contains many terms specific to professional training. Evaluating semantic relatedness on a test set based on this corpus may reveal the ability of a measure to adapt to a very special domain. The GIRT (German Indexing and Retrieval Testdatabase) corpus (Kluck, 2004) is a collection of abstracts of social science papers. It is a standard corpus for evaluating German information retrieval systems. The third corpus is compiled from 106 arbitrarily selected scientific PowerPoint presentations (SPP). They cover a wide range of topics from bio genetics to computer science and contain many technical terms. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. The three preprocessing steps (tokenization, POS-tagging, lemmatization) are pe","@endWordPosition":"2376","@position":"14903","annotationId":"T17","@startWordPosition":"2375","@citStr":"Kluck, 2004"}},"booktitle":{"#tail":"\n","#text":"The GIRT Data in the Evaluation of CLIR Systems - from"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Kluck"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Claudia Kunze, 2004. Lexikalisch-semantische Wortnetze, chapter Computerlinguistik und Sprachtechnologie, pages 423?431. Spektrum Akademischer Verlag."},"#text":"\n","pages":{"#tail":"\n","#text":"423--431"},"marker":{"#tail":"\n","#text":"Kunze, 2004"},"publisher":{"#tail":"\n","#text":"Spektrum Akademischer Verlag."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" NV = 0.15, NA = 0.15, V V = 0.1, V A = 0.05, AA = 0.05. That means 50% of the resulting word pairs for each corpus 7http://berufenet.arbeitsagentur.de 8l=logarithmic term frequency, t=logarithmic inverse document frequency, c=cosine normalization. 19 CORPUS # DOCS # TOKENS DOMAIN BN 9,022 7,728,501 descriptions of professions GIRT 151,319 19,645,417 abstracts of social science papers SPP 106 144,074 scientific .ppt presentations Table 2: Corpus statistics. were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet (Kunze, 2004), the German equivalent to WordNet, as a sense inventory for each word. It is the most complete resource of this type for German. GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e.g. for brother: ?brother, male sibling? vs. ?brother, comrade, friend? (Gurevych, 2005). We removed words which had more than three senses. Marginal manual post-processing was necessary, since the lemmatization process introduced some errors. Foreign words were tr","@endWordPosition":"2665","@position":"16730","annotationId":"T18","@startWordPosition":"2664","@citStr":"Kunze, 2004"}},"title":{"#tail":"\n","#text":"Lexikalisch-semantische Wortnetze, chapter Computerlinguistik und Sprachtechnologie,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Claudia Kunze"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Claudia Leacock and Martin Chodorow, 1998. WordNet: An Electronic Lexical Database, chapter Combining Local Context and WordNet Similarity for Word Sense Identification, pages 265?283. Cambridge: MIT Press."},"#text":"\n","pages":{"#tail":"\n","#text":"265--283"},"marker":{"#tail":"\n","#text":"Leacock, Chodorow, 1998"},"publisher":{"#tail":"\n","#text":"MIT Press."},"location":{"#tail":"\n","#text":"Cambridge:"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"d pairs from a corpus. Furthermore, the experimental setup leading to human judgments of semantic relatedness 3Nevertheless the two terms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whe","@endWordPosition":"596","@position":"3955","annotationId":"T19","@startWordPosition":"593","@citStr":"Leacock and Chodorow, 1998"}},"title":{"#tail":"\n","#text":"WordNet: An Electronic Lexical Database, chapter Combining Local Context and WordNet Similarity for Word Sense Identification,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Claudia Leacock"},{"#tail":"\n","#text":"Martin Chodorow"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"editor":{"#tail":"\n","#text":"In Robert Dale, editor,"},"rawString":{"#tail":"\n","#text":"Ludovic Lebart and Martin Rajman. 2000. Computing Similarity. In Robert Dale, editor, Handbook of NLP. Dekker: Basel."},"#text":"\n","marker":{"#tail":"\n","#text":"Lebart, Rajman, 2000"},"publisher":{"#tail":"\n","#text":"Dekker: Basel."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"sets.1 Experiments with human subjects show that the resulting datasets cover all degrees of relatedness. As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts. 1 Introduction Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction. It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts (Lebart and Rajman, 2000).2 Linguistic distance between words is inverse to their semantic similarity or relatedness. Semantic similarity is typically defined via the lexical relations of synonymy (automobile ? car) and hypernymy (vehicle ? car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be1In the near future, we are planning to make the software available to interested researchers. 2In this paper, word denotes the graphemic form of a token and concept refers to a particular sense of a word. tween two words (Gurevych, 2005).3 Dissimilar words can b","@endWordPosition":"179","@position":"1336","annotationId":"T20","@startWordPosition":"176","@citStr":"Lebart and Rajman, 2000"},{"#tail":"\n","#text":"ecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task. But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another. Application-based evaluation can only state the fact, but give little explanation about the reasons. The remaining approach - comparison with human judgments - is best suited for application independent evaluation of relatedness measures. Human annotators are asked to judge the relatednes","@endWordPosition":"737","@position":"4943","annotationId":"T21","@startWordPosition":"734","@citStr":"Lebart and Rajman (2000)"}]},"title":{"#tail":"\n","#text":"Computing Similarity."},"booktitle":{"#tail":"\n","#text":"Handbook of NLP."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ludovic Lebart"},{"#tail":"\n","#text":"Martin Rajman"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1986"},"rawString":{"#tail":"\n","#text":"Michael Lesk. 1986. Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual International Conference on Systems Documentation, pages 24?26, Toronto, Ontario, Canada."},"#text":"\n","pages":{"#tail":"\n","#text":"24--26"},"marker":{"#tail":"\n","#text":"Lesk, 1986"},"location":{"#tail":"\n","#text":"Toronto, Ontario, Canada."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cribes our system for automatically extracting word pairs from a corpus. Furthermore, the experimental setup leading to human judgments of semantic relatedness 3Nevertheless the two terms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metr","@endWordPosition":"587","@position":"3889","annotationId":"T22","@startWordPosition":"586","@citStr":"Lesk, 1986"}},"title":{"#tail":"\n","#text":"Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to tell a pine cone from an ice cream cone."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 5th Annual International Conference on Systems Documentation,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Lesk"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In Proceedings of International Conference on Machine Learning, Madison, Wisconsin."},"#text":"\n","marker":{"#tail":"\n","#text":"Lin, 1998"},"location":{"#tail":"\n","#text":"Madison, Wisconsin."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"tology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task. But they also note that evaluating a measure ","@endWordPosition":"672","@position":"4503","annotationId":"T23","@startWordPosition":"671","@citStr":"Lin, 1998"}},"title":{"#tail":"\n","#text":"An Information-Theoretic Definition of Similarity. In"},"booktitle":{"#tail":"\n","#text":"Proceedings of International Conference on Machine Learning,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dekang Lin"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2001"},"rawString":{"#tail":"\n","#text":"Rada Mihalcea and Dan Moldovan. 2001. Automatic Generation of a Coarse Grained WordNet. In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pittsburgh, PA, June."},"#text":"\n","marker":{"#tail":"\n","#text":"Mihalcea, Moldovan, 2001"},"location":{"#tail":"\n","#text":"Pittsburgh, PA,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ning only uppercase letters (mostly acronyms). Another filter can enforce a certain fraction of POS combinations to be present in the result set. As we want to obtain judgment scores for semantic relatedness of concepts instead of words, we have to include all word sense combinations of a pair in the list. An external dictionary of word senses is necessary for this step. It is also used to add a gloss for each word sense that enables test subjects to distinguish between senses. If differences in meaning between senses are very fine-grained, distinguishing between them is hard even for humans (Mihalcea and Moldovan, 2001).6 Pairs containing such words are not suitable for evaluation. To limit their impact on the experiment, a threshold for the maximal number of senses can be defined. Words with a number of senses above the threshold are removed from the list. The result of the extraction process is a list of sense disambiguated, POS-tagged pairs of concepts. 6E.g. the German verb ?halten? that can be translated as hold, maintain, present, sustain, etc. has 26 senses in GermaNet. 4.2 Experimental setup 4.2.1 Extraction of concept pairs We extracted word pairs from three different domain-specific corpora (see Ta","@endWordPosition":"2171","@position":"13594","annotationId":"T24","@startWordPosition":"2168","@citStr":"Mihalcea and Moldovan, 2001"}},"title":{"#tail":"\n","#text":"Automatic Generation of a Coarse Grained WordNet."},"booktitle":{"#tail":"\n","#text":"In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Rada Mihalcea"},{"#tail":"\n","#text":"Dan Moldovan"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"George A. Miller and Walter G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes, 6(1):1?28."},"#text":"\n","pages":{"#tail":"\n","#text":"6--1"},"marker":{"#tail":"\n","#text":"Miller, Charles, 1991"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rity instead of relatedness in mind. Word pairs consisted only of nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. 3 Related work In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards. Test subjects were instructed to order the cards according to the ?similarity of meaning? and then assign a continuous similarity value (0.0 - 4.0) to each card. Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects. Table 1 summarizes previous experiments. A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005","@endWordPosition":"1031","@position":"6720","annotationId":"T25","@startWordPosition":"1028","@citStr":"Miller and Charles (1991)"}},"booktitle":{"#tail":"\n","#text":"Contextual Correlates of Semantic Similarity. Language and Cognitive Processes,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"George A Miller"},{"#tail":"\n","#text":"Walter G Charles"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Jane Morris and Graeme Hirst. 2004. Non-Classical Lexical Semantic Relations. In Workshop on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of the ACL, Boston."},"#text":"\n","marker":{"#tail":"\n","#text":"Morris, Hirst, 2004"},"location":{"#tail":"\n","#text":"Boston."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" to cope with domain-specific or technical terms. This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of. Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations. Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e. other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity. Previous studies only considered semantic relatedness (or similarity) of words rather than concepts. However, polysemous or homonymous words should be annotated on the level of concepts. If we assume that bank has two meanings (?financial institution? vs. ?river bank?)5 and it is paired with money, the result is two sense quali5WordNet lists 10 meanings. fied pairs (bankfinancial ? money) and (bankr","@endWordPosition":"1573","@position":"10149","annotationId":"T26","@startWordPosition":"1570","@citStr":"Morris and Hirst (2004)"}},"title":{"#tail":"\n","#text":"Non-Classical Lexical Semantic Relations."},"booktitle":{"#tail":"\n","#text":"In Workshop on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of the ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jane Morris"},{"#tail":"\n","#text":"Graeme Hirst"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2003"},"rawString":{"#tail":"\n","#text":"Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using Measures of Semantic Relatedness for Word Sense Disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City."},"#text":"\n","marker":{"#tail":"\n","#text":"Patwardhan, Banerjee, Pedersen, 2003"},"location":{"#tail":"\n","#text":"Mexico City."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"lent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task. But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another. Application-based evaluation can only state the fact, but give little explanation about the reasons. The remaining approach - comparison with human judgments - is best suited for application independent evalua","@endWordPosition":"726","@position":"4863","annotationId":"T27","@startWordPosition":"723","@citStr":"Patwardhan et al., 2003"}},"title":{"#tail":"\n","#text":"Using Measures of Semantic Relatedness for Word Sense Disambiguation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Siddharth Patwardhan"},{"#tail":"\n","#text":"Satanjeev Banerjee"},{"#tail":"\n","#text":"Ted Pedersen"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448?453, Montreal, Canada."},"#text":"\n","pages":{"#tail":"\n","#text":"448--453"},"marker":{"#tail":"\n","#text":"Resnik, 1995"},"location":{"#tail":"\n","#text":"Montreal, Canada."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ntal setup leading to human judgments of semantic relatedness 3Nevertheless the two terms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles ","@endWordPosition":"599","@position":"3988","annotationId":"T28","@startWordPosition":"598","@citStr":"Resnik, 1995"},{"#tail":"\n","#text":" to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. 3 Related work In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards. Test subjects were instructed to order the cards according to the ?similarity of meaning? and then assign a continuous similarity value (0.0 - 4.0) to each card. Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects. Table 1 summarizes previous experiments. A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subje","@endWordPosition":"1061","@position":"6888","annotationId":"T29","@startWordPosition":"1060","@citStr":"Resnik (1995)"},{"#tail":"\n","#text":"s r=.478 (cf. Table 3), which 9http://www.wikipedia.de 20 CONCEPTS WORDS INTER INTRA INTER INTRA all .478 .647 .490 .675 BN .469 .695 .501 .718 GIRT .451 .598 .463 .625 SPP .535 .649 .523 .679 AA .556 .890 .597 .887 NA .547 .773 .511 .758 NV .510 .658 .540 .647 NN .463 .620 .476 .661 VA .317 .318 .391 .212 VV .278 .494 .301 .476 Table 3: Summarized correlation coefficients for all pairs, grouped by corpus and grouped by POS combinations. is statistically significant at p < .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=.9026.10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically. Evaluating the influence of using concept pairs instead of word pairs is complicated because word level judgments ar","@endWordPosition":"3288","@position":"20538","annotationId":"T30","@startWordPosition":"3287","@citStr":"Resnik (1995)"}]},"title":{"#tail":"\n","#text":"Using Information Content to Evaluate Semantic Similarity."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 14th International Joint Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Philip Resnik"}}},{"volume":{"#tail":"\n","#text":"8"},"#tail":"\n","date":{"#tail":"\n","#text":"1965"},"rawString":{"#tail":"\n","#text":"Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Communications of the ACM, 8(10):627?633."},"journal":{"#tail":"\n","#text":"Communications of the ACM,"},"#text":"\n","issue":{"#tail":"\n","#text":"10"},"marker":{"#tail":"\n","#text":"Rubenstein, Goodenough, 1965"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":") ? 0; d(x, y) = 0 ? x = y; d(x, y) = d(y, x); d(x, z) ? d(x, y) + d(y, z). insights into the nature of semantic relatedness. However, creating datasets for evaluation has so far been limited in a number of respects. Only a small number of word pairs was manually selected, with semantic similarity instead of relatedness in mind. Word pairs consisted only of nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. 3 Related work In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards. Test subjects were instructed to order the cards according to the ?similarity of meaning? and then assign a continuous similarity value (0.0 - 4.0) to each card. Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects. Table 1 summarizes previous experiments. A comprehensive evaluation of SR measures requires a higher number of word pairs. How","@endWordPosition":"984","@position":"6433","annotationId":"T31","@startWordPosition":"980","@citStr":"Rubenstein and Goodenough (1965)"},{"#tail":"\n","#text":"igure 4: Distribution of averaged human judgments with standard deviation < 0.8. that (car ? vehicle) are highly related, a strong connection between (parts ? speech) may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore, inter-subject correlation is lower than the results obtained by Gurevych (2006). In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. Rubenstein and Goodenough (1965) reported an intra-subject correlation of r=.85 for 15 subjects judging the similarity of a subset (36) of the original 65 word pairs. The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low. The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chai","@endWordPosition":"3658","@position":"22968","annotationId":"T32","@startWordPosition":"3655","@citStr":"Rubenstein and Goodenough (1965)"}]},"title":{"#tail":"\n","#text":"Contextual Correlates of Synonymy."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Herbert Rubenstein"},{"#tail":"\n","#text":"John B Goodenough"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1989"},"rawString":{"#tail":"\n","#text":"Gerard Salton. 1989. Automatic Text Processing: the Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley Longman Publishing, Boston, MA, USA."},"#text":"\n","marker":{"#tail":"\n","#text":"Salton, 1989"},"publisher":{"#tail":"\n","#text":"Addison-Wesley Longman Publishing,"},"location":{"#tail":"\n","#text":"Boston, MA, USA."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"d corpus is compiled from 106 arbitrarily selected scientific PowerPoint presentations (SPP). They cover a wide range of topics from bio genetics to computer science and contain many technical terms. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995). The resulting list of POS-tagged lemmas is weighted using the SMART ?ltc?8 tf.idf-weighting scheme (Salton, 1989). We implemented a set of filters for word pairs. One group of filters removed unwanted word pairs. Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist. Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters: NN = 0.5, NV = 0.15, NA = 0.15, V V = 0.1, V A = 0.05, AA = 0.05. That means 50% of the resulting word pairs for each corpus 7http://berufenet.arbei","@endWordPosition":"2488","@position":"15657","annotationId":"T33","@startWordPosition":"2487","@citStr":"Salton, 1989"}},"title":{"#tail":"\n","#text":"Automatic Text Processing: the Transformation, Analysis, and Retrieval of Information by Computer."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Gerard Salton"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Helmut Schmid. 1995. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, Manchester, UK."},"#text":"\n","marker":{"#tail":"\n","#text":"Schmid, 1995"},"location":{"#tail":"\n","#text":"Manchester, UK."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"cts of social science papers. It is a standard corpus for evaluating German information retrieval systems. The third corpus is compiled from 106 arbitrarily selected scientific PowerPoint presentations (SPP). They cover a wide range of topics from bio genetics to computer science and contain many technical terms. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995). The resulting list of POS-tagged lemmas is weighted using the SMART ?ltc?8 tf.idf-weighting scheme (Salton, 1989). We implemented a set of filters for word pairs. One group of filters removed unwanted word pairs. Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist. Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters: NN = 0.5, NV = 0.15, NA = 0.15, V","@endWordPosition":"2472","@position":"15542","annotationId":"T34","@startWordPosition":"2471","@citStr":"Schmid, 1995"}},"title":{"#tail":"\n","#text":"Probabilistic Part-of-Speech Tagging Using Decision Trees."},"booktitle":{"#tail":"\n","#text":"In International Conference on New Methods in Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Helmut Schmid"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Sabine Schulte im Walde and Alissa Melinger. 2005. Identifying Semantic Relations and Functional Properties of Human Verb Associations. In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in NLP, pages 612?619, Vancouver, Canada."},"#text":"\n","pages":{"#tail":"\n","#text":"612--619"},"marker":{"#tail":"\n","#text":"Walde, Melinger, 2005"},"location":{"#tail":"\n","#text":"Vancouver, Canada."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"oodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005). Results of such experiments are hard to quantify and cannot easily serve as the basis for evaluating SR measures. Rubenstein and Goodenough selected word pairs analytically to cover the whole spectrum of 17 CORRELATION PAPER LANGUAGE PAIRS POS REL-TYPE SCORES # SUBJECTS INTER INTRA R/G (1965) English 65 N sim continuous 0?4 51 - .850 M/C (1991) English 30 N sim continuous 0?4 38 - - Res (1995) English 30 N sim continuous 0?4 10 .903 - Fin (2002) English 353 N, V, A relat continuous 0?10 16 - - Gur (2005) German 65 N sim discrete {0,1,2,3,4} 24 .810 - Gur (2006) German 350 N, V, A relat discr","@endWordPosition":"1234","@position":"7992","annotationId":"T35","@startWordPosition":"1231","@citStr":"Walde and Melinger, 2005"}},"title":{"#tail":"\n","#text":"Identifying Semantic Relations and Functional Properties of Human Verb Associations."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in NLP,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Sabine Schulte im Walde"},{"#tail":"\n","#text":"Alissa Melinger"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"note":{"#tail":"\n","#text":"URL http://www.cre-elearning. tu-darmstadt.de/elearning/sir/."},"rawString":{"#tail":"\n","#text":"SIR Project. 2006. Project ?Semantic Information Retrieval?. URL http://www.cre-elearning. tu-darmstadt.de/elearning/sir/."},"#text":"\n","marker":{"#tail":"\n","#text":"Project, 2006"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ith a number of senses above the threshold are removed from the list. The result of the extraction process is a list of sense disambiguated, POS-tagged pairs of concepts. 6E.g. the German verb ?halten? that can be translated as hold, maintain, present, sustain, etc. has 26 senses in GermaNet. 4.2 Experimental setup 4.2.1 Extraction of concept pairs We extracted word pairs from three different domain-specific corpora (see Table 2). This is motivated by the aim to enable research in information retrieval incorporating SR measures. In particular, the ?Semantic Information Retrieval? project (SIR Project, 2006) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems. The BERUFEnet (BN) corpus7 consists of descriptions of 5,800 professions in Germany and therefore contains many terms specific to professional training. Evaluating semantic relatedness on a test set based on this corpus may reveal the ability of a measure to adapt to a very special domain. The GIRT (German Indexing and Retrieval Testdatabase) corpus (Kluck, 2004) is a collection of abstracts of social science papers. It is a standard corpu","@endWordPosition":"2297","@position":"14381","annotationId":"T36","@startWordPosition":"2296","@citStr":"Project, 2006"}},"title":{"#tail":"\n","#text":"Project ?Semantic Information Retrieval?."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"SIR Project"}}},{"volume":{"#tail":"\n","#text":"31"},"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Julie Weeds and David Weir. 2005. Co-occurrence Retrieval: A Flexible Framework For Lexical Distributional Similarity. Computational Linguistics, 31(4):439?475, December."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Weeds, Weir, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ess 3Nevertheless the two terms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain ","@endWordPosition":"609","@position":"4055","annotationId":"T37","@startWordPosition":"606","@citStr":"Weeds and Weir, 2005"}},"title":{"#tail":"\n","#text":"Co-occurrence Retrieval: A Flexible Framework For Lexical Distributional Similarity."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Julie Weeds"},{"#tail":"\n","#text":"David Weir"}]}},{"date":{"#tail":"\n","#text":"1994"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ically extracting word pairs from a corpus. Furthermore, the experimental setup leading to human judgments of semantic relatedness 3Nevertheless the two terms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematica","@endWordPosition":"592","@position":"3926","annotationId":"T38","@startWordPosition":"589","@citStr":"Wu and Palmer, 1994"}},"title":{"#tail":"\n","#text":"Verb Semantics and Lexical Selection."},"#tail":"\n","institution":{"#tail":"\n","#text":"NewMexico State University, Las Cruces,"},"rawString":{"#tail":"\n","#text":"Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and Lexical Selection. In 32nd Annual Meeting of the ACL, pages 133?138, NewMexico State University, Las Cruces, New Mexico."},"#text":"\n","pages":{"#tail":"\n","#text":"133--138"},"marker":{"#tail":"\n","#text":"Wu, Palmer, 1994"},"location":{"#tail":"\n","#text":"New Mexico."},"booktitle":{"#tail":"\n","#text":"In 32nd Annual Meeting of the ACL,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Zhibiao Wu"},{"#tail":"\n","#text":"Martha Palmer"}]}}]}}]}}
