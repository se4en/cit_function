 for better alignment and thus a better reference annotation. This increases the actual prediction performance and, furthermore, reduces the number of label predictions that are erroneously counted as a misprediction. Thus, it is to be expected that manual correction of the automatically created annotation results in significant performance gains. Preliminary annotation experiments have shown that this is indeed the case. 6.4 Segmentation quality Accuracy is not the best measure to assess segmentation quality, therefore we also conducted experiments using the WindowDiff measure as proposed by Pevzner and Hearst (2002). WindowDiff returns 0 in case of a perfect segmentation; 1 is the worst possible score. However, it only takes into account segment boundaries and disregards segment types. Table 3 shows the WindowDiff scores for CCOR-ALL and CRCG-ALL. Overall, the scores are quite good and are consistently below 0.2. Furthermore, CRCG-ALL scores do not suffer as badly from inaccurate reference annotation, since “near misses” are penalized less strongly. 8 Converged (%) Iterations (0) CCOR-ALL 0.999 15.4 CRCG-ALL 0.911 66.5 CCOR-BEST 0.999 14.2 CRCG-BEST 0.971 37.5 Table 4: Convergence behaviour of loopy BP 6
