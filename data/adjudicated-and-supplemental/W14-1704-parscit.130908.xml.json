{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"A. Rozovskaya and D. Roth. 2011. Algorithm selection and model adaptation for esl correction tasks. In ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Rozovskaya, Roth, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"preprocessed with a part-of-speech tag10 denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants \u201ca\u201d and \u201can\u201d are conflated and are restored later. ger2 and shallow parser3 (Punyakanok and Roth, 2001). The other system components use the preprocessing tools only as part of candidate generation (e.g., to identify all nouns in the data for the noun classifier). The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011). Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)). The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features","@endWordPosition":"1568","@position":"9781","annotationId":"T1","@startWordPosition":"1565","@citStr":"Rozovskaya and Roth, 2011"},{"#tail":"\n","#text":"ution on the training set. This method prevents the source feature from dominating the context features, and improves the recall of the system. The other classifiers in the baseline system \u2013 noun number, verb agreement, verb form, and preposition \u2013 are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm. All models use word ngram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). The modules targeting verb agreement and 2http://cogcomp.cs.illinois.edu/page/ software view/POS 3http://cogcomp.cs.illinois.edu/page/ software view/Chunker 36 verb form mistakes draw on the linguisticallymotivated approach to correcting verb errors proposed in Rozovskaya et. al (2014). 4 The CoNLL-2014 System The system in the CoNLL-2014 shared task is improved in three ways: 1) Additional error-specific classifiers: word form, orthography/punctuation, and style; 2) Model combination; and 3) Joint inference to address interacting errors. Table 3 summarizes the Illinois and the Illinois-Colu","@endWordPosition":"1862","@position":"11551","annotationId":"T2","@startWordPosition":"1859","@citStr":"Rozovskaya and Roth, 2011"}]},"title":{"#tail":"\n","#text":"Algorithm selection and model adaptation for esl correction tasks."},"booktitle":{"#tail":"\n","#text":"In ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Rozovskaya"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2011"},"rawString":{"#tail":"\n","#text":"A. Rozovskaya and D. Roth. 2011. Algorithm selection and model adaptation for esl correction tasks. In ACL."},"#text":"\n","marker":{"#tail":"\n","#text":"Rozovskaya, Roth, 2011"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"preprocessed with a part-of-speech tag10 denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants \u201ca\u201d and \u201can\u201d are conflated and are restored later. ger2 and shallow parser3 (Punyakanok and Roth, 2001). The other system components use the preprocessing tools only as part of candidate generation (e.g., to identify all nouns in the data for the noun classifier). The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011). Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)). The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features","@endWordPosition":"1568","@position":"9781","annotationId":"T3","@startWordPosition":"1565","@citStr":"Rozovskaya and Roth, 2011"},{"#tail":"\n","#text":"ution on the training set. This method prevents the source feature from dominating the context features, and improves the recall of the system. The other classifiers in the baseline system \u2013 noun number, verb agreement, verb form, and preposition \u2013 are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm. All models use word ngram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). The modules targeting verb agreement and 2http://cogcomp.cs.illinois.edu/page/ software view/POS 3http://cogcomp.cs.illinois.edu/page/ software view/Chunker 36 verb form mistakes draw on the linguisticallymotivated approach to correcting verb errors proposed in Rozovskaya et. al (2014). 4 The CoNLL-2014 System The system in the CoNLL-2014 shared task is improved in three ways: 1) Additional error-specific classifiers: word form, orthography/punctuation, and style; 2) Model combination; and 3) Joint inference to address interacting errors. Table 3 summarizes the Illinois and the Illinois-Colu","@endWordPosition":"1862","@position":"11551","annotationId":"T4","@startWordPosition":"1859","@citStr":"Rozovskaya and Roth, 2011"}]},"title":{"#tail":"\n","#text":"Algorithm selection and model adaptation for esl correction tasks."},"booktitle":{"#tail":"\n","#text":"In ACL."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Rozovskaya"},{"#tail":"\n","#text":"D Roth"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2012"},"rawString":{"#tail":"\n","#text":"A. Rozovskaya, M. Sammons, and D. Roth. 2012. The UI system in the HOO 2012 shared task on error correction. In Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) Workshop on Innovative Use of NLP for Building Educational Applications."},"#text":"\n","marker":{"#tail":"\n","#text":"Rozovskaya, Sammons, Roth, 2012"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011). Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)). The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles). The original word choice (the source article) used by the writer is also used as a feature. Since the errors are sparse, this feature causes the model to abstain from flagging mistakes, resulting in low recall. To avoid this problem, we adopt the approach proposed in Rozovskaya et al. (2012), ","@endWordPosition":"1640","@position":"10216","annotationId":"T5","@startWordPosition":"1637","@citStr":"Rozovskaya et al. (2012)"}},"title":{"#tail":"\n","#text":"The UI system in the HOO 2012 shared task on error correction."},"booktitle":{"#tail":"\n","#text":"In Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) Workshop on Innovative Use of NLP for Building Educational Applications."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"A Rozovskaya"},{"#tail":"\n","#text":"M Sammons"},{"#tail":"\n","#text":"D Roth"}]}}]}}}}
