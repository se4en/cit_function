{"algorithms":{"#text":"\n","@version":"110505","algorithm":[{"#tail":"\n","@name":"SectLabel","#text":"\n","@version":"110505","variant":{"@no":"0","note":{"#tail":"\n","@confidence":"0.326227","#text":"\nProceedings of the ACL 2010 Conference Short Papers, pages 318?324,\nUppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics\n"},"address":[{"#tail":"\n","@confidence":"0.88552","#text":"\nNjalsgade 140, 2300-DK Copenhagen\n"},{"#tail":"\n","@confidence":"0.891203","#text":"\nNjalsgade 140, 2300-DK Copenhagen\n"}],"author":[{"#tail":"\n","@confidence":"0.87996","#text":"\nCostanza Navarretta\n"},{"#tail":"\n","@confidence":"0.911908","#text":"\nPatrizia Paggio\n"}],"subsectionHeader":[{"#tail":"\n","@confidence":"0.989541","#text":"\n3.1 Inter-coder agreement on feedback\n"},{"#tail":"\n","@confidence":"0.997916","#text":"\n4.1 Measuring inter-coder agreement on\n"}],"title":{"#tail":"\n","@confidence":"0.666605","#text":"\nClassification of Feedback Expressions in Multimodal Data\n"},"@confidence":"0.000054","reference":[{"#tail":"\n","@confidence":"0.997392084905661","#text":"\nJens Allwood, Loredana Cerrato, Kristiina Jokinen,\nCostanza Navarretta, and Patrizia Paggio. 2007.\nThe MUMIN Coding Scheme for the Annotation of\nFeedback, Turn Management and Sequencing. Mul-\ntimodal Corpora for Modelling Human Multimodal\nBehaviour. Special Issue of the International Jour-\nnal of Language Resources and Evaluation, 41(3?\n4):273?287.\nAnne H. Anderson, Miles Bader, Ellen Gurman Bard,\nElizabeth Boyle, Gwyneth Doherty, Simon Garrod,\nStephen Isard, Jacqueline Kowtko, Jan McAllister,\nJim Miller, Catherine Sotillo, Henry S. Thompson,\nand Regina Weinert. 1991. The HCRC Map Task\nCorpus. Language and Speech, 34:351?366.\nRon Artstein and Massimo Poesio. 2008. Inter-Coder\nAgreement for Computational Linguistics. Compu-\ntational Linguistics, 34(4):555?596.\nPaul Boersma and David Weenink, 2009. Praat: do-\ning phonetics by computer. Retrieved May 1, 2009,\nfrom http://www.praat.org/.\nRobert L. Brennan and Dale J. Prediger. 1981. Co-\nefficient Kappa: Some uses, misuses, and alterna-\ntives. Educational and Psychological Measurement,\n41:687?699.\nJacob Cohen. 1960. A coefficient of agreement\nfor nominal scales. Educational and Psychological\nMeasurement, 20(1):37?46.\nJoseph L. Fleiss. 1971. Measuring nominal scale\nagreement among many raters. Psychological Bul-\nlettin, 76(5):378?382.\nShinya Fujie, Y. Ejiri, K. Nakajima, Y Matsusaka, and\nTetsunor Kobayashi. 2004. A conversation robot\nusing head gesture recognition as para-linguistic in-\nformation. In Proceedings of the 13th IEEE Inter-\nnational Workshop on Robot and Human Interactive\nCommunication, pages 159 ? 164, september.\nAgustin Gravano and Julia Hirschberg. 2009. Turn-\nyielding cues in task-oriented dialogue. In Pro-\nceedings of SIGDIAL 2009: the 10th Annual Meet-\ning of the Special Interest Group in Discourse and\nDialogue, September 2009, pages 253?261, Queen\nMary University of London.\nNina Gr?nnum. 2006. DanPASS - a Danish pho-\nnetically annotated spontaneous speech corpus. In\nN. Calzolari, K. Choukri, A. Gangemi, B. Maegaard,\nJ. Mariani, J. Odijk, and D. Tapias, editors, Pro-\nceedings of the 5th LREC, pages 1578?1583, Genoa,\nMay.\nKristiina Jokinen and Anton Ragni. 2007. Cluster-\ning experiments on the communicative prop- erties\nof gaze and gestures. In Proceeding of the 3rd.\nBaltic Conference on Human Language Technolo-\ngies, Kaunas, Lithuania, October.\nKristiina Jokinen, Costanza Navarretta, and Patrizia\nPaggio. 2008. Distinguishing the communica-\ntive functions of gestures. In Proceedings of the\n5th MLMI, LNCS 5237, pages 38?49, Utrecht, The\nNetherlands, September. Springer.\nMichael Kipp. 2004. Gesture Generation by Imita-\ntion - From Human Behavior to Computer Charac-\nter Animation. Ph.D. thesis, Saarland University,\nSaarbruecken, Germany, Boca Raton, Florida, dis-\nsertation.com.\nMax M. Louwerse, Patrick Jeuniaux, Mohammed E.\nHoque, Jie Wu, and Gwineth Lewis. 2006. Mul-\ntimodal communication in computer-mediated map\ntask scenarios. In R. Sun and N. Miyake, editors,\nProceedings of the 28th Annual Conference of the\nCognitive Science Society, pages 1717?1722, Mah-\nwah, NJ: Erlbaum.\nMax M. Louwerse, Nick Benesh, Mohammed E.\nHoque, Patrick Jeuniaux, Gwineth Lewis, Jie Wu,\nand Megan Zirnstein. 2007. Multimodal communi-\ncation in face-to-face conversations. In R. Sun and\nN. Miyake, editors, Proceedings of the 29th Annual\nConference of the Cognitive Science Society, pages\n1235?1240, Mahwah, NJ: Erlbaum.\nEvelyn McClave. 2000. Linguistic functions of head\nmovements in the context of speech. Journal of\nPragmatics, 32:855?878.\nLouis-Philippe Morency, Candace Sidner, Christopher\nLee, and Trevor Darrell. 2005. Contextual Recog-\nnition of Head Gestures. In Proceedings of the In-\nternational Conference on Multi-modal Interfaces.\nLouis-Philippe Morency, Candace Sidner, Christopher\nLee, and Trevor Darrell. 2007. Head gestures for\nperceptual interfaces: The role of context in im-\nproving recognition. Artificial Intelligence, 171(8?\n9):568?585.\nLouis-Philippe Morency, Iwan de Kok, and Jonathan\nGratch. 2009. A probabilistic multimodal ap-\nproach for predicting listener backchannels. Au-\ntonomous Agents and Multi-Agent Systems, 20:70?\n84, Springer.\nGabriel Murray and Steve Renals. 2008. Detecting\nAction Meetings in Meetings. In Proceedings of\nthe 5th MLMI, LNCS 5237, pages 208?213, Utrecht,\nThe Netherlands, September. Springer.\nHarm Rieks op den Akker and Christian Schulz. 2008.\nExploring features and classifiers for dialogue act\nsegmentation. In Proceedings of the 5th MLMI,\npages 196?207.\nPatrizia Paggio and Costanza Navarretta. 2010. Feed-\nback in Head Gesture and Speech. To appear in Pro-\nceedings of 7th Conference on Language Resources\nand Evaluation (LREC-2010), Malta, May.\n"},{"#tail":"\n","@confidence":"0.99857255","#text":"\nDennis Reidsma, Dirk Heylen, and Harm Rieks op den\nAkker. 2009. On the Contextual Analysis of Agree-\nment Scores. In Michael Kipp, Jean-Claude Mar-\ntin, Patrizia Paggio, and Dirk Heylen, editors, Multi-\nmodal Corpora From Models of Natural Interaction\nto Systems and Applications, number 5509 in Lec-\nture Notes in Artificial Intelligence, pages 122?137.\nSpringer.\nVivek Kumar Rangarajan Sridhar, Srinivas Bangaloreb,\nand Shrikanth Narayanan. 2009. Combining lexi-\ncal, syntactic and prosodic cues for improved online\ndialog act tagging. Computer Speech & Language,\n23(4):407?422.\nIan H. Witten and Eibe Frank. 2005. Data Mining:\nPractical machine learning tools and techniques.\nMorgan Kaufmann, San Francisco, second edition.\nHarry Zhang, Liangxiao Jiang, and Jiang Su. 2005.\nHidden Naive Bayes. In Proceedings of the Twen-\ntieth National Conference on Artificial Intelligence,\npages 919?924.\n"}],"#tail":"\n","bodyText":[{"#tail":"\n","@confidence":"0.9997598","#text":"\nThis paper addresses the issue of how lin-\nguistic feedback expressions, prosody and\nhead gestures, i.e. head movements and\nface expressions, relate to one another in\na collection of eight video-recorded Dan-\nish map-task dialogues. The study shows\nthat in these data, prosodic features and\nhead gestures significantly improve auto-\nmatic classification of dialogue act labels\nfor linguistic expressions of feedback.\n"},{"#tail":"\n","@confidence":"0.998457147058824","#text":"\nSeveral authors in communication studies have\npointed out that head movements are relevant to\nfeedback phenomena (see McClave (2000) for an\noverview). Others have looked at the application\nof machine learning algorithms to annotated mul-\ntimodal corpora. For example, Jokinen and Ragni\n(2007) and Jokinen et al (2008) find that machine\nlearning algorithms can be trained to recognise\nsome of the functions of head movements, while\nReidsma et al (2009) show that there is a depen-\ndence between focus of attention and assignment\nof dialogue act labels. Related are also the stud-\nies by Rieks op den Akker and Schulz (2008) and\nMurray and Renals (2008): both achieve promis-\ning results in the automatic segmentation of dia-\nlogue acts using the annotations in a large multi-\nmodal corpus.\nWork has also been done on prosody and ges-\ntures in the specific domain of map-task dialogues,\nalso targeted in this paper. Sridhar et al (2009)\nobtain promising results in dialogue act tagging\nof the Switchboard-DAMSL corpus using lexical,\nsyntactic and prosodic cues, while Gravano and\nHirschberg (2009) examine the relation between\nparticular acoustic and prosodic turn-yielding cues\nand turn taking in a large corpus of task-oriented\ndialogues. Louwerse et al (2006) and Louwerse\net al (2007) study the relation between eye gaze,\nfacial expression, pauses and dialogue structure\nin annotated English map-task dialogues (Ander-\nson et al, 1991) and find correlations between the\nvarious modalities both within and across speak-\ners. Finally, feedback expressions (head nods and\nshakes) are successfully predicted from speech,\nprosody and eye gaze in interaction with Embod-\nied Communication Agents as well as human com-\nmunication (Fujie et al, 2004; Morency et al,\n2005; Morency et al, 2007; Morency et al, 2009).\nOur work is in line with these studies, all of\nwhich focus on the relation between linguistic\nexpressions, prosody, dialogue content and ges-\ntures. In this paper, we investigate how feedback\nexpressions can be classified into different dia-\nlogue act categories based on prosodic and ges-\nture features. Our data are made up by a collec-\ntion of eight video-recorded map-task dialogues in\nDanish, which were annotated with phonetic and\nprosodic information. We find that prosodic fea-\ntures improve the classification of dialogue acts\nand that head gestures, where they occur, con-\ntribute to the semantic interpretation of feedback\nexpressions. The results, which partly confirm\nthose obtained on a smaller dataset in Paggio and\nNavarretta (2010), must be seen in light of the\nfact that our gesture annotation scheme comprises\nmore fine-grained categories than most of the stud-\nies mentioned earlier for both head movements\nand face expressions. The classification results\nimprove, however, if similar categories such as\nhead nods and jerks are collapsed into a more gen-\neral category.\nIn Section 2 we describe the multimodal Dan-\nish corpus. In Section 3, we describe how the\nprosody of feedback expressions is annotated, how\ntheir content is coded in terms of dialogue act, turn\nand agreement labels, and we provide inter-coder\nagreement measures. In Section 4 we account for\nthe annotation of head gestures, including inter-\n"},{"#tail":"\n","@confidence":"0.999544","#text":"\ncoder agreements results. Section 5 contains a de-\nscription of the resulting datasets and a discussion\nof the results obtained in the classification experi-\nments. Section 6 is the conclusion.\n"},{"#tail":"\n","@confidence":"0.999313020408164","#text":"\nThe Danish map-task dialogues from the Dan-\nPASS corpus (Gr?nnum, 2006) are a collection\nof dialogues in which 11 speaker pairs cooper-\nate on a map task. The dialogue participants\nare seated in different rooms and cannot see each\nother. They talk through headsets, and one of them\nis recorded with a video camera. Each pair goes\nthrough four different sets of maps, and changes\nroles each time, with one subject giving instruc-\ntions and the other following them. The material\nis transcribed orthographically with an indication\nof stress, articulatory hesitations and pauses. In\naddition to this, the acoustic signals are segmented\ninto words, syllables and prosodic phrases, and an-\nnotated with POS-tags, phonological and phonetic\ntranscriptions, pitch and intonation contours.\nPhonetic and prosodic segmentation and anno-\ntation were performed independently and in paral-\nlel by two annotators and then an agreed upon ver-\nsion was produced with the supervision of an ex-\npert annotator, for more information see Gr?nnum\n(2006). The Praat tool was used (Boersma and\nWeenink, 2009).\nThe feedback expressions we analyse here are\nYes and No expressions, i.e. in Danish words like\nja (yes), jo (yes in a negative context), jamen (yes\nbut, well), nej (no), n?h (no). They can be single\nwords or multi-word expressions.\nYes and No feedback expressions represent\nabout 9% of the approximately 47,000 running\nwords in the corpus. This is a rather high pro-\nportion compared to other corpora, both spoken\nand written, and a reason why we decided to use\nthe DanPASS videos in spite of the fact that the\ngesture behaviour is relatively limited given the\nfact that the two dialogue participants cannot see\neach other. Furthermore, the restricted contexts\nin which feedback expressions occur in these di-\nalogues allow for a very fine-grained analysis of\nthe relation of these expressions with prosody and\ngestures. Feedback behaviour, both in speech and\ngestures, can be observed especially in the person\nwho is receiving the instructions (the follower).\nTherefore, we decided to focus our analysis only\non the follower?s part of the interaction. Because\nof time restrictions, we limited the study to four\ndifferent subject pairs and two interactions per\npair, for a total of about an hour of video-recorded\ninteraction.\n"},{"#tail":"\n","@confidence":"0.995635214285714","#text":"\nAs already mentioned, all words in DanPASS are\nphonetically and prosodically annotated. In the\nsubset of the corpus considered here, 82% of the\nfeedback expressions bear stress or tone informa-\ntion, and 12% are unstressed; 7% of them are\nmarked with onset or offset hesitation, or both.\nFor this study, we added semantic labels ? includ-\ning dialogue acts ? and gesture annotation. Both\nkinds of annotation were carried out using ANVIL\n(Kipp, 2004). To distinguish among the various\nfunctions that feedback expressions have in the di-\nalogues, we selected a subset of the categories de-\nfined in the emerging ISO 24617-2 standard for\nsemantic annotation of language resources. This\nsubset comprises the categories Accept, Decline,\nRepeatRephrase and Answer. Moreover, all feed-\nback expressions were annotated with an agree-\nment feature (Agree, NonAgree) where relevant.\nFinally, the two turn management categories Turn-\nTake and TurnElicit were also coded.\nIt should be noted that the same expression may\nbe annotated with a label for each of the three se-\nmantic dimensions. For example, a yes can be an\nAnswer to a question, an Agree and a TurnElicit at\nthe same time, thus making the semantic classifi-\ncation very fine-grained. Table 1 shows how the\nvarious types are distributed across the 466 feed-\nback expressions in our data.\n"},{"#tail":"\n","@confidence":"0.987639117647059","#text":"\nexpression annotation\nIn general, dialogue act, agreement and turn anno-\ntations were coded by an expert annotator and the\nannotations were subsequently checked by a sec-\nond expert annotator. However, one dialogue was\ncoded independently and in parallel by two expert\nannotators to measure inter-coder agreement. A\nmeasure was derived for each annotated feature\nusing the agreement analysis facility provided in\nANVIL. Agreement between two annotation sets\nis calculated here in terms of Cohen?s kappa (Co-\nhen, 1960)1 and corrected kappa (Brennan and\nPrediger, 1981)2. Anvil divides the annotations in\nslices and compares each slice. We used slices of\n0.04 seconds. The inter-coder agreement figures\nobtained for the three types of annotation are given\nin Table 2.\n"},{"#tail":"\n","@confidence":"0.97792425","#text":"\npression annotation\nAlthough researchers do not totally agree on\nhow to measure agreement in various types of an-\nnotated data and on how to interpret the resulting\nfigures, see Artstein and Poesio (2008), it is usu-\nally assumed that Cohen?s kappa figures over 60\nare good while those over 75 are excellent (Fleiss,\n1971). Looking at the cases of disagreement we\ncould see that many of these are due to the fact\nthat the annotators had forgotten to remove some\nof the features automatically proposed by ANVIL\nfrom the latest annotated element.\n"},{"#tail":"\n","@confidence":"0.985859472222222","#text":"\nAll communicative head gestures in the videos\nwere found and annotated with ANVIL using a\nsubset of the attributes defined in the MUMIN an-\nnotation scheme (Allwood et al, 2007). The MU-\nMIN scheme is a general framework for the study\nof gestures in interpersonal communication. In\nthis study, we do not deal with functional classi-\nfication of the gestures in themselves, but rather\n1(Pa? Pe)/(1? Pe).\n2(Po ? 1/c)/(1 ? 1/c) where c is the number of cate-\ngories.\nwith how gestures contribute to the semantic in-\nterpretations of linguistic expressions. Therefore,\nonly a subset of the MUMIN attributes has been\nused, i.e. Smile, Laughter, Scowl, FaceOther for\nfacial expressions, and Nod, Jerk, Tilt, SideTurn,\nShake, Waggle, Other for head movements.\nA link was also established in ANVIL between\nthe gesture under consideration and the relevant\nspeech sequence where appropriate. The link was\nthen used to extract gesture information together\nwith the relevant linguistic annotations on which\nto apply machine learning.\nThe total number of head gestures annotated is\n264. Of these, 114 (43%) co-occur with feedback\nexpressions, with Nod as by far the most frequent\ntype (70 occurrences) followed by FaceOther as\nthe second most frequent (16). The other tokens\nare distributed more or less evenly, with a few oc-\ncurrences (2-8) per type. The remaining 150 ges-\ntures, linked to different linguistic expressions or\nto no expression at all, comprise many face ex-\npressions and a number of tilts. A rough prelim-\ninary analysis shows that their main functions are\nrelated to focusing or to different emotional atti-\ntudes. They will be ignored in what follows.\n"},{"#tail":"\n","@confidence":"0.990072","#text":"\ngesture annotation\nThe head gestures in the DanPASS data have been\ncoded by non expert annotators (one annotator\nper video) and subsequently controlled by a sec-\nond annotator, with the exception of one video\nwhich was annotated independently and in parallel\nby two annotators. The annotations of this video\nwere then used to measure inter-coder agreement\nin ANVIL as it was the case for the annotations\non feedback expressions. In the case of gestures\nwe also measured agreement on gesture segmen-\ntation. The figures obtained are given in Table 3.\n"},{"#tail":"\n","@confidence":"0.8369105","#text":"\nannotation\nThese results are slightly worse than those ob-\ntained in previous studies using the same annota-\ntion scheme (Jokinen et al, 2008), but are still sat-\n"},{"#tail":"\n","@confidence":"0.997862444444444","#text":"\nisfactory given the high number of categories pro-\nvided by the scheme.\nA distinction that seemed particularly difficult\nwas that between nods and jerks: although the\ndirection of the two movement types is different\n(down-up and up-down, respectively), the move-\nment quality is very similar, and makes it difficult\nto see the direction clearly. We return to this point\nbelow, in connection with our data analysis.\n"},{"#tail":"\n","@confidence":"0.983355142857143","#text":"\nThe multimodal data we obtained by combining\nthe linguistic annotations from DanPASS with the\ngesture annotation created in ANVIL, resulted into\ntwo different groups of data, one containing all Yes\nand No expressions, and the other the subset of\nthose that are accompanied by a face expression\nor a head movement, as shown in Table 4.\n"},{"#tail":"\n","@confidence":"0.996359612903226","#text":"\nThese two sets of data were used for automatic\ndialogue act classification, which was run in the\nWeka system (Witten and Frank, 2005). We exper-\nimented with various Weka classifiers, compris-\ning Hidden Naive Bayes, SMO, ID3, LADTree\nand Decision Table. The best results on most of\nour data were obtained using Hidden Naive Bayes\n(HNB) (Zhang et al, 2005). Therefore, here we\nshow the results of this classifier. Ten-folds cross-\nvalidation was applied throughout.\nIn the first group of experiments we took into\nconsideration all the Yes and No expressions (420\nYes and 46 No) without, however, considering ges-\nture information. The purpose was to see how\nprosodic information contributes to the classifica-\ntion of dialogue acts. We started by totally leav-\ning out prosody, i.e. only the orthographic tran-\nscription (Yes and No expressions) was consid-\nered; then we included information about stress\n(stressed or unstressed); in the third run we added\ntone attributes, and in the fourth information on\nhesitation. Agreement and turn attributes were\nused in all experiments, while Dialogue act anno-\ntation was only used in the training phase. The\nbaseline for the evaluation are the results provided\nby Weka?s ZeroR classifier, which always selects\nthe most frequent nominal class.\nIn Table 5 we provide results in terms of preci-\nsion (P), recall (R) and F-measure (F). These are\ncalculated in Weka as weighted averages of the re-\nsults obtained for each class.\n"},{"#tail":"\n","@confidence":"0.998696121212121","#text":"\nThe results indicate that prosodic information\nimproves the classification of dialogue acts with\nrespect to the baseline in all four experiments with\nimprovements of 10, 10.6, 10.9 and 10.8%, re-\nspectively. The best results are obtained using\ninformation on stress and tone, although the de-\ncrease in accuracy when hesitations are introduced\nis not significant. The confusion matrices show\nthat the classifier is best at identifying Accept,\nwhile it is very bad at identifying RepeatRephrase.\nThis result if not surprising since the former type\nis much more frequent in the data than the latter,\nand since prosodic information does not correlate\nwith RepeatRephrase in any systematic way.\nThe second group of experiments was con-\nducted on the dataset where feedback expressions\nare accompanied by gestures (102 Yes and 12 No).\nThe purpose this time was to see whether ges-\nture information improves dialogue act classifica-\ntion. We believe it makes sense to perform the\ntest based on this restricted dataset, rather than the\nentire material, because the portion of data where\ngestures do accompany feedback expressions is\nrather small (about 20%). In a different domain,\nwhere subjects are less constrained by the techni-\ncal setting, we expect gestures would make for a\nstronger and more widespread effect.\nThe Precision, Recall and F-measure of the Ze-\nroR classifier on these data are 31.5, 56.1 and 40.4,\nrespectively. For these experiments, however, we\nused as a baseline the results obtained based on\nstress, tone and hesitation information, the com-\nbination that gave the best results on the larger\n"},{"#tail":"\n","@confidence":"0.994073555555556","#text":"\ndataset. Together with the prosodic information,\nAgreement and turn attributes were included just\nas earlier, while the dialogue act annotation was\nonly used in the training phase. Face expression\nand head movement attributes were disregarded\nin the baseline. We then added face expression\nalone, head movement alone, and finally both ges-\nture types together. The results are shown in Ta-\nble 6.\n"},{"#tail":"\n","@confidence":"0.992941142857143","#text":"\nThese results indicate that adding head ges-\nture information improves the classification of di-\nalogue acts in this reduced dataset, although the\nimprovement is not impressive. The best results\nare achieved when both face expressions and head\nmovements are taken into consideration.\nThe confusion matrices show that although the\nrecognition of both Answer and None improve, it\nis only the None class which is recognised quite\nreliably. We already explained that in our annota-\ntion a large number of feedback utterances have an\nagreement or turn label without necessarily having\nbeen assigned to one of our task-related dialogue\nact categories. This means that head gestures\nhelp distinguishing utterances with an agreement\nor turn function from other kinds. Looking closer\nat these utterances, we can see that nods and jerks\noften occur together with TurnElicit, while tilts,\nside turns and smiles tend to occur with Agree.\nAn issue that worries us is the granularity of\nthe annotation categories. To investigate this, in\na third group of experiments we collapsed Nod\nand Jerk into a more general category: the distinc-\ntion had proven difficult for the annotators, and we\ndon?t have many jerks in the data. The results, dis-\nplayed in Table 7, show as expected an improve-\nment. The class which is recognised best is still\nNone.\n"},{"#tail":"\n","@confidence":"0.994544333333333","#text":"\nIn this study we have experimented with the au-\ntomatic classification of feedback expressions into\ndifferent dialogue acts in a multimodal corpus of\n"},{"#tail":"\n","@confidence":"0.977119466666667","#text":"\nmovements\nDanish. We have conducted three sets of experi-\nments, first looking at how prosodic features con-\ntribute to the classification, then testing whether\nthe use of head gesture information improved the\naccuracy of the classifier, finally running the clas-\nsification on a dataset in which the head move-\nment types were slightly more general. The re-\nsults indicate that prosodic features improve the\nclassification, and that in those cases where feed-\nback expressions are accompanied by head ges-\ntures, gesture information is also useful. The re-\nsults also show that using a more coarse-grained\ndistinction of head movements improves classifi-\ncation in these data.\nSlightly more than half of the head gestures in\nour data co-occur with other linguistic utterances\nthan those targeted in this study. Extending our in-\nvestigation to those, as we plan to do, will provide\nus with a larger dataset and therefore presumably\nwith even more interesting and reliable results.\nThe occurrence of gestures in the data stud-\nied here is undoubtedly limited by the technical\nsetup, since the two speakers do not see each other.\nTherefore, we want to investigate the role played\nby head gestures in other types of video and larger\nmaterials. Extending the analysis to larger datasets\nwill also shed more light on whether our gesture\nannotation categories are too fine-grained for au-\ntomatic classification.\n"},{"#tail":"\n","@confidence":"0.984857363636364","#text":"\nThis research has been done under the project\nVKK (Verbal and Bodily Communication) funded\nby the Danish Council for Independent Research\nin the Humanities, and the NOMCO project, a\ncollaborative Nordic project with participating re-\nsearch groups at the universities of Gothenburg,\nCopenhagen and Helsinki which is funded by the\nNOS-HS NORDCORP programme. We would\nalso like to thank Nina Gr?nnum for allowing us to\nuse the DanPASS corpus, and our gesture annota-\ntors Josephine B?dker Arrild and Sara Andersen.\n"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.9835855","#text":"\nUniversity of Copenhagen\nCentre for Language Technology (CST)\n"},{"#tail":"\n","@confidence":"0.985883","#text":"\nUniversity of Copenhagen\nCentre for Language Technology (CST)\n"}],"sectionHeader":[{"#tail":"\n","@confidence":"0.990093","@genericHeader":"abstract","#text":"\nAbstract\n"},{"#tail":"\n","@confidence":"0.998101","@genericHeader":"introduction","#text":"\n1 Introduction\n"},{"#tail":"\n","@confidence":"0.867147","@genericHeader":"method","#text":"\n2 The multimodal corpus\n"},{"#tail":"\n","@confidence":"0.808932","@genericHeader":"method","#text":"\n3 Annotation of feedback expressions\n"},{"#tail":"\n","@confidence":"0.979487","@genericHeader":"method","#text":"\n4 Gesture annotation\n"},{"#tail":"\n","@confidence":"0.614823","@genericHeader":"method","#text":"\n5 Analysis of the data\n"},{"#tail":"\n","@confidence":"0.990706","@genericHeader":"conclusions","#text":"\n6 Conclusion\n"},{"#tail":"\n","@confidence":"0.944289","@genericHeader":"acknowledgments","#text":"\nAcknowledgements\n"},{"#tail":"\n","@confidence":"0.972467","@genericHeader":"references","#text":"\nReferences\n"}],"tableCaption":[{"#tail":"\n","@confidence":"0.999609","#text":"\nTable 1: Distribution of semantic categories\n"},{"#tail":"\n","@confidence":"0.987967","#text":"\nTable 2: Inter-coder agreement on feedback ex-\n"},{"#tail":"\n","@confidence":"0.991892","#text":"\nTable 3: Inter-coder agreement on head gesture\n"},{"#tail":"\n","@confidence":"0.998816","#text":"\nTable 4: Yes and No datasets\n"},{"#tail":"\n","@confidence":"0.7309935","#text":"\nTable 5: Classification results with prosodic fea-\ntures\n"},{"#tail":"\n","@confidence":"0.76377","#text":"\nTable 6: Classification results with head gesture\nfeatures\n"},{"#tail":"\n","@confidence":"0.999011","#text":"\nTable 7: Classification results with fewer head\n"}],"page":[{"#tail":"\n","@confidence":"0.998819","#text":"\n318\n"},{"#tail":"\n","@confidence":"0.999146","#text":"\n319\n"},{"#tail":"\n","@confidence":"0.964703","#text":"\n320\n"},{"#tail":"\n","@confidence":"0.991869","#text":"\n321\n"},{"#tail":"\n","@confidence":"0.995725","#text":"\n322\n"},{"#tail":"\n","@confidence":"0.982848","#text":"\n323\n"},{"#tail":"\n","@confidence":"0.999709","#text":"\n324\n"}],"table":[{"#tail":"\n","@confidence":"0.983028230769231","#text":"\nDialogue Act\nAnswer 70 15%\nRepeatRephrase 57 12%\nAccept 127 27%\nNone 212 46%\nAgreement\nAgree 166 36%\nNonAgree 14 3%\nNone 286 61%\nTurn Management\nTurnTake 113 24%\nTurnElicit 85 18%\nNone 268 58%\n"},{"#tail":"\n","@confidence":"0.96057075","#text":"\nfeature Cohen?s k corrected k\nagreement 73.59 98.74\ndial act 84.53 98.87\nturn 73.52 99.16\n"},{"#tail":"\n","@confidence":"0.9757486","#text":"\nfeature Cohen?s k corrected k\nface segment 69.89 91.37\nface annotate 71.53 94.25\nhead mov segment 71.21 91.75\nhead mov annotate 71.65 95.14\n"},{"#tail":"\n","@confidence":"0.996738571428571","#text":"\nExpression Count %\nYes 420 90\nNo 46 10\nTotal 466 100\nYes with gestures 102 90\nNo with gestures 12 10\nTotal with gestures 114 100\n"},{"#tail":"\n","@confidence":"0.977363","#text":"\ndataset Algor P R F\nYesNo ZeroR 27.8 52.8 36.5\nHNB 47.2 53 46.4\n+stress HNB 47.5 54.1 47.1\n+stress+tone HNB 47.8 54.3 47.4\n+stress+tone+hes HNB 47.7 54.5 47.3\n"},{"#tail":"\n","@confidence":"0.960743","#text":"\ndataset Algor P R F\nYesNo HNB 43.1 56.1 46.4\n+face HNB 43.7 56.1 46.9\n+headm HNB 44.7 55.3 48.2\n+face+headm HNB 49.9 57 50.3\n"},{"#tail":"\n","@confidence":"0.9635222","#text":"\ndataset Algor P R F\nYesNo HNB 43.1 56.1 46.4\n+face HNB 43.7 56.1 46.9\n+headm HNB 47 57.9 51\n+face+headm HNB 51.6 57.9 53.9\n"}],"email":[{"#tail":"\n","@confidence":"0.97325","#text":"\ncostanza@hum.ku.dk\n"},{"#tail":"\n","@confidence":"0.982726","#text":"\npaggio@hum.ku.dk\n"}]}},{"#tail":"\n","@name":"ParsHed","#text":"\n","@version":"110505","variant":{"@confidence":"0.801770","#tail":"\n","@no":"0","note":{"#tail":"\n","@confidence":"0.978269","#text":"Proceedings of the ACL 2010 Conference Short Papers, pages 318?324, Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics"},"address":[{"#tail":"\n","@confidence":"0.985459","#text":"Njalsgade 140, 2300-DK Copenhagen"},{"#tail":"\n","@confidence":"0.985422","#text":"Njalsgade 140, 2300-DK Copenhagen"}],"#text":"\n","affiliation":[{"#tail":"\n","@confidence":"0.998583","#text":"University of Copenhagen Centre for Language Technology (CST)"},{"#tail":"\n","@confidence":"0.998592","#text":"University of Copenhagen Centre for Language Technology (CST)"}],"author":[{"#tail":"\n","@confidence":"0.953709","#text":"Costanza Navarretta"},{"#tail":"\n","@confidence":"0.991758","#text":"Patrizia Paggio"}],"abstract":{"#tail":"\n","@confidence":"0.996253454545455","#text":"This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback."},"title":{"#tail":"\n","@confidence":"0.948026","#text":"Classification of Feedback Expressions in Multimodal Data"},"email":[{"#tail":"\n","@confidence":"0.979297","#text":"costanza@hum.ku.dk"},{"#tail":"\n","@confidence":"0.98997","#text":"paggio@hum.ku.dk"}]}},{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"date":{"#tail":"\n","#text":"2007"},"issue":{"#tail":"\n","#text":"3"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ed data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen?s kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element. 4 Gesture annotation All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme (Allwood et al, 2007). The MUMIN scheme is a general framework for the study of gestures in interpersonal communication. In this study, we do not deal with functional classification of the gestures in themselves, but rather 1(Pa? Pe)/(1? Pe). 2(Po ? 1/c)/(1 ? 1/c) where c is the number of categories. with how gestures contribute to the semantic interpretations of linguistic expressions. Therefore, only a subset of the MUMIN attributes has been used, i.e. Smile, Laughter, Scowl, FaceOther for facial expressions, and Nod, Jerk, Tilt, SideTurn, Shake, Waggle, Other for head movements. A link was also established in A","@endWordPosition":"1568","@position":"9886","annotationId":"T1","@startWordPosition":"1565","@citStr":"Allwood et al, 2007"}},"title":{"#tail":"\n","#text":"The MUMIN Coding Scheme for the Annotation of Feedback, Turn Management and Sequencing. Multimodal Corpora for Modelling Human Multimodal Behaviour."},"volume":{"#tail":"\n","#text":"41"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Jens Allwood, Loredana Cerrato, Kristiina Jokinen, Costanza Navarretta, and Patrizia Paggio. 2007. The MUMIN Coding Scheme for the Annotation of Feedback, Turn Management and Sequencing. Multimodal Corpora for Modelling Human Multimodal Behaviour. Special Issue of the International Journal of Language Resources and Evaluation, 41(3? 4):273?287."},"journal":{"#tail":"\n","#text":"Special Issue of the International Journal of Language Resources and Evaluation,"},"#text":"\n","pages":{"#tail":"\n","#text":"4--273"},"marker":{"#tail":"\n","#text":"Allwood, Cerrato, Jokinen, Navarretta, Paggio, 2007"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jens Allwood"},{"#tail":"\n","#text":"Loredana Cerrato"},{"#tail":"\n","#text":"Kristiina Jokinen"},{"#tail":"\n","#text":"Costanza Navarretta"},{"#tail":"\n","#text":"Patrizia Paggio"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1991"},"rawString":{"#tail":"\n","#text":"Anne H. Anderson, Miles Bader, Ellen Gurman Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller, Catherine Sotillo, Henry S. Thompson, and Regina Weinert. 1991. The HCRC Map Task Corpus. Language and Speech, 34:351?366."},"#text":"\n","pages":{"#tail":"\n","#text":"34--351"},"marker":{"#tail":"\n","#text":"Anderson, Bader, Bard, Boyle, Doherty, Garrod, Isard, Kowtko, McAllister, Miller, Sotillo, Thompson, Weinert, 1991"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al (2006) and Louwerse et al (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al, 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al, 2004; Morency et al, 2005; Morency et al, 2007; Morency et al, 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different","@endWordPosition":"347","@position":"2329","annotationId":"T2","@startWordPosition":"343","@citStr":"Anderson et al, 1991"}},"title":{"#tail":"\n","#text":"The HCRC Map Task Corpus. Language and Speech,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Anne H Anderson"},{"#tail":"\n","#text":"Miles Bader"},{"#tail":"\n","#text":"Ellen Gurman Bard"},{"#tail":"\n","#text":"Elizabeth Boyle"},{"#tail":"\n","#text":"Gwyneth Doherty"},{"#tail":"\n","#text":"Simon Garrod"},{"#tail":"\n","#text":"Stephen Isard"},{"#tail":"\n","#text":"Jacqueline Kowtko"},{"#tail":"\n","#text":"Jan McAllister"},{"#tail":"\n","#text":"Jim Miller"},{"#tail":"\n","#text":"Catherine Sotillo"},{"#tail":"\n","#text":"Henry S Thompson"},{"#tail":"\n","#text":"Regina Weinert"}]}},{"volume":{"#tail":"\n","#text":"34"},"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational Linguistics, 34(4):555?596."},"journal":{"#tail":"\n","#text":"Computational Linguistics,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Artstein, Poesio, 2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" in terms of Cohen?s kappa (Cohen, 1960)1 and corrected kappa (Brennan and Prediger, 1981)2. Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2. feature Cohen?s k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2: Inter-coder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen?s kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element. 4 Gesture annotation All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme (Allwood et al, 2007). The MUMIN scheme is a general framework for the study of gestur","@endWordPosition":"1476","@position":"9351","annotationId":"T3","@startWordPosition":"1473","@citStr":"Artstein and Poesio (2008)"}},"title":{"#tail":"\n","#text":"Inter-Coder Agreement for Computational Linguistics."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ron Artstein"},{"#tail":"\n","#text":"Massimo Poesio"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"note":{"#tail":"\n","#text":"from http://www.praat.org/."},"rawString":{"#tail":"\n","#text":"Paul Boersma and David Weenink, 2009. Praat: doing phonetics by computer. Retrieved May 1, 2009, from http://www.praat.org/."},"#text":"\n","marker":{"#tail":"\n","#text":"Boersma, Weenink, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" following them. The material is transcribed orthographically with an indication of stress, articulatory hesitations and pauses. In addition to this, the acoustic signals are segmented into words, syllables and prosodic phrases, and annotated with POS-tags, phonological and phonetic transcriptions, pitch and intonation contours. Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr?nnum (2006). The Praat tool was used (Boersma and Weenink, 2009). The feedback expressions we analyse here are Yes and No expressions, i.e. in Danish words like ja (yes), jo (yes in a negative context), jamen (yes but, well), nej (no), n?h (no). They can be single words or multi-word expressions. Yes and No feedback expressions represent about 9% of the approximately 47,000 running words in the corpus. This is a rather high proportion compared to other corpora, both spoken and written, and a reason why we decided to use the DanPASS videos in spite of the fact that the gesture behaviour is relatively limited given the fact that the two dialogue participants","@endWordPosition":"836","@position":"5393","annotationId":"T4","@startWordPosition":"833","@citStr":"Boersma and Weenink, 2009"}},"title":{"#tail":"\n","#text":"Praat: doing phonetics by computer. Retrieved"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Paul Boersma"},{"#tail":"\n","#text":"David Weenink"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1981"},"rawString":{"#tail":"\n","#text":"Robert L. Brennan and Dale J. Prediger. 1981. Coefficient Kappa: Some uses, misuses, and alternatives. Educational and Psychological Measurement, 41:687?699."},"#text":"\n","pages":{"#tail":"\n","#text":"41--687"},"marker":{"#tail":"\n","#text":"Brennan, Prediger, 1981"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"of semantic categories 319 3.1 Inter-coder agreement on feedback expression annotation In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen?s kappa (Cohen, 1960)1 and corrected kappa (Brennan and Prediger, 1981)2. Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2. feature Cohen?s k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2: Inter-coder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen?s kappa figures over 60 are g","@endWordPosition":"1389","@position":"8815","annotationId":"T5","@startWordPosition":"1386","@citStr":"Brennan and Prediger, 1981"}},"title":{"#tail":"\n","#text":"Coefficient Kappa: Some uses, misuses, and alternatives."},"booktitle":{"#tail":"\n","#text":"Educational and Psychological Measurement,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Robert L Brennan"},{"#tail":"\n","#text":"Dale J Prediger"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1960"},"rawString":{"#tail":"\n","#text":"Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37?46."},"#text":"\n","pages":{"#tail":"\n","#text":"20--1"},"marker":{"#tail":"\n","#text":"Cohen, 1960"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"None 268 58% Table 1: Distribution of semantic categories 319 3.1 Inter-coder agreement on feedback expression annotation In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen?s kappa (Cohen, 1960)1 and corrected kappa (Brennan and Prediger, 1981)2. Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2. feature Cohen?s k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2: Inter-coder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usuall","@endWordPosition":"1382","@position":"8765","annotationId":"T6","@startWordPosition":"1380","@citStr":"Cohen, 1960"}},"title":{"#tail":"\n","#text":"A coefficient of agreement for nominal scales."},"booktitle":{"#tail":"\n","#text":"Educational and Psychological Measurement,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Jacob Cohen"}}},{"volume":{"#tail":"\n","#text":"76"},"#tail":"\n","date":{"#tail":"\n","#text":"1971"},"rawString":{"#tail":"\n","#text":"Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bullettin, 76(5):378?382."},"journal":{"#tail":"\n","#text":"Psychological Bullettin,"},"#text":"\n","issue":{"#tail":"\n","#text":"5"},"marker":{"#tail":"\n","#text":"Fleiss, 1971"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ces and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2. feature Cohen?s k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2: Inter-coder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen?s kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element. 4 Gesture annotation All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme (Allwood et al, 2007). The MUMIN scheme is a general framework for the study of gestures in interpersonal communication. In this study, we do not deal with functional classification of the gestures in t","@endWordPosition":"1497","@position":"9467","annotationId":"T7","@startWordPosition":"1496","@citStr":"Fleiss, 1971"}},"title":{"#tail":"\n","#text":"Measuring nominal scale agreement among many raters."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Joseph L Fleiss"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2004"},"rawString":{"#tail":"\n","#text":"Shinya Fujie, Y. Ejiri, K. Nakajima, Y Matsusaka, and Tetsunor Kobayashi. 2004. A conversation robot using head gesture recognition as para-linguistic information. In Proceedings of the 13th IEEE International Workshop on Robot and Human Interactive Communication, pages 159 ? 164, september."},"#text":"\n","pages":{"#tail":"\n","#text":"159--164"},"marker":{"#tail":"\n","#text":"Fujie, Ejiri, Nakajima, Matsusaka, Kobayashi, 2004"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"etween particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al (2006) and Louwerse et al (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al, 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al, 2004; Morency et al, 2005; Morency et al, 2007; Morency et al, 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that","@endWordPosition":"393","@position":"2629","annotationId":"T8","@startWordPosition":"390","@citStr":"Fujie et al, 2004"}},"title":{"#tail":"\n","#text":"A conversation robot using head gesture recognition as para-linguistic information."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 13th IEEE International Workshop on Robot and Human Interactive Communication,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Shinya Fujie"},{"#tail":"\n","#text":"Y Ejiri"},{"#tail":"\n","#text":"K Nakajima"},{"#tail":"\n","#text":"Y Matsusaka"},{"#tail":"\n","#text":"Tetsunor Kobayashi"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"institution":{"#tail":"\n","#text":"Queen Mary University of London."},"rawString":{"#tail":"\n","#text":"Agustin Gravano and Julia Hirschberg. 2009. Turnyielding cues in task-oriented dialogue. In Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, September 2009, pages 253?261, Queen Mary University of London."},"#text":"\n","pages":{"#tail":"\n","#text":"253--261"},"marker":{"#tail":"\n","#text":"Gravano, Hirschberg, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ow that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al (2006) and Louwerse et al (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al, 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well a","@endWordPosition":"296","@position":"1989","annotationId":"T9","@startWordPosition":"293","@citStr":"Gravano and Hirschberg (2009)"}},"title":{"#tail":"\n","#text":"Turnyielding cues in task-oriented dialogue."},"booktitle":{"#tail":"\n","#text":"In Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Agustin Gravano"},{"#tail":"\n","#text":"Julia Hirschberg"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"editor":{"#tail":"\n","#text":"In N. Calzolari, K. Choukri, A. Gangemi, B. Maegaard, J. Mariani, J. Odijk, and D. Tapias, editors,"},"rawString":{"#tail":"\n","#text":"Nina Gr?nnum. 2006. DanPASS - a Danish phonetically annotated spontaneous speech corpus. In N. Calzolari, K. Choukri, A. Gangemi, B. Maegaard, J. Mariani, J. Odijk, and D. Tapias, editors, Proceedings of the 5th LREC, pages 1578?1583, Genoa, May."},"#text":"\n","pages":{"#tail":"\n","#text":"1578--1583"},"marker":{"#tail":"\n","#text":"Grnnum, 2006"},"location":{"#tail":"\n","#text":"Genoa,"},"title":{"#tail":"\n","#text":"DanPASS - a Danish phonetically annotated spontaneous speech corpus."},"booktitle":{"#tail":"\n","#text":"Proceedings of the 5th LREC,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Nina Grnnum"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2007"},"rawString":{"#tail":"\n","#text":"Kristiina Jokinen and Anton Ragni. 2007. Clustering experiments on the communicative prop- erties of gaze and gestures. In Proceeding of the 3rd. Baltic Conference on Human Language Technologies, Kaunas, Lithuania, October."},"#text":"\n","marker":{"#tail":"\n","#text":"Jokinen, Ragni, 2007"},"location":{"#tail":"\n","#text":"Kaunas, Lithuania,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback. 1 Introduction Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview). Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example, Jokinen and Ragni (2007) and Jokinen et al (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in th","@endWordPosition":"163","@position":"1198","annotationId":"T10","@startWordPosition":"160","@citStr":"Jokinen and Ragni (2007)"}},"title":{"#tail":"\n","#text":"Clustering experiments on the communicative prop- erties of gaze and gestures."},"booktitle":{"#tail":"\n","#text":"In Proceeding of the 3rd. Baltic Conference on Human Language Technologies,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kristiina Jokinen"},{"#tail":"\n","#text":"Anton Ragni"}]}},{"date":{"#tail":"\n","#text":"2008"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback. 1 Introduction Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview). Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example, Jokinen and Ragni (2007) and Jokinen et al (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al (","@endWordPosition":"168","@position":"1223","annotationId":"T11","@startWordPosition":"165","@citStr":"Jokinen et al (2008)"},{"#tail":"\n","#text":"allel by two annotators. The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions. In the case of gestures we also measured agreement on gesture segmentation. The figures obtained are given in Table 3. feature Cohen?s k corrected k face segment 69.89 91.37 face annotate 71.53 94.25 head mov segment 71.21 91.75 head mov annotate 71.65 95.14 Table 3: Inter-coder agreement on head gesture annotation These results are slightly worse than those obtained in previous studies using the same annotation scheme (Jokinen et al, 2008), but are still sat320 isfactory given the high number of categories provided by the scheme. A distinction that seemed particularly difficult was that between nods and jerks: although the direction of the two movement types is different (down-up and up-down, respectively), the movement quality is very similar, and makes it difficult to see the direction clearly. We return to this point below, in connection with our data analysis. 5 Analysis of the data The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted int","@endWordPosition":"1955","@position":"12262","annotationId":"T12","@startWordPosition":"1952","@citStr":"Jokinen et al, 2008"}]},"title":{"#tail":"\n","#text":"Distinguishing the communicative functions of gestures."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Kristiina Jokinen, Costanza Navarretta, and Patrizia Paggio. 2008. Distinguishing the communicative functions of gestures. In Proceedings of the 5th MLMI, LNCS 5237, pages 38?49, Utrecht, The Netherlands, September. Springer."},"#text":"\n","pages":{"#tail":"\n","#text":"38--49"},"marker":{"#tail":"\n","#text":"Jokinen, Navarretta, Paggio, 2008"},"publisher":{"#tail":"\n","#text":"Springer."},"location":{"#tail":"\n","#text":"Utrecht, The Netherlands,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 5th MLMI, LNCS 5237,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Kristiina Jokinen"},{"#tail":"\n","#text":"Costanza Navarretta"},{"#tail":"\n","#text":"Patrizia Paggio"}]}},{"#tail":"\n","tech":{"#tail":"\n","#text":"Ph.D. thesis,"},"date":{"#tail":"\n","#text":"2004"},"institution":{"#tail":"\n","#text":"Saarland University,"},"rawString":{"#tail":"\n","#text":"Michael Kipp. 2004. Gesture Generation by Imitation - From Human Behavior to Computer Character Animation. Ph.D. thesis, Saarland University, Saarbruecken, Germany, Boca Raton, Florida, dissertation.com. Max M. Louwerse, Patrick Jeuniaux, Mohammed E."},"#text":"\n","marker":{"#tail":"\n","#text":"Kipp, 2004"},"location":{"#tail":"\n","#text":"Saarbruecken, Germany, Boca Raton, Florida,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"study to four different subject pairs and two interactions per pair, for a total of about an hour of video-recorded interaction. 3 Annotation of feedback expressions As already mentioned, all words in DanPASS are phonetically and prosodically annotated. In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels ? including dialogue acts ? and gesture annotation. Both kinds of annotation were carried out using ANVIL (Kipp, 2004). To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant. Finally, the two turn management categories TurnTake and TurnElicit were also coded. It should be noted that the same expression may be annotated with a label for each of the three sem","@endWordPosition":"1115","@position":"7100","annotationId":"T13","@startWordPosition":"1114","@citStr":"Kipp, 2004"}},"title":{"#tail":"\n","#text":"Gesture Generation by Imitation - From Human Behavior to Computer Character Animation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Michael Kipp"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2006"},"editor":{"#tail":"\n","#text":"In R. Sun and N. Miyake, editors,"},"rawString":{"#tail":"\n","#text":"Hoque, Jie Wu, and Gwineth Lewis. 2006. Multimodal communication in computer-mediated map task scenarios. In R. Sun and N. Miyake, editors, Proceedings of the 28th Annual Conference of the Cognitive Science Society, pages 1717?1722, Mahwah, NJ: Erlbaum. Max M. Louwerse, Nick Benesh, Mohammed E."},"#text":"\n","pages":{"#tail":"\n","#text":"1717--1722"},"marker":{"#tail":"\n","#text":"Hoque, Lewis, 2006"},"title":{"#tail":"\n","#text":"Multimodal communication in computer-mediated map task scenarios."},"booktitle":{"#tail":"\n","#text":"Proceedings of the 28th Annual Conference of the Cognitive Science Society,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Jie Wu Hoque"},{"#tail":"\n","#text":"Gwineth Lewis"}]}},{"date":{"#tail":"\n","#text":"2007"},"editor":{"#tail":"\n","#text":"In R. Sun and N. Miyake, editors,"},"title":{"#tail":"\n","#text":"Multimodal communication in face-to-face conversations."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Hoque, Patrick Jeuniaux, Gwineth Lewis, Jie Wu, and Megan Zirnstein. 2007. Multimodal communication in face-to-face conversations. In R. Sun and N. Miyake, editors, Proceedings of the 29th Annual Conference of the Cognitive Science Society, pages 1235?1240, Mahwah, NJ: Erlbaum."},"#text":"\n","pages":{"#tail":"\n","#text":"1235--1240"},"marker":{"#tail":"\n","#text":"Hoque, Lewis, Wu, Zirnstein, 2007"},"publisher":{"#tail":"\n","#text":"Erlbaum."},"location":{"#tail":"\n","#text":"Mahwah, NJ:"},"booktitle":{"#tail":"\n","#text":"Proceedings of the 29th Annual Conference of the Cognitive Science Society,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Patrick Jeuniaux Hoque"},{"#tail":"\n","#text":"Gwineth Lewis"},{"#tail":"\n","#text":"Jie Wu"},{"#tail":"\n","#text":"Megan Zirnstein"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2000"},"rawString":{"#tail":"\n","#text":"Evelyn McClave. 2000. Linguistic functions of head movements in the context of speech. Journal of Pragmatics, 32:855?878."},"journal":{"#tail":"\n","#text":"Journal of Pragmatics,"},"#text":"\n","pages":{"#tail":"\n","#text":"32--855"},"marker":{"#tail":"\n","#text":"McClave, 2000"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"sgade 140, 2300-DK Copenhagen paggio@hum.ku.dk Abstract This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback. 1 Introduction Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview). Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example, Jokinen and Ragni (2007) and Jokinen et al (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using t","@endWordPosition":"139","@position":"1040","annotationId":"T14","@startWordPosition":"138","@citStr":"McClave (2000)"}},"title":{"#tail":"\n","#text":"Linguistic functions of head movements in the context of speech."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Evelyn McClave"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Louis-Philippe Morency, Candace Sidner, Christopher Lee, and Trevor Darrell. 2005. Contextual Recognition of Head Gestures. In Proceedings of the International Conference on Multi-modal Interfaces."},"#text":"\n","marker":{"#tail":"\n","#text":"Morency, Sidner, Lee, Darrell, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"coustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al (2006) and Louwerse et al (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al, 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al, 2004; Morency et al, 2005; Morency et al, 2007; Morency et al, 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where","@endWordPosition":"397","@position":"2650","annotationId":"T15","@startWordPosition":"394","@citStr":"Morency et al, 2005"}},"title":{"#tail":"\n","#text":"Contextual Recognition of Head Gestures."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the International Conference on Multi-modal Interfaces."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Louis-Philippe Morency"},{"#tail":"\n","#text":"Candace Sidner"},{"#tail":"\n","#text":"Christopher Lee"},{"#tail":"\n","#text":"Trevor Darrell"}]}},{"date":{"#tail":"\n","#text":"2007"},"issue":{"#tail":"\n","#text":"8"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al (2006) and Louwerse et al (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al, 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al, 2004; Morency et al, 2005; Morency et al, 2007; Morency et al, 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribu","@endWordPosition":"401","@position":"2671","annotationId":"T16","@startWordPosition":"398","@citStr":"Morency et al, 2007"}},"title":{"#tail":"\n","#text":"Head gestures for perceptual interfaces: The role of context in improving recognition."},"volume":{"#tail":"\n","#text":"171"},"#tail":"\n","rawString":{"#tail":"\n","#text":"Louis-Philippe Morency, Candace Sidner, Christopher Lee, and Trevor Darrell. 2007. Head gestures for perceptual interfaces: The role of context in improving recognition. Artificial Intelligence, 171(8? 9):568?585."},"journal":{"#tail":"\n","#text":"Artificial Intelligence,"},"#text":"\n","pages":{"#tail":"\n","#text":"9--568"},"marker":{"#tail":"\n","#text":"Morency, Sidner, Lee, Darrell, 2007"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Louis-Philippe Morency"},{"#tail":"\n","#text":"Candace Sidner"},{"#tail":"\n","#text":"Christopher Lee"},{"#tail":"\n","#text":"Trevor Darrell"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Louis-Philippe Morency, Iwan de Kok, and Jonathan Gratch. 2009. A probabilistic multimodal approach for predicting listener backchannels. Autonomous Agents and Multi-Agent Systems, 20:70? 84, Springer."},"#text":"\n","pages":{"#tail":"\n","#text":"84"},"marker":{"#tail":"\n","#text":"Morency, de Kok, Gratch, 2009"},"publisher":{"#tail":"\n","#text":"Springer."},"title":{"#tail":"\n","#text":"A probabilistic multimodal approach for predicting listener backchannels. Autonomous Agents and Multi-Agent Systems,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Louis-Philippe Morency"},{"#tail":"\n","#text":"Iwan de Kok"},{"#tail":"\n","#text":"Jonathan Gratch"}]}},{"date":{"#tail":"\n","#text":"2008"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"mmunication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview). Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example, Jokinen and Ragni (2007) and Jokinen et al (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al (2006","@endWordPosition":"226","@position":"1553","annotationId":"T17","@startWordPosition":"223","@citStr":"Murray and Renals (2008)"}},"title":{"#tail":"\n","#text":"Detecting Action Meetings in Meetings."},"#tail":"\n","rawString":{"#tail":"\n","#text":"Gabriel Murray and Steve Renals. 2008. Detecting Action Meetings in Meetings. In Proceedings of the 5th MLMI, LNCS 5237, pages 208?213, Utrecht, The Netherlands, September. Springer."},"#text":"\n","pages":{"#tail":"\n","#text":"208--213"},"marker":{"#tail":"\n","#text":"Murray, Renals, 2008"},"publisher":{"#tail":"\n","#text":"Springer."},"location":{"#tail":"\n","#text":"Utrecht, The Netherlands,"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 5th MLMI, LNCS 5237,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Gabriel Murray"},{"#tail":"\n","#text":"Steve Renals"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2008"},"rawString":{"#tail":"\n","#text":"Harm Rieks op den Akker and Christian Schulz. 2008. Exploring features and classifiers for dialogue act segmentation. In Proceedings of the 5th MLMI, pages 196?207."},"#text":"\n","pages":{"#tail":"\n","#text":"196--207"},"marker":{"#tail":"\n","#text":"den Akker, Schulz, 2008"},"title":{"#tail":"\n","#text":"Exploring features and classifiers for dialogue act segmentation."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 5th MLMI,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Harm Rieks op den Akker"},{"#tail":"\n","#text":"Christian Schulz"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2010"},"rawString":{"#tail":"\n","#text":"Patrizia Paggio and Costanza Navarretta. 2010. Feedback in Head Gesture and Speech. To appear in Proceedings of 7th Conference on Language Resources and Evaluation (LREC-2010), Malta, May."},"#text":"\n","marker":{"#tail":"\n","#text":"Paggio, Navarretta, 2010"},"location":{"#tail":"\n","#text":"Malta,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"rosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions. The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category. In Section 2 we describe the multimodal Danish corpus. In Section 3, we describe how the prosody of feedback expressions is annotated, how their content is coded in terms of dialogue act, turn and agreement labels, and we provide inter-coder agreement measures. ","@endWordPosition":"521","@position":"3431","annotationId":"T18","@startWordPosition":"518","@citStr":"Paggio and Navarretta (2010)"}},"title":{"#tail":"\n","#text":"Feedback in Head Gesture and Speech. To appear in"},"booktitle":{"#tail":"\n","#text":"Proceedings of 7th Conference on Language Resources and Evaluation (LREC-2010),"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Patrizia Paggio"},{"#tail":"\n","#text":"Costanza Navarretta"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"editor":{"#tail":"\n","#text":"In Michael Kipp, Jean-Claude Martin, Patrizia Paggio, and Dirk Heylen, editors,"},"rawString":{"#tail":"\n","#text":"Dennis Reidsma, Dirk Heylen, and Harm Rieks op den Akker. 2009. On the Contextual Analysis of Agreement Scores. In Michael Kipp, Jean-Claude Martin, Patrizia Paggio, and Dirk Heylen, editors, Multimodal Corpora From Models of Natural Interaction to Systems and Applications, number 5509 in Lecture Notes in Artificial Intelligence, pages 122?137. Springer."},"#text":"\n","pages":{"#tail":"\n","#text":"122--137"},"marker":{"#tail":"\n","#text":"Reidsma, 2009"},"publisher":{"#tail":"\n","#text":"Springer."},"title":{"#tail":"\n","#text":"Dirk Heylen, and Harm Rieks op den Akker."},"booktitle":{"#tail":"\n","#text":"Multimodal Corpora From Models of Natural Interaction to Systems and Applications, number 5509 in Lecture Notes in Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Dennis Reidsma"}}},{"volume":{"#tail":"\n","#text":"23"},"#tail":"\n","date":{"#tail":"\n","#text":"2009"},"rawString":{"#tail":"\n","#text":"Vivek Kumar Rangarajan Sridhar, Srinivas Bangaloreb, and Shrikanth Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4):407?422."},"journal":{"#tail":"\n","#text":"Computer Speech & Language,"},"#text":"\n","issue":{"#tail":"\n","#text":"4"},"marker":{"#tail":"\n","#text":"Sridhar, Bangaloreb, Narayanan, 2009"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"en et al (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al (2006) and Louwerse et al (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al, 1991) and find correlations between the various modalities both within and across speakers. Finally, fee","@endWordPosition":"274","@position":"1828","annotationId":"T19","@startWordPosition":"271","@citStr":"Sridhar et al (2009)"}},"title":{"#tail":"\n","#text":"Combining lexical, syntactic and prosodic cues for improved online dialog act tagging."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Vivek Kumar Rangarajan Sridhar"},{"#tail":"\n","#text":"Srinivas Bangaloreb"},{"#tail":"\n","#text":"Shrikanth Narayanan"}]}},{"date":{"#tail":"\n","#text":"2005"},"#tail":"\n","note":{"#tail":"\n","#text":"second edition."},"rawString":{"#tail":"\n","#text":"Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco, second edition."},"#text":"\n","marker":{"#tail":"\n","#text":"Witten, Frank, 2005"},"publisher":{"#tail":"\n","#text":"Morgan Kaufmann,"},"location":{"#tail":"\n","#text":"San Francisco,"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. Expression Count % Yes 420 90 No 46 10 Total 466 100 Yes with gestures 102 90 No with gestures 12 10 Total with gestures 114 100 Table 4: Yes and No datasets These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (Witten and Frank, 2005). We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table. The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al, 2005). Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout. In the first group of experiments we took into consideration all the Yes and No expressions (420 Yes and 46 No) without, however, considering gesture information. The purpose was to see how prosodic information contributes to the classification of dialogue acts. We started","@endWordPosition":"2143","@position":"13345","annotationId":"T20","@startWordPosition":"2140","@citStr":"Witten and Frank, 2005"}},"title":{"#tail":"\n","#text":"Data Mining: Practical machine learning tools and techniques."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Ian H Witten"},{"#tail":"\n","#text":"Eibe Frank"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"2005"},"rawString":{"#tail":"\n","#text":"Harry Zhang, Liangxiao Jiang, and Jiang Su. 2005. Hidden Naive Bayes. In Proceedings of the Twentieth National Conference on Artificial Intelligence, pages 919?924."},"#text":"\n","pages":{"#tail":"\n","#text":"919--924"},"marker":{"#tail":"\n","#text":"Zhang, Jiang, Su, 2005"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ther the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. Expression Count % Yes 420 90 No 46 10 Total 466 100 Yes with gestures 102 90 No with gestures 12 10 Total with gestures 114 100 Table 4: Yes and No datasets These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (Witten and Frank, 2005). We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table. The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al, 2005). Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout. In the first group of experiments we took into consideration all the Yes and No expressions (420 Yes and 46 No) without, however, considering gesture information. The purpose was to see how prosodic information contributes to the classification of dialogue acts. We started by totally leaving out prosody, i.e. only the orthographic transcription (Yes and No expressions) was considered; then we included information about stress (stressed or unstressed); in the third run we added tone attri","@endWordPosition":"2180","@position":"13564","annotationId":"T21","@startWordPosition":"2177","@citStr":"Zhang et al, 2005"}},"title":{"#tail":"\n","#text":"Hidden Naive Bayes."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Twentieth National Conference on Artificial Intelligence,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Harry Zhang"},{"#tail":"\n","#text":"Liangxiao Jiang"},{"#tail":"\n","#text":"Jiang Su"}]}}]}}]}}
